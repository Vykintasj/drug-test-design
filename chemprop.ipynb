{"cells":[{"cell_type":"code","source":["import os\nimport pandas as pd\nimport argparse\nimport pickle\nimport os\nimport sys\nimport torch\nimport rdkit\nimport numpy as np\n#import warnings\nfrom collections import OrderedDict\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom hyperopt import fmin, hp, tpe\nfrom argparse import ArgumentParser, Namespace\nfrom copy import deepcopy\nimport json\nfrom typing import Dict, Union\nimport os\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem import DataStructs\nfrom rdkit.Chem import PandasTools"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["datasets = [\"Homopiperazines\",\"Piperazines\",\"Piperidines\",\"Sulphamides\"]\nnames = [\"JAK1 EC50 nM 1027\",\"JAK2 EC50 nM 1024\",\"JAK3 EC50 nM 1026\"]\nnames = names + ['TYK2 EC50 nM 1025']\nfiles = [\"/dbfs/FileStore/tables/Homopiperazines_cleaned_Feb_2019.sdf\",\n         \"/dbfs/FileStore/tables/Piperazines_cleaned_Feb_2019.sdf\",\n         \"/dbfs/FileStore/tables/Piperidines_cleaned_Feb_2019.sdf\",\n         \"/dbfs/FileStore/tables/Sulphamides_cleaned_Feb_2019.sdf\",\n        ]\nPARENT_DIR = '/dbfs/FileStore/tables'\nPICKLES_DIR = '/dbfs/FileStore/pickles'\nMOSES_DIR = '/dbfs/FileStore/moses'\nCHEMPROP_DIR = '/dbfs/FileStore/chemprop'\nGITCLONES_DIR = '/dbfs/FileStore/git-clones'\nZINC_DIR = '/dbfs/FileStore/ZINC'\nZINC_Models = os.path.join(ZINC_DIR,'models')\nVIRTUAL_SCREENING = os.path.join(ZINC_DIR,'virtual_screening')\ntargets=['JAK1','JAK2','JAK3','TYK2']\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["%run /Users/vxjdk@leo-pharma.com/mol_utils"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["%sh\ncd /dbfs/FileStore/tables\nrm data.tar.gz*\nwget https://github.com/swansonk14/chemprop/raw/master/data.tar.gz\ntar xvzf data.tar.gz"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["# Preprocessing"],"metadata":{}},{"cell_type":"code","source":["X_train = pd.read_csv(os.path.join(PARENT_DIR,'X_train_smiles.txt'), \n                      header=None).rename(columns={0:'smiles'})\nX_test = pd.read_csv(os.path.join(PARENT_DIR,'X_test_smiles.txt'), \n                     header=None).rename(columns={0:'smiles'})\nfor name in names:\n  #full_internal[name]=pd.read_csv(os.path.join(PARENT_DIR,name+'-full.txt'), header=None,index=False)\n  X_train[name]=-np.log10(np.array(pd.read_csv(os.path.join(PARENT_DIR,name+'-y_train.txt'), \n                            header=None, squeeze=True)).astype(float)*1e-9)\n  X_test[name]=-np.log10(np.array(pd.read_csv(os.path.join(PARENT_DIR,name+'-y_test.txt'), \n                           header=None, squeeze=True)).astype(float)*1e-9)\nprint(X_train.shape)\nprint(X_test.shape)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">(1460, 5)\n(365, 5)\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\nval, test = train_test_split(X_test, test_size=0.5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["os.mkdir(CHEMPROP_DIR)\nos.mkdir(os.path.join(CHEMPROP_DIR,'JAK'))\nos.mkdir(os.path.join(CHEMPROP_DIR,'JAK','checkpoints'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["X_all = X_train.append(X_test)\nX_all.columns = ['smiles']+targets\nX_all.to_csv(os.path.join(CHEMPROP_DIR,'JAK','all_1825-regression.csv'),index=None)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["val.to_csv(os.path.join(CHEMPROP_DIR,'JAK','val-182.csv'),index=None)\ntest.to_csv(os.path.join(CHEMPROP_DIR,'JAK','test-183.csv'),index=None)\nX_train.to_csv(os.path.join(CHEMPROP_DIR,'JAK','train-1460.csv'),index=None)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":10},{"cell_type":"code","source":["frame = pd.read_csv('/dbfs/FileStore/tables/frame.csv')\n#frame.columns = targets + ['SMILES']\n#frame = frame[['SMILES']+targets]\nprint(len(frame))\nfor name in names:\n  frame[name]=-np.log10(np.array(frame[name]).astype(float)*1e-9)\nframe.columns = targets+['smiles']\nframe = frame[['smiles']+targets]\nprint(frame.columns)\nframe.to_csv(os.path.join(CHEMPROP_DIR,'JAK','all_2188-sparse-regression.csv'),index=False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2188\nIndex([&#39;smiles&#39;, &#39;JAK1&#39;, &#39;JAK2&#39;, &#39;JAK3&#39;, &#39;TYK2&#39;], dtype=&#39;object&#39;)\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["for name in targets:\n  frame[name] = frame[name].apply(lambda x: 1 if x > 7.6 else 0)\nframe.to_csv(os.path.join(CHEMPROP_DIR,'JAK','all_2188-sparse-bin_76.csv'),index=False)\ndisplay(frame.head())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>smiles</th><th>JAK1</th><th>JAK2</th><th>JAK3</th><th>TYK2</th></tr></thead><tbody><tr><td>CN(C[C@H]1C[C@H](F)CN1S(N)(=O)=O)S(=O)(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>CN(C[C@H]1C[C@@H](F)CN1S(N)(=O)=O)S(=O)(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>CN(C[C@H]1CCCN1S(N)(=O)=O)S(=O)(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td>N#CCC(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>CNS(=O)(=O)N1CCN(c2ncnc3[nH]ccc23)C2CC21</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":12},{"cell_type":"markdown","source":["### Binarization"],"metadata":{}},{"cell_type":"code","source":["%sh head '/dbfs/FileStore/tables/TYK2 EC50 nM 1025-smiles.txt'"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">SMILES\nCN(C[C@H]1C[C@H](F)CN1S(N)(=O)=O)S(=O)(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2\nCN(C[C@H]1C[C@@H](F)CN1S(N)(=O)=O)S(=O)(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2\nCN(C[C@H]1CCCN1S(N)(=O)=O)S(=O)(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2\nN#CCC(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2\nCN(CC1CC(F)(F)C1)S(=O)(=O)N1CCN(c2ncnc3[nH]ccc23)C2CC21\nN#Cc1ccc(CC(=O)N2CCN(c3ncnc4[nH]ccc34)C3CC32)cc1\nc1nc(N2CCNC3CC32)c2cc[nH]c2n1\nCN(C[C@@H]1CC(F)(F)CN1)S(=O)(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2\nCS(=O)(=O)N1CC(F)(F)C[C@H]1COC(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["for name in names:\n  df = frame.dropna(subset=[name])\n  df[name]=-np.log10(np.array(df[name]).astype(float)*1e-9)\n  df[name] = df[name].apply(lambda x: 1 if x > 7.6 else 0)\n  df.filter(['SMILES',name]).rename(columns = {'SMILES':'smiles'}).to_csv('/dbfs/FileStore/tables/'+name+'-trues_bin76.csv',index=False)\n  df.filter(['SMILES']).rename(columns = {'SMILES':'smiles'}).to_csv('/dbfs/FileStore/tables/'+name+'-smiles.txt',index=False)\ndisplay(df.head())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>JAK1 EC50 nM 1027</th><th>JAK2 EC50 nM 1024</th><th>JAK3 EC50 nM 1026</th><th>TYK2 EC50 nM 1025</th><th>SMILES</th></tr></thead><tbody><tr><td>47.2</td><td>49.3</td><td>73.5</td><td>0</td><td>CN(C[C@H]1C[C@H](F)CN1S(N)(=O)=O)S(=O)(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2</td></tr><tr><td>26.4</td><td>16.6</td><td>99.8</td><td>0</td><td>CN(C[C@H]1C[C@@H](F)CN1S(N)(=O)=O)S(=O)(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2</td></tr><tr><td>31.2</td><td>24.1</td><td>148.0</td><td>0</td><td>CN(C[C@H]1CCCN1S(N)(=O)=O)S(=O)(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2</td></tr><tr><td>330.0</td><td>512.0</td><td>1350.0</td><td>0</td><td>N#CCC(=O)N1CCCN(c2ncnc3[nH]ccc23)CC12CC2</td></tr><tr><td>116.0</td><td>259.0</td><td>710.0</td><td>0</td><td>CN(CC1CC(F)(F)C1)S(=O)(=O)N1CCN(c2ncnc3[nH]ccc23)C2CC21</td></tr></tbody></table></div>"]}}],"execution_count":15},{"cell_type":"code","source":["val = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','val-182.csv'))\ntest = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','test-183.csv'))\ntrain = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','train-1460.csv'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":16},{"cell_type":"code","source":["dfs = [val,test,train]\nfor df in dfs:\n  for name in names:\n    df[name] = df[name].apply(lambda x: 1 if x > 8 else 0)\ndisplay(dfs[0].head())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>smiles</th><th>JAK1 EC50 nM 1027</th><th>JAK2 EC50 nM 1024</th><th>JAK3 EC50 nM 1026</th><th>TYK2 EC50 nM 1025</th></tr></thead><tbody><tr><td>CN(CC1CCN(C(=O)CCO)CC1)S(=O)(=O)N1CCN(c2ncnc3[nH]ccc23)CC12CC2</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>O=C(c1cnc2ccccc2n1)N1CCN(c2ncnc3[nH]cc(Cl)c23)CC12CC2</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Cc1cnc(C(=O)N2[C@H]3CC[C@@H]2CN(c2ncnc4[nH]ccc24)C3)cn1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>N#CCC(=O)N1CC[C@@H](NC(=O)[C@H]2CCN(c3ncnc4[nH]ccc34)CC23CC3)C1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>C[C@H](NC(=O)C1CCN(c2ncnc3[nH]ccc23)CC12CC2)c1ccc(C#N)cc1</td><td>1</td><td>1</td><td>1</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":17},{"cell_type":"code","source":["dfs[0].to_csv(os.path.join(CHEMPROP_DIR,'JAK','val-182_binary.csv'),index=None)\ndfs[1].to_csv(os.path.join(CHEMPROP_DIR,'JAK','test-183_binary.csv'),index=None)\ndfs[2].to_csv(os.path.join(CHEMPROP_DIR,'JAK','train-1460_binary.csv'),index=None)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"markdown","source":["Adding PubChem dataset"],"metadata":{}},{"cell_type":"code","source":["bin_train = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','train-1460_binary.csv'))\ndisplay(bin_train.head())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>smiles</th><th>JAK1 EC50 nM 1027</th><th>JAK2 EC50 nM 1024</th><th>JAK3 EC50 nM 1026</th><th>TYK2 EC50 nM 1025</th></tr></thead><tbody><tr><td>Cc1c[nH]c(C(=O)N2CCCN(c3ncnc4[nH]ccc34)CC23CC3)c1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>O=S(=O)(N(CCN1CCOCC1)C1CCC1)N1CCN(c2ncnc3[nH]ccc23)CC12CC2</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>CN(C[C@@H]1CCCN1S(C)(=O)=O)S(=O)(=O)N1CCN(c2ncnc3[nH]ccc23)CC12CC2</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>CC(=O)N1CC[C@H]1COC(=O)N1C2CCC1CN(c1ncnc3[nH]ccc13)C2</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>N#Cc1ccc(CC(=O)N2CCN(c3ncnc4[nH]ccc34)C3(CC3)C2)cc1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":20},{"cell_type":"code","source":["csvs = ['JAK1_20190311.csv', \n        'JAK2_20190311.csv', \n        'JAK3_20190311.csv', \n        'TYK2_20190311.csv']\nsdfs = ['JAK1_20190311_sdf-d4409.gz', \n        'JAK2_20190311_sdf-3d49e.gz', \n        'JAK3_20190311_sdf-08df5.gz', \n        'TYK2_20190311_sdf-6ca3e.gz']\n\next_dfs = []\nfor j, file in enumerate(csvs):\n  frame = external_prep('/'.join([PARENT_DIR, file]), \n                        '/'.join([PARENT_DIR, sdfs[j]]))\n  frame.drop_duplicates(subset='cid', inplace=True)\n  frame.dropna(subset=['acvalue'],inplace=True)\n  frame[sdfs[j][:4]] = -np.log10(np.array([val*1e-6 for val in frame['acvalue']]).astype(float))\n  ext_dfs.append(frame)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["all_internal = train.append(val)\nall_internal = all_internal.append(test)\nall_internal['Molecule'] = [Chem.MolFromSmiles(smi) for smi in all_internal['smiles']]\nfor df in ext_dfs:\n  print(any(df['Molecule'].isin(all_internal['Molecule'])))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">False\nFalse\nFalse\nFalse\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["#ext_all = ext_dfs[0]\nfrom functools import reduce\nfor j, df in enumerate(ext_dfs):\n  #df['smiles'] = [Chem.MolToSmiles(mol) for mol in df['Molecule']]\n  ext_dfs[j] = df.filter(['smiles',sdfs[j][:4]]).dropna()\ndf_all = reduce(lambda left,right: pd.merge(left,right,on='smiles', how='outer'), ext_dfs)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":23},{"cell_type":"code","source":["for col in sdfs:\n  df_all[col[:4]] = df_all[col[:4]].apply(lambda x: int(0) if x < 7.1 else x)\n  df_all[col[:4]] = df_all[col[:4]].apply(lambda x: int(1) if x >= 8.1 else x)\n  df_all[col[:4]] = df_all[col[:4]].apply(lambda x: None if (x < 8.1 and x >= 7.1) else x)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":24},{"cell_type":"code","source":["display(df_all.head())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>smiles</th><th>JAK1</th><th>JAK2</th><th>JAK3</th><th>TYK2</th></tr></thead><tbody><tr><td>COCCOC1CCCN(S(=O)(=O)CC2CCC(N(C)c3ncnc4[nH]ccc34)CC2)C1</td><td>1.0</td><td>1.0</td><td>1.0</td><td>NaN</td></tr><tr><td>CC(C)COC1CCCN(S(=O)(=O)CC2CCC(N(C)c3ncnc4[nH]ccc34)CC2)C1</td><td>1.0</td><td>1.0</td><td>1.0</td><td>NaN</td></tr><tr><td>C[C@H]1CN(S(=O)(=O)CC2CCC(N(C)c3ncnc4[nH]ccc34)CC2)C[C@@H]1CO</td><td>1.0</td><td>1.0</td><td>1.0</td><td>NaN</td></tr><tr><td>CN(c1ncnc2[nH]ccc12)C1CCC(CS(=O)(=O)N2CCC(C)(O)C2)CC1</td><td>1.0</td><td>1.0</td><td>1.0</td><td>NaN</td></tr><tr><td>N#CCC1(n2cc(C(N)=O)c(Nc3ccc(-c4cn[nH]c4)nc3)n2)CCN(C(=O)OCC(F)(F)F)CC1</td><td>1.0</td><td>1.0</td><td>NaN</td><td>NaN</td></tr></tbody></table></div>"]}}],"execution_count":25},{"cell_type":"code","source":["for df in dfs:\n  df.columns = ['smiles','JAK1','JAK2','JAK3','TYK2']\n\ndfs[2] = dfs[2].append(df_all)\ndfs[2].shape"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[83]: (8396, 5)\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["dfs[0].to_csv(os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'),index=None,float_format='%.0f')\ndfs[1].to_csv(os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),index=None,float_format='%.0f')\ndfs[2].to_csv(os.path.join(CHEMPROP_DIR,'JAK','train-8396_bin76.csv'),index=None,float_format='%.0f')\ntrain.to_csv(os.path.join(CHEMPROP_DIR,'JAK','train-1460_bin76.csv'),index=None,float_format='%.0f')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":27},{"cell_type":"markdown","source":["# Hyperparameter tuning"],"metadata":{}},{"cell_type":"code","source":["from chemprop.models import build_model\nfrom chemprop.nn_utils import param_count\nfrom chemprop.parsing import add_train_args, modify_train_args\nfrom chemprop.train import cross_validate\nfrom chemprop.utils import create_logger, makedirs\nfrom chemprop.features import get_available_features_generators"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":29},{"cell_type":"code","source":["SPACE = {\n    'hidden_size': hp.quniform('hidden_size', low=300, high=2400, q=100),\n    'depth': hp.quniform('depth', low=2, high=6, q=1),\n    'dropout': hp.quniform('dropout', low=0.0, high=0.4, q=0.05),\n    'ffn_num_layers': hp.quniform('ffn_num_layers', low=1, high=3, q=1)\n}\nINT_KEYS = ['hidden_size', 'depth', 'ffn_num_layers']"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["def grid_search(args: Namespace):\n    # Create loggers\n    logger = create_logger(name='hyperparameter_optimization', save_dir=args.log_dir, quiet=True)\n    train_logger = create_logger(name='train', save_dir=args.save_dir, quiet=args.quiet)\n\n    # Run grid search\n    results = []\n\n    # Define hyperparameter optimization\n    def objective(hyperparams: Dict[str, Union[int, float]]) -> float:\n        # Convert hyperparams from float to int when necessary\n        for key in INT_KEYS:\n            hyperparams[key] = int(hyperparams[key])\n\n        # Update args with hyperparams\n        hyper_args = deepcopy(args)\n        if args.save_dir is not None:\n            folder_name = '_'.join(f'{key}_{value}' for key, value in hyperparams.items())\n            hyper_args.save_dir = os.path.join(hyper_args.save_dir, folder_name)\n        for key, value in hyperparams.items():\n            setattr(hyper_args, key, value)\n\n        # Record hyperparameters\n        logger.info(hyperparams)\n\n        # Cross validate\n        mean_score, std_score = cross_validate(hyper_args, train_logger)\n\n        # Record results\n        temp_model = build_model(hyper_args)\n        num_params = param_count(temp_model)\n        logger.info(f'num params: {num_params:,}')\n        logger.info(f'{mean_score} +/- {std_score} {hyper_args.metric}')\n\n        results.append({\n            'mean_score': mean_score,\n            'std_score': std_score,\n            'hyperparams': hyperparams,\n            'num_params': num_params\n        })\n\n        # Deal with nan\n        if np.isnan(mean_score):\n            if hyper_args.dataset_type == 'classification':\n                mean_score = 0\n            else:\n                raise ValueError('Can\\'t handle nan score for non-classification dataset.')\n\n        return (1 if hyper_args.minimize_score else -1) * mean_score\n\n    fmin(objective, SPACE, algo=tpe.suggest, max_evals=args.num_iters)\n\n    # Report best result\n    results = [result for result in results if not np.isnan(result['mean_score'])]\n    best_result = min(results, key=lambda result: (1 if args.minimize_score else -1) * result['mean_score'])\n    logger.info('best')\n    logger.info(best_result['hyperparams'])\n    logger.info(f'num params: {best_result[\"num_params\"]:,}')\n    logger.info(f'{best_result[\"mean_score\"]} +/- {best_result[\"std_score\"]} {args.metric}')\n\n    # Save best hyperparameter settings as JSON config file\n    makedirs(args.config_save_path, isfile=True)\n\n    with open(args.config_save_path, 'w') as f:\n        json.dump(best_result['hyperparams'], f, indent=4, sort_keys=True)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["parser = ArgumentParser()\nadd_train_args(parser)\nparser.add_argument('--num_iters', type=int, default=40,\n                    help='Number of hyperparameter choices to try')\nparser.add_argument('--config_save_path', type=str, required=True,\n                    help='Path to .json file where best hyperparameter settings will be written')\nparser.add_argument('--log_dir', type=str,\n                    help='(Optional) Path to a directory where all results of the hyperparameter optimization will be written')\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-1460.csv'),\n                         '--dataset_type','regression',\n                         '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','checkpoints'),\n                         '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182.csv'),\n                         '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183.csv'),\n                         '--config_save_path',os.path.join(CHEMPROP_DIR,'JAK','configs','regression-4x.json'),\n                         '--log_dir',os.path.join(CHEMPROP_DIR,'JAK','configs')])\nmodify_train_args(args)\n\ngrid_search(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]{&#39;depth&#39;: 4, &#39;dropout&#39;: 0.15000000000000002, &#39;ffn_num_layers&#39;: 3, &#39;hidden_size&#39;: 900}\n{&#39;depth&#39;: 4, &#39;dropout&#39;: 0.15000000000000002, &#39;ffn_num_layers&#39;: 3, &#39;hidden_size&#39;: 900}\nFold 0\nFold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 900,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/depth_4_dropout_0.15000000000000002_ffn_num_layers_3_hidden_size_900/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 900,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/depth_4_dropout_0.15000000000000002_ffn_num_layers_3_hidden_size_900/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 900,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/depth_4_dropout_0.15000000000000002_ffn_num_layers_3_hidden_size_900/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\nLoading data\n\r                                                    \r\r  0%|          | 0/1460 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 23%|##3       | 341/1460 [00:00&lt;00:00, 3407.31it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 45%|####5     | 664/1460 [00:00&lt;00:00, 3348.86it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 68%|######8   | 1000/1460 [00:00&lt;00:00, 3347.12it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 91%|#########1| 1333/1460 [00:00&lt;00:00, 3340.84it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 1460/1460 [00:00&lt;00:00, 3320.96it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]Number of tasks = 4\nNumber of tasks = 4\nNumber of tasks = 4\nSplitting data with seed 0\nSplitting data with seed 0\nSplitting data with seed 0\n\r                                                    \r\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 183/183 [00:00&lt;00:00, 3343.28it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 182/182 [00:00&lt;00:00, 3332.14it/s]\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]Total size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nTotal size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nTotal size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nFitting scaler\nFitting scaler\nFitting scaler\nBuilding model 0\nBuilding model 0\nBuilding model 0\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=900, bias=False)\n      (W_h): Linear(in_features=900, out_features=900, bias=False)\n      (W_o): Linear(in_features=1033, out_features=900, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=900, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=300, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.15000000000000002)\n    (7): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=900, bias=False)\n      (W_h): Linear(in_features=900, out_features=900, bias=False)\n      (W_o): Linear(in_features=1033, out_features=900, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=900, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=300, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.15000000000000002)\n    (7): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=900, bias=False)\n      (W_h): Linear(in_features=900, out_features=900, bias=False)\n      (W_o): Linear(in_features=1033, out_features=900, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=900, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=300, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.15000000000000002)\n    (7): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 2,234,704\nNumber of parameters = 2,234,704\nNumber of parameters = 2,234,704\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\r                                                    \r\r  0%|          | 0/30 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:02&lt;?, ?it/s, best loss: ?]Epoch 0\nEpoch 0\nEpoch 0\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 10%|#         | 3/29 [00:00&lt;00:01, 24.21it/s]\n\r  0%|          | 0/20 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:00, 24.30it/s]\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 31%|###1      | 9/29 [00:00&lt;00:00, 24.35it/s]\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]Loss = 2.1320e-02, PNorm = 53.6053, GNorm = 2.3518, lr_0 = 2.5517e-04\nLoss = 2.1320e-02, PNorm = 53.6053, GNorm = 2.3518, lr_0 = 2.5517e-04\nLoss = 2.1320e-02, PNorm = 53.6053, GNorm = 2.3518, lr_0 = 2.5517e-04\n\r                                                    \r\r 41%|####1     | 12/29 [00:00&lt;00:00, 24.17it/s]\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 52%|#####1    | 15/29 [00:00&lt;00:00, 24.32it/s]\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 62%|######2   | 18/29 [00:00&lt;00:00, 24.60it/s]\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]Loss = 1.9253e-02, PNorm = 53.6186, GNorm = 1.1598, lr_0 = 4.1034e-04\nLoss = 1.9253e-02, PNorm = 53.6186, GNorm = 1.1598, lr_0 = 4.1034e-04\nLoss = 1.9253e-02, PNorm = 53.6186, GNorm = 1.1598, lr_0 = 4.1034e-04\n\r                                                    \r\r 72%|#######2  | 21/29 [00:00&lt;00:00, 24.54it/s]\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 83%|########2 | 24/29 [00:00&lt;00:00, 24.50it/s]\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 93%|#########3| 27/29 [00:01&lt;00:00, 24.71it/s]\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 29/29 [00:01&lt;00:00, 24.58it/s]\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 4/4 [00:00&lt;00:00, 58.59it/s]\n\r  0%|          | 0/20 [00:04&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:04&lt;?, ?it/s, best loss: ?]Validation rmse = 0.960224\nValidation rmse = 0.960224\nValidation rmse = 0.960224\n\r                                                    \r\r  3%|3         | 1/30 [00:02&lt;01:23,  2.87s/it]\n\r  0%|          | 0/20 [00:05&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:05&lt;?, ?it/s, best loss: ?]Epoch 1\nEpoch 1\nEpoch 1\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:05&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:05&lt;?, ?it/s, best loss: ?]Loss = 2.3822e-02, PNorm = 53.6451, GNorm = 1.1030, lr_0 = 5.6552e-04\nLoss = 2.3822e-02, PNorm = 53.6451, GNorm = 1.1030, lr_0 = 5.6552e-04\nLoss = 2.3822e-02, PNorm = 53.6451, GNorm = 1.1030, lr_0 = 5.6552e-04\n\r                                                    \r\r 10%|#         | 3/29 [00:00&lt;00:01, 23.95it/s]\n\r  0%|          | 0/20 [00:05&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:05&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:00, 24.23it/s]\n\r  0%|          | 0/20 [00:05&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:05&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 31%|###1      | 9/29 [00:00&lt;00:00, 24.55it/s]\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]Loss = 1.7664e-02, PNorm = 53.6905, GNorm = 0.9262, lr_0 = 7.2069e-04\nLoss = 1.7664e-02, PNorm = 53.6905, GNorm = 0.9262, lr_0 = 7.2069e-04\nLoss = 1.7664e-02, PNorm = 53.6905, GNorm = 0.9262, lr_0 = 7.2069e-04\n\r                                                    \r\r 41%|####1     | 12/29 [00:00&lt;00:00, 24.47it/s]\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 52%|#####1    | 15/29 [00:00&lt;00:00, 24.47it/s]\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 62%|######2   | 18/29 [00:00&lt;00:00, 24.51it/s]\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]Loss = 2.0067e-02, PNorm = 53.7509, GNorm = 2.5521, lr_0 = 8.7586e-04\nLoss = 2.0067e-02, PNorm = 53.7509, GNorm = 2.5521, lr_0 = 8.7586e-04\nLoss = 2.0067e-02, PNorm = 53.7509, GNorm = 2.5521, lr_0 = 8.7586e-04\n\r                                                    \r\r 72%|#######2  | 21/29 [00:00&lt;00:00, 24.55it/s]\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 83%|########2 | 24/29 [00:00&lt;00:00, 24.60it/s]\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 93%|#########3| 27/29 [00:01&lt;00:00, 24.64it/s]\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 29/29 [00:01&lt;00:00, 24.67it/s]\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 4/4 [00:00&lt;00:00, 60.27it/s]\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:06&lt;?, ?it/s, best loss: ?]Validation rmse = 0.931707\nValidation rmse = 0.931707\nValidation rmse = 0.931707\n\r                                                    \r\r  7%|6         | 2/30 [00:05&lt;01:19,  2.85s/it]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]Epoch 2\nEpoch 2\nEpoch 2\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]Loss = 1.4753e-02, PNorm = 53.8372, GNorm = 0.6518, lr_0 = 9.9434e-04\nLoss = 1.4753e-02, PNorm = 53.8372, GNorm = 0.6518, lr_0 = 9.9434e-04\nLoss = 1.4753e-02, PNorm = 53.8372, GNorm = 0.6518, lr_0 = 9.9434e-04\n\r                                                    \r\r 10%|#         | 3/29 [00:00&lt;00:01, 24.58it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:00, 24.72it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 31%|###1      | 9/29 [00:00&lt;00:00, 24.79it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]Loss = 1.8147e-02, PNorm = 53.9260, GNorm = 2.0769, lr_0 = 9.6654e-04\nLoss = 1.8147e-02, PNorm = 53.9260, GNorm = 2.0769, lr_0 = 9.6654e-04\nLoss = 1.8147e-02, PNorm = 53.9260, GNorm = 2.0769, lr_0 = 9.6654e-04\n\r                                                    \r\r 41%|####1     | 12/29 [00:00&lt;00:00, 24.67it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 52%|#####1    | 15/29 [00:00&lt;00:00, 24.49it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 62%|######2   | 18/29 [00:00&lt;00:00, 15.99it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 72%|#######2  | 21/29 [00:01&lt;00:00, 17.82it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]Loss = 1.6992e-02, PNorm = 54.0284, GNorm = 1.5197, lr_0 = 9.3952e-04\nLoss = 1.6992e-02, PNorm = 54.0284, GNorm = 1.5197, lr_0 = 9.3952e-04\nLoss = 1.6992e-02, PNorm = 54.0284, GNorm = 1.5197, lr_0 = 9.3952e-04\n\r                                                    \r\r 83%|########2 | 24/29 [00:01&lt;00:00, 19.34it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 93%|#########3| 27/29 [00:01&lt;00:00, 20.84it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 29/29 [00:01&lt;00:00, 20.80it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 4/4 [00:00&lt;00:00, 59.75it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]Validation rmse = 0.851933\nValidation rmse = 0.851933\nValidation rmse = 0.851933\n\r                                                    \r\r 10%|#         | 3/30 [00:08&lt;01:18,  2.91s/it]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]Epoch 3\nEpoch 3\nEpoch 3\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]Loss = 1.6931e-02, PNorm = 54.1370, GNorm = 2.2555, lr_0 = 9.1325e-04\nLoss = 1.6931e-02, PNorm = 54.1370, GNorm = 2.2555, lr_0 = 9.1325e-04\nLoss = 1.6931e-02, PNorm = 54.1370, GNorm = 2.2555, lr_0 = 9.1325e-04\n\r                                                    \r\r 10%|#         | 3/29 [00:00&lt;00:01, 23.15it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:00, 23.51it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\n*** WARNING: skipped 4515172 bytes of output ***\n\n\r 95%|█████████▌| 19/20 [39:01&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 55%|#####5    | 16/29 [00:02&lt;00:01,  7.44it/s]\n\r 95%|█████████▌| 19/20 [39:01&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:01&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 59%|#####8    | 17/29 [00:02&lt;00:01,  7.43it/s]\n\r 95%|█████████▌| 19/20 [39:01&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:01&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]Loss = 3.3439e-03, PNorm = 82.9455, GNorm = 0.8945, lr_0 = 1.1201e-04\nLoss = 3.3439e-03, PNorm = 82.9455, GNorm = 0.8945, lr_0 = 1.1201e-04\nLoss = 3.3439e-03, PNorm = 82.9455, GNorm = 0.8945, lr_0 = 1.1201e-04\n\r                                                                               \r\r 62%|######2   | 18/29 [00:02&lt;00:01,  7.35it/s]\n\r 95%|█████████▌| 19/20 [39:01&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:01&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 66%|######5   | 19/29 [00:02&lt;00:01,  7.38it/s]\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 69%|######8   | 20/29 [00:02&lt;00:01,  7.37it/s]\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 72%|#######2  | 21/29 [00:02&lt;00:01,  7.38it/s]\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 76%|#######5  | 22/29 [00:02&lt;00:00,  7.38it/s]\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 79%|#######9  | 23/29 [00:03&lt;00:00,  7.40it/s]\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 83%|########2 | 24/29 [00:03&lt;00:00,  7.41it/s]\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 86%|########6 | 25/29 [00:03&lt;00:00,  7.42it/s]\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:02&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 90%|########9 | 26/29 [00:03&lt;00:00,  7.46it/s]\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 93%|#########3| 27/29 [00:03&lt;00:00,  7.47it/s]\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]Loss = 4.1138e-03, PNorm = 82.9571, GNorm = 1.5557, lr_0 = 1.0888e-04\nLoss = 4.1138e-03, PNorm = 82.9571, GNorm = 1.5557, lr_0 = 1.0888e-04\nLoss = 4.1138e-03, PNorm = 82.9571, GNorm = 1.5557, lr_0 = 1.0888e-04\n\r                                                                               \r\r 97%|#########6| 28/29 [00:03&lt;00:00,  7.40it/s]\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r100%|##########| 29/29 [00:03&lt;00:00,  7.39it/s]\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 75%|#######5  | 3/4 [00:00&lt;00:00, 26.09it/s]\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r100%|##########| 4/4 [00:00&lt;00:00, 28.34it/s]\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:03&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]Validation rmse = 0.482766\nValidation rmse = 0.482766\nValidation rmse = 0.482766\n\r                                                                               \r\r 97%|#########6| 29/30 [04:04&lt;00:08,  8.47s/it]\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]Epoch 29\nEpoch 29\nEpoch 29\n\r                                                                               \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r  3%|3         | 1/29 [00:00&lt;00:03,  7.08it/s]\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r  7%|6         | 2/29 [00:00&lt;00:03,  7.16it/s]\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 10%|#         | 3/29 [00:00&lt;00:03,  7.26it/s]\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 14%|#3        | 4/29 [00:00&lt;00:03,  7.27it/s]\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 17%|#7        | 5/29 [00:00&lt;00:03,  7.27it/s]\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:11&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 21%|##        | 6/29 [00:00&lt;00:03,  7.33it/s]\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 24%|##4       | 7/29 [00:00&lt;00:03,  7.18it/s]\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 28%|##7       | 8/29 [00:01&lt;00:02,  7.24it/s]\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]Loss = 3.5687e-03, PNorm = 82.9709, GNorm = 0.8550, lr_0 = 1.0584e-04\nLoss = 3.5687e-03, PNorm = 82.9709, GNorm = 0.8550, lr_0 = 1.0584e-04\nLoss = 3.5687e-03, PNorm = 82.9709, GNorm = 0.8550, lr_0 = 1.0584e-04\n\r                                                                               \r\r 31%|###1      | 9/29 [00:01&lt;00:02,  7.20it/s]\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 34%|###4      | 10/29 [00:01&lt;00:02,  7.28it/s]\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 38%|###7      | 11/29 [00:01&lt;00:02,  7.28it/s]\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 41%|####1     | 12/29 [00:01&lt;00:02,  7.31it/s]\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 45%|####4     | 13/29 [00:01&lt;00:02,  7.35it/s]\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:12&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 48%|####8     | 14/29 [00:01&lt;00:02,  7.33it/s]\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 52%|#####1    | 15/29 [00:02&lt;00:01,  7.29it/s]\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 55%|#####5    | 16/29 [00:02&lt;00:01,  7.32it/s]\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 59%|#####8    | 17/29 [00:02&lt;00:01,  7.27it/s]\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 62%|######2   | 18/29 [00:02&lt;00:01,  7.33it/s]\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]Loss = 3.7846e-03, PNorm = 82.9817, GNorm = 0.9376, lr_0 = 1.0288e-04\nLoss = 3.7846e-03, PNorm = 82.9817, GNorm = 0.9376, lr_0 = 1.0288e-04\nLoss = 3.7846e-03, PNorm = 82.9817, GNorm = 0.9376, lr_0 = 1.0288e-04\n\r                                                                               \r\r 66%|######5   | 19/29 [00:02&lt;00:01,  7.25it/s]\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 69%|######8   | 20/29 [00:02&lt;00:01,  7.25it/s]\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:13&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 72%|#######2  | 21/29 [00:02&lt;00:01,  7.32it/s]\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 76%|#######5  | 22/29 [00:03&lt;00:00,  7.29it/s]\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 79%|#######9  | 23/29 [00:03&lt;00:00,  7.37it/s]\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 83%|########2 | 24/29 [00:03&lt;00:00,  7.36it/s]\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 86%|########6 | 25/29 [00:03&lt;00:00,  7.39it/s]\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 90%|########9 | 26/29 [00:03&lt;00:00,  7.33it/s]\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 93%|#########3| 27/29 [00:03&lt;00:00,  7.42it/s]\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:14&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 97%|#########6| 28/29 [00:03&lt;00:00,  7.45it/s]\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]Loss = 3.7586e-03, PNorm = 82.9919, GNorm = 1.1141, lr_0 = 1.0000e-04\nLoss = 3.7586e-03, PNorm = 82.9919, GNorm = 1.1141, lr_0 = 1.0000e-04\nLoss = 3.7586e-03, PNorm = 82.9919, GNorm = 1.1141, lr_0 = 1.0000e-04\n\r                                                                               \r\r100%|##########| 29/29 [00:03&lt;00:00,  7.37it/s]\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 75%|#######5  | 3/4 [00:00&lt;00:00, 25.83it/s]\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r100%|##########| 4/4 [00:00&lt;00:00, 28.09it/s]\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]Validation rmse = 0.496593\nValidation rmse = 0.496593\nValidation rmse = 0.496593\n\r                                                                               \r\r100%|##########| 30/30 [04:08&lt;00:00,  7.16s/it]\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:15&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]Model 0 best validation rmse = 0.482766 on epoch 28\nModel 0 best validation rmse = 0.482766 on epoch 28\nModel 0 best validation rmse = 0.482766 on epoch 28\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\r                                                                               \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 95%|█████████▌| 19/20 [39:17&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:17&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r 75%|#######5  | 3/4 [00:00&lt;00:00, 25.37it/s]\n\r 95%|█████████▌| 19/20 [39:17&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:17&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\r100%|##########| 4/4 [00:00&lt;00:00, 27.60it/s]\n\r 95%|█████████▌| 19/20 [39:17&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [39:17&lt;02:45, 165.69s/it, best loss: 0.5338780642467457]Model 0 test rmse = 0.564631\nModel 0 test rmse = 0.564631\nModel 0 test rmse = 0.564631\nEnsemble test rmse = 0.564631\nEnsemble test rmse = 0.564631\nEnsemble test rmse = 0.564631\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\nSeed 0 ==&gt; test rmse = 0.564631\nSeed 0 ==&gt; test rmse = 0.564631\nSeed 0 ==&gt; test rmse = 0.564631\nOverall test rmse = 0.564631 +/- 0.000000\nOverall test rmse = 0.564631 +/- 0.000000\nOverall test rmse = 0.564631 +/- 0.000000\nnum params: 13,006,204\nnum params: 13,006,204\n0.5646305549208138 +/- 0.0 rmse\n0.5646305549208138 +/- 0.0 rmse\n\r100%|██████████| 20/20 [39:17&lt;00:00, 193.65s/it, best loss: 0.5338780642467457]\nbest\nbest\n{&#39;depth&#39;: 4, &#39;dropout&#39;: 0.1, &#39;ffn_num_layers&#39;: 2, &#39;hidden_size&#39;: 2000}\n{&#39;depth&#39;: 4, &#39;dropout&#39;: 0.1, &#39;ffn_num_layers&#39;: 2, &#39;hidden_size&#39;: 2000}\nnum params: 9,163,504\nnum params: 9,163,504\n0.5338780642467457 +/- 0.0 rmse\n0.5338780642467457 +/- 0.0 rmse\n</div>"]}}],"execution_count":32},{"cell_type":"markdown","source":["### Classification hyperparameter tuning"],"metadata":{}},{"cell_type":"code","source":["parser = ArgumentParser()\nadd_train_args(parser)\nos.mkdir(os.path.join(CHEMPROP_DIR,'JAK','checkpoints','binary-4x'))\nos.mkdir(os.path.join(CHEMPROP_DIR,'JAK','configs','binary-4x'))\nparser.add_argument('--num_iters', type=int, default=40,\n                    help='Number of hyperparameter choices to try')\nparser.add_argument('--config_save_path', type=str, required=True,\n                    help='Path to .json file where best hyperparameter settings will be written')\nparser.add_argument('--log_dir', type=str,\n                    help='(Optional) Path to a directory where all results of the hyperparameter optimization will be written')\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-1460_binary.csv'),\n                         '--dataset_type','classification',\n                         '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','checkpoints','binary-4x'),\n                         '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_binary.csv'),\n                         '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_binary.csv'),\n                         '--config_save_path',os.path.join(CHEMPROP_DIR,'JAK','configs','binary-4x.json'),\n                         '--log_dir',os.path.join(CHEMPROP_DIR,'JAK','configs','binary-4x')])\nmodify_train_args(args)\n\ngrid_search(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]{&#39;depth&#39;: 2, &#39;dropout&#39;: 0.2, &#39;ffn_num_layers&#39;: 1, &#39;hidden_size&#39;: 1800}\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/binary-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 2,\n &#39;dropout&#39;: 0.2,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1800,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/binary-4x&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/binary-4x/depth_2_dropout_0.2_ffn_num_layers_1_hidden_size_1800/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\n\r                                                    \r\r  0%|          | 0/1460 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 305/1460 [00:00&lt;00:00, 3042.82it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 43%|####3     | 628/1460 [00:00&lt;00:00, 3096.42it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 65%|######4   | 947/1460 [00:00&lt;00:00, 3123.83it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 87%|########7 | 1271/1460 [00:00&lt;00:00, 3157.59it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 1460/1460 [00:00&lt;00:00, 3185.60it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]Number of tasks = 4\nSplitting data with seed 0\n\r                                                    \r\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 183/183 [00:00&lt;00:00, 3273.07it/s]\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 182/182 [00:00&lt;00:00, 3237.81it/s]\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]Class sizes\nJAK1 EC50 nM 1027 0: 68.56%, 1: 31.44%\nJAK2 EC50 nM 1024 0: 72.40%, 1: 27.60%\nJAK3 EC50 nM 1026 0: 93.15%, 1: 6.85%\nTYK2 EC50 nM 1025 0: 98.08%, 1: 1.92%\nTotal size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nBuilding model 0\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.2)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1800, bias=False)\n      (W_h): Linear(in_features=1800, out_features=1800, bias=False)\n      (W_o): Linear(in_features=1933, out_features=1800, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.2)\n    (1): Linear(in_features=1800, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 6,993,004\nMoving model to cuda\n\r                                                    \r\r  0%|          | 0/30 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]Epoch 0\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  3%|3         | 1/29 [00:00&lt;00:03,  7.79it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  7%|6         | 2/29 [00:00&lt;00:03,  8.05it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 10%|#         | 3/29 [00:00&lt;00:03,  8.16it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 14%|#3        | 4/29 [00:00&lt;00:03,  8.25it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 17%|#7        | 5/29 [00:00&lt;00:02,  8.24it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:02,  8.18it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 24%|##4       | 7/29 [00:00&lt;00:02,  8.21it/s]\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 28%|##7       | 8/29 [00:01&lt;00:03,  6.67it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 31%|###1      | 9/29 [00:01&lt;00:02,  7.13it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]Loss = 9.2414e-03, PNorm = 62.8212, GNorm = 0.5150, lr_0 = 2.5517e-04\n\r                                                    \r\r 34%|###4      | 10/29 [00:01&lt;00:02,  7.24it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 38%|###7      | 11/29 [00:01&lt;00:02,  7.53it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 41%|####1     | 12/29 [00:01&lt;00:02,  7.76it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 45%|####4     | 13/29 [00:01&lt;00:02,  7.95it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 48%|####8     | 14/29 [00:01&lt;00:01,  8.15it/s]\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 52%|#####1    | 15/29 [00:01&lt;00:01,  8.25it/s]\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 55%|#####5    | 16/29 [00:02&lt;00:01,  8.36it/s]\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 59%|#####8    | 17/29 [00:02&lt;00:01,  8.40it/s]\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 62%|######2   | 18/29 [00:02&lt;00:01,  8.30it/s]\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 66%|######5   | 19/29 [00:02&lt;00:01,  6.10it/s]\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]Loss = 8.5434e-03, PNorm = 62.8569, GNorm = 0.2707, lr_0 = 4.1034e-04\n\r                                                    \r\r 69%|######8   | 20/29 [00:02&lt;00:01,  6.62it/s]\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 72%|#######2  | 21/29 [00:02&lt;00:01,  7.00it/s]\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 76%|#######5  | 22/29 [00:02&lt;00:00,  7.44it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 79%|#######9  | 23/29 [00:03&lt;00:00,  7.74it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 83%|########2 | 24/29 [00:03&lt;00:00,  7.75it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 86%|########6 | 25/29 [00:03&lt;00:00,  7.92it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 90%|########9 | 26/29 [00:03&lt;00:00,  8.00it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 93%|#########3| 27/29 [00:03&lt;00:00,  8.14it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 97%|#########6| 28/29 [00:03&lt;00:00,  8.13it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 29/29 [00:03&lt;00:00,  8.29it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 25%|##5       | 1/4 [00:00&lt;00:00,  3.41it/s]\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 50%|#####     | 2/4 [00:00&lt;00:00,  4.24it/s]\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 4/4 [00:00&lt;00:00,  5.29it/s]\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]Validation auc = 0.658798\n\r                                                    \r\r  3%|3         | 1/30 [00:08&lt;04:20,  8.97s/it]\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]Epoch 1\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]Loss = 8.9521e-03, PNorm = 62.9170, GNorm = 0.3367, lr_0 = 5.6552e-04\n\r                                                    \r\r 10%|#         | 3/29 [00:00&lt;00:01, 23.58it/s]\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:00, 24.05it/s]\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 31%|###1      | 9/29 [00:00&lt;00:00, 24.64it/s]\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]Loss = 7.3450e-03, PNorm = 62.9962, GNorm = 0.5946, lr_0 = 7.2069e-04\n\r                                                    \r\r 41%|####1     | 12/29 [00:00&lt;00:00, 24.93it/s]\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 52%|#####1    | 15/29 [00:00&lt;00:00, 25.11it/s]\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 62%|######2   | 18/29 [00:00&lt;00:00, 25.27it/s]\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]Loss = 7.6861e-03, PNorm = 63.0957, GNorm = 0.2812, lr_0 = 8.7586e-04\n\r                                                    \r\r 72%|#######2  | 21/29 [00:00&lt;00:00, 25.32it/s]\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:17&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 83%|########2 | 24/29 [00:00&lt;00:00, 25.19it/s]\n\r  0%|          | 0/20 [00:18&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:18&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 93%|#########3| 27/29 [00:01&lt;00:00, 25.16it/s]\n\r  0%|          | 0/20 [00:18&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:18&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 29/29 [00:01&lt;00:00, 25.28it/s]\n\r  0%|          | 0/20 [00:18&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:18&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:18&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:18&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 4/4 [00:00&lt;00:00, 59.20it/s]\n\r  0%|          | 0/20 [00:18&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:18&lt;?, ?it/s, best loss: ?]Validation auc = 0.730066\n\r                                                    \r\r  7%|6         | 2/30 [00:13&lt;03:36,  7.72s/it]\n\r  0%|          | 0/20 [00:21&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:21&lt;?, ?it/s, best loss: ?]Epoch 2\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:21&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:21&lt;?, ?it/s, best loss: ?]Loss = 6.3367e-03, PNorm = 63.2330, GNorm = 0.4132, lr_0 = 9.9434e-04\n\r                                                    \r\r 10%|#         | 3/29 [00:00&lt;00:01, 25.19it/s]\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:00, 25.24it/s]\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 31%|###1      | 9/29 [00:00&lt;00:00, 25.43it/s]\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]Loss = 7.1372e-03, PNorm = 63.3964, GNorm = 0.1546, lr_0 = 9.6654e-04\n\r                                                    \r\r 41%|####1     | 12/29 [00:00&lt;00:00, 25.49it/s]\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 52%|#####1    | 15/29 [00:00&lt;00:00, 25.69it/s]\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 62%|######2   | 18/29 [00:00&lt;00:00, 25.92it/s]\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 72%|#######2  | 21/29 [00:00&lt;00:00, 25.80it/s]\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]Loss = 7.4121e-03, PNorm = 63.5757, GNorm = 0.5500, lr_0 = 9.3952e-04\n\r                                                    \r\r 83%|########2 | 24/29 [00:00&lt;00:00, 25.89it/s]\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 93%|#########3| 27/29 [00:01&lt;00:00, 25.92it/s]\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 29/29 [00:01&lt;00:00, 25.85it/s]\n\r  0%|          | 0/20 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 4/4 [00:00&lt;00:00, 58.51it/s]\n\r  0%|          | 0/20 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:23&lt;?, ?it/s, best loss: ?]Validation auc = 0.795176\n\r                                                    \r\r 10%|#         | 3/30 [00:18&lt;03:05,  6.85s/it]\n\r  0%|          | 0/20 [00:26&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:26&lt;?, ?it/s, best loss: ?]Epoch 3\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/20 [00:26&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:26&lt;?, ?it/s, best loss: ?]Loss = 7.0080e-03, PNorm = 63.7639, GNorm = 0.2591, lr_0 = 9.1325e-04\n\r                                                    \r\r 10%|#         | 3/29 [00:00&lt;00:01, 25.89it/s]\n\r  0%|          | 0/20 [00:26&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:26&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:00, 26.04it/s]\n\r  0%|          | 0/20 [00:26&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:26&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 31%|###1      | 9/29 [00:00&lt;00:00, 26.05it/s]\n\r  0%|          | 0/20 [00:27&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:27&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 41%|####1     | 12/29 [00:00&lt;00:00, 26.06it/s]\n\r  0%|          | 0/20 [00:27&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:27&lt;?, ?it/s, best loss: ?]Loss = 6.7152e-03, PNorm = 63.9699, GNorm = 0.2668, lr_0 = 8.8772e-04\n\r                                                    \r\r 52%|#####1    | 15/29 [00:00&lt;00:00, 25.94it/s]\n\r  0%|          | 0/20 [00:27&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/20 [00:27&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 62%|######2   | 18/29 [00:00&lt;00:00, 26.01it/s]\n\r  0%|          | 0/20 [00:27&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\n*** WARNING: skipped 5162885 bytes of output ***\n\n\r                                                                                \r\r 28%|##7       | 8/29 [00:00&lt;00:02,  9.06it/s]\n\r 95%|█████████▌| 19/20 [46:25&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:25&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 31%|###1      | 9/29 [00:00&lt;00:02,  9.10it/s]\n\r 95%|█████████▌| 19/20 [46:25&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:25&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 34%|###4      | 10/29 [00:01&lt;00:02,  9.20it/s]\n\r 95%|█████████▌| 19/20 [46:25&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:25&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 38%|###7      | 11/29 [00:01&lt;00:01,  9.20it/s]\n\r 95%|█████████▌| 19/20 [46:25&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:25&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 41%|####1     | 12/29 [00:01&lt;00:01,  9.03it/s]\n\r 95%|█████████▌| 19/20 [46:25&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:25&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 45%|####4     | 13/29 [00:01&lt;00:01,  9.15it/s]\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 48%|####8     | 14/29 [00:01&lt;00:01,  9.20it/s]\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 52%|#####1    | 15/29 [00:01&lt;00:01,  9.20it/s]\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 55%|#####5    | 16/29 [00:01&lt;00:01,  9.18it/s]\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 59%|#####8    | 17/29 [00:01&lt;00:01,  9.17it/s]\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]Loss = 2.4704e-03, PNorm = 73.0380, GNorm = 0.4904, lr_0 = 1.1201e-04\n\r                                                                                \r\r 62%|######2   | 18/29 [00:01&lt;00:01,  9.03it/s]\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 66%|######5   | 19/29 [00:02&lt;00:01,  9.12it/s]\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 69%|######8   | 20/29 [00:02&lt;00:00,  9.19it/s]\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 72%|#######2  | 21/29 [00:02&lt;00:00,  9.15it/s]\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:26&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 76%|#######5  | 22/29 [00:02&lt;00:00,  9.16it/s]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 79%|#######9  | 23/29 [00:02&lt;00:00,  9.19it/s]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 83%|########2 | 24/29 [00:02&lt;00:00,  9.30it/s]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 86%|########6 | 25/29 [00:02&lt;00:00,  9.31it/s]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 90%|########9 | 26/29 [00:02&lt;00:00,  9.28it/s]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 93%|#########3| 27/29 [00:02&lt;00:00,  9.33it/s]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]Loss = 2.4225e-03, PNorm = 73.0644, GNorm = 0.8058, lr_0 = 1.0888e-04\n\r                                                                                \r\r 97%|#########6| 28/29 [00:03&lt;00:00,  9.24it/s]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r100%|##########| 29/29 [00:03&lt;00:00,  9.18it/s]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r100%|##########| 4/4 [00:00&lt;00:00, 34.04it/s]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]Validation auc = 0.945787\n\r                                                                                \r\r 97%|#########6| 29/30 [03:07&lt;00:03,  3.57s/it]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]Epoch 29\n\r                                                                                \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:27&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r  3%|3         | 1/29 [00:00&lt;00:03,  8.99it/s]\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r  7%|6         | 2/29 [00:00&lt;00:02,  9.04it/s]\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 10%|#         | 3/29 [00:00&lt;00:02,  9.09it/s]\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 14%|#3        | 4/29 [00:00&lt;00:02,  9.12it/s]\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 17%|#7        | 5/29 [00:00&lt;00:02,  9.11it/s]\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 21%|##        | 6/29 [00:00&lt;00:02,  9.19it/s]\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 24%|##4       | 7/29 [00:00&lt;00:02,  9.18it/s]\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 28%|##7       | 8/29 [00:00&lt;00:02,  9.20it/s]\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]Loss = 2.3617e-03, PNorm = 73.0872, GNorm = 0.6301, lr_0 = 1.0584e-04\n\r                                                                                \r\r 31%|###1      | 9/29 [00:00&lt;00:02,  9.20it/s]\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:28&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 34%|###4      | 10/29 [00:01&lt;00:02,  9.17it/s]\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 38%|###7      | 11/29 [00:01&lt;00:01,  9.07it/s]\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 41%|####1     | 12/29 [00:01&lt;00:01,  9.17it/s]\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 45%|####4     | 13/29 [00:01&lt;00:01,  8.94it/s]\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 48%|####8     | 14/29 [00:01&lt;00:01,  9.02it/s]\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 52%|#####1    | 15/29 [00:01&lt;00:01,  9.06it/s]\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 55%|#####5    | 16/29 [00:01&lt;00:01,  9.05it/s]\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 59%|#####8    | 17/29 [00:01&lt;00:01,  9.17it/s]\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 62%|######2   | 18/29 [00:01&lt;00:01,  9.14it/s]\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:29&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]Loss = 2.4887e-03, PNorm = 73.1114, GNorm = 0.4993, lr_0 = 1.0288e-04\n\r                                                                                \r\r 66%|######5   | 19/29 [00:02&lt;00:01,  9.16it/s]\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 69%|######8   | 20/29 [00:02&lt;00:00,  9.26it/s]\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 72%|#######2  | 21/29 [00:02&lt;00:00,  9.17it/s]\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 76%|#######5  | 22/29 [00:02&lt;00:00,  9.08it/s]\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 79%|#######9  | 23/29 [00:02&lt;00:00,  9.20it/s]\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 83%|########2 | 24/29 [00:02&lt;00:00,  9.21it/s]\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 86%|########6 | 25/29 [00:02&lt;00:00,  9.20it/s]\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 90%|########9 | 26/29 [00:02&lt;00:00,  9.25it/s]\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 93%|#########3| 27/29 [00:02&lt;00:00,  9.28it/s]\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:30&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 97%|#########6| 28/29 [00:03&lt;00:00,  9.22it/s]\n\r 95%|█████████▌| 19/20 [46:31&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:31&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]Loss = 2.6892e-03, PNorm = 73.1371, GNorm = 0.4482, lr_0 = 1.0000e-04\n\r                                                                                \r\r100%|##########| 29/29 [00:03&lt;00:00,  9.18it/s]\n\r 95%|█████████▌| 19/20 [46:31&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:31&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 95%|█████████▌| 19/20 [46:31&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:31&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 75%|#######5  | 3/4 [00:00&lt;00:00, 29.85it/s]\n\r 95%|█████████▌| 19/20 [46:31&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:31&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r100%|##########| 4/4 [00:00&lt;00:00, 32.57it/s]\n\r 95%|█████████▌| 19/20 [46:31&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:31&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]Validation auc = 0.951015\n\r                                                                                \r\r100%|##########| 30/30 [03:16&lt;00:00,  5.16s/it]\n\r 95%|█████████▌| 19/20 [46:36&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:36&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]Model 0 best validation auc = 0.951015 on epoch 29\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\r                                                                                \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 95%|█████████▌| 19/20 [46:39&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:39&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r 75%|#######5  | 3/4 [00:00&lt;00:00, 27.56it/s]\n\r 95%|█████████▌| 19/20 [46:39&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:39&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\r100%|##########| 4/4 [00:00&lt;00:00, 30.17it/s]\n\r 95%|█████████▌| 19/20 [46:39&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]\r                                                                                \r\n\r 95%|█████████▌| 19/20 [46:39&lt;02:10, 130.84s/it, best loss: -0.9377547781216816]Model 0 test auc = 0.933746\nEnsemble test auc = 0.933746\n1-fold cross validation\nSeed 0 ==&gt; test auc = 0.933746\nOverall test auc = 0.933746 +/- 0.000000\nnum params: 8,570,004\n0.9337455908289242 +/- 0.0 auc\n\r100%|██████████| 20/20 [46:39&lt;00:00, 152.78s/it, best loss: -0.9377547781216816]\nbest\n{&#39;depth&#39;: 6, &#39;dropout&#39;: 0.30000000000000004, &#39;ffn_num_layers&#39;: 1, &#39;hidden_size&#39;: 2200}\nnum params: 10,307,004\n0.9377547781216816 +/- 0.0 auc\n</div>"]}}],"execution_count":34},{"cell_type":"markdown","source":["#### With PubChem dataset"],"metadata":{}},{"cell_type":"code","source":["parser = ArgumentParser()\nadd_train_args(parser)\nparser.add_argument('--num_iters', type=int, default=40,\n                    help='Number of hyperparameter choices to try')\nparser.add_argument('--config_save_path', type=str, required=True,\n                    help='Path to .json file where best hyperparameter settings will be written')\nparser.add_argument('--log_dir', type=str,\n                    help='(Optional) Path to a directory where all results of the hyperparameter optimization will be written')\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-8396_bin76.csv'),\n                         '--dataset_type','classification',\n                         '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','checkpoints','bin76_ext'),\n                         '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'),\n                         '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                         '--config_save_path',os.path.join(CHEMPROP_DIR,'JAK','configs','bin76_ext.json'),\n                         '--log_dir',os.path.join(CHEMPROP_DIR,'JAK','configs','bin76_ext'),\n                         #'--num_folds','3',\n                         '--seed','13'])\nmodify_train_args(args)\n\ngrid_search(args)\n#dfs[2].to_csv(os.path.join(CHEMPROP_DIR,'JAK','train-8396_bin76.csv'),index=None)\n#train.to_csv(os.path.join(CHEMPROP_DIR,'JAK','train-1460_bin76.csv'),index=None)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]{&#39;depth&#39;: 2, &#39;dropout&#39;: 0.15000000000000002, &#39;ffn_num_layers&#39;: 3, &#39;hidden_size&#39;: 1100}\n{&#39;depth&#39;: 2, &#39;dropout&#39;: 0.15000000000000002, &#39;ffn_num_layers&#39;: 3, &#39;hidden_size&#39;: 1100}\nFold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-8396_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 2,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1100,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 40,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/bin76_ext/depth_2_dropout_0.15000000000000002_ffn_num_layers_3_hidden_size_1100/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-8396_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 2,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1100,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 40,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/bin76_ext/depth_2_dropout_0.15000000000000002_ffn_num_layers_3_hidden_size_1100/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\n\r                                                    \r\r  0%|          | 0/8396 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  4%|4         | 336/8396 [00:00&lt;00:02, 3358.31it/s]\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  8%|7         | 664/8396 [00:00&lt;00:02, 3331.61it/s]\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 12%|#1        | 995/8396 [00:00&lt;00:02, 3323.76it/s]\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 16%|#5        | 1330/8396 [00:00&lt;00:02, 3330.32it/s]\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 20%|#9        | 1663/8396 [00:00&lt;00:02, 3327.57it/s]\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 23%|##3       | 1940/8396 [00:00&lt;00:02, 3135.63it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 26%|##6       | 2217/8396 [00:00&lt;00:02, 2991.95it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 30%|##9       | 2505/8396 [00:00&lt;00:01, 2954.39it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 34%|###3      | 2829/8396 [00:00&lt;00:01, 3033.25it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 38%|###7      | 3163/8396 [00:01&lt;00:01, 3117.27it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 41%|####1     | 3468/8396 [00:01&lt;00:01, 2864.39it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 45%|####4     | 3761/8396 [00:01&lt;00:01, 2881.70it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 49%|####8     | 4105/8396 [00:01&lt;00:01, 3028.82it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 53%|#####2    | 4439/8396 [00:01&lt;00:01, 3114.31it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 57%|#####6    | 4785/8396 [00:01&lt;00:01, 3208.50it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 61%|######1   | 5134/8396 [00:01&lt;00:00, 3287.44it/s]\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 65%|######5   | 5481/8396 [00:01&lt;00:00, 3337.65it/s]\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 69%|######9   | 5817/8396 [00:01&lt;00:00, 3323.64it/s]\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 73%|#######3  | 6151/8396 [00:01&lt;00:00, 3254.54it/s]\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 77%|#######7  | 6478/8396 [00:02&lt;00:00, 3155.30it/s]\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 81%|########1 | 6803/8396 [00:02&lt;00:00, 3182.40it/s]\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 85%|########4 | 7133/8396 [00:02&lt;00:00, 3215.20it/s]\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 89%|########9 | 7486/8396 [00:02&lt;00:00, 3302.97it/s]\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 94%|#########3| 7861/8396 [00:02&lt;00:00, 3424.40it/s]\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 98%|#########7| 8206/8396 [00:02&lt;00:00, 1816.60it/s]\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 8396/8396 [00:02&lt;00:00, 2883.82it/s]\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]Number of tasks = 4\nNumber of tasks = 4\nSplitting data with seed 13\nSplitting data with seed 13\n\r                                                    \r\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 183/183 [00:00&lt;00:00, 3428.65it/s]\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 182/182 [00:00&lt;00:00, 3388.30it/s]\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]Class sizes\nClass sizes\nJAK1 0: 33.55%, 1: 66.45%\nJAK1 0: 33.55%, 1: 66.45%\nJAK2 0: 60.19%, 1: 39.81%\nJAK2 0: 60.19%, 1: 39.81%\nJAK3 0: 77.42%, 1: 22.58%\nJAK3 0: 77.42%, 1: 22.58%\nTYK2 0: 92.62%, 1: 7.38%\nTYK2 0: 92.62%, 1: 7.38%\nTotal size = 8,396 | train size = 8,396 | val size = 182 | test size = 183\nTotal size = 8,396 | train size = 8,396 | val size = 182 | test size = 183\nBuilding model 0\nBuilding model 0\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1100, bias=False)\n      (W_h): Linear(in_features=1100, out_features=1100, bias=False)\n      (W_o): Linear(in_features=1233, out_features=1100, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=1100, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=300, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.15000000000000002)\n    (7): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1100, bias=False)\n      (W_h): Linear(in_features=1100, out_features=1100, bias=False)\n      (W_o): Linear(in_features=1233, out_features=1100, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=1100, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=300, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.15000000000000002)\n    (7): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 3,150,904\nNumber of parameters = 3,150,904\nMoving model to cuda\nMoving model to cuda\n\r                                                    \r\r  0%|          | 0/30 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:08&lt;?, ?it/s, best loss: ?]Epoch 0\nEpoch 0\n\r                                                    \r\r  0%|          | 0/167 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  1%|          | 1/167 [00:00&lt;00:19,  8.67it/s]\n\r  0%|          | 0/40 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:08&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  1%|1         | 2/167 [00:00&lt;00:18,  8.78it/s]\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  2%|1         | 3/167 [00:00&lt;00:18,  8.89it/s]\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  2%|2         | 4/167 [00:00&lt;00:18,  8.88it/s]\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  3%|2         | 5/167 [00:00&lt;00:18,  8.78it/s]\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  4%|3         | 6/167 [00:00&lt;00:18,  8.68it/s]\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  4%|4         | 7/167 [00:00&lt;00:18,  8.80it/s]\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  5%|4         | 8/167 [00:00&lt;00:18,  8.81it/s]\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  5%|5         | 9/167 [00:01&lt;00:18,  8.53it/s]\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:09&lt;?, ?it/s, best loss: ?]Loss = 1.2437e-02, PNorm = 57.4378, GNorm = 0.6137, lr_0 = 1.2695e-04\nLoss = 1.2437e-02, PNorm = 57.4378, GNorm = 0.6137, lr_0 = 1.2695e-04\n\r                                                    \r\r  6%|5         | 10/167 [00:01&lt;00:23,  6.75it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  7%|6         | 11/167 [00:01&lt;00:21,  7.41it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  7%|7         | 12/167 [00:01&lt;00:19,  7.90it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  8%|7         | 13/167 [00:01&lt;00:18,  8.21it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  8%|8         | 14/167 [00:01&lt;00:18,  8.42it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  9%|8         | 15/167 [00:01&lt;00:17,  8.51it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 10%|9         | 16/167 [00:01&lt;00:17,  8.73it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 10%|#         | 17/167 [00:02&lt;00:17,  8.65it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 11%|#         | 18/167 [00:02&lt;00:17,  8.68it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 11%|#1        | 19/167 [00:02&lt;00:16,  8.74it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]Loss = 1.1772e-02, PNorm = 57.4454, GNorm = 1.1178, lr_0 = 1.5389e-04\nLoss = 1.1772e-02, PNorm = 57.4454, GNorm = 1.1178, lr_0 = 1.5389e-04\n\r                                                    \r\r 12%|#1        | 20/167 [00:02&lt;00:16,  8.74it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 13%|#2        | 21/167 [00:02&lt;00:24,  6.07it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 13%|#3        | 22/167 [00:02&lt;00:21,  6.63it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 14%|#3        | 23/167 [00:02&lt;00:19,  7.26it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 14%|#4        | 24/167 [00:02&lt;00:18,  7.66it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 15%|#4        | 25/167 [00:03&lt;00:17,  8.04it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 16%|#5        | 26/167 [00:03&lt;00:16,  8.45it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 16%|#6        | 27/167 [00:03&lt;00:16,  8.60it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 17%|#6        | 28/167 [00:03&lt;00:15,  8.74it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 17%|#7        | 29/167 [00:03&lt;00:15,  8.84it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]Loss = 1.1325e-02, PNorm = 57.4526, GNorm = 0.8288, lr_0 = 1.8084e-04\nLoss = 1.1325e-02, PNorm = 57.4526, GNorm = 0.8288, lr_0 = 1.8084e-04\n\r                                                    \r\r 18%|#7        | 30/167 [00:03&lt;00:15,  8.83it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 19%|#8        | 31/167 [00:03&lt;00:15,  8.90it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 19%|#9        | 32/167 [00:03&lt;00:15,  8.89it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 20%|#9        | 33/167 [00:04&lt;00:24,  5.56it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 20%|##        | 34/167 [00:04&lt;00:21,  6.32it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 35/167 [00:04&lt;00:19,  6.91it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 22%|##1       | 36/167 [00:04&lt;00:17,  7.31it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 22%|##2       | 37/167 [00:04&lt;00:16,  7.68it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 23%|##2       | 38/167 [00:04&lt;00:16,  8.03it/s]\n\n*** WARNING: skipped 34166060 bytes of output ***\n\n\r 82%|████████▎ | 33/40 [3:09:39&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Loss = 7.1957e-03, PNorm = 75.8810, GNorm = 0.3070, lr_0 = 9.5854e-04\nLoss = 7.1957e-03, PNorm = 75.8810, GNorm = 0.3070, lr_0 = 9.5854e-04\n\r                                                                                 \r\r 51%|#####1    | 86/167 [00:04&lt;00:04, 17.97it/s]\n\r 82%|████████▎ | 33/40 [3:09:39&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:39&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 53%|#####2    | 88/167 [00:04&lt;00:04, 17.94it/s]\n\r 82%|████████▎ | 33/40 [3:09:39&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:39&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 54%|#####3    | 90/167 [00:05&lt;00:04, 17.88it/s]\n\r 82%|████████▎ | 33/40 [3:09:39&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:39&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 55%|#####5    | 92/167 [00:05&lt;00:04, 17.88it/s]\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 56%|#####6    | 94/167 [00:05&lt;00:04, 17.96it/s]\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Loss = 6.7536e-03, PNorm = 76.2904, GNorm = 0.5512, lr_0 = 9.5383e-04\nLoss = 6.7536e-03, PNorm = 76.2904, GNorm = 0.5512, lr_0 = 9.5383e-04\n\r                                                                                 \r\r 57%|#####7    | 96/167 [00:05&lt;00:03, 17.87it/s]\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 59%|#####8    | 98/167 [00:05&lt;00:03, 17.93it/s]\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 60%|#####9    | 100/167 [00:05&lt;00:03, 17.93it/s]\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 61%|######1   | 102/167 [00:05&lt;00:03, 18.00it/s]\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 62%|######2   | 104/167 [00:05&lt;00:03, 18.10it/s]\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Loss = 7.4243e-03, PNorm = 76.6505, GNorm = 0.3223, lr_0 = 9.4914e-04\nLoss = 7.4243e-03, PNorm = 76.6505, GNorm = 0.3223, lr_0 = 9.4914e-04\n\r                                                                                 \r\r 63%|######3   | 106/167 [00:05&lt;00:03, 17.93it/s]\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 65%|######4   | 108/167 [00:06&lt;00:03, 17.95it/s]\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:40&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 66%|######5   | 110/167 [00:06&lt;00:03, 17.98it/s]\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 67%|######7   | 112/167 [00:06&lt;00:03, 17.93it/s]\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 68%|######8   | 114/167 [00:06&lt;00:02, 17.88it/s]\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Loss = 7.7858e-03, PNorm = 77.0265, GNorm = 0.7599, lr_0 = 9.4448e-04\nLoss = 7.7858e-03, PNorm = 77.0265, GNorm = 0.7599, lr_0 = 9.4448e-04\n\r                                                                                 \r\r 69%|######9   | 116/167 [00:06&lt;00:02, 17.70it/s]\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 71%|#######   | 118/167 [00:06&lt;00:02, 17.78it/s]\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 72%|#######1  | 120/167 [00:06&lt;00:02, 17.66it/s]\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 73%|#######3  | 122/167 [00:06&lt;00:02, 17.80it/s]\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 74%|#######4  | 124/167 [00:06&lt;00:02, 18.00it/s]\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Loss = 7.3006e-03, PNorm = 77.4152, GNorm = 0.3882, lr_0 = 9.3984e-04\nLoss = 7.3006e-03, PNorm = 77.4152, GNorm = 0.3882, lr_0 = 9.3984e-04\n\r                                                                                 \r\r 75%|#######5  | 126/167 [00:07&lt;00:02, 17.86it/s]\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:41&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 77%|#######6  | 128/167 [00:07&lt;00:02, 17.85it/s]\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 78%|#######7  | 130/167 [00:07&lt;00:02, 17.97it/s]\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 79%|#######9  | 132/167 [00:07&lt;00:01, 17.83it/s]\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 80%|########  | 134/167 [00:07&lt;00:01, 17.86it/s]\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Loss = 7.3477e-03, PNorm = 77.8595, GNorm = 0.6671, lr_0 = 9.3522e-04\nLoss = 7.3477e-03, PNorm = 77.8595, GNorm = 0.6671, lr_0 = 9.3522e-04\n\r                                                                                 \r\r 81%|########1 | 136/167 [00:07&lt;00:01, 17.71it/s]\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 83%|########2 | 138/167 [00:07&lt;00:01, 17.81it/s]\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 84%|########3 | 140/167 [00:07&lt;00:01, 17.58it/s]\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 85%|########5 | 142/167 [00:07&lt;00:01, 17.50it/s]\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 86%|########6 | 144/167 [00:08&lt;00:01, 17.66it/s]\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:42&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Loss = 7.9537e-03, PNorm = 78.2560, GNorm = 0.3311, lr_0 = 9.3063e-04\nLoss = 7.9537e-03, PNorm = 78.2560, GNorm = 0.3311, lr_0 = 9.3063e-04\n\r                                                                                 \r\r 87%|########7 | 146/167 [00:08&lt;00:01, 17.63it/s]\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 89%|########8 | 148/167 [00:08&lt;00:01, 17.67it/s]\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 90%|########9 | 150/167 [00:08&lt;00:00, 17.79it/s]\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 91%|#########1| 152/167 [00:08&lt;00:00, 17.96it/s]\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 92%|#########2| 154/167 [00:08&lt;00:00, 17.97it/s]\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Loss = 7.4481e-03, PNorm = 78.6353, GNorm = 0.3692, lr_0 = 9.2606e-04\nLoss = 7.4481e-03, PNorm = 78.6353, GNorm = 0.3692, lr_0 = 9.2606e-04\n\r                                                                                 \r\r 93%|#########3| 156/167 [00:08&lt;00:00, 17.67it/s]\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 95%|#########4| 158/167 [00:08&lt;00:00, 17.80it/s]\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 96%|#########5| 160/167 [00:08&lt;00:00, 17.98it/s]\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 97%|#########7| 162/167 [00:09&lt;00:00, 17.99it/s]\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:43&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 98%|#########8| 164/167 [00:09&lt;00:00, 17.96it/s]\n\r 82%|████████▎ | 33/40 [3:09:44&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:44&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Loss = 6.7000e-03, PNorm = 79.0133, GNorm = 0.4560, lr_0 = 9.2151e-04\nLoss = 6.7000e-03, PNorm = 79.0133, GNorm = 0.4560, lr_0 = 9.2151e-04\n\r                                                                                 \r\r 99%|#########9| 166/167 [00:09&lt;00:00, 17.79it/s]\n\r 82%|████████▎ | 33/40 [3:09:44&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:44&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r100%|##########| 167/167 [00:09&lt;00:00, 17.83it/s]\n\r 82%|████████▎ | 33/40 [3:09:44&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:44&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 82%|████████▎ | 33/40 [3:09:44&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:44&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r100%|##########| 4/4 [00:00&lt;00:00, 52.29it/s]\n\r 82%|████████▎ | 33/40 [3:09:44&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:44&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Validation auc = 0.826980\nValidation auc = 0.826980\n\r                                                                                 \r\r 10%|#         | 3/30 [00:45&lt;06:52, 15.28s/it]\n\r 82%|████████▎ | 33/40 [3:09:49&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:49&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Epoch 3\nEpoch 3\n\r                                                                                 \r\r  0%|          | 0/167 [00:00&lt;?, ?it/s]\n\r 82%|████████▎ | 33/40 [3:09:49&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:49&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r  1%|1         | 2/167 [00:00&lt;00:09, 17.57it/s]\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r  2%|2         | 4/167 [00:00&lt;00:09, 17.63it/s]\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r  4%|3         | 6/167 [00:00&lt;00:09, 17.49it/s]\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r  5%|4         | 8/167 [00:00&lt;00:09, 17.62it/s]\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Loss = 7.3947e-03, PNorm = 79.3027, GNorm = 0.3352, lr_0 = 9.1698e-04\nLoss = 7.3947e-03, PNorm = 79.3027, GNorm = 0.3352, lr_0 = 9.1698e-04\n\r                                                                                 \r\r  6%|5         | 10/167 [00:00&lt;00:08, 17.48it/s]\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r  7%|7         | 12/167 [00:00&lt;00:08, 17.41it/s]\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r  8%|8         | 14/167 [00:00&lt;00:08, 17.54it/s]\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 10%|9         | 16/167 [00:00&lt;00:08, 17.63it/s]\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 11%|#         | 18/167 [00:01&lt;00:08, 17.62it/s]\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:50&lt;42:26, 363.72s/it, best loss: -0.894491226675029]Loss = 6.4818e-03, PNorm = 79.6524, GNorm = 0.2762, lr_0 = 9.1248e-04\nLoss = 6.4818e-03, PNorm = 79.6524, GNorm = 0.2762, lr_0 = 9.1248e-04\n\r                                                                                 \r\r 12%|#1        | 20/167 [00:01&lt;00:08, 17.48it/s]\n\r 82%|████████▎ | 33/40 [3:09:51&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:51&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 13%|#3        | 22/167 [00:01&lt;00:08, 17.42it/s]\n\r 82%|████████▎ | 33/40 [3:09:51&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:51&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 14%|#4        | 24/167 [00:01&lt;00:08, 17.60it/s]\n\r 82%|████████▎ | 33/40 [3:09:51&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:51&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 16%|#5        | 26/167 [00:01&lt;00:08, 17.42it/s]\n\r 82%|████████▎ | 33/40 [3:09:51&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:51&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\r 17%|#6        | 28/167 [00:01&lt;00:07, 17.63it/s]\n\r 82%|████████▎ | 33/40 [3:09:51&lt;42:26, 363.72s/it, best loss: -0.894491226675029]\r                                                                                 \r\n\r 82%|████████▎ | 33/40 [3:09:51&lt;42:26, 363.72s/it, best loss: -0.894491226675029]</div>"]}}],"execution_count":36},{"cell_type":"code","source":["#Ext feat_slogp\nparser = ArgumentParser()\nadd_train_args(parser)\nparser.add_argument('--num_iters', type=int, default=40,\n                    help='Number of hyperparameter choices to try')\nparser.add_argument('--config_save_path', type=str, required=True,\n                    help='Path to .json file where best hyperparameter settings will be written')\nparser.add_argument('--log_dir', type=str,\n                    help='(Optional) Path to a directory where all results of the hyperparameter optimization will be written')\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-8396_bin76.csv'),\n                          '--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtrain-8396.csv'),\n                          '--dataset_type','classification',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','checkpoints','bin76_ext_Feat_SLogP'),\n                          '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'),\n                          '--separate_val_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPval-182.csv'),\n                          '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                          '--separate_test_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtest-183.csv'),\n                          '--config_save_path',os.path.join(CHEMPROP_DIR,'JAK','configs','bin76_ext_Feat_SLogP.json'),\n                          '--log_dir',os.path.join(CHEMPROP_DIR,'JAK','configs','bin76_ext_Feat_SLogP')])\nmodify_train_args(args)\n\ngrid_search(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]{&#39;depth&#39;: 6, &#39;dropout&#39;: 0.15000000000000002, &#39;ffn_num_layers&#39;: 3, &#39;hidden_size&#39;: 300}\n{&#39;depth&#39;: 6, &#39;dropout&#39;: 0.15000000000000002, &#39;ffn_num_layers&#39;: 3, &#39;hidden_size&#39;: 300}\nFold 0\nFold 0\nFold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext_Feat_SLogP.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-8396_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 300,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext_Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 40,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/bin76_ext_Feat_SLogP/depth_6_dropout_0.15000000000000002_ffn_num_layers_3_hidden_size_300/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext_Feat_SLogP.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-8396_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 300,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext_Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 40,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/bin76_ext_Feat_SLogP/depth_6_dropout_0.15000000000000002_ffn_num_layers_3_hidden_size_300/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext_Feat_SLogP.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-8396_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 300,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext_Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 40,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/bin76_ext_Feat_SLogP/depth_6_dropout_0.15000000000000002_ffn_num_layers_3_hidden_size_300/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext_Feat_SLogP.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-8396_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 300,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_ext_Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 40,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/bin76_ext_Feat_SLogP/depth_6_dropout_0.15000000000000002_ffn_num_layers_3_hidden_size_300/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\nLoading data\nLoading data\n\n\n\r                                                    \r\r  0%|          | 0/8396 [00:00&lt;?, ?it/s]\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r  4%|3         | 310/8396 [00:00&lt;00:02, 3093.39it/s]\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r  7%|7         | 628/8396 [00:00&lt;00:02, 3118.80it/s]\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 11%|#1        | 947/8396 [00:00&lt;00:02, 3137.08it/s]\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 15%|#5        | 1266/8396 [00:00&lt;00:02, 3152.37it/s]\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 19%|#8        | 1588/8396 [00:00&lt;00:02, 3170.90it/s]\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 22%|##2       | 1872/8396 [00:00&lt;00:02, 3060.62it/s]\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 26%|##5       | 2143/8396 [00:00&lt;00:02, 2859.25it/s]\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 29%|##8       | 2415/8396 [00:00&lt;00:02, 2815.01it/s]\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 32%|###2      | 2702/8396 [00:00&lt;00:02, 2829.35it/s]\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 36%|###5      | 3016/8396 [00:01&lt;00:01, 2914.84it/s]\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 39%|###9      | 3301/8396 [00:01&lt;00:01, 2851.16it/s]\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 43%|####2     | 3582/8396 [00:01&lt;00:01, 2668.35it/s]\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 46%|####6     | 3882/8396 [00:01&lt;00:01, 2759.42it/s]\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 50%|#####     | 4211/8396 [00:01&lt;00:01, 2898.53it/s]\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 54%|#####3    | 4526/8396 [00:01&lt;00:01, 2968.13it/s]\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 58%|#####7    | 4853/8396 [00:01&lt;00:01, 3052.27it/s]\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 62%|######1   | 5170/8396 [00:01&lt;00:01, 3085.65it/s]\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 66%|######5   | 5502/8396 [00:01&lt;00:00, 3149.58it/s]\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 69%|######9   | 5819/8396 [00:01&lt;00:00, 3077.24it/s]\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 73%|#######2  | 6129/8396 [00:02&lt;00:00, 3006.31it/s]\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 77%|#######6  | 6431/8396 [00:02&lt;00:00, 2962.24it/s]\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 80%|########  | 6729/8396 [00:02&lt;00:00, 2952.03it/s]\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 84%|########3 | 7048/8396 [00:02&lt;00:00, 3019.25it/s]\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 88%|########7 | 7375/8396 [00:02&lt;00:00, 3090.21it/s]\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:02&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 92%|#########1| 7721/8396 [00:02&lt;00:00, 3190.35it/s]\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 96%|#########5| 8049/8396 [00:02&lt;00:00, 3215.73it/s]\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r100%|#########9| 8372/8396 [00:02&lt;00:00, 3091.63it/s]\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r100%|##########| 8396/8396 [00:02&lt;00:00, 3016.31it/s]\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]Number of tasks = 4\nNumber of tasks = 4\nNumber of tasks = 4\nNumber of tasks = 4\nSplitting data with seed 0\nSplitting data with seed 0\nSplitting data with seed 0\nSplitting data with seed 0\n\n\n\r                                                    \r\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r100%|##########| 183/183 [00:00&lt;00:00, 3105.22it/s]\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r100%|##########| 182/182 [00:00&lt;00:00, 3011.34it/s]\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:03&lt;?, ?it/s, best loss: ?]Class sizes\nClass sizes\nClass sizes\nClass sizes\nJAK1 0: 33.55%, 1: 66.45%\nJAK1 0: 33.55%, 1: 66.45%\nJAK1 0: 33.55%, 1: 66.45%\nJAK1 0: 33.55%, 1: 66.45%\nJAK2 0: 60.19%, 1: 39.81%\nJAK2 0: 60.19%, 1: 39.81%\nJAK2 0: 60.19%, 1: 39.81%\nJAK2 0: 60.19%, 1: 39.81%\nJAK3 0: 77.42%, 1: 22.58%\nJAK3 0: 77.42%, 1: 22.58%\nJAK3 0: 77.42%, 1: 22.58%\nJAK3 0: 77.42%, 1: 22.58%\nTYK2 0: 92.62%, 1: 7.38%\nTYK2 0: 92.62%, 1: 7.38%\nTYK2 0: 92.62%, 1: 7.38%\nTYK2 0: 92.62%, 1: 7.38%\nTotal size = 8,396 | train size = 8,396 | val size = 182 | test size = 183\nTotal size = 8,396 | train size = 8,396 | val size = 182 | test size = 183\nTotal size = 8,396 | train size = 8,396 | val size = 182 | test size = 183\nTotal size = 8,396 | train size = 8,396 | val size = 182 | test size = 183\nBuilding model 0\nBuilding model 0\nBuilding model 0\nBuilding model 0\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=300, bias=False)\n      (W_h): Linear(in_features=300, out_features=300, bias=False)\n      (W_o): Linear(in_features=433, out_features=300, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=301, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=300, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.15000000000000002)\n    (7): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=300, bias=False)\n      (W_h): Linear(in_features=300, out_features=300, bias=False)\n      (W_o): Linear(in_features=433, out_features=300, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=301, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=300, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.15000000000000002)\n    (7): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=300, bias=False)\n      (W_h): Linear(in_features=300, out_features=300, bias=False)\n      (W_o): Linear(in_features=433, out_features=300, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=301, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=300, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.15000000000000002)\n    (7): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=300, bias=False)\n      (W_h): Linear(in_features=300, out_features=300, bias=False)\n      (W_o): Linear(in_features=433, out_features=300, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=301, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=300, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.15000000000000002)\n    (7): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 446,404\nNumber of parameters = 446,404\nNumber of parameters = 446,404\nNumber of parameters = 446,404\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\n\n\r                                                    \r\r  0%|          | 0/30 [00:00&lt;?, ?it/s]\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]Epoch 0\nEpoch 0\nEpoch 0\nEpoch 0\n\n\n\r                                                    \r\r  0%|          | 0/167 [00:00&lt;?, ?it/s]\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r  2%|1         | 3/167 [00:00&lt;00:05, 28.98it/s]\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r  4%|4         | 7/167 [00:00&lt;00:05, 29.58it/s]\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]Loss = 1.3272e-02, PNorm = 38.1627, GNorm = 1.1868, lr_0 = 1.2695e-04\nLoss = 1.3272e-02, PNorm = 38.1627, GNorm = 1.1868, lr_0 = 1.2695e-04\nLoss = 1.3272e-02, PNorm = 38.1627, GNorm = 1.1868, lr_0 = 1.2695e-04\nLoss = 1.3272e-02, PNorm = 38.1627, GNorm = 1.1868, lr_0 = 1.2695e-04\n\n\n\r                                                    \r\r  7%|6         | 11/167 [00:00&lt;00:05, 29.70it/s]\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r  8%|8         | 14/167 [00:00&lt;00:05, 29.53it/s]\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 11%|#         | 18/167 [00:00&lt;00:05, 29.74it/s]\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/40 [00:04&lt;?, ?it/s, best loss: ?]Loss = 1.1678e-02, PNorm = 38.1653, GNorm = 0.9134, lr_0 = 1.5389e-04\nLoss = 1.1678e-02, PNorm = 38.1653, GNorm = 0.9134, lr_0 = 1.5389e-04\nLoss = 1.1678e-02, PNorm = 38.1653, GNorm = 0.9134, lr_0 = 1.5389e-04\nLoss = 1.1678e-02, PNorm = 38.1653, GNorm = 0.9134, lr_0 = 1.5389e-04\n\n\n\r                                                    \r\r 13%|#2        | 21/167 [00:00&lt;00:04, 29.82it/s]\n\n\n\r  0%|          | 0/40 [00:05&lt;?, ?it/s, best loss: ?]\n\n*** WARNING: skipped 48833628 bytes of output ***\n\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 2.0700e-03, PNorm = 73.1557, GNorm = 0.5250, lr_0 = 1.0557e-04\nLoss = 2.0700e-03, PNorm = 73.1557, GNorm = 0.5250, lr_0 = 1.0557e-04\nLoss = 2.0700e-03, PNorm = 73.1557, GNorm = 0.5250, lr_0 = 1.0557e-04\nLoss = 2.0700e-03, PNorm = 73.1557, GNorm = 0.5250, lr_0 = 1.0557e-04\n\n\n\r                                                                                  \r\r 34%|###4      | 57/167 [00:02&lt;00:04, 24.34it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 36%|###5      | 60/167 [00:02&lt;00:04, 24.67it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 38%|###7      | 63/167 [00:02&lt;00:04, 24.74it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 40%|###9      | 66/167 [00:02&lt;00:04, 24.00it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 1.9004e-03, PNorm = 73.1643, GNorm = 0.5100, lr_0 = 1.0505e-04\nLoss = 1.9004e-03, PNorm = 73.1643, GNorm = 0.5100, lr_0 = 1.0505e-04\nLoss = 1.9004e-03, PNorm = 73.1643, GNorm = 0.5100, lr_0 = 1.0505e-04\nLoss = 1.9004e-03, PNorm = 73.1643, GNorm = 0.5100, lr_0 = 1.0505e-04\n\n\n\r                                                                                  \r\r 41%|####1     | 69/167 [00:02&lt;00:04, 24.28it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 43%|####3     | 72/167 [00:02&lt;00:03, 24.55it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:56&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 45%|####4     | 75/167 [00:03&lt;00:03, 24.92it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 2.0960e-03, PNorm = 73.1736, GNorm = 0.5746, lr_0 = 1.0453e-04\nLoss = 2.0960e-03, PNorm = 73.1736, GNorm = 0.5746, lr_0 = 1.0453e-04\nLoss = 2.0960e-03, PNorm = 73.1736, GNorm = 0.5746, lr_0 = 1.0453e-04\nLoss = 2.0960e-03, PNorm = 73.1736, GNorm = 0.5746, lr_0 = 1.0453e-04\n\n\n\r                                                                                  \r\r 47%|####6     | 78/167 [00:03&lt;00:03, 24.54it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 49%|####8     | 81/167 [00:03&lt;00:03, 24.78it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 50%|#####     | 84/167 [00:03&lt;00:03, 24.91it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 1.9734e-03, PNorm = 73.1829, GNorm = 0.8164, lr_0 = 1.0402e-04\nLoss = 1.9734e-03, PNorm = 73.1829, GNorm = 0.8164, lr_0 = 1.0402e-04\nLoss = 1.9734e-03, PNorm = 73.1829, GNorm = 0.8164, lr_0 = 1.0402e-04\nLoss = 1.9734e-03, PNorm = 73.1829, GNorm = 0.8164, lr_0 = 1.0402e-04\n\n\n\r                                                                                  \r\r 52%|#####2    | 87/167 [00:03&lt;00:03, 24.87it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 54%|#####3    | 90/167 [00:03&lt;00:03, 24.06it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 56%|#####5    | 93/167 [00:03&lt;00:03, 24.54it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 57%|#####7    | 96/167 [00:03&lt;00:02, 24.71it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:57&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 2.0685e-03, PNorm = 73.1913, GNorm = 0.8062, lr_0 = 1.0351e-04\nLoss = 2.0685e-03, PNorm = 73.1913, GNorm = 0.8062, lr_0 = 1.0351e-04\nLoss = 2.0685e-03, PNorm = 73.1913, GNorm = 0.8062, lr_0 = 1.0351e-04\nLoss = 2.0685e-03, PNorm = 73.1913, GNorm = 0.8062, lr_0 = 1.0351e-04\n\n\n\r                                                                                  \r\r 59%|#####9    | 99/167 [00:04&lt;00:02, 24.71it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 61%|######1   | 102/167 [00:04&lt;00:02, 24.00it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 63%|######2   | 105/167 [00:04&lt;00:02, 24.28it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 2.0624e-03, PNorm = 73.1981, GNorm = 0.6934, lr_0 = 1.0300e-04\nLoss = 2.0624e-03, PNorm = 73.1981, GNorm = 0.6934, lr_0 = 1.0300e-04\nLoss = 2.0624e-03, PNorm = 73.1981, GNorm = 0.6934, lr_0 = 1.0300e-04\nLoss = 2.0624e-03, PNorm = 73.1981, GNorm = 0.6934, lr_0 = 1.0300e-04\n\n\n\r                                                                                  \r\r 65%|######4   | 108/167 [00:04&lt;00:02, 24.50it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 66%|######6   | 111/167 [00:04&lt;00:02, 24.78it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 68%|######8   | 114/167 [00:04&lt;00:02, 24.45it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 1.9055e-03, PNorm = 73.2041, GNorm = 0.3753, lr_0 = 1.0249e-04\nLoss = 1.9055e-03, PNorm = 73.2041, GNorm = 0.3753, lr_0 = 1.0249e-04\nLoss = 1.9055e-03, PNorm = 73.2041, GNorm = 0.3753, lr_0 = 1.0249e-04\nLoss = 1.9055e-03, PNorm = 73.2041, GNorm = 0.3753, lr_0 = 1.0249e-04\n\n\n\r                                                                                  \r\r 70%|#######   | 117/167 [00:04&lt;00:02, 24.61it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 72%|#######1  | 120/167 [00:04&lt;00:01, 24.87it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:58&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 74%|#######3  | 123/167 [00:05&lt;00:01, 24.97it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 75%|#######5  | 126/167 [00:05&lt;00:01, 25.13it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 2.3051e-03, PNorm = 73.2102, GNorm = 0.6514, lr_0 = 1.0199e-04\nLoss = 2.3051e-03, PNorm = 73.2102, GNorm = 0.6514, lr_0 = 1.0199e-04\nLoss = 2.3051e-03, PNorm = 73.2102, GNorm = 0.6514, lr_0 = 1.0199e-04\nLoss = 2.3051e-03, PNorm = 73.2102, GNorm = 0.6514, lr_0 = 1.0199e-04\n\n\n\r                                                                                  \r\r 77%|#######7  | 129/167 [00:05&lt;00:01, 24.02it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 79%|#######9  | 132/167 [00:05&lt;00:01, 24.45it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 81%|########  | 135/167 [00:05&lt;00:01, 24.79it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 2.2502e-03, PNorm = 73.2155, GNorm = 0.6641, lr_0 = 1.0149e-04\nLoss = 2.2502e-03, PNorm = 73.2155, GNorm = 0.6641, lr_0 = 1.0149e-04\nLoss = 2.2502e-03, PNorm = 73.2155, GNorm = 0.6641, lr_0 = 1.0149e-04\nLoss = 2.2502e-03, PNorm = 73.2155, GNorm = 0.6641, lr_0 = 1.0149e-04\n\n\n\r                                                                                  \r\r 83%|########2 | 138/167 [00:05&lt;00:01, 24.69it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 84%|########4 | 141/167 [00:05&lt;00:01, 24.10it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 86%|########6 | 144/167 [00:05&lt;00:00, 24.44it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:49:59&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 2.0592e-03, PNorm = 73.2218, GNorm = 0.4083, lr_0 = 1.0099e-04\nLoss = 2.0592e-03, PNorm = 73.2218, GNorm = 0.4083, lr_0 = 1.0099e-04\nLoss = 2.0592e-03, PNorm = 73.2218, GNorm = 0.4083, lr_0 = 1.0099e-04\nLoss = 2.0592e-03, PNorm = 73.2218, GNorm = 0.4083, lr_0 = 1.0099e-04\n\n\n\r                                                                                  \r\r 88%|########8 | 147/167 [00:06&lt;00:00, 24.58it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 90%|########9 | 150/167 [00:06&lt;00:00, 24.67it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 92%|#########1| 153/167 [00:06&lt;00:00, 24.08it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 93%|#########3| 156/167 [00:06&lt;00:00, 24.39it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 2.0581e-03, PNorm = 73.2289, GNorm = 0.3619, lr_0 = 1.0049e-04\nLoss = 2.0581e-03, PNorm = 73.2289, GNorm = 0.3619, lr_0 = 1.0049e-04\nLoss = 2.0581e-03, PNorm = 73.2289, GNorm = 0.3619, lr_0 = 1.0049e-04\nLoss = 2.0581e-03, PNorm = 73.2289, GNorm = 0.3619, lr_0 = 1.0049e-04\n\n\n\r                                                                                  \r\r 95%|#########5| 159/167 [00:06&lt;00:00, 24.47it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 97%|#########7| 162/167 [00:06&lt;00:00, 24.88it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r 99%|#########8| 165/167 [00:06&lt;00:00, 24.47it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Loss = 2.5972e-03, PNorm = 73.2348, GNorm = 0.9199, lr_0 = 1.0000e-04\nLoss = 2.5972e-03, PNorm = 73.2348, GNorm = 0.9199, lr_0 = 1.0000e-04\nLoss = 2.5972e-03, PNorm = 73.2348, GNorm = 0.9199, lr_0 = 1.0000e-04\nLoss = 2.5972e-03, PNorm = 73.2348, GNorm = 0.9199, lr_0 = 1.0000e-04\n\n\n\r                                                                                  \r\r100%|##########| 167/167 [00:06&lt;00:00, 24.47it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r100%|##########| 4/4 [00:00&lt;00:00, 60.25it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Validation auc = 0.903302\nValidation auc = 0.903302\nValidation auc = 0.903302\nValidation auc = 0.903302\n\n\n\r                                                                                  \r\r100%|##########| 30/30 [03:49&lt;00:00,  7.11s/it]\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:00&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Model 0 best validation auc = 0.906356 on epoch 19\nModel 0 best validation auc = 0.906356 on epoch 19\nModel 0 best validation auc = 0.906356 on epoch 19\nModel 0 best validation auc = 0.906356 on epoch 19\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\n\n\r                                                                                  \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:01&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:01&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\r100%|##########| 4/4 [00:00&lt;00:00, 58.87it/s]\n\n\n\r 98%|█████████▊| 39/40 [3:50:01&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]\n\n\r                                                                                  \r\n\n\n\r 98%|█████████▊| 39/40 [3:50:01&lt;05:42, 342.78s/it, best loss: -0.8910388397151274]Model 0 test auc = 0.885072\nModel 0 test auc = 0.885072\nModel 0 test auc = 0.885072\nModel 0 test auc = 0.885072\nEnsemble test auc = 0.885072\nEnsemble test auc = 0.885072\nEnsemble test auc = 0.885072\nEnsemble test auc = 0.885072\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\nSeed 0 ==&gt; test auc = 0.885072\nSeed 0 ==&gt; test auc = 0.885072\nSeed 0 ==&gt; test auc = 0.885072\nSeed 0 ==&gt; test auc = 0.885072\nOverall test auc = 0.885072 +/- 0.000000\nOverall test auc = 0.885072 +/- 0.000000\nOverall test auc = 0.885072 +/- 0.000000\nOverall test auc = 0.885072 +/- 0.000000\nnum params: 1,876,508\nnum params: 1,876,508\n0.8850718824099508 +/- 0.0 auc\n0.8850718824099508 +/- 0.0 auc\n\n\n\r100%|██████████| 40/40 [3:50:01&lt;00:00, 310.76s/it, best loss: -0.8910388397151274]best\nbest\n{&#39;depth&#39;: 5, &#39;dropout&#39;: 0.15000000000000002, &#39;ffn_num_layers&#39;: 1, &#39;hidden_size&#39;: 2300}\n{&#39;depth&#39;: 5, &#39;dropout&#39;: 0.15000000000000002, &#39;ffn_num_layers&#39;: 1, &#39;hidden_size&#39;: 2300}\nnum params: 11,235,508\nnum params: 11,235,508\n0.8910388397151274 +/- 0.0 auc\n0.8910388397151274 +/- 0.0 auc\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["#Int feat_slogp\nparser = ArgumentParser()\nadd_train_args(parser)\nparser.add_argument('--num_iters', type=int, default=40,\n                    help='Number of hyperparameter choices to try')\nparser.add_argument('--config_save_path', type=str, required=True,\n                    help='Path to .json file where best hyperparameter settings will be written')\nparser.add_argument('--log_dir', type=str,\n                    help='(Optional) Path to a directory where all results of the hyperparameter optimization will be written')\n\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-1460_bin76.csv'),\n                          '--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtrain-1460.csv'),\n                          '--dataset_type','classification',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','checkpoints','bin76_int_Feat_SLogP'),\n                          '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'),\n                          '--separate_val_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPval-182.csv'),\n                          '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                          '--separate_test_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtest-183.csv'),\n                          '--config_save_path',os.path.join(CHEMPROP_DIR,'JAK','configs','bin76_int_Feat_SLogP.json'),\n                          '--log_dir',os.path.join(CHEMPROP_DIR,'JAK','configs','bin76_int_Feat_SLogP')])\nmodify_train_args(args)\n\ngrid_search(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]{&#39;depth&#39;: 3, &#39;dropout&#39;: 0.0, &#39;ffn_num_layers&#39;: 3, &#39;hidden_size&#39;: 2100}\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_int_Feat_SLogP.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 3,\n &#39;dropout&#39;: 0.0,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2100,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/configs/bin76_int_Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 40,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/bin76_int_Feat_SLogP/depth_3_dropout_0.0_ffn_num_layers_3_hidden_size_2100/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\n\r                                                    \r\r  0%|          | 0/1460 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 17%|#7        | 251/1460 [00:00&lt;00:00, 2503.60it/s]\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 39%|###8      | 564/1460 [00:00&lt;00:00, 2663.12it/s]\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 60%|#####9    | 873/1460 [00:00&lt;00:00, 2777.67it/s]\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:00&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 81%|########  | 1176/1460 [00:00&lt;00:00, 2848.03it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 1460/1460 [00:00&lt;00:00, 2962.82it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]Number of tasks = 4\nSplitting data with seed 0\n\r                                                    \r\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 183/183 [00:00&lt;00:00, 3097.38it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 182/182 [00:00&lt;00:00, 3026.24it/s]\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:01&lt;?, ?it/s, best loss: ?]Class sizes\nJAK1 0: 50.89%, 1: 49.11%\nJAK2 0: 55.27%, 1: 44.73%\nJAK3 0: 83.56%, 1: 16.44%\nTYK2 0: 93.56%, 1: 6.44%\nTotal size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nBuilding model 0\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.0)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=2100, bias=False)\n      (W_h): Linear(in_features=2100, out_features=2100, bias=False)\n      (W_o): Linear(in_features=2233, out_features=2100, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.0)\n    (1): Linear(in_features=2101, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.0)\n    (4): Linear(in_features=300, out_features=300, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.0)\n    (7): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 10,132,204\nMoving model to cuda\n\r                                                    \r\r  0%|          | 0/30 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]Epoch 0\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  3%|3         | 1/29 [00:00&lt;00:04,  6.97it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  7%|6         | 2/29 [00:00&lt;00:03,  7.00it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 10%|#         | 3/29 [00:00&lt;00:03,  7.05it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 14%|#3        | 4/29 [00:00&lt;00:03,  7.09it/s]\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:10&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 17%|#7        | 5/29 [00:00&lt;00:03,  7.15it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:03,  7.10it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 24%|##4       | 7/29 [00:00&lt;00:03,  7.15it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 28%|##7       | 8/29 [00:01&lt;00:03,  5.81it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 31%|###1      | 9/29 [00:01&lt;00:03,  6.19it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]Loss = 1.1040e-02, PNorm = 73.2803, GNorm = 0.7917, lr_0 = 2.5517e-04\n\r                                                    \r\r 34%|###4      | 10/29 [00:01&lt;00:02,  6.39it/s]\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:11&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 38%|###7      | 11/29 [00:01&lt;00:02,  6.66it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 41%|####1     | 12/29 [00:01&lt;00:02,  6.86it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 45%|####4     | 13/29 [00:01&lt;00:02,  6.94it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 48%|####8     | 14/29 [00:02&lt;00:02,  6.97it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 52%|#####1    | 15/29 [00:02&lt;00:01,  7.01it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 55%|#####5    | 16/29 [00:02&lt;00:01,  7.08it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 59%|#####8    | 17/29 [00:02&lt;00:01,  7.12it/s]\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:12&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 62%|######2   | 18/29 [00:02&lt;00:01,  5.55it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 66%|######5   | 19/29 [00:02&lt;00:01,  5.98it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]Loss = 1.0423e-02, PNorm = 73.3254, GNorm = 0.8006, lr_0 = 4.1034e-04\n\r                                                    \r\r 69%|######8   | 20/29 [00:03&lt;00:01,  6.20it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 72%|#######2  | 21/29 [00:03&lt;00:01,  6.48it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 76%|#######5  | 22/29 [00:03&lt;00:01,  6.73it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 79%|#######9  | 23/29 [00:03&lt;00:00,  6.81it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 83%|########2 | 24/29 [00:03&lt;00:00,  6.91it/s]\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:13&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 86%|########6 | 25/29 [00:03&lt;00:00,  7.01it/s]\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 90%|########9 | 26/29 [00:03&lt;00:00,  7.10it/s]\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 93%|#########3| 27/29 [00:04&lt;00:00,  7.14it/s]\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 97%|#########6| 28/29 [00:04&lt;00:00,  7.18it/s]\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 29/29 [00:04&lt;00:00,  7.14it/s]\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 25%|##5       | 1/4 [00:00&lt;00:00,  3.52it/s]\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:14&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 50%|#####     | 2/4 [00:00&lt;00:00,  4.37it/s]\n\r  0%|          | 0/40 [00:15&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:15&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 75%|#######5  | 3/4 [00:00&lt;00:00,  5.24it/s]\n\r  0%|          | 0/40 [00:15&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:15&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 4/4 [00:00&lt;00:00,  7.31it/s]\n\r  0%|          | 0/40 [00:15&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:15&lt;?, ?it/s, best loss: ?]Validation auc = 0.706550\n\r                                                    \r\r  3%|3         | 1/30 [00:11&lt;05:35, 11.56s/it]\n\r  0%|          | 0/40 [00:21&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:21&lt;?, ?it/s, best loss: ?]Epoch 1\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:21&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:21&lt;?, ?it/s, best loss: ?]Loss = 9.3410e-03, PNorm = 73.3951, GNorm = 0.3663, lr_0 = 5.6552e-04\n\r                                                    \r\r  7%|6         | 2/29 [00:00&lt;00:01, 15.51it/s]\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 14%|#3        | 4/29 [00:00&lt;00:01, 15.66it/s]\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:01, 16.05it/s]\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 28%|##7       | 8/29 [00:00&lt;00:01, 16.37it/s]\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 34%|###4      | 10/29 [00:00&lt;00:01, 16.51it/s]\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]Loss = 9.6724e-03, PNorm = 73.4987, GNorm = 1.0892, lr_0 = 7.2069e-04\n\r                                                    \r\r 41%|####1     | 12/29 [00:00&lt;00:01, 16.38it/s]\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 48%|####8     | 14/29 [00:00&lt;00:00, 16.57it/s]\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 55%|#####5    | 16/29 [00:00&lt;00:00, 16.66it/s]\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:22&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 62%|######2   | 18/29 [00:01&lt;00:00, 16.67it/s]\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 69%|######8   | 20/29 [00:01&lt;00:00, 16.58it/s]\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]Loss = 1.0198e-02, PNorm = 73.6388, GNorm = 0.4546, lr_0 = 8.7586e-04\n\r                                                    \r\r 76%|#######5  | 22/29 [00:01&lt;00:00, 16.47it/s]\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 83%|########2 | 24/29 [00:01&lt;00:00, 16.39it/s]\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 90%|########9 | 26/29 [00:01&lt;00:00, 16.52it/s]\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 97%|#########6| 28/29 [00:01&lt;00:00, 16.57it/s]\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 29/29 [00:01&lt;00:00, 16.53it/s]\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\r100%|##########| 4/4 [00:00&lt;00:00, 48.98it/s]\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:23&lt;?, ?it/s, best loss: ?]Validation auc = 0.787579\n\r                                                    \r\r  7%|6         | 2/30 [00:18&lt;04:44, 10.17s/it]\n\r  0%|          | 0/40 [00:28&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:28&lt;?, ?it/s, best loss: ?]Epoch 2\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r  0%|          | 0/40 [00:28&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:28&lt;?, ?it/s, best loss: ?]Loss = 9.6861e-03, PNorm = 73.8612, GNorm = 0.1441, lr_0 = 9.9434e-04\n\r                                                    \r\r  7%|6         | 2/29 [00:00&lt;00:01, 16.59it/s]\n\r  0%|          | 0/40 [00:28&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:28&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 14%|#3        | 4/29 [00:00&lt;00:01, 16.70it/s]\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:01, 16.78it/s]\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 28%|##7       | 8/29 [00:00&lt;00:01, 16.85it/s]\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 34%|###4      | 10/29 [00:00&lt;00:01, 16.87it/s]\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]Loss = 9.5263e-03, PNorm = 74.1444, GNorm = 0.4222, lr_0 = 9.6654e-04\n\r                                                    \r\r 41%|####1     | 12/29 [00:00&lt;00:01, 16.67it/s]\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 48%|####8     | 14/29 [00:00&lt;00:00, 16.69it/s]\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 55%|#####5    | 16/29 [00:00&lt;00:00, 16.50it/s]\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 62%|######2   | 18/29 [00:01&lt;00:00, 16.62it/s]\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:29&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 69%|######8   | 20/29 [00:01&lt;00:00, 16.68it/s]\n\r  0%|          | 0/40 [00:30&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:30&lt;?, ?it/s, best loss: ?]Loss = 9.0906e-03, PNorm = 74.5115, GNorm = 0.5997, lr_0 = 9.3952e-04\n\r                                                    \r\r 76%|#######5  | 22/29 [00:01&lt;00:00, 16.58it/s]\n\r  0%|          | 0/40 [00:30&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:30&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 83%|########2 | 24/29 [00:01&lt;00:00, 16.65it/s]\n\r  0%|          | 0/40 [00:30&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\r  0%|          | 0/40 [00:30&lt;?, ?it/s, best loss: ?]\r                                                    \r\r 90%|########9 | 26/29 [00:01&lt;00:00, 16.82it/s]\n\r  0%|          | 0/40 [00:30&lt;?, ?it/s, best loss: ?]\r                                                    \r\n\n*** WARNING: skipped 9417819 bytes of output ***\n\n\r                                                                                  \r\r 90%|######### | 27/30 [01:45&lt;00:07,  2.53s/it]\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Epoch 27\n\r                                                                                  \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r  7%|6         | 2/29 [00:00&lt;00:02, 12.65it/s]\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 14%|#3        | 4/29 [00:00&lt;00:01, 12.54it/s]\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 21%|##        | 6/29 [00:00&lt;00:01, 12.52it/s]\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Loss = 3.9704e-03, PNorm = 63.9455, GNorm = 0.5579, lr_0 = 1.2546e-04\n\r                                                                                  \r\r 28%|##7       | 8/29 [00:00&lt;00:01, 12.55it/s]\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:53&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 34%|###4      | 10/29 [00:00&lt;00:01, 12.50it/s]\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 41%|####1     | 12/29 [00:00&lt;00:01, 12.60it/s]\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 48%|####8     | 14/29 [00:01&lt;00:01, 12.64it/s]\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 55%|#####5    | 16/29 [00:01&lt;00:01, 12.53it/s]\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Loss = 4.0467e-03, PNorm = 63.9714, GNorm = 0.7879, lr_0 = 1.2196e-04\n\r                                                                                  \r\r 62%|######2   | 18/29 [00:01&lt;00:00, 12.48it/s]\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 69%|######8   | 20/29 [00:01&lt;00:00, 12.45it/s]\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:54&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 76%|#######5  | 22/29 [00:01&lt;00:00, 12.38it/s]\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 83%|########2 | 24/29 [00:01&lt;00:00, 12.55it/s]\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 90%|########9 | 26/29 [00:02&lt;00:00, 12.60it/s]\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Loss = 3.8937e-03, PNorm = 63.9956, GNorm = 0.5266, lr_0 = 1.1855e-04\n\r                                                                                  \r\r 97%|#########6| 28/29 [00:02&lt;00:00, 12.52it/s]\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r100%|##########| 29/29 [00:02&lt;00:00, 12.48it/s]\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r100%|##########| 4/4 [00:00&lt;00:00, 42.17it/s]\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:55&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Validation auc = 0.918667\n\r                                                                                  \r\r 93%|#########3| 28/30 [01:51&lt;00:07,  3.50s/it]\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Epoch 28\n\r                                                                                  \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r  7%|6         | 2/29 [00:00&lt;00:02, 12.18it/s]\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 14%|#3        | 4/29 [00:00&lt;00:02, 12.28it/s]\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 21%|##        | 6/29 [00:00&lt;00:01, 12.28it/s]\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Loss = 3.9191e-03, PNorm = 64.0188, GNorm = 0.8906, lr_0 = 1.1523e-04\n\r                                                                                  \r\r 28%|##7       | 8/29 [00:00&lt;00:01, 12.21it/s]\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 34%|###4      | 10/29 [00:00&lt;00:01, 12.29it/s]\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:14:59&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 41%|####1     | 12/29 [00:00&lt;00:01, 12.38it/s]\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 48%|####8     | 14/29 [00:01&lt;00:01, 12.22it/s]\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 55%|#####5    | 16/29 [00:01&lt;00:01, 12.08it/s]\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Loss = 3.7201e-03, PNorm = 64.0399, GNorm = 0.8670, lr_0 = 1.1201e-04\n\r                                                                                  \r\r 62%|######2   | 18/29 [00:01&lt;00:00, 12.01it/s]\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 69%|######8   | 20/29 [00:01&lt;00:00, 12.24it/s]\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 76%|#######5  | 22/29 [00:01&lt;00:00, 12.33it/s]\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:00&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 83%|########2 | 24/29 [00:01&lt;00:00, 12.37it/s]\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 90%|########9 | 26/29 [00:02&lt;00:00, 12.30it/s]\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Loss = 3.3986e-03, PNorm = 64.0633, GNorm = 0.8162, lr_0 = 1.0888e-04\n\r                                                                                  \r\r 97%|#########6| 28/29 [00:02&lt;00:00, 12.33it/s]\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r100%|##########| 29/29 [00:02&lt;00:00, 12.28it/s]\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r100%|##########| 4/4 [00:00&lt;00:00, 42.47it/s]\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Validation auc = 0.909429\n\r                                                                                  \r\r 97%|#########6| 29/30 [01:53&lt;00:03,  3.19s/it]\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Epoch 29\n\r                                                                                  \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r  7%|6         | 2/29 [00:00&lt;00:02, 12.26it/s]\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 14%|#3        | 4/29 [00:00&lt;00:02, 12.41it/s]\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:01&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 21%|##        | 6/29 [00:00&lt;00:01, 12.44it/s]\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 28%|##7       | 8/29 [00:00&lt;00:01, 12.32it/s]\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Loss = 3.4589e-03, PNorm = 64.0827, GNorm = 0.5223, lr_0 = 1.0584e-04\n\r                                                                                  \r\r 34%|###4      | 10/29 [00:00&lt;00:01, 12.36it/s]\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 41%|####1     | 12/29 [00:00&lt;00:01, 12.46it/s]\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 48%|####8     | 14/29 [00:01&lt;00:01, 12.40it/s]\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 55%|#####5    | 16/29 [00:01&lt;00:01, 12.37it/s]\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:02&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 62%|######2   | 18/29 [00:01&lt;00:00, 12.37it/s]\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Loss = 3.8101e-03, PNorm = 64.1042, GNorm = 0.5160, lr_0 = 1.0288e-04\n\r                                                                                  \r\r 69%|######8   | 20/29 [00:01&lt;00:00, 12.42it/s]\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 76%|#######5  | 22/29 [00:01&lt;00:00, 12.49it/s]\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 83%|########2 | 24/29 [00:01&lt;00:00, 12.53it/s]\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 90%|########9 | 26/29 [00:02&lt;00:00, 12.42it/s]\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r 97%|#########6| 28/29 [00:02&lt;00:00, 12.47it/s]\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Loss = 3.6807e-03, PNorm = 64.1240, GNorm = 0.4226, lr_0 = 1.0000e-04\n\r                                                                                  \r\r100%|##########| 29/29 [00:02&lt;00:00, 12.43it/s]\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:03&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r100%|##########| 4/4 [00:00&lt;00:00, 42.07it/s]\n\r 98%|█████████▊| 39/40 [1:15:04&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:04&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Validation auc = 0.908841\n\r                                                                                  \r\r100%|##########| 30/30 [01:56&lt;00:00,  2.97s/it]\n\r 98%|█████████▊| 39/40 [1:15:04&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:04&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Model 0 best validation auc = 0.918667 on epoch 27\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\r                                                                                  \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 98%|█████████▊| 39/40 [1:15:05&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:05&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\r100%|##########| 4/4 [00:00&lt;00:00, 37.86it/s]\n\r 98%|█████████▊| 39/40 [1:15:05&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]\r                                                                                  \r\n\r 98%|█████████▊| 39/40 [1:15:05&lt;01:57, 117.58s/it, best loss: -0.8767466105577009]Model 0 test auc = 0.870860\nEnsemble test auc = 0.870860\n1-fold cross validation\nSeed 0 ==&gt; test auc = 0.870860\nOverall test auc = 0.870860 +/- 0.000000\nnum params: 4,927,508\n0.8708595445935224 +/- 0.0 auc\n\r100%|██████████| 40/40 [1:15:05&lt;00:00, 118.71s/it, best loss: -0.8767466105577009]\nbest\n{&#39;depth&#39;: 6, &#39;dropout&#39;: 0.25, &#39;ffn_num_layers&#39;: 1, &#39;hidden_size&#39;: 2200}\nnum params: 10,307,008\n0.8767466105577009 +/- 0.0 auc\n</div>"]}}],"execution_count":38},{"cell_type":"markdown","source":["# Training"],"metadata":{}},{"cell_type":"code","source":["from argparse import ArgumentParser, Namespace\nfrom chemprop.parsing import parse_train_args\nfrom chemprop.parsing import add_train_args\nfrom chemprop.parsing import modify_train_args\n\nfrom chemprop.train import cross_validate\nfrom chemprop.utils import create_logger"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":40},{"cell_type":"code","source":["parser = ArgumentParser()\nadd_train_args(parser)\nargs = parser.parse_args(['-h'])\nargs = modify_train_args(args)\n\ncross_validate(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">An exception has occurred, use %tb to see the full traceback.\n\n<span class=\"ansi-red-fg\">SystemExit</span><span class=\"ansi-red-fg\">:</span> 0\n</div>"]}}],"execution_count":41},{"cell_type":"code","source":["%sh cp /dbfs/FileStore/chemprop/JAK/train-1460.csv /dbfs/FileStore/chemprop/JAK/all-1825.csv \nsed 1d /dbfs/FileStore/chemprop/JAK/test-183.csv >> /dbfs/FileStore/chemprop/JAK/all-1825.csv \nsed 1d /dbfs/FileStore/chemprop/JAK/val-182.csv >> /dbfs/FileStore/chemprop/JAK/all-1825.csv \nwc /dbfs/FileStore/chemprop/JAK/all-1825.csv "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">  1826   1838 235037 /dbfs/FileStore/chemprop/JAK/all-1825.csv\n</div>"]}}],"execution_count":42},{"cell_type":"code","source":["%sh cat /dbfs/FileStore/chemprop/JAK/configs/regression-4x.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{\n    &#34;depth&#34;: 4,\n    &#34;dropout&#34;: 0.1,\n    &#34;ffn_num_layers&#34;: 2,\n    &#34;hidden_size&#34;: 2000\n}</div>"]}}],"execution_count":43},{"cell_type":"markdown","source":["#### Classification external vs internal pEC 7.6"],"metadata":{}},{"cell_type":"code","source":["%sh cat /dbfs/FileStore/chemprop/JAK/configs/bin76_ext.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{\n    &#34;depth&#34;: 5,\n    &#34;dropout&#34;: 0.05,\n    &#34;ffn_num_layers&#34;: 2,\n    &#34;hidden_size&#34;: 1500\n}</div>"]}}],"execution_count":45},{"cell_type":"code","source":["%sh cat /dbfs/FileStore/chemprop/JAK/configs/bin76_int.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{\n    &#34;depth&#34;: 5,\n    &#34;dropout&#34;: 0.15000000000000002,\n    &#34;ffn_num_layers&#34;: 1,\n    &#34;hidden_size&#34;: 1300\n}</div>"]}}],"execution_count":46},{"cell_type":"code","source":["#External\nparser = ArgumentParser()\nadd_train_args(parser)\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-8396_bin76.csv'),\n                          '--dataset_type','classification',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext'),\n                          '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'),\n                          '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                          '--log_frequency','1',\n                          '--depth','5',\n                          '--dropout','0.05',\n                          '--hidden_size','1500',\n                          '--ffn_num_layers','2',\n                          '--epochs','50'\n                        #,'--atom_messages'\n                        #,'--ensemble_size','3'\n                        #,'--features_generator','rdkit_2d'\n                         ,'--seed','13',\n                          ])\nmodify_train_args(args)\nlogger = create_logger(name='train', save_dir=args.save_dir, quiet=args.quiet)\n\ncross_validate(args, logger)\n#test auc = 0.889156 - 50epoch\n#test auc = 0.879814 - 30epoch\n#test auc = 0.873914 - 11epoch\n#best validation auc = 0.915431 on epoch 26\n#best validation auc = 0.938879 on epoch 11\n#best validation auc = 0.920827 on epoch 9"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-8396_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 5,\n &#39;dropout&#39;: 0.05,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 50,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 1500,\n &#39;ffn_num_layers&#39;: 2,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1500,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_ext/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\n\r  0%|          | 0/8396 [00:00&lt;?, ?it/s]\r  4%|▍         | 315/8396 [00:00&lt;00:02, 3145.29it/s]\r  8%|▊         | 649/8396 [00:00&lt;00:02, 3201.09it/s]\r 12%|█▏        | 980/8396 [00:00&lt;00:02, 3231.99it/s]\r 16%|█▌        | 1307/8396 [00:00&lt;00:02, 3242.27it/s]\r 20%|█▉        | 1638/8396 [00:00&lt;00:02, 3260.32it/s]\r 23%|██▎       | 1917/8396 [00:00&lt;00:02, 3103.35it/s]\r 26%|██▌       | 2193/8396 [00:00&lt;00:02, 2953.81it/s]\r 30%|██▉       | 2483/8396 [00:00&lt;00:02, 2934.35it/s]\r 33%|███▎      | 2803/8396 [00:00&lt;00:01, 3007.49it/s]\r 37%|███▋      | 3136/8396 [00:01&lt;00:01, 3095.88it/s]\r 41%|████      | 3439/8396 [00:01&lt;00:01, 2890.28it/s]\r 44%|████▍     | 3726/8396 [00:01&lt;00:01, 2864.23it/s]\r 48%|████▊     | 4058/8396 [00:01&lt;00:01, 2985.24it/s]\r 52%|█████▏    | 4380/8396 [00:01&lt;00:01, 3051.72it/s]\r 56%|█████▋    | 4727/8396 [00:01&lt;00:01, 3163.64it/s]\r 60%|██████    | 5079/8396 [00:01&lt;00:01, 3260.08it/s]\r 65%|██████▍   | 5434/8396 [00:01&lt;00:00, 3341.02it/s]\r 69%|██████▊   | 5770/8396 [00:01&lt;00:00, 3338.82it/s]\r 73%|███████▎  | 6106/8396 [00:01&lt;00:00, 3284.21it/s]\r 77%|███████▋  | 6436/8396 [00:02&lt;00:00, 3209.43it/s]\r 81%|████████  | 6759/8396 [00:02&lt;00:00, 3214.21it/s]\r 85%|████████▍ | 7095/8396 [00:02&lt;00:00, 3255.23it/s]\r 89%|████████▉ | 7452/8396 [00:02&lt;00:00, 3341.27it/s]\r 93%|█████████▎| 7827/8396 [00:02&lt;00:00, 3452.53it/s]\r 97%|█████████▋| 8174/8396 [00:02&lt;00:00, 3442.89it/s]\r100%|██████████| 8396/8396 [00:02&lt;00:00, 3204.48it/s]\nNumber of tasks = 4\nSplitting data with seed 13\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 3246.42it/s]\n\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\r100%|██████████| 182/182 [00:00&lt;00:00, 3254.25it/s]\nClass sizes\nJAK1 0: 33.55%, 1: 66.45%\nJAK2 0: 60.19%, 1: 39.81%\nJAK3 0: 77.42%, 1: 22.58%\nTYK2 0: 92.62%, 1: 7.38%\nTotal size = 8,396 | train size = 8,396 | val size = 182 | test size = 183\nBuilding model 0\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.05)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1500, bias=False)\n      (W_h): Linear(in_features=1500, out_features=1500, bias=False)\n      (W_o): Linear(in_features=1633, out_features=1500, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.05)\n    (1): Linear(in_features=1500, out_features=1500, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.05)\n    (4): Linear(in_features=1500, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 7,179,004\nMoving model to cuda\n\r  0%|          | 0/50 [00:00&lt;?, ?it/s]Epoch 0\n\n\r  0%|          | 0/167 [00:00&lt;?, ?it/s]Loss = 1.3349e-02, PNorm = 69.5572, GNorm = 2.3014, lr_0 = 1.0269e-04\n\n\r  1%|          | 1/167 [00:00&lt;00:26,  6.20it/s]Loss = 1.2354e-02, PNorm = 69.5578, GNorm = 0.7841, lr_0 = 1.0539e-04\n\n\r  1%|          | 2/167 [00:00&lt;00:26,  6.26it/s]Loss = 9.8677e-03, PNorm = 69.5586, GNorm = 0.9328, lr_0 = 1.0808e-04\n\n\r  2%|▏         | 3/167 [00:00&lt;00:26,  6.29it/s]Loss = 1.4017e-02, PNorm = 69.5589, GNorm = 2.4804, lr_0 = 1.1078e-04\n\n\r  2%|▏         | 4/167 [00:00&lt;00:25,  6.33it/s]Loss = 1.0350e-02, PNorm = 69.5594, GNorm = 1.0664, lr_0 = 1.1347e-04\n\n\r  3%|▎         | 5/167 [00:00&lt;00:29,  5.44it/s]Loss = 1.1165e-02, PNorm = 69.5600, GNorm = 1.0690, lr_0 = 1.1617e-04\n\n\r  4%|▎         | 6/167 [00:01&lt;00:28,  5.67it/s]Loss = 1.3059e-02, PNorm = 69.5603, GNorm = 1.1202, lr_0 = 1.1886e-04\n\n\r  4%|▍         | 7/167 [00:01&lt;00:27,  5.79it/s]Loss = 1.1674e-02, PNorm = 69.5606, GNorm = 2.2655, lr_0 = 1.2156e-04\n\n\r  5%|▍         | 8/167 [00:01&lt;00:26,  6.00it/s]Loss = 1.1609e-02, PNorm = 69.5609, GNorm = 0.4431, lr_0 = 1.2425e-04\n\n\r  5%|▌         | 9/167 [00:01&lt;00:26,  6.04it/s]Loss = 1.1691e-02, PNorm = 69.5613, GNorm = 1.2187, lr_0 = 1.2695e-04\n\n\r  6%|▌         | 10/167 [00:01&lt;00:25,  6.22it/s]Loss = 1.1440e-02, PNorm = 69.5617, GNorm = 0.6540, lr_0 = 1.2964e-04\n\n\r  7%|▋         | 11/167 [00:01&lt;00:24,  6.35it/s]Loss = 1.1120e-02, PNorm = 69.5621, GNorm = 0.6472, lr_0 = 1.3234e-04\n\n\r  7%|▋         | 12/167 [00:01&lt;00:24,  6.35it/s]Loss = 1.2041e-02, PNorm = 69.5626, GNorm = 0.9200, lr_0 = 1.3503e-04\n\n\r  8%|▊         | 13/167 [00:02&lt;00:24,  6.38it/s]Loss = 1.1744e-02, PNorm = 69.5631, GNorm = 0.7038, lr_0 = 1.3772e-04\n\n\r  8%|▊         | 14/167 [00:02&lt;00:23,  6.38it/s]Loss = 1.0988e-02, PNorm = 69.5637, GNorm = 1.4280, lr_0 = 1.4042e-04\n\n\r  9%|▉         | 15/167 [00:02&lt;00:29,  5.10it/s]Loss = 1.2406e-02, PNorm = 69.5643, GNorm = 0.5834, lr_0 = 1.4311e-04\n\n\r 10%|▉         | 16/167 [00:02&lt;00:27,  5.45it/s]Loss = 1.0485e-02, PNorm = 69.5651, GNorm = 0.6936, lr_0 = 1.4581e-04\n\n\r 10%|█         | 17/167 [00:02&lt;00:26,  5.75it/s]Loss = 1.1718e-02, PNorm = 69.5659, GNorm = 0.4401, lr_0 = 1.4850e-04\n\n\r 11%|█         | 18/167 [00:03&lt;00:25,  5.90it/s]Loss = 1.1664e-02, PNorm = 69.5667, GNorm = 0.3887, lr_0 = 1.5120e-04\n\n\r 11%|█▏        | 19/167 [00:03&lt;00:24,  6.03it/s]Loss = 1.1535e-02, PNorm = 69.5675, GNorm = 0.3910, lr_0 = 1.5389e-04\n\n\r 12%|█▏        | 20/167 [00:03&lt;00:23,  6.13it/s]Loss = 1.0220e-02, PNorm = 69.5683, GNorm = 0.5338, lr_0 = 1.5659e-04\n\n\r 13%|█▎        | 21/167 [00:03&lt;00:23,  6.26it/s]Loss = 1.1031e-02, PNorm = 69.5690, GNorm = 1.0326, lr_0 = 1.5928e-04\n\n\r 13%|█▎        | 22/167 [00:03&lt;00:22,  6.45it/s]Loss = 1.2809e-02, PNorm = 69.5696, GNorm = 0.8568, lr_0 = 1.6198e-04\n\n\r 14%|█▍        | 23/167 [00:03&lt;00:22,  6.38it/s]Loss = 1.1622e-02, PNorm = 69.5702, GNorm = 0.9174, lr_0 = 1.6467e-04\n\n\r 14%|█▍        | 24/167 [00:03&lt;00:22,  6.38it/s]Loss = 1.1007e-02, PNorm = 69.5710, GNorm = 0.4212, lr_0 = 1.6737e-04\n\n\r 15%|█▍        | 25/167 [00:04&lt;00:29,  4.80it/s]Loss = 1.0547e-02, PNorm = 69.5719, GNorm = 0.4994, lr_0 = 1.7006e-04\n\n\r 16%|█▌        | 26/167 [00:04&lt;00:27,  5.21it/s]Loss = 1.1582e-02, PNorm = 69.5728, GNorm = 0.3251, lr_0 = 1.7275e-04\n\n\r 16%|█▌        | 27/167 [00:04&lt;00:25,  5.54it/s]Loss = 1.3577e-02, PNorm = 69.5737, GNorm = 0.7029, lr_0 = 1.7545e-04\n\n\r 17%|█▋        | 28/167 [00:04&lt;00:24,  5.72it/s]Loss = 1.2063e-02, PNorm = 69.5746, GNorm = 0.6475, lr_0 = 1.7814e-04\n\n\r 17%|█▋        | 29/167 [00:04&lt;00:23,  5.93it/s]Loss = 1.0820e-02, PNorm = 69.5757, GNorm = 0.3088, lr_0 = 1.8084e-04\n\n\r 18%|█▊        | 30/167 [00:05&lt;00:22,  6.00it/s]Loss = 1.0927e-02, PNorm = 69.5769, GNorm = 0.3589, lr_0 = 1.8353e-04\n\n\r 19%|█▊        | 31/167 [00:05&lt;00:22,  6.16it/s]Loss = 1.1020e-02, PNorm = 69.5782, GNorm = 0.4802, lr_0 = 1.8623e-04\n\n\r 19%|█▉        | 32/167 [00:05&lt;00:21,  6.28it/s]Loss = 1.0572e-02, PNorm = 69.5797, GNorm = 0.3522, lr_0 = 1.8892e-04\n\n\r 20%|█▉        | 33/167 [00:05&lt;00:21,  6.25it/s]Loss = 1.3742e-02, PNorm = 69.5810, GNorm = 0.5909, lr_0 = 1.9162e-04\n\n\r 20%|██        | 34/167 [00:05&lt;00:20,  6.40it/s]Loss = 1.1562e-02, PNorm = 69.5824, GNorm = 0.3364, lr_0 = 1.9431e-04\n\n\r 21%|██        | 35/167 [00:05&lt;00:20,  6.44it/s]Loss = 1.1845e-02, PNorm = 69.5837, GNorm = 0.3153, lr_0 = 1.9701e-04\n\n\r 22%|██▏       | 36/167 [00:05&lt;00:20,  6.47it/s]Loss = 1.0006e-02, PNorm = 69.5851, GNorm = 0.2925, lr_0 = 1.9970e-04\n\n\r 22%|██▏       | 37/167 [00:06&lt;00:20,  6.47it/s]Loss = 1.0640e-02, PNorm = 69.5867, GNorm = 0.3340, lr_0 = 2.0240e-04\n\n\r 23%|██▎       | 38/167 [00:06&lt;00:19,  6.52it/s]Loss = 1.1785e-02, PNorm = 69.5881, GNorm = 0.5480, lr_0 = 2.0509e-04\n\n\r 23%|██▎       | 39/167 [00:06&lt;00:28,  4.43it/s]Loss = 1.0856e-02, PNorm = 69.5895, GNorm = 0.6108, lr_0 = 2.0778e-04\n\n\r 24%|██▍       | 40/167 [00:06&lt;00:25,  4.93it/s]Loss = 1.1373e-02, PNorm = 69.5909, GNorm = 0.2534, lr_0 = 2.1048e-04\n\n\r 25%|██▍       | 41/167 [00:06&lt;00:23,  5.33it/s]Loss = 1.1063e-02, PNorm = 69.5924, GNorm = 0.2512, lr_0 = 2.1317e-04\n\n\r 25%|██▌       | 42/167 [00:07&lt;00:22,  5.57it/s]Loss = 1.3576e-02, PNorm = 69.5936, GNorm = 0.6283, lr_0 = 2.1587e-04\n\n\r 26%|██▌       | 43/167 [00:07&lt;00:21,  5.83it/s]Loss = 9.7097e-03, PNorm = 69.5949, GNorm = 0.6983, lr_0 = 2.1856e-04\n\n\r 26%|██▋       | 44/167 [00:07&lt;00:20,  6.01it/s]Loss = 1.1874e-02, PNorm = 69.5962, GNorm = 0.4931, lr_0 = 2.2126e-04\n\n\r 27%|██▋       | 45/167 [00:07&lt;00:19,  6.25it/s]Loss = 1.0854e-02, PNorm = 69.5979, GNorm = 0.5929, lr_0 = 2.2395e-04\n\n\r 28%|██▊       | 46/167 [00:07&lt;00:19,  6.29it/s]Loss = 1.0140e-02, PNorm = 69.5998, GNorm = 0.4064, lr_0 = 2.2665e-04\n\n\r 28%|██▊       | 47/167 [00:07&lt;00:19,  6.23it/s]Loss = 1.3451e-02, PNorm = 69.6016, GNorm = 0.7983, lr_0 = 2.2934e-04\n\n\r 29%|██▊       | 48/167 [00:08&lt;00:18,  6.33it/s]Loss = 1.1266e-02, PNorm = 69.6035, GNorm = 0.2957, lr_0 = 2.3204e-04\n\n\r 29%|██▉       | 49/167 [00:08&lt;00:18,  6.38it/s]Loss = 1.1733e-02, PNorm = 69.6056, GNorm = 0.3959, lr_0 = 2.3473e-04\n\n\r 30%|██▉       | 50/167 [00:08&lt;00:18,  6.50it/s]Loss = 1.0619e-02, PNorm = 69.6077, GNorm = 0.4465, lr_0 = 2.3743e-04\n\n\r 31%|███       | 51/167 [00:08&lt;00:17,  6.48it/s]Loss = 1.0477e-02, PNorm = 69.6096, GNorm = 0.7125, lr_0 = 2.4012e-04\n\n\r 31%|███       | 52/167 [00:08&lt;00:17,  6.53it/s]Loss = 1.1415e-02, PNorm = 69.6116, GNorm = 0.3017, lr_0 = 2.4281e-04\n\n\r 32%|███▏      | 53/167 [00:08&lt;00:17,  6.55it/s]Loss = 8.7693e-03, PNorm = 69.6140, GNorm = 0.5978, lr_0 = 2.4551e-04\n\n\r 32%|███▏      | 54/167 [00:08&lt;00:17,  6.47it/s]Loss = 1.1837e-02, PNorm = 69.6163, GNorm = 0.3372, lr_0 = 2.4820e-04\n\n\r 33%|███▎      | 55/167 [00:09&lt;00:27,  4.04it/s]Loss = 1.2129e-02, PNorm = 69.6188, GNorm = 0.4308, lr_0 = 2.5090e-04\n\n\r 34%|███▎      | 56/167 [00:09&lt;00:24,  4.57it/s]Loss = 1.0405e-02, PNorm = 69.6213, GNorm = 0.2583, lr_0 = 2.5359e-04\n\n\r 34%|███▍      | 57/167 [00:09&lt;00:21,  5.04it/s]Loss = 1.0528e-02, PNorm = 69.6239, GNorm = 0.4661, lr_0 = 2.5629e-04\n\n\r 35%|███▍      | 58/167 [00:09&lt;00:20,  5.44it/s]Loss = 1.1544e-02, PNorm = 69.6264, GNorm = 0.5944, lr_0 = 2.5898e-04\n\n\r 35%|███▌      | 59/167 [00:10&lt;00:18,  5.70it/s]Loss = 1.0047e-02, PNorm = 69.6292, GNorm = 0.2465, lr_0 = 2.6168e-04\n\n\r 36%|███▌      | 60/167 [00:10&lt;00:17,  5.98it/s]Loss = 1.2668e-02, PNorm = 69.6321, GNorm = 0.6490, lr_0 = 2.6437e-04\n\n\r 37%|███▋      | 61/167 [00:10&lt;00:17,  6.17it/s]Loss = 1.1318e-02, PNorm = 69.6353, GNorm = 0.3192, lr_0 = 2.6707e-04\n\n\r 37%|███▋      | 62/167 [00:10&lt;00:16,  6.26it/s]Loss = 1.0069e-02, PNorm = 69.6385, GNorm = 0.6355, lr_0 = 2.6976e-04\n\n\r 38%|███▊      | 63/167 [00:10&lt;00:16,  6.30it/s]Loss = 1.0286e-02, PNorm = 69.6420, GNorm = 0.2699, lr_0 = 2.7246e-04\n\n\r 38%|███▊      | 64/167 [00:10&lt;00:16,  6.28it/s]Loss = 1.1021e-02, PNorm = 69.6461, GNorm = 0.4128, lr_0 = 2.7515e-04\n\n\r 39%|███▉      | 65/167 [00:10&lt;00:16,  6.33it/s]Loss = 1.2143e-02, PNorm = 69.6502, GNorm = 0.4999, lr_0 = 2.7784e-04\n\n\r 40%|███▉      | 66/167 [00:11&lt;00:15,  6.47it/s]Loss = 1.0561e-02, PNorm = 69.6542, GNorm = 0.7733, lr_0 = 2.8054e-04\n\n\r 40%|████      | 67/167 [00:11&lt;00:15,  6.54it/s]Loss = 1.1216e-02, PNorm = 69.6585, GNorm = 0.7719, lr_0 = 2.8323e-04\n\n\r 41%|████      | 68/167 [00:11&lt;00:15,  6.43it/s]Loss = 1.1785e-02, PNorm = 69.6629, GNorm = 0.4726, lr_0 = 2.8593e-04\n\n\r 41%|████▏     | 69/167 [00:11&lt;00:15,  6.49it/s]Loss = 1.1279e-02, PNorm = 69.6668, GNorm = 0.8841, lr_0 = 2.8862e-04\n\n\r 42%|████▏     | 70/167 [00:11&lt;00:14,  6.49it/s]Loss = 9.1958e-03, PNorm = 69.6712, GNorm = 0.6658, lr_0 = 2.9132e-04\n\n\r 43%|████▎     | 71/167 [00:11&lt;00:14,  6.51it/s]Loss = 1.0665e-02, PNorm = 69.6764, GNorm = 0.3853, lr_0 = 2.9401e-04\n\n\r 43%|████▎     | 72/167 [00:12&lt;00:14,  6.57it/s]Loss = 1.2685e-02, PNorm = 69.6820, GNorm = 1.8322, lr_0 = 2.9671e-04\n\n\r 44%|████▎     | 73/167 [00:12&lt;00:14,  6.47it/s]Loss = 1.4260e-02, PNorm = 69.6873, GNorm = 1.4908, lr_0 = 2.9940e-04\n\n\r 44%|████▍     | 74/167 [00:12&lt;00:25,  3.62it/s]Loss = 1.1061e-02, PNorm = 69.6917, GNorm = 1.1551, lr_0 = 3.0210e-04\n\n\r 45%|████▍     | 75/167 [00:12&lt;00:22,  4.15it/s]Loss = 1.1487e-02, PNorm = 69.6953, GNorm = 1.1866, lr_0 = 3.0479e-04\n\n\r 46%|████▌     | 76/167 [00:13&lt;00:19,  4.65it/s]Loss = 9.4717e-03, PNorm = 69.6997, GNorm = 0.2945, lr_0 = 3.0749e-04\n\n\r 46%|████▌     | 77/167 [00:13&lt;00:17,  5.05it/s]Loss = 1.1236e-02, PNorm = 69.7038, GNorm = 1.0484, lr_0 = 3.1018e-04\n\n\r 47%|████▋     | 78/167 [00:13&lt;00:16,  5.44it/s]Loss = 1.1220e-02, PNorm = 69.7084, GNorm = 0.3820, lr_0 = 3.1287e-04\n\n\r 47%|████▋     | 79/167 [00:13&lt;00:15,  5.65it/s]Loss = 1.1039e-02, PNorm = 69.7133, GNorm = 0.5222, lr_0 = 3.1557e-04\n\n\r 48%|████▊     | 80/167 [00:13&lt;00:14,  6.00it/s]Loss = 1.0796e-02, PNorm = 69.7183, GNorm = 0.5733, lr_0 = 3.1826e-04\n\n\r 49%|████▊     | 81/167 [00:13&lt;00:13,  6.18it/s]Loss = 1.0921e-02, PNorm = 69.7241, GNorm = 0.5250, lr_0 = 3.2096e-04\n\n\r 49%|████▉     | 82/167 [00:14&lt;00:13,  6.20it/s]Loss = 1.3752e-02, PNorm = 69.7300, GNorm = 1.1869, lr_0 = 3.2365e-04\n\n\r 50%|████▉     | 83/167 [00:14&lt;00:13,  6.29it/s]Loss = 1.0350e-02, PNorm = 69.7368, GNorm = 0.4138, lr_0 = 3.2635e-04\n\n\r 50%|█████     | 84/167 [00:14&lt;00:12,  6.40it/s]Loss = 1.1948e-02, PNorm = 69.7442, GNorm = 0.5635, lr_0 = 3.2904e-04\n\n\r 51%|█████     | 85/167 [00:14&lt;00:12,  6.36it/s]Loss = 1.0926e-02, PNorm = 69.7521, GNorm = 0.2135, lr_0 = 3.3174e-04\n\n\r 51%|█████▏    | 86/167 [00:14&lt;00:12,  6.36it/s]Loss = 1.1086e-02, PNorm = 69.7597, GNorm = 0.5191, lr_0 = 3.3443e-04\n\n\r 52%|█████▏    | 87/167 [00:14&lt;00:12,  6.32it/s]Loss = 1.0025e-02, PNorm = 69.7669, GNorm = 0.7819, lr_0 = 3.3713e-04\n\n\r 53%|█████▎    | 88/167 [00:14&lt;00:12,  6.42it/s]Loss = 1.1260e-02, PNorm = 69.7739, GNorm = 0.4252, lr_0 = 3.3982e-04\n\n\r 53%|█████▎    | 89/167 [00:15&lt;00:12,  6.37it/s]Loss = 1.2614e-02, PNorm = 69.7793, GNorm = 0.7207, lr_0 = 3.4251e-04\n\n\r 54%|█████▍    | 90/167 [00:15&lt;00:11,  6.47it/s]Loss = 1.0132e-02, PNorm = 69.7853, GNorm = 0.3156, lr_0 = 3.4521e-04\n\n\r 54%|█████▍    | 91/167 [00:15&lt;00:11,  6.38it/s]Loss = 8.8211e-03, PNorm = 69.7923, GNorm = 0.3294, lr_0 = 3.4790e-04\n\n\r 55%|█████▌    | 92/167 [00:15&lt;00:11,  6.36it/s]Loss = 1.0574e-02, PNorm = 69.8002, GNorm = 0.3933, lr_0 = 3.5060e-04\n\n\r 56%|█████▌    | 93/167 [00:15&lt;00:11,  6.37it/s]Loss = 1.1375e-02, PNorm = 69.8074, GNorm = 0.4886, lr_0 = 3.5329e-04\n\n\r 56%|█████▋    | 94/167 [00:15&lt;00:11,  6.41it/s]Loss = 8.4872e-03, PNorm = 69.8156, GNorm = 0.5425, lr_0 = 3.5599e-04\n\n\r 57%|█████▋    | 95/167 [00:16&lt;00:11,  6.29it/s]Loss = 1.0819e-02, PNorm = 69.8230, GNorm = 0.4158, lr_0 = 3.5868e-04\n\n\r 57%|█████▋    | 96/167 [00:16&lt;00:11,  6.36it/s]Loss = 1.2919e-02, PNorm = 69.8290, GNorm = 0.5419, lr_0 = 3.6138e-04\n\n\r 58%|█████▊    | 97/167 [00:16&lt;00:11,  6.34it/s]Loss = 1.1418e-02, PNorm = 69.8348, GNorm = 0.2967, lr_0 = 3.6407e-04\n\n\r 59%|█████▊    | 98/167 [00:17&lt;00:21,  3.22it/s]Loss = 1.1007e-02, PNorm = 69.8401, GNorm = 0.3353, lr_0 = 3.6677e-04\n\n\r 59%|█████▉    | 99/167 [00:17&lt;00:17,  3.80it/s]Loss = 1.0384e-02, PNorm = 69.8458, GNorm = 0.4757, lr_0 = 3.6946e-04\n\n\r 60%|█████▉    | 100/167 [00:17&lt;00:15,  4.32it/s]Loss = 1.0265e-02, PNorm = 69.8521, GNorm = 0.2877, lr_0 = 3.7216e-04\n\n\r 60%|██████    | 101/167 [00:17&lt;00:13,  4.74it/s]Loss = 1.1808e-02, PNorm = 69.8590, GNorm = 0.5920, lr_0 = 3.7485e-04\n\n\r 61%|██████    | 102/167 [00:17&lt;00:12,  5.12it/s]Loss = 1.0453e-02, PNorm = 69.8662, GNorm = 0.7807, lr_0 = 3.7754e-04\n\n\r 62%|██████▏   | 103/167 [00:17&lt;00:12,  5.29it/s]Loss = 1.0856e-02, PNorm = 69.8730, GNorm = 0.4396, lr_0 = 3.8024e-04\n\n\r 62%|██████▏   | 104/167 [00:17&lt;00:11,  5.54it/s]Loss = 9.3205e-03, PNorm = 69.8798, GNorm = 0.6181, lr_0 = 3.8293e-04\n\n\r 63%|██████▎   | 105/167 [00:18&lt;00:10,  5.72it/s]Loss = 1.1123e-02, PNorm = 69.8868, GNorm = 0.5573, lr_0 = 3.8563e-04\n\n\r 63%|██████▎   | 106/167 [00:18&lt;00:10,  5.96it/s]Loss = 1.1358e-02, PNorm = 69.8948, GNorm = 0.8832, lr_0 = 3.8832e-04\n\n\r 64%|██████▍   | 107/167 [00:18&lt;00:09,  6.04it/s]Loss = 1.0272e-02, PNorm = 69.9029, GNorm = 0.3880, lr_0 = 3.9102e-04\n\n\r 65%|██████▍   | 108/167 [00:18&lt;00:09,  6.10it/s]Loss = 8.5978e-03, PNorm = 69.9116, GNorm = 0.4665, lr_0 = 3.9371e-04\n\n\r 65%|██████▌   | 109/167 [00:18&lt;00:09,  6.12it/s]Loss = 1.0550e-02, PNorm = 69.9214, GNorm = 0.5152, lr_0 = 3.9641e-04\n\n\r 66%|██████▌   | 110/167 [00:18&lt;00:09,  6.27it/s]Loss = 9.6090e-03, PNorm = 69.9305, GNorm = 0.9820, lr_0 = 3.9910e-04\n\n\r 66%|██████▋   | 111/167 [00:19&lt;00:08,  6.31it/s]Loss = 9.2322e-03, PNorm = 69.9402, GNorm = 0.6082, lr_0 = 4.0180e-04\n\n\r 67%|██████▋   | 112/167 [00:19&lt;00:08,  6.39it/s]Loss = 9.5379e-03, PNorm = 69.9479, GNorm = 0.8188, lr_0 = 4.0449e-04\n\n\r 68%|██████▊   | 113/167 [00:19&lt;00:08,  6.43it/s]Loss = 8.2801e-03, PNorm = 69.9559, GNorm = 0.4484, lr_0 = 4.0719e-04\n\n\r 68%|██████▊   | 114/167 [00:19&lt;00:08,  6.38it/s]Loss = 9.6294e-03, PNorm = 69.9638, GNorm = 0.7338, lr_0 = 4.0988e-04\n\n\r 69%|██████▉   | 115/167 [00:19&lt;00:07,  6.51it/s]Loss = 1.4773e-02, PNorm = 69.9710, GNorm = 1.5384, lr_0 = 4.1257e-04\n\n\r 69%|██████▉   | 116/167 [00:19&lt;00:07,  6.51it/s]Loss = 8.7750e-03, PNorm = 69.9787, GNorm = 0.4894, lr_0 = 4.1527e-04\n\n\r 70%|███████   | 117/167 [00:20&lt;00:07,  6.37it/s]Loss = 9.8519e-03, PNorm = 69.9864, GNorm = 0.5076, lr_0 = 4.1796e-04\n\n\r 71%|███████   | 118/167 [00:20&lt;00:07,  6.42it/s]Loss = 1.0600e-02, PNorm = 69.9936, GNorm = 0.3970, lr_0 = 4.2066e-04\n\n\r 71%|███████▏  | 119/167 [00:20&lt;00:07,  6.38it/s]Loss = 9.7537e-03, PNorm = 69.9996, GNorm = 0.6947, lr_0 = 4.2335e-04\n\n\r 72%|███████▏  | 120/167 [00:20&lt;00:07,  6.45it/s]Loss = 9.7964e-03, PNorm = 70.0052, GNorm = 0.4771, lr_0 = 4.2605e-04\n\n\r 72%|███████▏  | 121/167 [00:20&lt;00:07,  6.41it/s]Loss = 9.8942e-03, PNorm = 70.0116, GNorm = 0.3639, lr_0 = 4.2874e-04\n\n\r 73%|███████▎  | 122/167 [00:20&lt;00:07,  6.29it/s]Loss = 1.0356e-02, PNorm = 70.0186, GNorm = 0.4490, lr_0 = 4.3144e-04\n\n\r 74%|███████▎  | 123/167 [00:20&lt;00:06,  6.29it/s]Loss = 9.7376e-03, PNorm = 70.0269, GNorm = 0.3462, lr_0 = 4.3413e-04\n\n\r 74%|███████▍  | 124/167 [00:21&lt;00:06,  6.30it/s]Loss = 9.9052e-03, PNorm = 70.0356, GNorm = 0.5104, lr_0 = 4.3683e-04\n\n\r 75%|███████▍  | 125/167 [00:21&lt;00:06,  6.36it/s]Loss = 1.0816e-02, PNorm = 70.0453, GNorm = 0.4772, lr_0 = 4.3952e-04\n\n\r 75%|███████▌  | 126/167 [00:21&lt;00:06,  6.54it/s]Loss = 8.5089e-03, PNorm = 70.0562, GNorm = 0.4891, lr_0 = 4.4222e-04\n\n\r 76%|███████▌  | 127/167 [00:21&lt;00:06,  6.50it/s]Loss = 9.4309e-03, PNorm = 70.0667, GNorm = 0.4959, lr_0 = 4.4491e-04\n\n\r 77%|███████▋  | 128/167 [00:22&lt;00:13,  2.88it/s]Loss = 1.0920e-02, PNorm = 70.0771, GNorm = 0.4257, lr_0 = 4.4760e-04\n\n\r 77%|███████▋  | 129/167 [00:22&lt;00:11,  3.42it/s]Loss = 1.1396e-02, PNorm = 70.0870, GNorm = 0.8938, lr_0 = 4.5030e-04\n\n\r 78%|███████▊  | 130/167 [00:22&lt;00:09,  3.97it/s]Loss = 1.1425e-02, PNorm = 70.0962, GNorm = 0.9985, lr_0 = 4.5299e-04\n\n\r 78%|███████▊  | 131/167 [00:22&lt;00:07,  4.53it/s]Loss = 9.9997e-03, PNorm = 70.1068, GNorm = 0.3741, lr_0 = 4.5569e-04\n\n\r 79%|███████▉  | 132/167 [00:23&lt;00:07,  4.91it/s]Loss = 1.2477e-02, PNorm = 70.1185, GNorm = 1.1017, lr_0 = 4.5838e-04\n\n\r 80%|███████▉  | 133/167 [00:23&lt;00:06,  5.29it/s]Loss = 8.8317e-03, PNorm = 70.1314, GNorm = 0.2833, lr_0 = 4.6108e-04\n\n\r 80%|████████  | 134/167 [00:23&lt;00:05,  5.54it/s]Loss = 9.0596e-03, PNorm = 70.1426, GNorm = 0.7128, lr_0 = 4.6377e-04\n\n\r 81%|████████  | 135/167 [00:23&lt;00:05,  5.80it/s]Loss = 1.1047e-02, PNorm = 70.1515, GNorm = 0.8297, lr_0 = 4.6647e-04\n\n\r 81%|████████▏ | 136/167 [00:23&lt;00:05,  6.03it/s]Loss = 8.5955e-03, PNorm = 70.1618, GNorm = 0.4094, lr_0 = 4.6916e-04\n\n\r 82%|████████▏ | 137/167 [00:23&lt;00:04,  6.14it/s]Loss = 7.6591e-03, PNorm = 70.1735, GNorm = 0.4314, lr_0 = 4.7186e-04\n\n\r 83%|████████▎ | 138/167 [00:23&lt;00:04,  6.23it/s]Loss = 1.1132e-02, PNorm = 70.1857, GNorm = 0.7459, lr_0 = 4.7455e-04\n\n\r 83%|████████▎ | 139/167 [00:24&lt;00:04,  6.24it/s]Loss = 1.0093e-02, PNorm = 70.1968, GNorm = 0.2557, lr_0 = 4.7725e-04\n\n\r 84%|████████▍ | 140/167 [00:24&lt;00:04,  6.24it/s]Loss = 1.2233e-02, PNorm = 70.2064, GNorm = 0.5451, lr_0 = 4.7994e-04\n\n\r 84%|████████▍ | 141/167 [00:24&lt;00:04,  6.35it/s]Loss = 1.0199e-02, PNorm = 70.2165, GNorm = 0.4446, lr_0 = 4.8263e-04\n\n\r 85%|████████▌ | 142/167 [00:24&lt;00:04,  6.24it/s]Loss = 9.7282e-03, PNorm = 70.2267, GNorm = 0.2432, lr_0 = 4.8533e-04\n\n\r 86%|████████▌ | 143/167 [00:24&lt;00:03,  6.28it/s]Loss = 1.0372e-02, PNorm = 70.2371, GNorm = 0.2980, lr_0 = 4.8802e-04\n\n\r 86%|████████▌ | 144/167 [00:24&lt;00:03,  6.41it/s]Loss = 1.0011e-02, PNorm = 70.2459, GNorm = 0.4992, lr_0 = 4.9072e-04\n\n\r 87%|████████▋ | 145/167 [00:25&lt;00:03,  6.39it/s]Loss = 9.6542e-03, PNorm = 70.2547, GNorm = 0.3009, lr_0 = 4.9341e-04\n\n\r 87%|████████▋ | 146/167 [00:25&lt;00:03,  6.47it/s]Loss = 1.1171e-02, PNorm = 70.2623, GNorm = 0.6216, lr_0 = 4.9611e-04\n\n\r 88%|████████▊ | 147/167 [00:25&lt;00:03,  6.58it/s]Loss = 9.0065e-03, PNorm = 70.2705, GNorm = 0.2784, lr_0 = 4.9880e-04\n\n\r 89%|████████▊ | 148/167 [00:25&lt;00:02,  6.40it/s]Loss = 1.0669e-02, PNorm = 70.2798, GNorm = 0.2719, lr_0 = 5.0150e-04\n\n\r 89%|████████▉ | 149/167 [00:25&lt;00:02,  6.38it/s]Loss = 8.3385e-03, PNorm = 70.2899, GNorm = 0.3950, lr_0 = 5.0419e-04\n\n\r 90%|████████▉ | 150/167 [00:25&lt;00:02,  6.34it/s]Loss = 1.1152e-02, PNorm = 70.3003, GNorm = 0.3279, lr_0 = 5.0689e-04\n\n\r 90%|█████████ | 151/167 [00:25&lt;00:02,  6.32it/s]Loss = 9.6220e-03, PNorm = 70.3113, GNorm = 0.3748, lr_0 = 5.0958e-04\n\n\r 91%|█████████ | 152/167 [00:26&lt;00:02,  6.39it/s]Loss = 8.7196e-03, PNorm = 70.3232, GNorm = 0.4073, lr_0 = 5.1228e-04\n\n\r 92%|█████████▏| 153/167 [00:26&lt;00:02,  6.34it/s]Loss = 1.1765e-02, PNorm = 70.3360, GNorm = 0.7082, lr_0 = 5.1497e-04\n\n\r 92%|█████████▏| 154/167 [00:26&lt;00:02,  6.40it/s]Loss = 1.0537e-02, PNorm = 70.3513, GNorm = 0.8881, lr_0 = 5.1766e-04\n\n\r 93%|█████████▎| 155/167 [00:26&lt;00:01,  6.26it/s]Loss = 8.5187e-03, PNorm = 70.3652, GNorm = 0.8024, lr_0 = 5.2036e-04\n\n\r 93%|█████████▎| 156/167 [00:26&lt;00:01,  6.26it/s]Loss = 1.1175e-02, PNorm = 70.3797, GNorm = 0.3105, lr_0 = 5.2305e-04\n\n\r 94%|█████████▍| 157/167 [00:26&lt;00:01,  6.30it/s]Loss = 9.3279e-03, PNorm = 70.3924, GNorm = 0.7184, lr_0 = 5.2575e-04\n\n\r 95%|█████████▍| 158/167 [00:27&lt;00:01,  6.34it/s]Loss = 8.0544e-03, PNorm = 70.4035, GNorm = 0.9651, lr_0 = 5.2844e-04\n\n\r 95%|█████████▌| 159/167 [00:27&lt;00:01,  6.38it/s]Loss = 8.8337e-03, PNorm = 70.4146, GNorm = 0.8875, lr_0 = 5.3114e-04\n\n\r 96%|█████████▌| 160/167 [00:27&lt;00:01,  6.42it/s]Loss = 7.6717e-03, PNorm = 70.4273, GNorm = 0.5083, lr_0 = 5.3383e-04\n\n\r 96%|█████████▋| 161/167 [00:27&lt;00:00,  6.44it/s]Loss = 1.3246e-02, PNorm = 70.4414, GNorm = 1.3336, lr_0 = 5.3653e-04\n\n\r 97%|█████████▋| 162/167 [00:27&lt;00:00,  6.44it/s]Loss = 8.7304e-03, PNorm = 70.4559, GNorm = 0.3664, lr_0 = 5.3922e-04\n\n\r 98%|█████████▊| 163/167 [00:27&lt;00:00,  6.48it/s]Loss = 1.1528e-02, PNorm = 70.4708, GNorm = 0.4629, lr_0 = 5.4192e-04\n\n\r 98%|█████████▊| 164/167 [00:28&lt;00:00,  6.40it/s]Loss = 7.6355e-03, PNorm = 70.4848, GNorm = 0.4198, lr_0 = 5.4461e-04\n\n\r 99%|█████████▉| 165/167 [00:28&lt;00:00,  2.53it/s]Loss = 9.9611e-03, PNorm = 70.4984, GNorm = 0.5927, lr_0 = 5.4731e-04\n\n\r 99%|█████████▉| 166/167 [00:29&lt;00:00,  3.10it/s]Loss = 9.9958e-03, PNorm = 70.5105, GNorm = 0.6000, lr_0 = 5.5000e-04\n\n\r100%|██████████| 167/167 [00:29&lt;00:00,  3.67it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 25%|██▌       | 1/4 [00:00&lt;00:00,  9.91it/s]\n\r 50%|█████     | 2/4 [00:00&lt;00:00,  9.75it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00,  9.72it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 10.63it/s]Validation auc = 0.730748\n\r  2%|▏         | 1/50 [00:34&lt;28:08, 34.46s/it]Epoch 1\n\n\r  0%|          | 0/167 [00:00&lt;?, ?it/s]Loss = 9.2897e-03, PNorm = 70.5233, GNorm = 0.2772, lr_0 = 5.5269e-04\n\n*** WARNING: skipped 785781 bytes of output ***\n\nLoss = 1.1361e-05, PNorm = 123.4802, GNorm = 0.0227, lr_0 = 1.0714e-04\n\n\r 56%|█████▋    | 94/167 [00:06&lt;00:05, 13.81it/s]Loss = 3.8864e-05, PNorm = 123.4808, GNorm = 0.1554, lr_0 = 1.0711e-04\nLoss = 2.3263e-06, PNorm = 123.4813, GNorm = 0.0051, lr_0 = 1.0708e-04\n\n\r 57%|█████▋    | 96/167 [00:06&lt;00:05, 13.79it/s]Loss = 3.7276e-06, PNorm = 123.4817, GNorm = 0.0045, lr_0 = 1.0704e-04\nLoss = 4.1653e-06, PNorm = 123.4822, GNorm = 0.0070, lr_0 = 1.0701e-04\n\n\r 59%|█████▊    | 98/167 [00:07&lt;00:04, 13.83it/s]Loss = 1.3883e-05, PNorm = 123.4826, GNorm = 0.0205, lr_0 = 1.0698e-04\nLoss = 8.9834e-06, PNorm = 123.4830, GNorm = 0.0105, lr_0 = 1.0695e-04\n\n\r 60%|█████▉    | 100/167 [00:07&lt;00:04, 13.73it/s]Loss = 1.4914e-05, PNorm = 123.4834, GNorm = 0.0658, lr_0 = 1.0692e-04\nLoss = 2.0111e-05, PNorm = 123.4838, GNorm = 0.0317, lr_0 = 1.0689e-04\n\n\r 61%|██████    | 102/167 [00:07&lt;00:04, 13.79it/s]Loss = 1.2247e-05, PNorm = 123.4842, GNorm = 0.0362, lr_0 = 1.0686e-04\nLoss = 2.9152e-06, PNorm = 123.4845, GNorm = 0.0068, lr_0 = 1.0683e-04\n\n\r 62%|██████▏   | 104/167 [00:07&lt;00:04, 13.69it/s]Loss = 7.2421e-07, PNorm = 123.4849, GNorm = 0.0010, lr_0 = 1.0680e-04\nLoss = 1.6236e-05, PNorm = 123.4852, GNorm = 0.0214, lr_0 = 1.0677e-04\n\n\r 63%|██████▎   | 106/167 [00:07&lt;00:04, 13.67it/s]Loss = 2.2276e-04, PNorm = 123.4856, GNorm = 0.1335, lr_0 = 1.0674e-04\nLoss = 5.1780e-06, PNorm = 123.4860, GNorm = 0.0120, lr_0 = 1.0671e-04\n\n\r 65%|██████▍   | 108/167 [00:07&lt;00:04, 13.61it/s]Loss = 5.6623e-05, PNorm = 123.4864, GNorm = 0.0770, lr_0 = 1.0668e-04\nLoss = 1.6385e-05, PNorm = 123.4869, GNorm = 0.0276, lr_0 = 1.0665e-04\n\n\r 66%|██████▌   | 110/167 [00:07&lt;00:04, 13.64it/s]Loss = 1.2043e-05, PNorm = 123.4873, GNorm = 0.0094, lr_0 = 1.0662e-04\nLoss = 7.2788e-06, PNorm = 123.4877, GNorm = 0.0101, lr_0 = 1.0658e-04\n\n\r 67%|██████▋   | 112/167 [00:08&lt;00:04, 13.70it/s]Loss = 1.8317e-05, PNorm = 123.4881, GNorm = 0.0501, lr_0 = 1.0655e-04\nLoss = 2.5546e-05, PNorm = 123.4885, GNorm = 0.0480, lr_0 = 1.0652e-04\n\n\r 68%|██████▊   | 114/167 [00:08&lt;00:03, 13.72it/s]Loss = 4.0367e-06, PNorm = 123.4889, GNorm = 0.0070, lr_0 = 1.0649e-04\nLoss = 1.5893e-05, PNorm = 123.4893, GNorm = 0.0312, lr_0 = 1.0646e-04\n\n\r 69%|██████▉   | 116/167 [00:08&lt;00:03, 13.80it/s]Loss = 8.6503e-06, PNorm = 123.4896, GNorm = 0.0062, lr_0 = 1.0643e-04\nLoss = 3.0149e-04, PNorm = 123.4900, GNorm = 0.1692, lr_0 = 1.0640e-04\n\n\r 71%|███████   | 118/167 [00:08&lt;00:03, 13.97it/s]Loss = 1.0254e-04, PNorm = 123.4904, GNorm = 0.1054, lr_0 = 1.0637e-04\nLoss = 3.7178e-06, PNorm = 123.4908, GNorm = 0.0080, lr_0 = 1.0634e-04\n\n\r 72%|███████▏  | 120/167 [00:08&lt;00:03, 13.92it/s]Loss = 3.5065e-05, PNorm = 123.4912, GNorm = 0.0486, lr_0 = 1.0631e-04\nLoss = 3.2530e-05, PNorm = 123.4916, GNorm = 0.0441, lr_0 = 1.0628e-04\n\n\r 73%|███████▎  | 122/167 [00:08&lt;00:03, 13.86it/s]Loss = 4.9675e-05, PNorm = 123.4919, GNorm = 0.0605, lr_0 = 1.0625e-04\nLoss = 6.3050e-05, PNorm = 123.4924, GNorm = 0.1725, lr_0 = 1.0622e-04\n\n\r 74%|███████▍  | 124/167 [00:08&lt;00:03, 13.70it/s]Loss = 4.7649e-06, PNorm = 123.4928, GNorm = 0.0121, lr_0 = 1.0619e-04\nLoss = 2.7037e-05, PNorm = 123.4932, GNorm = 0.0355, lr_0 = 1.0616e-04\n\n\r 75%|███████▌  | 126/167 [00:09&lt;00:02, 13.86it/s]Loss = 1.0369e-03, PNorm = 123.4932, GNorm = 0.3929, lr_0 = 1.0613e-04\nLoss = 2.1999e-04, PNorm = 123.4933, GNorm = 0.1786, lr_0 = 1.0610e-04\n\n\r 77%|███████▋  | 128/167 [00:09&lt;00:02, 13.92it/s]Loss = 3.9828e-06, PNorm = 123.4935, GNorm = 0.0059, lr_0 = 1.0607e-04\nLoss = 5.5993e-06, PNorm = 123.4936, GNorm = 0.0154, lr_0 = 1.0603e-04\n\n\r 78%|███████▊  | 130/167 [00:09&lt;00:02, 14.09it/s]Loss = 1.8828e-05, PNorm = 123.4938, GNorm = 0.0516, lr_0 = 1.0600e-04\nLoss = 8.7366e-07, PNorm = 123.4940, GNorm = 0.0014, lr_0 = 1.0597e-04\n\n\r 79%|███████▉  | 132/167 [00:09&lt;00:02, 14.02it/s]Loss = 9.4265e-06, PNorm = 123.4942, GNorm = 0.0207, lr_0 = 1.0594e-04\nLoss = 2.8268e-06, PNorm = 123.4944, GNorm = 0.0033, lr_0 = 1.0591e-04\n\n\r 80%|████████  | 134/167 [00:09&lt;00:02, 14.04it/s]Loss = 1.0314e-05, PNorm = 123.4946, GNorm = 0.0159, lr_0 = 1.0588e-04\nLoss = 2.7495e-06, PNorm = 123.4948, GNorm = 0.0043, lr_0 = 1.0585e-04\n\n\r 81%|████████▏ | 136/167 [00:09&lt;00:02, 13.96it/s]Loss = 1.0758e-05, PNorm = 123.4950, GNorm = 0.0166, lr_0 = 1.0582e-04\nLoss = 4.1585e-06, PNorm = 123.4952, GNorm = 0.0074, lr_0 = 1.0579e-04\n\n\r 83%|████████▎ | 138/167 [00:09&lt;00:02, 13.89it/s]Loss = 9.1805e-04, PNorm = 123.4950, GNorm = 0.3987, lr_0 = 1.0576e-04\nLoss = 4.6182e-05, PNorm = 123.4948, GNorm = 0.0350, lr_0 = 1.0573e-04\n\n\r 84%|████████▍ | 140/167 [00:10&lt;00:01, 13.84it/s]Loss = 2.4495e-05, PNorm = 123.4948, GNorm = 0.0302, lr_0 = 1.0570e-04\nLoss = 2.1651e-06, PNorm = 123.4947, GNorm = 0.0023, lr_0 = 1.0567e-04\n\n\r 85%|████████▌ | 142/167 [00:10&lt;00:01, 13.86it/s]Loss = 1.7413e-05, PNorm = 123.4947, GNorm = 0.0490, lr_0 = 1.0564e-04\nLoss = 6.6566e-06, PNorm = 123.4947, GNorm = 0.0111, lr_0 = 1.0561e-04\n\n\r 86%|████████▌ | 144/167 [00:10&lt;00:01, 13.69it/s]Loss = 3.7948e-06, PNorm = 123.4948, GNorm = 0.0053, lr_0 = 1.0558e-04\nLoss = 6.0741e-06, PNorm = 123.4948, GNorm = 0.0080, lr_0 = 1.0555e-04\n\n\r 87%|████████▋ | 146/167 [00:10&lt;00:01, 13.77it/s]Loss = 2.6130e-06, PNorm = 123.4949, GNorm = 0.0033, lr_0 = 1.0552e-04\nLoss = 2.0205e-06, PNorm = 123.4950, GNorm = 0.0024, lr_0 = 1.0549e-04\n\n\r 89%|████████▊ | 148/167 [00:10&lt;00:01, 13.76it/s]Loss = 1.9429e-04, PNorm = 123.4951, GNorm = 0.2813, lr_0 = 1.0546e-04\nLoss = 1.9895e-05, PNorm = 123.4952, GNorm = 0.0486, lr_0 = 1.0543e-04\n\n\r 90%|████████▉ | 150/167 [00:10&lt;00:01, 13.77it/s]Loss = 3.4932e-05, PNorm = 123.4954, GNorm = 0.0540, lr_0 = 1.0540e-04\nLoss = 2.3171e-05, PNorm = 123.4956, GNorm = 0.0688, lr_0 = 1.0537e-04\n\n\r 91%|█████████ | 152/167 [00:10&lt;00:01, 13.59it/s]Loss = 1.9320e-06, PNorm = 123.4958, GNorm = 0.0026, lr_0 = 1.0534e-04\nLoss = 1.8467e-05, PNorm = 123.4960, GNorm = 0.0192, lr_0 = 1.0531e-04\n\n\r 92%|█████████▏| 154/167 [00:11&lt;00:00, 13.68it/s]Loss = 4.0648e-05, PNorm = 123.4963, GNorm = 0.0943, lr_0 = 1.0528e-04\nLoss = 4.2398e-06, PNorm = 123.4965, GNorm = 0.0068, lr_0 = 1.0525e-04\n\n\r 93%|█████████▎| 156/167 [00:11&lt;00:00, 13.68it/s]Loss = 2.5183e-05, PNorm = 123.4968, GNorm = 0.0518, lr_0 = 1.0522e-04\nLoss = 5.1769e-06, PNorm = 123.4970, GNorm = 0.0157, lr_0 = 1.0519e-04\n\n\r 95%|█████████▍| 158/167 [00:11&lt;00:00, 13.56it/s]Loss = 4.6723e-05, PNorm = 123.4973, GNorm = 0.0757, lr_0 = 1.0516e-04\nLoss = 8.9867e-06, PNorm = 123.4975, GNorm = 0.0156, lr_0 = 1.0513e-04\n\n\r 96%|█████████▌| 160/167 [00:11&lt;00:00, 13.65it/s]Loss = 2.9711e-05, PNorm = 123.4978, GNorm = 0.0517, lr_0 = 1.0509e-04\nLoss = 1.6076e-05, PNorm = 123.4980, GNorm = 0.0209, lr_0 = 1.0506e-04\n\n\r 97%|█████████▋| 162/167 [00:11&lt;00:00, 13.65it/s]Loss = 1.1069e-04, PNorm = 123.4984, GNorm = 0.1066, lr_0 = 1.0503e-04\nLoss = 1.8711e-06, PNorm = 123.4987, GNorm = 0.0036, lr_0 = 1.0500e-04\n\n\r 98%|█████████▊| 164/167 [00:11&lt;00:00, 13.78it/s]Loss = 5.7824e-06, PNorm = 123.4990, GNorm = 0.0088, lr_0 = 1.0497e-04\nLoss = 2.9458e-06, PNorm = 123.4993, GNorm = 0.0069, lr_0 = 1.0494e-04\n\n\r 99%|█████████▉| 166/167 [00:11&lt;00:00, 13.79it/s]Loss = 3.6128e-05, PNorm = 123.4995, GNorm = 0.1180, lr_0 = 1.0491e-04\n\n\r100%|██████████| 167/167 [00:12&lt;00:00, 13.84it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 45.36it/s]Validation auc = 0.906663\n\r 98%|█████████▊| 49/50 [11:09&lt;00:12, 12.43s/it]Epoch 49\n\n\r  0%|          | 0/167 [00:00&lt;?, ?it/s]Loss = 1.0149e-04, PNorm = 123.4998, GNorm = 0.0699, lr_0 = 1.0488e-04\nLoss = 4.2739e-06, PNorm = 123.5000, GNorm = 0.0110, lr_0 = 1.0485e-04\n\n\r  1%|          | 2/167 [00:00&lt;00:11, 14.06it/s]Loss = 3.5400e-05, PNorm = 123.5003, GNorm = 0.0397, lr_0 = 1.0482e-04\nLoss = 1.9332e-05, PNorm = 123.5006, GNorm = 0.0372, lr_0 = 1.0479e-04\n\n\r  2%|▏         | 4/167 [00:00&lt;00:11, 14.00it/s]Loss = 2.0947e-06, PNorm = 123.5008, GNorm = 0.0033, lr_0 = 1.0476e-04\nLoss = 3.2201e-05, PNorm = 123.5011, GNorm = 0.0553, lr_0 = 1.0473e-04\n\n\r  4%|▎         | 6/167 [00:00&lt;00:11, 14.08it/s]Loss = 4.6385e-06, PNorm = 123.5015, GNorm = 0.0089, lr_0 = 1.0470e-04\nLoss = 6.6724e-06, PNorm = 123.5018, GNorm = 0.0073, lr_0 = 1.0467e-04\n\n\r  5%|▍         | 8/167 [00:00&lt;00:11, 14.12it/s]Loss = 1.4406e-05, PNorm = 123.5021, GNorm = 0.0129, lr_0 = 1.0464e-04\nLoss = 4.1398e-05, PNorm = 123.5024, GNorm = 0.0386, lr_0 = 1.0461e-04\n\n\r  6%|▌         | 10/167 [00:00&lt;00:11, 13.99it/s]Loss = 1.1091e-04, PNorm = 123.5029, GNorm = 0.2680, lr_0 = 1.0458e-04\nLoss = 1.3203e-05, PNorm = 123.5033, GNorm = 0.0347, lr_0 = 1.0455e-04\n\n\r  7%|▋         | 12/167 [00:00&lt;00:11, 13.96it/s]Loss = 1.3721e-06, PNorm = 123.5038, GNorm = 0.0021, lr_0 = 1.0452e-04\nLoss = 7.9856e-06, PNorm = 123.5042, GNorm = 0.0128, lr_0 = 1.0449e-04\n\n\r  8%|▊         | 14/167 [00:01&lt;00:11, 13.76it/s]Loss = 1.4712e-04, PNorm = 123.5048, GNorm = 0.4379, lr_0 = 1.0446e-04\nLoss = 6.2189e-06, PNorm = 123.5054, GNorm = 0.0104, lr_0 = 1.0443e-04\n\n\r 10%|▉         | 16/167 [00:01&lt;00:10, 13.91it/s]Loss = 2.5649e-05, PNorm = 123.5060, GNorm = 0.0627, lr_0 = 1.0440e-04\nLoss = 1.4418e-05, PNorm = 123.5066, GNorm = 0.0117, lr_0 = 1.0437e-04\n\n\r 11%|█         | 18/167 [00:01&lt;00:10, 13.85it/s]Loss = 8.8630e-06, PNorm = 123.5072, GNorm = 0.0222, lr_0 = 1.0434e-04\nLoss = 2.0359e-05, PNorm = 123.5077, GNorm = 0.0597, lr_0 = 1.0431e-04\n\n\r 12%|█▏        | 20/167 [00:01&lt;00:10, 13.76it/s]Loss = 8.0370e-06, PNorm = 123.5082, GNorm = 0.0289, lr_0 = 1.0428e-04\nLoss = 4.9940e-06, PNorm = 123.5087, GNorm = 0.0070, lr_0 = 1.0425e-04\n\n\r 13%|█▎        | 22/167 [00:01&lt;00:10, 13.71it/s]Loss = 1.6973e-05, PNorm = 123.5092, GNorm = 0.0308, lr_0 = 1.0422e-04\nLoss = 1.6787e-05, PNorm = 123.5097, GNorm = 0.0232, lr_0 = 1.0419e-04\n\n\r 14%|█▍        | 24/167 [00:01&lt;00:10, 13.70it/s]Loss = 2.3433e-05, PNorm = 123.5101, GNorm = 0.0323, lr_0 = 1.0416e-04\nLoss = 1.5548e-04, PNorm = 123.5108, GNorm = 0.4204, lr_0 = 1.0413e-04\n\n\r 16%|█▌        | 26/167 [00:01&lt;00:10, 13.77it/s]Loss = 2.5965e-05, PNorm = 123.5115, GNorm = 0.0593, lr_0 = 1.0410e-04\nLoss = 1.3810e-06, PNorm = 123.5121, GNorm = 0.0044, lr_0 = 1.0407e-04\n\n\r 17%|█▋        | 28/167 [00:02&lt;00:10, 13.76it/s]Loss = 4.3047e-05, PNorm = 123.5128, GNorm = 0.1242, lr_0 = 1.0404e-04\nLoss = 8.3970e-05, PNorm = 123.5135, GNorm = 0.1086, lr_0 = 1.0401e-04\n\n\r 18%|█▊        | 30/167 [00:02&lt;00:09, 13.81it/s]Loss = 1.3044e-06, PNorm = 123.5141, GNorm = 0.0029, lr_0 = 1.0398e-04\nLoss = 2.4027e-04, PNorm = 123.5147, GNorm = 0.1317, lr_0 = 1.0395e-04\n\n\r 19%|█▉        | 32/167 [00:02&lt;00:09, 13.64it/s]Loss = 8.1813e-06, PNorm = 123.5153, GNorm = 0.0078, lr_0 = 1.0392e-04\nLoss = 4.0204e-06, PNorm = 123.5159, GNorm = 0.0074, lr_0 = 1.0389e-04\n\n\r 20%|██        | 34/167 [00:02&lt;00:09, 13.76it/s]Loss = 1.6050e-05, PNorm = 123.5165, GNorm = 0.0481, lr_0 = 1.0386e-04\nLoss = 1.2381e-06, PNorm = 123.5170, GNorm = 0.0016, lr_0 = 1.0383e-04\n\n\r 22%|██▏       | 36/167 [00:02&lt;00:09, 13.74it/s]Loss = 4.9171e-04, PNorm = 123.5175, GNorm = 0.2321, lr_0 = 1.0380e-04\nLoss = 3.3477e-06, PNorm = 123.5181, GNorm = 0.0037, lr_0 = 1.0378e-04\n\n\r 23%|██▎       | 38/167 [00:02&lt;00:09, 13.94it/s]Loss = 7.5587e-06, PNorm = 123.5186, GNorm = 0.0210, lr_0 = 1.0375e-04\nLoss = 8.4232e-06, PNorm = 123.5190, GNorm = 0.0160, lr_0 = 1.0372e-04\n\n\r 24%|██▍       | 40/167 [00:02&lt;00:09, 14.05it/s]Loss = 4.1445e-06, PNorm = 123.5195, GNorm = 0.0105, lr_0 = 1.0369e-04\nLoss = 1.1034e-05, PNorm = 123.5198, GNorm = 0.0462, lr_0 = 1.0366e-04\n\n\r 25%|██▌       | 42/167 [00:03&lt;00:08, 14.03it/s]Loss = 2.2504e-05, PNorm = 123.5202, GNorm = 0.0365, lr_0 = 1.0363e-04\nLoss = 7.2973e-06, PNorm = 123.5206, GNorm = 0.0064, lr_0 = 1.0360e-04\n\n\r 26%|██▋       | 44/167 [00:03&lt;00:08, 14.05it/s]Loss = 1.6986e-05, PNorm = 123.5209, GNorm = 0.0343, lr_0 = 1.0357e-04\nLoss = 6.1364e-05, PNorm = 123.5214, GNorm = 0.0912, lr_0 = 1.0354e-04\n\n\r 28%|██▊       | 46/167 [00:03&lt;00:08, 13.90it/s]Loss = 9.5234e-07, PNorm = 123.5218, GNorm = 0.0031, lr_0 = 1.0351e-04\nLoss = 1.8251e-05, PNorm = 123.5222, GNorm = 0.0467, lr_0 = 1.0348e-04\n\n\r 29%|██▊       | 48/167 [00:03&lt;00:08, 13.89it/s]Loss = 1.6301e-05, PNorm = 123.5225, GNorm = 0.0462, lr_0 = 1.0345e-04\nLoss = 1.1873e-05, PNorm = 123.5229, GNorm = 0.0253, lr_0 = 1.0342e-04\n\n\r 30%|██▉       | 50/167 [00:03&lt;00:08, 13.85it/s]Loss = 4.9356e-06, PNorm = 123.5232, GNorm = 0.0101, lr_0 = 1.0339e-04\nLoss = 3.7020e-05, PNorm = 123.5235, GNorm = 0.0343, lr_0 = 1.0336e-04\n\n\r 31%|███       | 52/167 [00:03&lt;00:08, 13.84it/s]Loss = 2.2155e-06, PNorm = 123.5238, GNorm = 0.0047, lr_0 = 1.0333e-04\nLoss = 2.4394e-05, PNorm = 123.5241, GNorm = 0.0344, lr_0 = 1.0330e-04\n\n\r 32%|███▏      | 54/167 [00:03&lt;00:08, 13.93it/s]Loss = 2.9891e-05, PNorm = 123.5243, GNorm = 0.0695, lr_0 = 1.0327e-04\nLoss = 6.6359e-05, PNorm = 123.5247, GNorm = 0.0957, lr_0 = 1.0324e-04\n\n\r 34%|███▎      | 56/167 [00:04&lt;00:08, 13.85it/s]Loss = 7.6197e-06, PNorm = 123.5250, GNorm = 0.0102, lr_0 = 1.0321e-04\nLoss = 2.9564e-05, PNorm = 123.5253, GNorm = 0.0462, lr_0 = 1.0318e-04\n\n\r 35%|███▍      | 58/167 [00:04&lt;00:07, 13.94it/s]Loss = 3.4276e-05, PNorm = 123.5257, GNorm = 0.0630, lr_0 = 1.0315e-04\nLoss = 2.1706e-04, PNorm = 123.5261, GNorm = 0.1560, lr_0 = 1.0312e-04\n\n\r 36%|███▌      | 60/167 [00:04&lt;00:07, 13.83it/s]Loss = 2.1018e-06, PNorm = 123.5266, GNorm = 0.0031, lr_0 = 1.0309e-04\nLoss = 6.4504e-06, PNorm = 123.5269, GNorm = 0.0081, lr_0 = 1.0306e-04\n\n\r 37%|███▋      | 62/167 [00:04&lt;00:07, 13.95it/s]Loss = 2.4432e-04, PNorm = 123.5272, GNorm = 0.1689, lr_0 = 1.0303e-04\nLoss = 1.9103e-05, PNorm = 123.5275, GNorm = 0.0308, lr_0 = 1.0300e-04\n\n\r 38%|███▊      | 64/167 [00:04&lt;00:07, 13.83it/s]Loss = 2.9220e-05, PNorm = 123.5278, GNorm = 0.1112, lr_0 = 1.0297e-04\nLoss = 6.5382e-06, PNorm = 123.5281, GNorm = 0.0108, lr_0 = 1.0294e-04\n\n\r 40%|███▉      | 66/167 [00:04&lt;00:07, 13.74it/s]Loss = 1.1213e-05, PNorm = 123.5284, GNorm = 0.0261, lr_0 = 1.0291e-04\nLoss = 1.0890e-05, PNorm = 123.5287, GNorm = 0.0202, lr_0 = 1.0288e-04\n\n\r 41%|████      | 68/167 [00:04&lt;00:07, 13.70it/s]Loss = 8.2438e-06, PNorm = 123.5290, GNorm = 0.0117, lr_0 = 1.0286e-04\nLoss = 1.7431e-05, PNorm = 123.5293, GNorm = 0.0505, lr_0 = 1.0283e-04\n\n\r 42%|████▏     | 70/167 [00:05&lt;00:07, 13.67it/s]Loss = 2.1802e-06, PNorm = 123.5295, GNorm = 0.0050, lr_0 = 1.0280e-04\nLoss = 6.6142e-06, PNorm = 123.5298, GNorm = 0.0104, lr_0 = 1.0277e-04\n\n\r 43%|████▎     | 72/167 [00:05&lt;00:06, 13.77it/s]Loss = 1.6297e-06, PNorm = 123.5301, GNorm = 0.0040, lr_0 = 1.0274e-04\nLoss = 4.1666e-06, PNorm = 123.5303, GNorm = 0.0065, lr_0 = 1.0271e-04\n\n\r 44%|████▍     | 74/167 [00:05&lt;00:06, 13.57it/s]Loss = 4.0009e-05, PNorm = 123.5305, GNorm = 0.1259, lr_0 = 1.0268e-04\nLoss = 1.0201e-05, PNorm = 123.5307, GNorm = 0.0206, lr_0 = 1.0265e-04\n\n\r 46%|████▌     | 76/167 [00:05&lt;00:06, 13.69it/s]Loss = 8.8163e-06, PNorm = 123.5309, GNorm = 0.0206, lr_0 = 1.0262e-04\nLoss = 1.3678e-05, PNorm = 123.5311, GNorm = 0.0300, lr_0 = 1.0259e-04\n\n\r 47%|████▋     | 78/167 [00:05&lt;00:06, 13.84it/s]Loss = 2.4254e-06, PNorm = 123.5313, GNorm = 0.0044, lr_0 = 1.0256e-04\nLoss = 5.9720e-06, PNorm = 123.5315, GNorm = 0.0127, lr_0 = 1.0253e-04\n\n\r 48%|████▊     | 80/167 [00:05&lt;00:06, 13.75it/s]Loss = 2.4683e-06, PNorm = 123.5317, GNorm = 0.0036, lr_0 = 1.0250e-04\nLoss = 3.3325e-06, PNorm = 123.5319, GNorm = 0.0068, lr_0 = 1.0247e-04\n\n\r 49%|████▉     | 82/167 [00:05&lt;00:06, 13.70it/s]Loss = 3.7937e-06, PNorm = 123.5320, GNorm = 0.0042, lr_0 = 1.0244e-04\nLoss = 2.1240e-05, PNorm = 123.5322, GNorm = 0.0357, lr_0 = 1.0241e-04\n\n\r 50%|█████     | 84/167 [00:06&lt;00:06, 13.75it/s]Loss = 5.3115e-04, PNorm = 123.5322, GNorm = 0.2935, lr_0 = 1.0238e-04\nLoss = 1.5263e-04, PNorm = 123.5323, GNorm = 0.0926, lr_0 = 1.0235e-04\n\n\r 51%|█████▏    | 86/167 [00:06&lt;00:05, 13.80it/s]Loss = 9.3907e-05, PNorm = 123.5325, GNorm = 0.2078, lr_0 = 1.0232e-04\nLoss = 6.5178e-06, PNorm = 123.5327, GNorm = 0.0065, lr_0 = 1.0230e-04\n\n\r 53%|█████▎    | 88/167 [00:06&lt;00:05, 13.90it/s]Loss = 6.2373e-05, PNorm = 123.5329, GNorm = 0.0897, lr_0 = 1.0227e-04\nLoss = 5.3971e-06, PNorm = 123.5331, GNorm = 0.0114, lr_0 = 1.0224e-04\n\n\r 54%|█████▍    | 90/167 [00:06&lt;00:05, 13.94it/s]Loss = 9.4946e-06, PNorm = 123.5333, GNorm = 0.0125, lr_0 = 1.0221e-04\nLoss = 1.0892e-06, PNorm = 123.5335, GNorm = 0.0024, lr_0 = 1.0218e-04\n\n\r 55%|█████▌    | 92/167 [00:06&lt;00:05, 13.73it/s]Loss = 2.5374e-06, PNorm = 123.5337, GNorm = 0.0050, lr_0 = 1.0215e-04\nLoss = 1.4503e-05, PNorm = 123.5339, GNorm = 0.0457, lr_0 = 1.0212e-04\n\n\r 56%|█████▋    | 94/167 [00:06&lt;00:05, 13.97it/s]Loss = 2.3144e-04, PNorm = 123.5342, GNorm = 0.1447, lr_0 = 1.0209e-04\nLoss = 2.4980e-05, PNorm = 123.5345, GNorm = 0.0205, lr_0 = 1.0206e-04\n\n\r 57%|█████▋    | 96/167 [00:06&lt;00:05, 13.93it/s]Loss = 6.5682e-06, PNorm = 123.5349, GNorm = 0.0149, lr_0 = 1.0203e-04\nLoss = 6.2060e-06, PNorm = 123.5352, GNorm = 0.0082, lr_0 = 1.0200e-04\n\n\r 59%|█████▊    | 98/167 [00:07&lt;00:04, 13.93it/s]Loss = 6.5434e-06, PNorm = 123.5355, GNorm = 0.0130, lr_0 = 1.0197e-04\nLoss = 1.1904e-05, PNorm = 123.5358, GNorm = 0.0285, lr_0 = 1.0194e-04\n\n\r 60%|█████▉    | 100/167 [00:07&lt;00:04, 13.92it/s]Loss = 7.3343e-06, PNorm = 123.5360, GNorm = 0.0156, lr_0 = 1.0191e-04\nLoss = 4.8992e-04, PNorm = 123.5365, GNorm = 0.2097, lr_0 = 1.0188e-04\n\n\r 61%|██████    | 102/167 [00:07&lt;00:04, 14.06it/s]Loss = 5.3815e-04, PNorm = 123.5370, GNorm = 0.3814, lr_0 = 1.0186e-04\nLoss = 1.6758e-04, PNorm = 123.5374, GNorm = 0.1193, lr_0 = 1.0183e-04\n\n\r 62%|██████▏   | 104/167 [00:07&lt;00:04, 13.85it/s]Loss = 7.4303e-06, PNorm = 123.5379, GNorm = 0.0279, lr_0 = 1.0180e-04\nLoss = 3.0527e-06, PNorm = 123.5383, GNorm = 0.0060, lr_0 = 1.0177e-04\n\n\r 63%|██████▎   | 106/167 [00:07&lt;00:04, 13.95it/s]Loss = 3.5255e-06, PNorm = 123.5387, GNorm = 0.0061, lr_0 = 1.0174e-04\nLoss = 3.4248e-06, PNorm = 123.5391, GNorm = 0.0123, lr_0 = 1.0171e-04\n\n\r 65%|██████▍   | 108/167 [00:07&lt;00:04, 13.96it/s]Loss = 6.1692e-05, PNorm = 123.5397, GNorm = 0.0995, lr_0 = 1.0168e-04\nLoss = 3.2987e-06, PNorm = 123.5402, GNorm = 0.0068, lr_0 = 1.0165e-04\n\n\r 66%|██████▌   | 110/167 [00:07&lt;00:04, 13.97it/s]Loss = 2.9488e-05, PNorm = 123.5407, GNorm = 0.0657, lr_0 = 1.0162e-04\nLoss = 3.6064e-07, PNorm = 123.5411, GNorm = 0.0009, lr_0 = 1.0159e-04\n\n\r 67%|██████▋   | 112/167 [00:08&lt;00:03, 13.85it/s]Loss = 6.2318e-06, PNorm = 123.5415, GNorm = 0.0136, lr_0 = 1.0156e-04\nLoss = 2.4000e-05, PNorm = 123.5420, GNorm = 0.0504, lr_0 = 1.0153e-04\n\n\r 68%|██████▊   | 114/167 [00:08&lt;00:03, 13.82it/s]Loss = 3.5579e-06, PNorm = 123.5424, GNorm = 0.0172, lr_0 = 1.0150e-04\nLoss = 4.2712e-06, PNorm = 123.5427, GNorm = 0.0060, lr_0 = 1.0148e-04\n\n\r 69%|██████▉   | 116/167 [00:08&lt;00:03, 13.79it/s]Loss = 1.5604e-05, PNorm = 123.5431, GNorm = 0.0285, lr_0 = 1.0145e-04\nLoss = 5.4351e-06, PNorm = 123.5434, GNorm = 0.0071, lr_0 = 1.0142e-04\n\n\r 71%|███████   | 118/167 [00:08&lt;00:03, 13.83it/s]Loss = 2.1889e-05, PNorm = 123.5438, GNorm = 0.0354, lr_0 = 1.0139e-04\nLoss = 1.0089e-03, PNorm = 123.5441, GNorm = 0.5019, lr_0 = 1.0136e-04\n\n\r 72%|███████▏  | 120/167 [00:08&lt;00:03, 13.96it/s]Loss = 8.2656e-06, PNorm = 123.5445, GNorm = 0.0122, lr_0 = 1.0133e-04\nLoss = 3.7733e-04, PNorm = 123.5447, GNorm = 0.1983, lr_0 = 1.0130e-04\n\n\r 73%|███████▎  | 122/167 [00:08&lt;00:03, 13.93it/s]Loss = 8.5021e-07, PNorm = 123.5450, GNorm = 0.0023, lr_0 = 1.0127e-04\nLoss = 1.7219e-06, PNorm = 123.5452, GNorm = 0.0026, lr_0 = 1.0124e-04\n\n\r 74%|███████▍  | 124/167 [00:08&lt;00:03, 13.99it/s]Loss = 1.2961e-05, PNorm = 123.5455, GNorm = 0.0189, lr_0 = 1.0121e-04\nLoss = 4.3602e-06, PNorm = 123.5457, GNorm = 0.0108, lr_0 = 1.0118e-04\n\n\r 75%|███████▌  | 126/167 [00:09&lt;00:02, 13.94it/s]Loss = 9.6690e-05, PNorm = 123.5460, GNorm = 0.0984, lr_0 = 1.0116e-04\nLoss = 2.4065e-06, PNorm = 123.5462, GNorm = 0.0060, lr_0 = 1.0113e-04\n\n\r 77%|███████▋  | 128/167 [00:09&lt;00:02, 13.86it/s]Loss = 1.8874e-06, PNorm = 123.5465, GNorm = 0.0032, lr_0 = 1.0110e-04\nLoss = 2.8061e-05, PNorm = 123.5468, GNorm = 0.0345, lr_0 = 1.0107e-04\n\n\r 78%|███████▊  | 130/167 [00:09&lt;00:02, 13.76it/s]Loss = 4.3721e-06, PNorm = 123.5470, GNorm = 0.0102, lr_0 = 1.0104e-04\nLoss = 6.2016e-06, PNorm = 123.5473, GNorm = 0.0081, lr_0 = 1.0101e-04\n\n\r 79%|███████▉  | 132/167 [00:09&lt;00:02, 13.74it/s]Loss = 9.4124e-06, PNorm = 123.5475, GNorm = 0.0150, lr_0 = 1.0098e-04\nLoss = 2.1184e-06, PNorm = 123.5477, GNorm = 0.0033, lr_0 = 1.0095e-04\n\n\r 80%|████████  | 134/167 [00:09&lt;00:02, 13.79it/s]Loss = 1.5210e-05, PNorm = 123.5480, GNorm = 0.0230, lr_0 = 1.0092e-04\nLoss = 3.3732e-06, PNorm = 123.5482, GNorm = 0.0036, lr_0 = 1.0089e-04\n\n\r 81%|████████▏ | 136/167 [00:09&lt;00:02, 13.85it/s]Loss = 1.2084e-05, PNorm = 123.5484, GNorm = 0.0259, lr_0 = 1.0087e-04\nLoss = 7.6386e-06, PNorm = 123.5487, GNorm = 0.0079, lr_0 = 1.0084e-04\n\n\r 83%|████████▎ | 138/167 [00:09&lt;00:02, 13.94it/s]Loss = 1.7283e-05, PNorm = 123.5489, GNorm = 0.0361, lr_0 = 1.0081e-04\nLoss = 3.0198e-05, PNorm = 123.5492, GNorm = 0.0422, lr_0 = 1.0078e-04\n\n\r 84%|████████▍ | 140/167 [00:10&lt;00:01, 13.79it/s]Loss = 1.2177e-05, PNorm = 123.5494, GNorm = 0.0256, lr_0 = 1.0075e-04\nLoss = 6.9264e-05, PNorm = 123.5497, GNorm = 0.0559, lr_0 = 1.0072e-04\n\n\r 85%|████████▌ | 142/167 [00:10&lt;00:01, 13.82it/s]Loss = 3.3721e-04, PNorm = 123.5498, GNorm = 0.1767, lr_0 = 1.0069e-04\nLoss = 1.4384e-06, PNorm = 123.5498, GNorm = 0.0023, lr_0 = 1.0066e-04\n\n\r 86%|████████▌ | 144/167 [00:10&lt;00:01, 13.95it/s]Loss = 4.3529e-05, PNorm = 123.5500, GNorm = 0.0648, lr_0 = 1.0063e-04\nLoss = 7.6424e-06, PNorm = 123.5502, GNorm = 0.0217, lr_0 = 1.0061e-04\n\n\r 87%|████████▋ | 146/167 [00:10&lt;00:01, 14.00it/s]Loss = 8.9360e-06, PNorm = 123.5504, GNorm = 0.0121, lr_0 = 1.0058e-04\nLoss = 8.2264e-04, PNorm = 123.5506, GNorm = 0.3311, lr_0 = 1.0055e-04\n\n\r 89%|████████▊ | 148/167 [00:10&lt;00:01, 14.11it/s]Loss = 1.6975e-05, PNorm = 123.5509, GNorm = 0.0284, lr_0 = 1.0052e-04\nLoss = 6.1486e-06, PNorm = 123.5512, GNorm = 0.0139, lr_0 = 1.0049e-04\n\n\r 90%|████████▉ | 150/167 [00:10&lt;00:01, 14.14it/s]Loss = 8.4949e-05, PNorm = 123.5514, GNorm = 0.0849, lr_0 = 1.0046e-04\nLoss = 5.5849e-05, PNorm = 123.5517, GNorm = 0.0306, lr_0 = 1.0043e-04\n\n\r 91%|█████████ | 152/167 [00:10&lt;00:01, 14.08it/s]Loss = 1.8206e-05, PNorm = 123.5520, GNorm = 0.0379, lr_0 = 1.0040e-04\nLoss = 8.9881e-05, PNorm = 123.5524, GNorm = 0.1339, lr_0 = 1.0037e-04\n\n\r 92%|█████████▏| 154/167 [00:11&lt;00:00, 14.10it/s]Loss = 2.1141e-05, PNorm = 123.5527, GNorm = 0.0359, lr_0 = 1.0035e-04\nLoss = 2.7397e-05, PNorm = 123.5531, GNorm = 0.0235, lr_0 = 1.0032e-04\n\n\r 93%|█████████▎| 156/167 [00:11&lt;00:00, 14.00it/s]Loss = 5.6894e-05, PNorm = 123.5537, GNorm = 0.0965, lr_0 = 1.0029e-04\nLoss = 1.3144e-06, PNorm = 123.5541, GNorm = 0.0022, lr_0 = 1.0026e-04\n\n\r 95%|█████████▍| 158/167 [00:11&lt;00:00, 13.99it/s]Loss = 3.5622e-06, PNorm = 123.5546, GNorm = 0.0041, lr_0 = 1.0023e-04\nLoss = 1.8432e-05, PNorm = 123.5550, GNorm = 0.0731, lr_0 = 1.0020e-04\n\n\r 96%|█████████▌| 160/167 [00:11&lt;00:00, 13.91it/s]Loss = 3.2301e-06, PNorm = 123.5554, GNorm = 0.0080, lr_0 = 1.0017e-04\nLoss = 4.0128e-07, PNorm = 123.5557, GNorm = 0.0008, lr_0 = 1.0014e-04\n\n\r 97%|█████████▋| 162/167 [00:11&lt;00:00, 13.89it/s]Loss = 8.8521e-05, PNorm = 123.5561, GNorm = 0.1379, lr_0 = 1.0011e-04\nLoss = 2.7780e-05, PNorm = 123.5566, GNorm = 0.0395, lr_0 = 1.0009e-04\n\n\r 98%|█████████▊| 164/167 [00:11&lt;00:00, 13.89it/s]Loss = 2.6152e-05, PNorm = 123.5571, GNorm = 0.0580, lr_0 = 1.0006e-04\nLoss = 5.0387e-04, PNorm = 123.5576, GNorm = 0.1709, lr_0 = 1.0003e-04\n\n\r 99%|█████████▉| 166/167 [00:11&lt;00:00, 13.78it/s]Loss = 6.3214e-05, PNorm = 123.5581, GNorm = 0.2247, lr_0 = 1.0000e-04\n\n\r100%|██████████| 167/167 [00:12&lt;00:00, 13.87it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 46.60it/s]Validation auc = 0.905501\n\r100%|██████████| 50/50 [11:22&lt;00:00, 12.34s/it]\nModel 0 best validation auc = 0.915431 on epoch 26\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\r 25%|██▌       | 1/4 [00:00&lt;00:00,  9.86it/s]\r 50%|█████     | 2/4 [00:01&lt;00:00,  2.81it/s]\r 75%|███████▌  | 3/4 [00:01&lt;00:00,  3.58it/s]\r100%|██████████| 4/4 [00:01&lt;00:00,  3.28it/s]\nModel 0 test auc = 0.889156\nEnsemble test auc = 0.889156\n1-fold cross validation\nSeed 13 ==&gt; test auc = 0.889156\nOverall test auc = 0.889156 +/- 0.000000\nOut[5]: (0.8891555746591703, 0.0)\n</div>"]}}],"execution_count":47},{"cell_type":"code","source":["#LEO + PubChem 0.8726888202188564\nparser = ArgumentParser()\nadd_train_args(parser)\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-8396_bin76.csv'),\n                          '--dataset_type','classification',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext-50e'),\n                          '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'),\n                          '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                          '--log_frequency','1',\n                          '--depth','5',\n                          '--dropout','0.05',\n                          '--hidden_size','1500',\n                          '--ffn_num_layers','2',\n                          '--epochs','50'\n                        #,'--atom_messages'\n                        #,'--ensemble_size','3'\n                        #,'--features_generator','rdkit_2d'\n                        # ,'--seed','13',\n                          ])\nmodify_train_args(args)\nlogger = create_logger(name='train', save_dir=args.save_dir, quiet=args.quiet)\n\ncross_validate(args, logger)\n#test auc = 0.889156 - 50epoch\n#test auc = 0.879814 - 30epoch\n#test auc = 0.873914 - 11epoch\n#best validation auc = 0.915431 on epoch 26\n#best validation auc = 0.938879 on epoch 11\n#best validation auc = 0.920827 on epoch 9\n#100e test auc = 0.868794"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-8396_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 5,\n &#39;dropout&#39;: 0.05,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 50,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 1500,\n &#39;ffn_num_layers&#39;: 2,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1500,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_ext-50e/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\n\r  0%|          | 0/8396 [00:00&lt;?, ?it/s]\r  3%|▎         | 279/8396 [00:00&lt;00:02, 2785.10it/s]\r  7%|▋         | 580/8396 [00:00&lt;00:02, 2846.85it/s]\r 10%|█         | 880/8396 [00:00&lt;00:02, 2889.87it/s]\r 14%|█▍        | 1172/8396 [00:00&lt;00:02, 2897.27it/s]\r 18%|█▊        | 1474/8396 [00:00&lt;00:02, 2931.27it/s]\r 21%|██        | 1758/8396 [00:00&lt;00:02, 2900.75it/s]\r 24%|██▍       | 2016/8396 [00:00&lt;00:02, 2722.05it/s]\r 27%|██▋       | 2268/8396 [00:00&lt;00:02, 2618.12it/s]\r 30%|███       | 2526/8396 [00:00&lt;00:02, 2604.70it/s]\r 34%|███▎      | 2815/8396 [00:01&lt;00:02, 2681.87it/s]\r 37%|███▋      | 3115/8396 [00:01&lt;00:01, 2767.35it/s]\r 40%|████      | 3389/8396 [00:01&lt;00:01, 2546.87it/s]\r 43%|████▎     | 3645/8396 [00:01&lt;00:01, 2436.67it/s]\r 47%|████▋     | 3946/8396 [00:01&lt;00:01, 2583.29it/s]\r 51%|█████     | 4258/8396 [00:01&lt;00:01, 2722.57it/s]\r 54%|█████▍    | 4556/8396 [00:01&lt;00:01, 2794.03it/s]\r 58%|█████▊    | 4869/8396 [00:01&lt;00:01, 2885.75it/s]\r 62%|██████▏   | 5175/8396 [00:01&lt;00:01, 2933.49it/s]\r 65%|██████▌   | 5487/8396 [00:01&lt;00:00, 2985.80it/s]\r 69%|██████▉   | 5789/8396 [00:02&lt;00:00, 2995.22it/s]\r 73%|███████▎  | 6090/8396 [00:02&lt;00:00, 2907.27it/s]\r 76%|███████▌  | 6383/8396 [00:02&lt;00:00, 2822.43it/s]\r 79%|███████▉  | 6667/8396 [00:02&lt;00:00, 2805.70it/s]\r 83%|████████▎ | 6958/8396 [00:02&lt;00:00, 2834.15it/s]\r 87%|████████▋ | 7265/8396 [00:02&lt;00:00, 2898.25it/s]\r 91%|█████████ | 7605/8396 [00:02&lt;00:00, 3029.63it/s]\r 95%|█████████▍| 7941/8396 [00:02&lt;00:00, 3121.47it/s]\r 98%|█████████▊| 8256/8396 [00:02&lt;00:00, 3010.74it/s]\r100%|██████████| 8396/8396 [00:02&lt;00:00, 2847.14it/s]\nNumber of tasks = 4\nSplitting data with seed 0\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 2991.70it/s]\n\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\r100%|██████████| 182/182 [00:00&lt;00:00, 2940.11it/s]\nClass sizes\nJAK1 0: 33.55%, 1: 66.45%\nJAK2 0: 60.19%, 1: 39.81%\nJAK3 0: 77.42%, 1: 22.58%\nTYK2 0: 92.62%, 1: 7.38%\nTotal size = 8,396 | train size = 8,396 | val size = 182 | test size = 183\nBuilding model 0\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.05)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1500, bias=False)\n      (W_h): Linear(in_features=1500, out_features=1500, bias=False)\n      (W_o): Linear(in_features=1633, out_features=1500, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.05)\n    (1): Linear(in_features=1500, out_features=1500, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.05)\n    (4): Linear(in_features=1500, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 7,179,004\nMoving model to cuda\n\r  0%|          | 0/50 [00:00&lt;?, ?it/s]Epoch 0\n\n\r  0%|          | 0/167 [00:00&lt;?, ?it/s]Loss = 1.5320e-02, PNorm = 69.5640, GNorm = 2.5873, lr_0 = 1.0269e-04\n\n\r  1%|          | 1/167 [00:00&lt;00:27,  6.08it/s]Loss = 1.1693e-02, PNorm = 69.5648, GNorm = 1.5199, lr_0 = 1.0539e-04\n\n\r  1%|          | 2/167 [00:00&lt;00:26,  6.17it/s]Loss = 1.2347e-02, PNorm = 69.5656, GNorm = 1.0565, lr_0 = 1.0808e-04\n\n\r  2%|▏         | 3/167 [00:00&lt;00:26,  6.12it/s]Loss = 1.1802e-02, PNorm = 69.5662, GNorm = 1.1681, lr_0 = 1.1078e-04\n\n\r  2%|▏         | 4/167 [00:00&lt;00:26,  6.07it/s]Loss = 1.3037e-02, PNorm = 69.5667, GNorm = 1.6037, lr_0 = 1.1347e-04\n\n\r  3%|▎         | 5/167 [00:00&lt;00:26,  6.17it/s]Loss = 1.1124e-02, PNorm = 69.5672, GNorm = 1.2579, lr_0 = 1.1617e-04\n\n\r  4%|▎         | 6/167 [00:01&lt;00:30,  5.34it/s]Loss = 1.1464e-02, PNorm = 69.5677, GNorm = 0.9691, lr_0 = 1.1886e-04\n\n\r  4%|▍         | 7/167 [00:01&lt;00:28,  5.53it/s]Loss = 1.2507e-02, PNorm = 69.5682, GNorm = 0.6799, lr_0 = 1.2156e-04\n\n\r  5%|▍         | 8/167 [00:01&lt;00:27,  5.70it/s]Loss = 1.2338e-02, PNorm = 69.5687, GNorm = 1.2465, lr_0 = 1.2425e-04\n\n\r  5%|▌         | 9/167 [00:01&lt;00:27,  5.76it/s]Loss = 1.3073e-02, PNorm = 69.5691, GNorm = 1.0896, lr_0 = 1.2695e-04\n\n\r  6%|▌         | 10/167 [00:01&lt;00:26,  5.97it/s]Loss = 1.2530e-02, PNorm = 69.5696, GNorm = 0.7612, lr_0 = 1.2964e-04\n\n\r  7%|▋         | 11/167 [00:01&lt;00:25,  6.10it/s]Loss = 1.1262e-02, PNorm = 69.5701, GNorm = 0.5256, lr_0 = 1.3234e-04\n\n\r  7%|▋         | 12/167 [00:02&lt;00:26,  5.95it/s]Loss = 1.2086e-02, PNorm = 69.5707, GNorm = 1.0444, lr_0 = 1.3503e-04\n\n\r  8%|▊         | 13/167 [00:02&lt;00:25,  5.99it/s]Loss = 1.1941e-02, PNorm = 69.5714, GNorm = 0.3992, lr_0 = 1.3772e-04\n\n\r  8%|▊         | 14/167 [00:02&lt;00:26,  5.82it/s]Loss = 1.0827e-02, PNorm = 69.5721, GNorm = 0.8368, lr_0 = 1.4042e-04\n\n\r  9%|▉         | 15/167 [00:02&lt;00:33,  4.59it/s]Loss = 1.2285e-02, PNorm = 69.5728, GNorm = 0.4991, lr_0 = 1.4311e-04\n\n\r 10%|▉         | 16/167 [00:02&lt;00:31,  4.86it/s]Loss = 1.0704e-02, PNorm = 69.5735, GNorm = 0.4651, lr_0 = 1.4581e-04\n\n\r 10%|█         | 17/167 [00:03&lt;00:29,  5.05it/s]Loss = 1.2029e-02, PNorm = 69.5743, GNorm = 0.5420, lr_0 = 1.4850e-04\n\n\r 11%|█         | 18/167 [00:03&lt;00:28,  5.24it/s]Loss = 1.1855e-02, PNorm = 69.5750, GNorm = 0.7492, lr_0 = 1.5120e-04\n\n\r 11%|█▏        | 19/167 [00:03&lt;00:27,  5.34it/s]Loss = 1.2671e-02, PNorm = 69.5757, GNorm = 0.5399, lr_0 = 1.5389e-04\n\n\r 12%|█▏        | 20/167 [00:03&lt;00:26,  5.46it/s]Loss = 1.2247e-02, PNorm = 69.5764, GNorm = 0.4228, lr_0 = 1.5659e-04\n\n\r 13%|█▎        | 21/167 [00:03&lt;00:26,  5.51it/s]Loss = 1.0686e-02, PNorm = 69.5773, GNorm = 0.3807, lr_0 = 1.5928e-04\n\n\r 13%|█▎        | 22/167 [00:03&lt;00:26,  5.51it/s]Loss = 1.0177e-02, PNorm = 69.5782, GNorm = 0.3909, lr_0 = 1.6198e-04\n\n\r 14%|█▍        | 23/167 [00:04&lt;00:25,  5.61it/s]Loss = 1.0971e-02, PNorm = 69.5793, GNorm = 0.3537, lr_0 = 1.6467e-04\n\n\r 14%|█▍        | 24/167 [00:04&lt;00:25,  5.64it/s]Loss = 1.0578e-02, PNorm = 69.5804, GNorm = 0.4875, lr_0 = 1.6737e-04\n\n\r 15%|█▍        | 25/167 [00:04&lt;00:25,  5.64it/s]Loss = 1.1966e-02, PNorm = 69.5815, GNorm = 0.7926, lr_0 = 1.7006e-04\n\n\r 16%|█▌        | 26/167 [00:04&lt;00:33,  4.17it/s]Loss = 1.0905e-02, PNorm = 69.5827, GNorm = 0.2703, lr_0 = 1.7275e-04\n\n\r 16%|█▌        | 27/167 [00:05&lt;00:30,  4.56it/s]Loss = 1.1332e-02, PNorm = 69.5839, GNorm = 0.3413, lr_0 = 1.7545e-04\n\n\r 17%|█▋        | 28/167 [00:05&lt;00:28,  4.90it/s]Loss = 1.0720e-02, PNorm = 69.5852, GNorm = 0.4382, lr_0 = 1.7814e-04\n\n\r 17%|█▋        | 29/167 [00:05&lt;00:26,  5.15it/s]Loss = 1.0723e-02, PNorm = 69.5865, GNorm = 0.4598, lr_0 = 1.8084e-04\n\n\r 18%|█▊        | 30/167 [00:05&lt;00:27,  5.04it/s]Loss = 1.1489e-02, PNorm = 69.5876, GNorm = 0.7015, lr_0 = 1.8353e-04\n\n\r 19%|█▊        | 31/167 [00:05&lt;00:25,  5.27it/s]Loss = 1.4096e-02, PNorm = 69.5884, GNorm = 0.8917, lr_0 = 1.8623e-04\n\n\r 19%|█▉        | 32/167 [00:05&lt;00:25,  5.39it/s]Loss = 1.0588e-02, PNorm = 69.5893, GNorm = 0.7107, lr_0 = 1.8892e-04\n\n\r 20%|█▉        | 33/167 [00:06&lt;00:24,  5.46it/s]Loss = 1.2686e-02, PNorm = 69.5903, GNorm = 0.5984, lr_0 = 1.9162e-04\n\n\r 20%|██        | 34/167 [00:06&lt;00:24,  5.52it/s]Loss = 1.1436e-02, PNorm = 69.5915, GNorm = 0.2908, lr_0 = 1.9431e-04\n\n\r 21%|██        | 35/167 [00:06&lt;00:23,  5.55it/s]Loss = 1.1989e-02, PNorm = 69.5927, GNorm = 0.5277, lr_0 = 1.9701e-04\n\n\r 22%|██▏       | 36/167 [00:06&lt;00:23,  5.66it/s]Loss = 1.1487e-02, PNorm = 69.5939, GNorm = 0.2981, lr_0 = 1.9970e-04\n\n\r 22%|██▏       | 37/167 [00:06&lt;00:22,  5.70it/s]Loss = 1.1382e-02, PNorm = 69.5952, GNorm = 0.4499, lr_0 = 2.0240e-04\n\n\r 23%|██▎       | 38/167 [00:06&lt;00:22,  5.75it/s]Loss = 1.1388e-02, PNorm = 69.5967, GNorm = 0.4037, lr_0 = 2.0509e-04\n\n\r 23%|██▎       | 39/167 [00:07&lt;00:22,  5.68it/s]Loss = 1.0639e-02, PNorm = 69.5983, GNorm = 0.4371, lr_0 = 2.0778e-04\n\n\r 24%|██▍       | 40/167 [00:07&lt;00:32,  3.95it/s]Loss = 1.2073e-02, PNorm = 69.6001, GNorm = 0.7746, lr_0 = 2.1048e-04\n\n\r 25%|██▍       | 41/167 [00:07&lt;00:29,  4.34it/s]Loss = 1.1720e-02, PNorm = 69.6019, GNorm = 0.3766, lr_0 = 2.1317e-04\n\n\r 25%|██▌       | 42/167 [00:07&lt;00:27,  4.61it/s]Loss = 9.0665e-03, PNorm = 69.6041, GNorm = 0.4823, lr_0 = 2.1587e-04\n\n\r 26%|██▌       | 43/167 [00:08&lt;00:25,  4.89it/s]Loss = 1.0962e-02, PNorm = 69.6063, GNorm = 0.2981, lr_0 = 2.1856e-04\n\n\r 26%|██▋       | 44/167 [00:08&lt;00:23,  5.14it/s]Loss = 1.2027e-02, PNorm = 69.6085, GNorm = 0.4868, lr_0 = 2.2126e-04\n\n\r 27%|██▋       | 45/167 [00:08&lt;00:22,  5.36it/s]Loss = 1.3000e-02, PNorm = 69.6107, GNorm = 0.9261, lr_0 = 2.2395e-04\n\n\r 28%|██▊       | 46/167 [00:08&lt;00:22,  5.44it/s]Loss = 1.1420e-02, PNorm = 69.6125, GNorm = 0.8796, lr_0 = 2.2665e-04\n\n\r 28%|██▊       | 47/167 [00:08&lt;00:21,  5.52it/s]Loss = 1.4294e-02, PNorm = 69.6138, GNorm = 0.8172, lr_0 = 2.2934e-04\n\n\r 29%|██▊       | 48/167 [00:08&lt;00:21,  5.61it/s]Loss = 1.2494e-02, PNorm = 69.6154, GNorm = 0.5197, lr_0 = 2.3204e-04\n\n\r 29%|██▉       | 49/167 [00:09&lt;00:20,  5.63it/s]Loss = 1.1034e-02, PNorm = 69.6173, GNorm = 0.4060, lr_0 = 2.3473e-04\n\n\r 30%|██▉       | 50/167 [00:09&lt;00:20,  5.63it/s]Loss = 1.1034e-02, PNorm = 69.6196, GNorm = 0.4487, lr_0 = 2.3743e-04\n\n\r 31%|███       | 51/167 [00:09&lt;00:20,  5.65it/s]Loss = 1.1318e-02, PNorm = 69.6220, GNorm = 0.3625, lr_0 = 2.4012e-04\n\n\r 31%|███       | 52/167 [00:09&lt;00:20,  5.73it/s]Loss = 1.0843e-02, PNorm = 69.6250, GNorm = 0.7301, lr_0 = 2.4281e-04\n\n\r 32%|███▏      | 53/167 [00:09&lt;00:19,  5.73it/s]Loss = 1.0861e-02, PNorm = 69.6278, GNorm = 0.6288, lr_0 = 2.4551e-04\n\n\r 32%|███▏      | 54/167 [00:10&lt;00:20,  5.61it/s]Loss = 1.0862e-02, PNorm = 69.6306, GNorm = 0.4445, lr_0 = 2.4820e-04\n\n\r 33%|███▎      | 55/167 [00:10&lt;00:20,  5.60it/s]Loss = 1.0531e-02, PNorm = 69.6334, GNorm = 0.6992, lr_0 = 2.5090e-04\n\n\r 34%|███▎      | 56/167 [00:10&lt;00:19,  5.63it/s]Loss = 1.1206e-02, PNorm = 69.6365, GNorm = 0.3483, lr_0 = 2.5359e-04\n\n\r 34%|███▍      | 57/167 [00:10&lt;00:30,  3.62it/s]Loss = 1.2106e-02, PNorm = 69.6398, GNorm = 0.4116, lr_0 = 2.5629e-04\n\n\r 35%|███▍      | 58/167 [00:11&lt;00:26,  4.06it/s]Loss = 1.1077e-02, PNorm = 69.6434, GNorm = 0.3861, lr_0 = 2.5898e-04\n\n\r 35%|███▌      | 59/167 [00:11&lt;00:24,  4.42it/s]Loss = 1.0248e-02, PNorm = 69.6472, GNorm = 0.3114, lr_0 = 2.6168e-04\n\n\r 36%|███▌      | 60/167 [00:11&lt;00:22,  4.74it/s]Loss = 9.4180e-03, PNorm = 69.6513, GNorm = 0.3181, lr_0 = 2.6437e-04\n\n\r 37%|███▋      | 61/167 [00:11&lt;00:21,  4.96it/s]Loss = 1.2034e-02, PNorm = 69.6554, GNorm = 0.6131, lr_0 = 2.6707e-04\n\n\r 37%|███▋      | 62/167 [00:11&lt;00:20,  5.13it/s]Loss = 8.7980e-03, PNorm = 69.6601, GNorm = 0.5197, lr_0 = 2.6976e-04\n\n\r 38%|███▊      | 63/167 [00:11&lt;00:19,  5.28it/s]Loss = 1.0838e-02, PNorm = 69.6636, GNorm = 0.7581, lr_0 = 2.7246e-04\n\n\r 38%|███▊      | 64/167 [00:12&lt;00:19,  5.42it/s]Loss = 1.1628e-02, PNorm = 69.6659, GNorm = 1.7194, lr_0 = 2.7515e-04\n\n\r 39%|███▉      | 65/167 [00:12&lt;00:18,  5.47it/s]Loss = 8.1442e-03, PNorm = 69.6694, GNorm = 0.4875, lr_0 = 2.7784e-04\n\n\r 40%|███▉      | 66/167 [00:12&lt;00:18,  5.51it/s]Loss = 1.0409e-02, PNorm = 69.6736, GNorm = 1.2708, lr_0 = 2.8054e-04\n\n\r 40%|████      | 67/167 [00:12&lt;00:17,  5.58it/s]Loss = 1.2349e-02, PNorm = 69.6783, GNorm = 1.3956, lr_0 = 2.8323e-04\n\n\r 41%|████      | 68/167 [00:12&lt;00:17,  5.59it/s]Loss = 1.2082e-02, PNorm = 69.6825, GNorm = 0.9108, lr_0 = 2.8593e-04\n\n\r 41%|████▏     | 69/167 [00:13&lt;00:17,  5.60it/s]Loss = 1.0646e-02, PNorm = 69.6853, GNorm = 1.4233, lr_0 = 2.8862e-04\n\n\r 42%|████▏     | 70/167 [00:13&lt;00:17,  5.58it/s]Loss = 1.3319e-02, PNorm = 69.6873, GNorm = 0.7438, lr_0 = 2.9132e-04\n\n\r 43%|████▎     | 71/167 [00:13&lt;00:17,  5.45it/s]Loss = 1.1156e-02, PNorm = 69.6896, GNorm = 0.3723, lr_0 = 2.9401e-04\n\n\r 43%|████▎     | 72/167 [00:13&lt;00:17,  5.49it/s]Loss = 9.0112e-03, PNorm = 69.6924, GNorm = 0.4014, lr_0 = 2.9671e-04\n\n\r 44%|████▎     | 73/167 [00:13&lt;00:17,  5.52it/s]Loss = 1.0459e-02, PNorm = 69.6962, GNorm = 0.5689, lr_0 = 2.9940e-04\n\n\r 44%|████▍     | 74/167 [00:13&lt;00:16,  5.52it/s]Loss = 1.0294e-02, PNorm = 69.7001, GNorm = 0.3166, lr_0 = 3.0210e-04\n\n\r 45%|████▍     | 75/167 [00:14&lt;00:16,  5.61it/s]Loss = 9.3251e-03, PNorm = 69.7047, GNorm = 0.7606, lr_0 = 3.0479e-04\n\n\r 46%|████▌     | 76/167 [00:14&lt;00:16,  5.62it/s]Loss = 1.0378e-02, PNorm = 69.7095, GNorm = 0.2683, lr_0 = 3.0749e-04\n\n\r 46%|████▌     | 77/167 [00:14&lt;00:16,  5.62it/s]Loss = 9.6998e-03, PNorm = 69.7148, GNorm = 0.2990, lr_0 = 3.1018e-04\n\n\r 47%|████▋     | 78/167 [00:15&lt;00:27,  3.29it/s]Loss = 1.1192e-02, PNorm = 69.7200, GNorm = 0.4638, lr_0 = 3.1287e-04\n\n\r 47%|████▋     | 79/167 [00:15&lt;00:23,  3.75it/s]Loss = 1.1540e-02, PNorm = 69.7251, GNorm = 0.4065, lr_0 = 3.1557e-04\n\n\r 48%|████▊     | 80/167 [00:15&lt;00:21,  4.07it/s]Loss = 1.0722e-02, PNorm = 69.7299, GNorm = 0.5286, lr_0 = 3.1826e-04\n\n\r 49%|████▊     | 81/167 [00:15&lt;00:19,  4.43it/s]Loss = 1.0543e-02, PNorm = 69.7345, GNorm = 0.5935, lr_0 = 3.2096e-04\n\n\r 49%|████▉     | 82/167 [00:15&lt;00:18,  4.71it/s]Loss = 9.7629e-03, PNorm = 69.7392, GNorm = 0.3232, lr_0 = 3.2365e-04\n\n\r 50%|████▉     | 83/167 [00:15&lt;00:16,  4.98it/s]Loss = 1.1501e-02, PNorm = 69.7438, GNorm = 0.3089, lr_0 = 3.2635e-04\n\n\r 50%|█████     | 84/167 [00:16&lt;00:15,  5.22it/s]Loss = 1.0564e-02, PNorm = 69.7481, GNorm = 0.5237, lr_0 = 3.2904e-04\n\n\r 51%|█████     | 85/167 [00:16&lt;00:15,  5.30it/s]Loss = 9.5308e-03, PNorm = 69.7526, GNorm = 0.4095, lr_0 = 3.3174e-04\n\n\r 51%|█████▏    | 86/167 [00:16&lt;00:14,  5.41it/s]Loss = 8.7120e-03, PNorm = 69.7578, GNorm = 0.5465, lr_0 = 3.3443e-04\n\n\r 52%|█████▏    | 87/167 [00:16&lt;00:14,  5.55it/s]Loss = 8.7471e-03, PNorm = 69.7640, GNorm = 0.4211, lr_0 = 3.3713e-04\n\n\r 53%|█████▎    | 88/167 [00:16&lt;00:14,  5.62it/s]Loss = 1.0468e-02, PNorm = 69.7704, GNorm = 0.7276, lr_0 = 3.3982e-04\n\n\r 53%|█████▎    | 89/167 [00:17&lt;00:13,  5.65it/s]Loss = 9.7950e-03, PNorm = 69.7779, GNorm = 0.8572, lr_0 = 3.4251e-04\n\n\r 54%|█████▍    | 90/167 [00:17&lt;00:13,  5.69it/s]Loss = 1.1675e-02, PNorm = 69.7863, GNorm = 0.5914, lr_0 = 3.4521e-04\n\n\r 54%|█████▍    | 91/167 [00:17&lt;00:13,  5.59it/s]Loss = 1.0619e-02, PNorm = 69.7941, GNorm = 0.6233, lr_0 = 3.4790e-04\n\n\r 55%|█████▌    | 92/167 [00:17&lt;00:13,  5.65it/s]Loss = 1.0780e-02, PNorm = 69.8014, GNorm = 0.7096, lr_0 = 3.5060e-04\n\n\r 56%|█████▌    | 93/167 [00:17&lt;00:13,  5.65it/s]Loss = 9.4941e-03, PNorm = 69.8091, GNorm = 0.6051, lr_0 = 3.5329e-04\n\n\r 56%|█████▋    | 94/167 [00:17&lt;00:12,  5.69it/s]Loss = 1.0843e-02, PNorm = 69.8153, GNorm = 1.0051, lr_0 = 3.5599e-04\n\n\r 57%|█████▋    | 95/167 [00:18&lt;00:12,  5.72it/s]Loss = 1.1808e-02, PNorm = 69.8209, GNorm = 0.8012, lr_0 = 3.5868e-04\n\n\r 57%|█████▋    | 96/167 [00:18&lt;00:12,  5.72it/s]Loss = 9.8473e-03, PNorm = 69.8264, GNorm = 0.3983, lr_0 = 3.6138e-04\n\n\r 58%|█████▊    | 97/167 [00:18&lt;00:12,  5.73it/s]Loss = 1.0429e-02, PNorm = 69.8330, GNorm = 0.2304, lr_0 = 3.6407e-04\n\n\r 59%|█████▊    | 98/167 [00:18&lt;00:12,  5.54it/s]Loss = 1.0098e-02, PNorm = 69.8400, GNorm = 0.3979, lr_0 = 3.6677e-04\n\n\r 59%|█████▉    | 99/167 [00:18&lt;00:12,  5.56it/s]Loss = 9.8195e-03, PNorm = 69.8479, GNorm = 0.2698, lr_0 = 3.6946e-04\n\n\r 60%|█████▉    | 100/167 [00:18&lt;00:12,  5.48it/s]Loss = 1.0439e-02, PNorm = 69.8545, GNorm = 0.3735, lr_0 = 3.7216e-04\n\n\r 60%|██████    | 101/167 [00:19&lt;00:12,  5.47it/s]Loss = 1.0148e-02, PNorm = 69.8610, GNorm = 0.4139, lr_0 = 3.7485e-04\n\n\r 61%|██████    | 102/167 [00:19&lt;00:11,  5.54it/s]Loss = 1.0102e-02, PNorm = 69.8684, GNorm = 0.4263, lr_0 = 3.7754e-04\n\n\r 62%|██████▏   | 103/167 [00:19&lt;00:11,  5.55it/s]Loss = 7.9760e-03, PNorm = 69.8774, GNorm = 0.6859, lr_0 = 3.8024e-04\n\n\r 62%|██████▏   | 104/167 [00:20&lt;00:21,  2.92it/s]Loss = 7.9466e-03, PNorm = 69.8871, GNorm = 0.3411, lr_0 = 3.8293e-04\n\n\r 63%|██████▎   | 105/167 [00:20&lt;00:18,  3.38it/s]Loss = 1.0765e-02, PNorm = 69.8970, GNorm = 0.5399, lr_0 = 3.8563e-04\n\n\r 63%|██████▎   | 106/167 [00:20&lt;00:15,  3.83it/s]Loss = 9.5741e-03, PNorm = 69.9050, GNorm = 1.0916, lr_0 = 3.8832e-04\n\n\r 64%|██████▍   | 107/167 [00:20&lt;00:14,  4.20it/s]Loss = 1.2473e-02, PNorm = 69.9103, GNorm = 1.2817, lr_0 = 3.9102e-04\n\n\r 65%|██████▍   | 108/167 [00:20&lt;00:13,  4.51it/s]Loss = 1.1324e-02, PNorm = 69.9158, GNorm = 0.7980, lr_0 = 3.9371e-04\n\n\r 65%|██████▌   | 109/167 [00:21&lt;00:12,  4.79it/s]Loss = 9.1455e-03, PNorm = 69.9223, GNorm = 0.2052, lr_0 = 3.9641e-04\n\n\r 66%|██████▌   | 110/167 [00:21&lt;00:11,  5.06it/s]Loss = 8.9582e-03, PNorm = 69.9296, GNorm = 0.2752, lr_0 = 3.9910e-04\n\n\r 66%|██████▋   | 111/167 [00:21&lt;00:10,  5.19it/s]Loss = 9.4504e-03, PNorm = 69.9376, GNorm = 0.3055, lr_0 = 4.0180e-04\n\n\r 67%|██████▋   | 112/167 [00:21&lt;00:10,  5.33it/s]Loss = 1.1134e-02, PNorm = 69.9466, GNorm = 0.8185, lr_0 = 4.0449e-04\n\n\r 68%|██████▊   | 113/167 [00:21&lt;00:10,  5.37it/s]Loss = 1.1111e-02, PNorm = 69.9571, GNorm = 0.7259, lr_0 = 4.0719e-04\n\n\r 68%|██████▊   | 114/167 [00:22&lt;00:09,  5.43it/s]Loss = 1.1636e-02, PNorm = 69.9683, GNorm = 0.3271, lr_0 = 4.0988e-04\n\n\r 69%|██████▉   | 115/167 [00:22&lt;00:09,  5.51it/s]Loss = 1.0402e-02, PNorm = 69.9787, GNorm = 0.6597, lr_0 = 4.1257e-04\n\n\r 69%|██████▉   | 116/167 [00:22&lt;00:09,  5.49it/s]Loss = 1.0034e-02, PNorm = 69.9890, GNorm = 0.5772, lr_0 = 4.1527e-04\n\n\r 70%|███████   | 117/167 [00:22&lt;00:08,  5.58it/s]Loss = 1.1018e-02, PNorm = 69.9983, GNorm = 0.6767, lr_0 = 4.1796e-04\n\n\r 71%|███████   | 118/167 [00:22&lt;00:08,  5.59it/s]Loss = 1.0522e-02, PNorm = 70.0077, GNorm = 0.4311, lr_0 = 4.2066e-04\n\n\r 71%|███████▏  | 119/167 [00:22&lt;00:08,  5.63it/s]Loss = 1.1088e-02, PNorm = 70.0173, GNorm = 0.4235, lr_0 = 4.2335e-04\n\n\r 72%|███████▏  | 120/167 [00:23&lt;00:08,  5.68it/s]Loss = 1.0623e-02, PNorm = 70.0274, GNorm = 0.4444, lr_0 = 4.2605e-04\n\n\r 72%|███████▏  | 121/167 [00:23&lt;00:08,  5.66it/s]Loss = 9.7311e-03, PNorm = 70.0363, GNorm = 0.7073, lr_0 = 4.2874e-04\n\n\r 73%|███████▎  | 122/167 [00:23&lt;00:07,  5.66it/s]Loss = 8.1731e-03, PNorm = 70.0463, GNorm = 0.3590, lr_0 = 4.3144e-04\n\n\r 74%|███████▎  | 123/167 [00:23&lt;00:07,  5.63it/s]Loss = 9.8610e-03, PNorm = 70.0562, GNorm = 0.3271, lr_0 = 4.3413e-04\n\n\r 74%|███████▍  | 124/167 [00:23&lt;00:07,  5.61it/s]Loss = 1.1204e-02, PNorm = 70.0660, GNorm = 0.9059, lr_0 = 4.3683e-04\n\n\r 75%|███████▍  | 125/167 [00:24&lt;00:07,  5.62it/s]Loss = 7.7637e-03, PNorm = 70.0774, GNorm = 0.2547, lr_0 = 4.3952e-04\n\n\r 75%|███████▌  | 126/167 [00:24&lt;00:07,  5.61it/s]Loss = 1.1248e-02, PNorm = 70.0886, GNorm = 0.5677, lr_0 = 4.4222e-04\n\n\r 76%|███████▌  | 127/167 [00:24&lt;00:07,  5.63it/s]Loss = 1.1612e-02, PNorm = 70.0986, GNorm = 0.5316, lr_0 = 4.4491e-04\n\n\r 77%|███████▋  | 128/167 [00:24&lt;00:06,  5.64it/s]Loss = 9.3521e-03, PNorm = 70.1081, GNorm = 0.7143, lr_0 = 4.4760e-04\n\n\r 77%|███████▋  | 129/167 [00:24&lt;00:06,  5.68it/s]Loss = 9.5568e-03, PNorm = 70.1174, GNorm = 0.5051, lr_0 = 4.5030e-04\n\n\r 78%|███████▊  | 130/167 [00:24&lt;00:06,  5.61it/s]Loss = 8.1427e-03, PNorm = 70.1275, GNorm = 0.6638, lr_0 = 4.5299e-04\n\n\r 78%|███████▊  | 131/167 [00:25&lt;00:06,  5.60it/s]Loss = 8.3956e-03, PNorm = 70.1381, GNorm = 0.4118, lr_0 = 4.5569e-04\n\n\r 79%|███████▉  | 132/167 [00:25&lt;00:06,  5.63it/s]Loss = 7.8838e-03, PNorm = 70.1505, GNorm = 1.0239, lr_0 = 4.5838e-04\n\n\r 80%|███████▉  | 133/167 [00:25&lt;00:06,  5.53it/s]Loss = 1.2030e-02, PNorm = 70.1626, GNorm = 0.4577, lr_0 = 4.6108e-04\n\n\r 80%|████████  | 134/167 [00:25&lt;00:05,  5.62it/s]Loss = 1.1341e-02, PNorm = 70.1740, GNorm = 0.9733, lr_0 = 4.6377e-04\n\n\r 81%|████████  | 135/167 [00:25&lt;00:05,  5.62it/s]Loss = 9.1026e-03, PNorm = 70.1854, GNorm = 0.5882, lr_0 = 4.6647e-04\n\n\r 81%|████████▏ | 136/167 [00:26&lt;00:11,  2.59it/s]Loss = 1.0538e-02, PNorm = 70.1966, GNorm = 0.3570, lr_0 = 4.6916e-04\n\n\r 82%|████████▏ | 137/167 [00:26&lt;00:09,  3.11it/s]Loss = 1.0427e-02, PNorm = 70.2077, GNorm = 0.8019, lr_0 = 4.7186e-04\n\n\r 83%|████████▎ | 138/167 [00:27&lt;00:08,  3.57it/s]Loss = 7.5527e-03, PNorm = 70.2191, GNorm = 0.5301, lr_0 = 4.7455e-04\n\n\r 83%|████████▎ | 139/167 [00:27&lt;00:06,  4.00it/s]Loss = 1.1495e-02, PNorm = 70.2300, GNorm = 0.6098, lr_0 = 4.7725e-04\n\n\r 84%|████████▍ | 140/167 [00:27&lt;00:06,  4.41it/s]Loss = 1.0826e-02, PNorm = 70.2401, GNorm = 0.6907, lr_0 = 4.7994e-04\n\n\r 84%|████████▍ | 141/167 [00:27&lt;00:05,  4.74it/s]Loss = 1.0317e-02, PNorm = 70.2510, GNorm = 0.4994, lr_0 = 4.8263e-04\n\n\r 85%|████████▌ | 142/167 [00:27&lt;00:05,  4.98it/s]Loss = 1.0726e-02, PNorm = 70.2622, GNorm = 0.4587, lr_0 = 4.8533e-04\n\n\r 86%|████████▌ | 143/167 [00:27&lt;00:04,  5.12it/s]Loss = 1.0330e-02, PNorm = 70.2724, GNorm = 1.3956, lr_0 = 4.8802e-04\n\n\r 86%|████████▌ | 144/167 [00:28&lt;00:04,  5.36it/s]Loss = 1.1721e-02, PNorm = 70.2840, GNorm = 0.6819, lr_0 = 4.9072e-04\n\n\r 87%|████████▋ | 145/167 [00:28&lt;00:04,  5.46it/s]Loss = 9.1087e-03, PNorm = 70.2956, GNorm = 0.3222, lr_0 = 4.9341e-04\n\n\r 87%|████████▋ | 146/167 [00:28&lt;00:03,  5.56it/s]Loss = 9.9946e-03, PNorm = 70.3075, GNorm = 0.2586, lr_0 = 4.9611e-04\n\n\r 88%|████████▊ | 147/167 [00:28&lt;00:03,  5.68it/s]Loss = 9.4210e-03, PNorm = 70.3204, GNorm = 0.7161, lr_0 = 4.9880e-04\n\n\r 89%|████████▊ | 148/167 [00:28&lt;00:03,  5.66it/s]Loss = 1.1227e-02, PNorm = 70.3350, GNorm = 0.6940, lr_0 = 5.0150e-04\n\n\r 89%|████████▉ | 149/167 [00:28&lt;00:03,  5.62it/s]Loss = 7.6758e-03, PNorm = 70.3499, GNorm = 0.5223, lr_0 = 5.0419e-04\n\n\r 90%|████████▉ | 150/167 [00:29&lt;00:02,  5.69it/s]Loss = 1.0100e-02, PNorm = 70.3640, GNorm = 0.4088, lr_0 = 5.0689e-04\n\n\r 90%|█████████ | 151/167 [00:29&lt;00:02,  5.67it/s]Loss = 1.0117e-02, PNorm = 70.3789, GNorm = 0.5438, lr_0 = 5.0958e-04\n\n\r 91%|█████████ | 152/167 [00:29&lt;00:02,  5.73it/s]Loss = 9.5740e-03, PNorm = 70.3926, GNorm = 0.6857, lr_0 = 5.1228e-04\n\n\r 92%|█████████▏| 153/167 [00:29&lt;00:02,  5.67it/s]Loss = 9.1445e-03, PNorm = 70.4055, GNorm = 0.4516, lr_0 = 5.1497e-04\n\n\r 92%|█████████▏| 154/167 [00:29&lt;00:02,  5.60it/s]Loss = 9.1299e-03, PNorm = 70.4180, GNorm = 0.3287, lr_0 = 5.1766e-04\n\n\r 93%|█████████▎| 155/167 [00:30&lt;00:02,  5.59it/s]Loss = 9.8822e-03, PNorm = 70.4299, GNorm = 0.6159, lr_0 = 5.2036e-04\n\n\r 93%|█████████▎| 156/167 [00:30&lt;00:01,  5.63it/s]Loss = 1.4913e-02, PNorm = 70.4411, GNorm = 1.7079, lr_0 = 5.2305e-04\n\n\r 94%|█████████▍| 157/167 [00:30&lt;00:01,  5.59it/s]Loss = 1.1303e-02, PNorm = 70.4557, GNorm = 1.2390, lr_0 = 5.2575e-04\n\n\r 95%|█████████▍| 158/167 [00:30&lt;00:01,  5.55it/s]Loss = 9.3124e-03, PNorm = 70.4706, GNorm = 0.3925, lr_0 = 5.2844e-04\n\n\r 95%|█████████▌| 159/167 [00:30&lt;00:01,  5.48it/s]Loss = 8.8536e-03, PNorm = 70.4857, GNorm = 0.2859, lr_0 = 5.3114e-04\n\n\r 96%|█████████▌| 160/167 [00:30&lt;00:01,  5.52it/s]Loss = 9.1982e-03, PNorm = 70.5006, GNorm = 0.4256, lr_0 = 5.3383e-04\n\n\r 96%|█████████▋| 161/167 [00:31&lt;00:01,  5.55it/s]Loss = 1.0267e-02, PNorm = 70.5145, GNorm = 0.6469, lr_0 = 5.3653e-04\n\n\r 97%|█████████▋| 162/167 [00:31&lt;00:00,  5.64it/s]Loss = 1.1496e-02, PNorm = 70.5269, GNorm = 0.9084, lr_0 = 5.3922e-04\n\n\r 98%|█████████▊| 163/167 [00:31&lt;00:00,  5.65it/s]Loss = 1.0352e-02, PNorm = 70.5399, GNorm = 0.2687, lr_0 = 5.4192e-04\n\n\r 98%|█████████▊| 164/167 [00:31&lt;00:00,  5.71it/s]Loss = 1.0658e-02, PNorm = 70.5539, GNorm = 0.2888, lr_0 = 5.4461e-04\n\n\r 99%|█████████▉| 165/167 [00:31&lt;00:00,  5.76it/s]Loss = 9.7455e-03, PNorm = 70.5688, GNorm = 0.5767, lr_0 = 5.4731e-04\n\n\r 99%|█████████▉| 166/167 [00:31&lt;00:00,  5.65it/s]Loss = 1.0401e-02, PNorm = 70.5842, GNorm = 0.6087, lr_0 = 5.5000e-04\n\n\r100%|██████████| 167/167 [00:32&lt;00:00,  5.71it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 25%|██▌       | 1/4 [00:00&lt;00:00,  9.52it/s]\n\r 50%|█████     | 2/4 [00:00&lt;00:00,  9.45it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00,  9.32it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 10.13it/s]Validation auc = 0.762413\n\n*** WARNING: skipped 891085 bytes of output ***\n\nLoss = 1.8698e-05, PNorm = 123.7520, GNorm = 0.0378, lr_0 = 1.0714e-04\n\n\r 56%|█████▋    | 94/167 [00:08&lt;00:06, 11.04it/s]Loss = 9.7638e-05, PNorm = 123.7525, GNorm = 0.1978, lr_0 = 1.0711e-04\nLoss = 1.2159e-05, PNorm = 123.7530, GNorm = 0.0421, lr_0 = 1.0708e-04\n\n\r 57%|█████▋    | 96/167 [00:08&lt;00:06, 11.21it/s]Loss = 2.3391e-05, PNorm = 123.7535, GNorm = 0.0269, lr_0 = 1.0704e-04\nLoss = 1.4116e-06, PNorm = 123.7540, GNorm = 0.0023, lr_0 = 1.0701e-04\n\n\r 59%|█████▊    | 98/167 [00:08&lt;00:06, 11.21it/s]Loss = 2.7265e-05, PNorm = 123.7545, GNorm = 0.0340, lr_0 = 1.0698e-04\nLoss = 3.1046e-05, PNorm = 123.7550, GNorm = 0.0442, lr_0 = 1.0695e-04\n\n\r 60%|█████▉    | 100/167 [00:08&lt;00:05, 11.26it/s]Loss = 1.7595e-05, PNorm = 123.7556, GNorm = 0.0342, lr_0 = 1.0692e-04\nLoss = 1.7076e-05, PNorm = 123.7561, GNorm = 0.0221, lr_0 = 1.0689e-04\n\n\r 61%|██████    | 102/167 [00:09&lt;00:05, 11.21it/s]Loss = 2.7471e-05, PNorm = 123.7566, GNorm = 0.0431, lr_0 = 1.0686e-04\nLoss = 5.8214e-05, PNorm = 123.7571, GNorm = 0.1479, lr_0 = 1.0683e-04\n\n\r 62%|██████▏   | 104/167 [00:09&lt;00:05, 11.24it/s]Loss = 1.7806e-05, PNorm = 123.7577, GNorm = 0.0233, lr_0 = 1.0680e-04\nLoss = 6.1008e-06, PNorm = 123.7582, GNorm = 0.0099, lr_0 = 1.0677e-04\n\n\r 63%|██████▎   | 106/167 [00:09&lt;00:05, 11.13it/s]Loss = 3.5623e-05, PNorm = 123.7586, GNorm = 0.0445, lr_0 = 1.0674e-04\nLoss = 2.7476e-05, PNorm = 123.7591, GNorm = 0.0367, lr_0 = 1.0671e-04\n\n\r 65%|██████▍   | 108/167 [00:09&lt;00:05, 11.07it/s]Loss = 1.7078e-05, PNorm = 123.7596, GNorm = 0.0374, lr_0 = 1.0668e-04\nLoss = 2.5218e-04, PNorm = 123.7601, GNorm = 0.1847, lr_0 = 1.0665e-04\n\n\r 66%|██████▌   | 110/167 [00:09&lt;00:05, 11.09it/s]Loss = 1.7736e-04, PNorm = 123.7607, GNorm = 0.1061, lr_0 = 1.0662e-04\nLoss = 1.1480e-05, PNorm = 123.7612, GNorm = 0.0190, lr_0 = 1.0658e-04\n\n\r 67%|██████▋   | 112/167 [00:10&lt;00:04, 11.13it/s]Loss = 2.1124e-06, PNorm = 123.7617, GNorm = 0.0034, lr_0 = 1.0655e-04\nLoss = 2.4305e-05, PNorm = 123.7622, GNorm = 0.0598, lr_0 = 1.0652e-04\n\n\r 68%|██████▊   | 114/167 [00:10&lt;00:04, 11.24it/s]Loss = 9.5274e-06, PNorm = 123.7627, GNorm = 0.0170, lr_0 = 1.0649e-04\nLoss = 6.0918e-06, PNorm = 123.7632, GNorm = 0.0068, lr_0 = 1.0646e-04\n\n\r 69%|██████▉   | 116/167 [00:10&lt;00:04, 11.28it/s]Loss = 2.7928e-05, PNorm = 123.7636, GNorm = 0.0452, lr_0 = 1.0643e-04\nLoss = 2.0477e-04, PNorm = 123.7642, GNorm = 0.1001, lr_0 = 1.0640e-04\n\n\r 71%|███████   | 118/167 [00:10&lt;00:04, 11.15it/s]Loss = 7.8758e-04, PNorm = 123.7640, GNorm = 0.3832, lr_0 = 1.0637e-04\nLoss = 3.8113e-06, PNorm = 123.7639, GNorm = 0.0060, lr_0 = 1.0634e-04\n\n\r 72%|███████▏  | 120/167 [00:10&lt;00:04, 11.22it/s]Loss = 4.0481e-04, PNorm = 123.7637, GNorm = 0.2021, lr_0 = 1.0631e-04\nLoss = 4.1744e-05, PNorm = 123.7637, GNorm = 0.0604, lr_0 = 1.0628e-04\n\n\r 73%|███████▎  | 122/167 [00:10&lt;00:04, 11.22it/s]Loss = 3.7180e-06, PNorm = 123.7636, GNorm = 0.0093, lr_0 = 1.0625e-04\nLoss = 5.2647e-06, PNorm = 123.7636, GNorm = 0.0101, lr_0 = 1.0622e-04\n\n\r 74%|███████▍  | 124/167 [00:11&lt;00:03, 11.39it/s]Loss = 4.7529e-04, PNorm = 123.7638, GNorm = 0.1868, lr_0 = 1.0619e-04\nLoss = 3.2940e-06, PNorm = 123.7639, GNorm = 0.0064, lr_0 = 1.0616e-04\n\n\r 75%|███████▌  | 126/167 [00:11&lt;00:03, 12.08it/s]Loss = 1.8620e-05, PNorm = 123.7641, GNorm = 0.0176, lr_0 = 1.0613e-04\nLoss = 1.9223e-04, PNorm = 123.7644, GNorm = 0.1068, lr_0 = 1.0610e-04\n\n\r 77%|███████▋  | 128/167 [00:11&lt;00:03, 12.59it/s]Loss = 2.0501e-05, PNorm = 123.7646, GNorm = 0.0392, lr_0 = 1.0607e-04\nLoss = 4.4092e-06, PNorm = 123.7648, GNorm = 0.0105, lr_0 = 1.0603e-04\n\n\r 78%|███████▊  | 130/167 [00:11&lt;00:02, 13.04it/s]Loss = 1.8881e-05, PNorm = 123.7651, GNorm = 0.0198, lr_0 = 1.0600e-04\nLoss = 3.2120e-05, PNorm = 123.7655, GNorm = 0.0805, lr_0 = 1.0597e-04\n\n\r 79%|███████▉  | 132/167 [00:11&lt;00:02, 13.29it/s]Loss = 9.3757e-06, PNorm = 123.7659, GNorm = 0.0109, lr_0 = 1.0594e-04\nLoss = 2.4890e-05, PNorm = 123.7662, GNorm = 0.0452, lr_0 = 1.0591e-04\n\n\r 80%|████████  | 134/167 [00:11&lt;00:02, 13.41it/s]Loss = 6.8397e-04, PNorm = 123.7670, GNorm = 0.4011, lr_0 = 1.0588e-04\nLoss = 1.9198e-05, PNorm = 123.7677, GNorm = 0.0303, lr_0 = 1.0585e-04\n\n\r 81%|████████▏ | 136/167 [00:11&lt;00:02, 13.57it/s]Loss = 1.5088e-06, PNorm = 123.7684, GNorm = 0.0058, lr_0 = 1.0582e-04\nLoss = 1.0155e-04, PNorm = 123.7689, GNorm = 0.5878, lr_0 = 1.0579e-04\n\n\r 83%|████████▎ | 138/167 [00:12&lt;00:02, 13.56it/s]Loss = 1.0684e-05, PNorm = 123.7695, GNorm = 0.0115, lr_0 = 1.0576e-04\nLoss = 4.5614e-06, PNorm = 123.7700, GNorm = 0.0092, lr_0 = 1.0573e-04\n\n\r 84%|████████▍ | 140/167 [00:12&lt;00:01, 13.64it/s]Loss = 1.3264e-05, PNorm = 123.7705, GNorm = 0.0451, lr_0 = 1.0570e-04\nLoss = 4.8473e-05, PNorm = 123.7710, GNorm = 0.0606, lr_0 = 1.0567e-04\n\n\r 85%|████████▌ | 142/167 [00:12&lt;00:01, 13.81it/s]Loss = 1.6703e-05, PNorm = 123.7715, GNorm = 0.0255, lr_0 = 1.0564e-04\nLoss = 4.0139e-05, PNorm = 123.7720, GNorm = 0.1591, lr_0 = 1.0561e-04\n\n\r 86%|████████▌ | 144/167 [00:12&lt;00:01, 13.96it/s]Loss = 2.6887e-06, PNorm = 123.7725, GNorm = 0.0043, lr_0 = 1.0558e-04\nLoss = 1.7560e-04, PNorm = 123.7730, GNorm = 0.2658, lr_0 = 1.0555e-04\n\n\r 87%|████████▋ | 146/167 [00:12&lt;00:01, 13.95it/s]Loss = 6.6167e-06, PNorm = 123.7734, GNorm = 0.0106, lr_0 = 1.0552e-04\nLoss = 3.1344e-06, PNorm = 123.7738, GNorm = 0.0033, lr_0 = 1.0549e-04\n\n\r 89%|████████▊ | 148/167 [00:12&lt;00:01, 13.78it/s]Loss = 2.1720e-05, PNorm = 123.7742, GNorm = 0.0345, lr_0 = 1.0546e-04\nLoss = 1.3201e-05, PNorm = 123.7747, GNorm = 0.0262, lr_0 = 1.0543e-04\n\n\r 90%|████████▉ | 150/167 [00:13&lt;00:01, 13.80it/s]Loss = 2.7077e-04, PNorm = 123.7752, GNorm = 0.1885, lr_0 = 1.0540e-04\nLoss = 1.3803e-05, PNorm = 123.7756, GNorm = 0.0163, lr_0 = 1.0537e-04\n\n\r 91%|█████████ | 152/167 [00:13&lt;00:01, 13.78it/s]Loss = 7.8343e-06, PNorm = 123.7761, GNorm = 0.0099, lr_0 = 1.0534e-04\nLoss = 2.5852e-05, PNorm = 123.7765, GNorm = 0.0498, lr_0 = 1.0531e-04\n\n\r 92%|█████████▏| 154/167 [00:13&lt;00:00, 13.80it/s]Loss = 4.0576e-05, PNorm = 123.7771, GNorm = 0.0930, lr_0 = 1.0528e-04\nLoss = 1.3729e-05, PNorm = 123.7776, GNorm = 0.0203, lr_0 = 1.0525e-04\n\n\r 93%|█████████▎| 156/167 [00:13&lt;00:00, 13.87it/s]Loss = 1.4916e-04, PNorm = 123.7781, GNorm = 0.1091, lr_0 = 1.0522e-04\nLoss = 5.6841e-06, PNorm = 123.7785, GNorm = 0.0117, lr_0 = 1.0519e-04\n\n\r 95%|█████████▍| 158/167 [00:13&lt;00:00, 13.70it/s]Loss = 3.7823e-05, PNorm = 123.7790, GNorm = 0.0635, lr_0 = 1.0516e-04\nLoss = 1.5263e-04, PNorm = 123.7796, GNorm = 0.2391, lr_0 = 1.0513e-04\n\n\r 96%|█████████▌| 160/167 [00:13&lt;00:00, 13.65it/s]Loss = 6.5227e-06, PNorm = 123.7802, GNorm = 0.0092, lr_0 = 1.0509e-04\nLoss = 6.4947e-06, PNorm = 123.7807, GNorm = 0.0093, lr_0 = 1.0506e-04\n\n\r 97%|█████████▋| 162/167 [00:13&lt;00:00, 13.68it/s]Loss = 1.4860e-04, PNorm = 123.7812, GNorm = 0.0835, lr_0 = 1.0503e-04\nLoss = 1.9147e-04, PNorm = 123.7816, GNorm = 0.1205, lr_0 = 1.0500e-04\n\n\r 98%|█████████▊| 164/167 [00:14&lt;00:00, 13.75it/s]Loss = 5.4513e-06, PNorm = 123.7821, GNorm = 0.0116, lr_0 = 1.0497e-04\nLoss = 2.7295e-04, PNorm = 123.7826, GNorm = 0.1989, lr_0 = 1.0494e-04\n\n\r 99%|█████████▉| 166/167 [00:14&lt;00:00, 13.56it/s]Loss = 9.7710e-05, PNorm = 123.7831, GNorm = 0.0701, lr_0 = 1.0491e-04\n\n\r100%|██████████| 167/167 [00:14&lt;00:00, 11.72it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 48.90it/s]Validation auc = 0.904668\n\r 98%|█████████▊| 49/50 [18:56&lt;00:15, 15.71s/it]Epoch 49\n\n\r  0%|          | 0/167 [00:00&lt;?, ?it/s]Loss = 4.7804e-06, PNorm = 123.7836, GNorm = 0.0064, lr_0 = 1.0488e-04\nLoss = 1.3364e-05, PNorm = 123.7841, GNorm = 0.0186, lr_0 = 1.0485e-04\n\n\r  1%|          | 2/167 [00:00&lt;00:12, 13.24it/s]Loss = 1.1981e-04, PNorm = 123.7847, GNorm = 0.1142, lr_0 = 1.0482e-04\nLoss = 2.1248e-05, PNorm = 123.7852, GNorm = 0.0646, lr_0 = 1.0479e-04\n\n\r  2%|▏         | 4/167 [00:00&lt;00:12, 12.99it/s]Loss = 2.4319e-04, PNorm = 123.7857, GNorm = 0.1136, lr_0 = 1.0476e-04\nLoss = 6.6462e-05, PNorm = 123.7862, GNorm = 0.0472, lr_0 = 1.0473e-04\n\n\r  4%|▎         | 6/167 [00:00&lt;00:12, 13.27it/s]Loss = 1.8473e-05, PNorm = 123.7867, GNorm = 0.0354, lr_0 = 1.0470e-04\nLoss = 2.0176e-05, PNorm = 123.7871, GNorm = 0.0320, lr_0 = 1.0467e-04\n\n\r  5%|▍         | 8/167 [00:00&lt;00:11, 13.46it/s]Loss = 8.4927e-05, PNorm = 123.7877, GNorm = 0.0813, lr_0 = 1.0464e-04\nLoss = 2.0502e-04, PNorm = 123.7883, GNorm = 0.0557, lr_0 = 1.0461e-04\n\n\r  6%|▌         | 10/167 [00:00&lt;00:11, 13.52it/s]Loss = 9.3508e-06, PNorm = 123.7889, GNorm = 0.0089, lr_0 = 1.0458e-04\nLoss = 7.1173e-05, PNorm = 123.7896, GNorm = 0.0859, lr_0 = 1.0455e-04\n\n\r  7%|▋         | 12/167 [00:00&lt;00:11, 13.51it/s]Loss = 8.8593e-06, PNorm = 123.7902, GNorm = 0.0079, lr_0 = 1.0452e-04\nLoss = 3.0649e-05, PNorm = 123.7909, GNorm = 0.0952, lr_0 = 1.0449e-04\n\n\r  8%|▊         | 14/167 [00:01&lt;00:11, 13.66it/s]Loss = 2.2601e-06, PNorm = 123.7914, GNorm = 0.0034, lr_0 = 1.0446e-04\nLoss = 2.7766e-06, PNorm = 123.7920, GNorm = 0.0040, lr_0 = 1.0443e-04\n\n\r 10%|▉         | 16/167 [00:01&lt;00:11, 13.72it/s]Loss = 1.7468e-05, PNorm = 123.7925, GNorm = 0.0237, lr_0 = 1.0440e-04\nLoss = 1.9841e-05, PNorm = 123.7930, GNorm = 0.0463, lr_0 = 1.0437e-04\n\n\r 11%|█         | 18/167 [00:01&lt;00:10, 13.57it/s]Loss = 5.3068e-05, PNorm = 123.7935, GNorm = 0.1520, lr_0 = 1.0434e-04\nLoss = 3.8970e-06, PNorm = 123.7940, GNorm = 0.0051, lr_0 = 1.0431e-04\n\n\r 12%|█▏        | 20/167 [00:01&lt;00:10, 13.57it/s]Loss = 1.1893e-05, PNorm = 123.7944, GNorm = 0.0215, lr_0 = 1.0428e-04\nLoss = 8.7931e-06, PNorm = 123.7949, GNorm = 0.0135, lr_0 = 1.0425e-04\n\n\r 13%|█▎        | 22/167 [00:01&lt;00:10, 13.68it/s]Loss = 2.8463e-06, PNorm = 123.7953, GNorm = 0.0033, lr_0 = 1.0422e-04\nLoss = 3.5693e-05, PNorm = 123.7957, GNorm = 0.0479, lr_0 = 1.0419e-04\n\n\r 14%|█▍        | 24/167 [00:01&lt;00:10, 13.62it/s]Loss = 6.7141e-06, PNorm = 123.7961, GNorm = 0.0095, lr_0 = 1.0416e-04\nLoss = 6.1749e-06, PNorm = 123.7965, GNorm = 0.0084, lr_0 = 1.0413e-04\n\n\r 16%|█▌        | 26/167 [00:01&lt;00:10, 13.66it/s]Loss = 1.0780e-05, PNorm = 123.7969, GNorm = 0.0253, lr_0 = 1.0410e-04\nLoss = 5.3942e-06, PNorm = 123.7972, GNorm = 0.0084, lr_0 = 1.0407e-04\n\n\r 17%|█▋        | 28/167 [00:02&lt;00:10, 13.73it/s]Loss = 2.5896e-04, PNorm = 123.7976, GNorm = 0.0859, lr_0 = 1.0404e-04\nLoss = 1.4569e-05, PNorm = 123.7981, GNorm = 0.0211, lr_0 = 1.0401e-04\n\n\r 18%|█▊        | 30/167 [00:02&lt;00:09, 13.80it/s]Loss = 3.6767e-06, PNorm = 123.7985, GNorm = 0.0064, lr_0 = 1.0398e-04\nLoss = 6.6821e-07, PNorm = 123.7989, GNorm = 0.0016, lr_0 = 1.0395e-04\n\n\r 19%|█▉        | 32/167 [00:02&lt;00:09, 13.73it/s]Loss = 1.5672e-04, PNorm = 123.7992, GNorm = 0.0794, lr_0 = 1.0392e-04\nLoss = 2.9406e-05, PNorm = 123.7996, GNorm = 0.0501, lr_0 = 1.0389e-04\n\n\r 20%|██        | 34/167 [00:02&lt;00:09, 13.74it/s]Loss = 6.0667e-05, PNorm = 123.8000, GNorm = 0.0772, lr_0 = 1.0386e-04\nLoss = 4.2263e-05, PNorm = 123.8004, GNorm = 0.0893, lr_0 = 1.0383e-04\n\n\r 22%|██▏       | 36/167 [00:02&lt;00:09, 13.73it/s]Loss = 1.7378e-05, PNorm = 123.8008, GNorm = 0.0182, lr_0 = 1.0380e-04\nLoss = 8.2953e-06, PNorm = 123.8012, GNorm = 0.0116, lr_0 = 1.0378e-04\n\n\r 23%|██▎       | 38/167 [00:02&lt;00:09, 13.58it/s]Loss = 1.3886e-05, PNorm = 123.8017, GNorm = 0.0322, lr_0 = 1.0375e-04\nLoss = 4.9908e-06, PNorm = 123.8021, GNorm = 0.0109, lr_0 = 1.0372e-04\n\n\r 24%|██▍       | 40/167 [00:02&lt;00:09, 13.42it/s]Loss = 6.5202e-06, PNorm = 123.8024, GNorm = 0.0071, lr_0 = 1.0369e-04\nLoss = 2.9436e-05, PNorm = 123.8028, GNorm = 0.0260, lr_0 = 1.0366e-04\n\n\r 25%|██▌       | 42/167 [00:03&lt;00:09, 13.44it/s]Loss = 1.7988e-06, PNorm = 123.8032, GNorm = 0.0024, lr_0 = 1.0363e-04\nLoss = 1.0577e-05, PNorm = 123.8036, GNorm = 0.0409, lr_0 = 1.0360e-04\n\n\r 26%|██▋       | 44/167 [00:03&lt;00:09, 13.48it/s]Loss = 4.9261e-05, PNorm = 123.8039, GNorm = 0.0629, lr_0 = 1.0357e-04\nLoss = 1.0696e-06, PNorm = 123.8043, GNorm = 0.0016, lr_0 = 1.0354e-04\n\n\r 28%|██▊       | 46/167 [00:03&lt;00:08, 13.52it/s]Loss = 9.1616e-05, PNorm = 123.8046, GNorm = 0.0692, lr_0 = 1.0351e-04\nLoss = 3.1401e-04, PNorm = 123.8048, GNorm = 0.8959, lr_0 = 1.0348e-04\n\n\r 29%|██▊       | 48/167 [00:03&lt;00:09, 13.13it/s]Loss = 8.8255e-07, PNorm = 123.8051, GNorm = 0.0016, lr_0 = 1.0345e-04\nLoss = 4.0091e-06, PNorm = 123.8053, GNorm = 0.0052, lr_0 = 1.0342e-04\n\n\r 30%|██▉       | 50/167 [00:03&lt;00:08, 13.00it/s]Loss = 1.5264e-06, PNorm = 123.8056, GNorm = 0.0029, lr_0 = 1.0339e-04\nLoss = 1.2445e-05, PNorm = 123.8059, GNorm = 0.0359, lr_0 = 1.0336e-04\n\n\r 31%|███       | 52/167 [00:03&lt;00:08, 12.87it/s]Loss = 4.6888e-06, PNorm = 123.8062, GNorm = 0.0100, lr_0 = 1.0333e-04\nLoss = 6.9698e-06, PNorm = 123.8065, GNorm = 0.0201, lr_0 = 1.0330e-04\n\n\r 32%|███▏      | 54/167 [00:04&lt;00:08, 12.94it/s]Loss = 6.8571e-05, PNorm = 123.8069, GNorm = 0.0733, lr_0 = 1.0327e-04\nLoss = 4.4645e-05, PNorm = 123.8074, GNorm = 0.3013, lr_0 = 1.0324e-04\n\n\r 34%|███▎      | 56/167 [00:04&lt;00:08, 12.91it/s]Loss = 3.5963e-05, PNorm = 123.8078, GNorm = 0.0219, lr_0 = 1.0321e-04\nLoss = 3.5698e-05, PNorm = 123.8083, GNorm = 0.0373, lr_0 = 1.0318e-04\n\n\r 35%|███▍      | 58/167 [00:04&lt;00:08, 12.64it/s]Loss = 3.2643e-04, PNorm = 123.8089, GNorm = 0.1960, lr_0 = 1.0315e-04\nLoss = 8.4641e-06, PNorm = 123.8095, GNorm = 0.0158, lr_0 = 1.0312e-04\n\n\r 36%|███▌      | 60/167 [00:04&lt;00:08, 12.76it/s]Loss = 7.9209e-06, PNorm = 123.8100, GNorm = 0.0198, lr_0 = 1.0309e-04\nLoss = 4.6716e-06, PNorm = 123.8105, GNorm = 0.0062, lr_0 = 1.0306e-04\n\n\r 37%|███▋      | 62/167 [00:04&lt;00:08, 12.42it/s]Loss = 2.4621e-05, PNorm = 123.8110, GNorm = 0.0362, lr_0 = 1.0303e-04\nLoss = 1.1686e-06, PNorm = 123.8115, GNorm = 0.0026, lr_0 = 1.0300e-04\n\n\r 38%|███▊      | 64/167 [00:04&lt;00:08, 12.67it/s]Loss = 2.0212e-05, PNorm = 123.8120, GNorm = 0.0224, lr_0 = 1.0297e-04\nLoss = 4.2273e-04, PNorm = 123.8125, GNorm = 0.1943, lr_0 = 1.0294e-04\n\n\r 40%|███▉      | 66/167 [00:04&lt;00:08, 12.53it/s]Loss = 5.4126e-05, PNorm = 123.8131, GNorm = 0.0736, lr_0 = 1.0291e-04\nLoss = 8.8551e-05, PNorm = 123.8136, GNorm = 0.0869, lr_0 = 1.0288e-04\n\n\r 41%|████      | 68/167 [00:05&lt;00:07, 12.84it/s]Loss = 2.1102e-05, PNorm = 123.8141, GNorm = 0.0358, lr_0 = 1.0286e-04\nLoss = 1.4015e-05, PNorm = 123.8146, GNorm = 0.0298, lr_0 = 1.0283e-04\n\n\r 42%|████▏     | 70/167 [00:05&lt;00:07, 12.51it/s]Loss = 9.4195e-06, PNorm = 123.8152, GNorm = 0.0336, lr_0 = 1.0280e-04\nLoss = 2.3013e-06, PNorm = 123.8157, GNorm = 0.0077, lr_0 = 1.0277e-04\n\n\r 43%|████▎     | 72/167 [00:05&lt;00:07, 12.71it/s]Loss = 3.8644e-05, PNorm = 123.8162, GNorm = 0.0649, lr_0 = 1.0274e-04\nLoss = 2.5075e-05, PNorm = 123.8167, GNorm = 0.0650, lr_0 = 1.0271e-04\n\n\r 44%|████▍     | 74/167 [00:05&lt;00:07, 12.77it/s]Loss = 5.2512e-06, PNorm = 123.8172, GNorm = 0.0051, lr_0 = 1.0268e-04\nLoss = 2.4794e-05, PNorm = 123.8177, GNorm = 0.0834, lr_0 = 1.0265e-04\n\n\r 46%|████▌     | 76/167 [00:05&lt;00:07, 12.40it/s]Loss = 1.3819e-06, PNorm = 123.8181, GNorm = 0.0019, lr_0 = 1.0262e-04\nLoss = 1.5289e-05, PNorm = 123.8186, GNorm = 0.0327, lr_0 = 1.0259e-04\n\n\r 47%|████▋     | 78/167 [00:05&lt;00:07, 12.63it/s]Loss = 8.8151e-05, PNorm = 123.8191, GNorm = 0.0840, lr_0 = 1.0256e-04\nLoss = 5.3137e-06, PNorm = 123.8195, GNorm = 0.0079, lr_0 = 1.0253e-04\n\n\r 48%|████▊     | 80/167 [00:06&lt;00:07, 12.33it/s]Loss = 1.5273e-05, PNorm = 123.8200, GNorm = 0.0195, lr_0 = 1.0250e-04\nLoss = 4.6308e-05, PNorm = 123.8206, GNorm = 0.1010, lr_0 = 1.0247e-04\n\n\r 49%|████▉     | 82/167 [00:06&lt;00:06, 12.42it/s]Loss = 1.6433e-06, PNorm = 123.8211, GNorm = 0.0024, lr_0 = 1.0244e-04\nLoss = 2.3731e-04, PNorm = 123.8215, GNorm = 0.2135, lr_0 = 1.0241e-04\n\n\r 50%|█████     | 84/167 [00:06&lt;00:06, 12.41it/s]Loss = 5.6181e-05, PNorm = 123.8219, GNorm = 0.0699, lr_0 = 1.0238e-04\nLoss = 2.7444e-06, PNorm = 123.8223, GNorm = 0.0045, lr_0 = 1.0235e-04\n\n\r 51%|█████▏    | 86/167 [00:06&lt;00:06, 12.29it/s]Loss = 3.4746e-06, PNorm = 123.8227, GNorm = 0.0053, lr_0 = 1.0232e-04\nLoss = 4.5043e-06, PNorm = 123.8230, GNorm = 0.0060, lr_0 = 1.0230e-04\n\n\r 53%|█████▎    | 88/167 [00:06&lt;00:06, 12.58it/s]Loss = 1.4620e-05, PNorm = 123.8234, GNorm = 0.0312, lr_0 = 1.0227e-04\nLoss = 5.8438e-06, PNorm = 123.8238, GNorm = 0.0144, lr_0 = 1.0224e-04\n\n\r 54%|█████▍    | 90/167 [00:06&lt;00:06, 12.46it/s]Loss = 1.8419e-05, PNorm = 123.8242, GNorm = 0.0248, lr_0 = 1.0221e-04\nLoss = 2.8859e-06, PNorm = 123.8245, GNorm = 0.0068, lr_0 = 1.0218e-04\n\n\r 55%|█████▌    | 92/167 [00:07&lt;00:05, 12.87it/s]Loss = 1.4391e-06, PNorm = 123.8249, GNorm = 0.0036, lr_0 = 1.0215e-04\nLoss = 1.6154e-04, PNorm = 123.8252, GNorm = 0.1226, lr_0 = 1.0212e-04\n\n\r 56%|█████▋    | 94/167 [00:07&lt;00:05, 12.55it/s]Loss = 1.4911e-05, PNorm = 123.8256, GNorm = 0.0236, lr_0 = 1.0209e-04\nLoss = 1.0989e-05, PNorm = 123.8259, GNorm = 0.0210, lr_0 = 1.0206e-04\n\n\r 57%|█████▋    | 96/167 [00:07&lt;00:05, 12.58it/s]Loss = 7.6395e-06, PNorm = 123.8263, GNorm = 0.0116, lr_0 = 1.0203e-04\nLoss = 4.0493e-05, PNorm = 123.8266, GNorm = 0.0369, lr_0 = 1.0200e-04\n\n\r 59%|█████▊    | 98/167 [00:07&lt;00:05, 12.33it/s]Loss = 1.3526e-05, PNorm = 123.8269, GNorm = 0.0150, lr_0 = 1.0197e-04\nLoss = 8.7942e-06, PNorm = 123.8272, GNorm = 0.0293, lr_0 = 1.0194e-04\n\n\r 60%|█████▉    | 100/167 [00:07&lt;00:05, 12.68it/s]Loss = 8.3548e-06, PNorm = 123.8276, GNorm = 0.0168, lr_0 = 1.0191e-04\nLoss = 2.4496e-05, PNorm = 123.8279, GNorm = 0.0581, lr_0 = 1.0188e-04\n\n\r 61%|██████    | 102/167 [00:07&lt;00:05, 12.38it/s]Loss = 2.2400e-06, PNorm = 123.8283, GNorm = 0.0039, lr_0 = 1.0186e-04\nLoss = 1.6953e-05, PNorm = 123.8286, GNorm = 0.0290, lr_0 = 1.0183e-04\n\n\r 62%|██████▏   | 104/167 [00:08&lt;00:05, 12.59it/s]Loss = 4.4222e-06, PNorm = 123.8289, GNorm = 0.0054, lr_0 = 1.0180e-04\nLoss = 2.3517e-05, PNorm = 123.8293, GNorm = 0.0421, lr_0 = 1.0177e-04\n\n\r 63%|██████▎   | 106/167 [00:08&lt;00:04, 12.50it/s]Loss = 2.8284e-05, PNorm = 123.8296, GNorm = 0.0213, lr_0 = 1.0174e-04\nLoss = 5.5786e-04, PNorm = 123.8298, GNorm = 0.1786, lr_0 = 1.0171e-04\n\n\r 65%|██████▍   | 108/167 [00:08&lt;00:04, 12.34it/s]Loss = 2.4215e-06, PNorm = 123.8300, GNorm = 0.0057, lr_0 = 1.0168e-04\nLoss = 8.5173e-06, PNorm = 123.8302, GNorm = 0.0138, lr_0 = 1.0165e-04\n\n\r 66%|██████▌   | 110/167 [00:08&lt;00:04, 12.51it/s]Loss = 8.4708e-06, PNorm = 123.8304, GNorm = 0.0113, lr_0 = 1.0162e-04\nLoss = 1.1864e-05, PNorm = 123.8306, GNorm = 0.0143, lr_0 = 1.0159e-04\n\n\r 67%|██████▋   | 112/167 [00:08&lt;00:04, 12.35it/s]Loss = 6.2925e-04, PNorm = 123.8306, GNorm = 0.2521, lr_0 = 1.0156e-04\nLoss = 8.6302e-06, PNorm = 123.8305, GNorm = 0.0268, lr_0 = 1.0153e-04\n\n\r 68%|██████▊   | 114/167 [00:08&lt;00:04, 12.54it/s]Loss = 1.4432e-05, PNorm = 123.8306, GNorm = 0.0214, lr_0 = 1.0150e-04\nLoss = 1.4767e-06, PNorm = 123.8306, GNorm = 0.0019, lr_0 = 1.0148e-04\n\n\r 69%|██████▉   | 116/167 [00:08&lt;00:04, 12.45it/s]Loss = 2.4512e-06, PNorm = 123.8307, GNorm = 0.0036, lr_0 = 1.0145e-04\nLoss = 3.5524e-06, PNorm = 123.8307, GNorm = 0.0178, lr_0 = 1.0142e-04\n\n\r 71%|███████   | 118/167 [00:09&lt;00:03, 12.80it/s]Loss = 2.2754e-05, PNorm = 123.8309, GNorm = 0.1264, lr_0 = 1.0139e-04\nLoss = 6.1734e-06, PNorm = 123.8310, GNorm = 0.0087, lr_0 = 1.0136e-04\n\n\r 72%|███████▏  | 120/167 [00:09&lt;00:03, 12.51it/s]Loss = 8.6412e-07, PNorm = 123.8311, GNorm = 0.0028, lr_0 = 1.0133e-04\nLoss = 1.4279e-05, PNorm = 123.8312, GNorm = 0.0221, lr_0 = 1.0130e-04\n\n\r 73%|███████▎  | 122/167 [00:09&lt;00:03, 12.60it/s]Loss = 5.4811e-06, PNorm = 123.8314, GNorm = 0.0095, lr_0 = 1.0127e-04\nLoss = 4.1280e-06, PNorm = 123.8315, GNorm = 0.0056, lr_0 = 1.0124e-04\n\n\r 74%|███████▍  | 124/167 [00:09&lt;00:03, 12.64it/s]Loss = 5.0957e-04, PNorm = 123.8312, GNorm = 0.3234, lr_0 = 1.0121e-04\nLoss = 4.3581e-05, PNorm = 123.8312, GNorm = 0.1004, lr_0 = 1.0118e-04\n\n\r 75%|███████▌  | 126/167 [00:09&lt;00:03, 12.36it/s]Loss = 4.9438e-04, PNorm = 123.8314, GNorm = 0.3317, lr_0 = 1.0116e-04\nLoss = 6.1217e-06, PNorm = 123.8317, GNorm = 0.0209, lr_0 = 1.0113e-04\n\n\r 77%|███████▋  | 128/167 [00:09&lt;00:03, 12.41it/s]Loss = 2.1244e-05, PNorm = 123.8321, GNorm = 0.0918, lr_0 = 1.0110e-04\nLoss = 2.0041e-05, PNorm = 123.8325, GNorm = 0.0491, lr_0 = 1.0107e-04\n\n\r 78%|███████▊  | 130/167 [00:10&lt;00:03, 12.29it/s]Loss = 1.8849e-06, PNorm = 123.8329, GNorm = 0.0033, lr_0 = 1.0104e-04\nLoss = 2.9864e-06, PNorm = 123.8333, GNorm = 0.0052, lr_0 = 1.0101e-04\n\n\r 79%|███████▉  | 132/167 [00:10&lt;00:02, 12.65it/s]Loss = 3.0624e-05, PNorm = 123.8337, GNorm = 0.0877, lr_0 = 1.0098e-04\nLoss = 3.9468e-05, PNorm = 123.8342, GNorm = 0.0626, lr_0 = 1.0095e-04\n\n\r 80%|████████  | 134/167 [00:10&lt;00:02, 12.44it/s]Loss = 1.3137e-05, PNorm = 123.8346, GNorm = 0.0235, lr_0 = 1.0092e-04\nLoss = 1.0295e-05, PNorm = 123.8350, GNorm = 0.0156, lr_0 = 1.0089e-04\n\n\r 81%|████████▏ | 136/167 [00:10&lt;00:02, 12.70it/s]Loss = 3.3533e-04, PNorm = 123.8355, GNorm = 0.1575, lr_0 = 1.0087e-04\nLoss = 5.1954e-06, PNorm = 123.8359, GNorm = 0.0115, lr_0 = 1.0084e-04\n\n\r 83%|████████▎ | 138/167 [00:10&lt;00:02, 12.46it/s]Loss = 1.2977e-05, PNorm = 123.8363, GNorm = 0.0185, lr_0 = 1.0081e-04\nLoss = 3.3836e-06, PNorm = 123.8367, GNorm = 0.0054, lr_0 = 1.0078e-04\n\n\r 84%|████████▍ | 140/167 [00:10&lt;00:02, 12.91it/s]Loss = 1.0212e-06, PNorm = 123.8370, GNorm = 0.0018, lr_0 = 1.0075e-04\nLoss = 4.7534e-05, PNorm = 123.8374, GNorm = 0.1291, lr_0 = 1.0072e-04\n\n\r 85%|████████▌ | 142/167 [00:11&lt;00:01, 12.55it/s]Loss = 4.9592e-05, PNorm = 123.8378, GNorm = 0.0799, lr_0 = 1.0069e-04\nLoss = 4.5370e-04, PNorm = 123.8382, GNorm = 0.1920, lr_0 = 1.0066e-04\n\n\r 86%|████████▌ | 144/167 [00:11&lt;00:01, 12.72it/s]Loss = 8.9903e-06, PNorm = 123.8386, GNorm = 0.0134, lr_0 = 1.0063e-04\nLoss = 3.2164e-06, PNorm = 123.8389, GNorm = 0.0037, lr_0 = 1.0061e-04\n\n\r 87%|████████▋ | 146/167 [00:11&lt;00:01, 12.34it/s]Loss = 1.8926e-06, PNorm = 123.8392, GNorm = 0.0034, lr_0 = 1.0058e-04\nLoss = 3.6602e-04, PNorm = 123.8395, GNorm = 0.1371, lr_0 = 1.0055e-04\n\n\r 89%|████████▊ | 148/167 [00:11&lt;00:01, 12.45it/s]Loss = 3.8428e-05, PNorm = 123.8398, GNorm = 0.0618, lr_0 = 1.0052e-04\nLoss = 6.4619e-06, PNorm = 123.8401, GNorm = 0.0102, lr_0 = 1.0049e-04\n\n\r 90%|████████▉ | 150/167 [00:11&lt;00:01, 12.42it/s]Loss = 1.9544e-05, PNorm = 123.8404, GNorm = 0.0364, lr_0 = 1.0046e-04\nLoss = 3.0509e-06, PNorm = 123.8407, GNorm = 0.0050, lr_0 = 1.0043e-04\n\n\r 91%|█████████ | 152/167 [00:11&lt;00:01, 12.51it/s]Loss = 1.8079e-04, PNorm = 123.8410, GNorm = 0.1122, lr_0 = 1.0040e-04\nLoss = 9.1925e-04, PNorm = 123.8418, GNorm = 0.4998, lr_0 = 1.0037e-04\n\n\r 92%|█████████▏| 154/167 [00:11&lt;00:01, 12.67it/s]Loss = 1.7689e-04, PNorm = 123.8428, GNorm = 0.3492, lr_0 = 1.0035e-04\nLoss = 1.2930e-05, PNorm = 123.8438, GNorm = 0.0134, lr_0 = 1.0032e-04\n\n\r 93%|█████████▎| 156/167 [00:12&lt;00:00, 12.84it/s]Loss = 3.4281e-06, PNorm = 123.8447, GNorm = 0.0043, lr_0 = 1.0029e-04\nLoss = 2.5390e-05, PNorm = 123.8455, GNorm = 0.0338, lr_0 = 1.0026e-04\n\n\r 95%|█████████▍| 158/167 [00:12&lt;00:00, 12.94it/s]Loss = 3.9904e-06, PNorm = 123.8463, GNorm = 0.0068, lr_0 = 1.0023e-04\nLoss = 8.8213e-05, PNorm = 123.8471, GNorm = 0.0604, lr_0 = 1.0020e-04\n\n\r 96%|█████████▌| 160/167 [00:12&lt;00:00, 12.92it/s]Loss = 1.2493e-05, PNorm = 123.8479, GNorm = 0.0180, lr_0 = 1.0017e-04\nLoss = 4.3197e-06, PNorm = 123.8486, GNorm = 0.0232, lr_0 = 1.0014e-04\n\n\r 97%|█████████▋| 162/167 [00:12&lt;00:00, 12.80it/s]Loss = 1.0319e-05, PNorm = 123.8493, GNorm = 0.0201, lr_0 = 1.0011e-04\nLoss = 2.3561e-05, PNorm = 123.8500, GNorm = 0.0584, lr_0 = 1.0009e-04\n\n\r 98%|█████████▊| 164/167 [00:12&lt;00:00, 12.75it/s]Loss = 1.4625e-06, PNorm = 123.8505, GNorm = 0.0035, lr_0 = 1.0006e-04\nLoss = 7.3938e-05, PNorm = 123.8512, GNorm = 0.1030, lr_0 = 1.0003e-04\n\n\r 99%|█████████▉| 166/167 [00:12&lt;00:00, 12.76it/s]Loss = 3.5748e-05, PNorm = 123.8519, GNorm = 0.0874, lr_0 = 1.0000e-04\n\n\r100%|██████████| 167/167 [00:13&lt;00:00, 12.84it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 48.64it/s]Validation auc = 0.905703\n\r100%|██████████| 50/50 [19:09&lt;00:00, 14.93s/it]\nModel 0 best validation auc = 0.938107 on epoch 8\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\r 25%|██▌       | 1/4 [00:00&lt;00:00,  9.65it/s]\r 50%|█████     | 2/4 [00:00&lt;00:00,  9.65it/s]\r100%|██████████| 4/4 [00:00&lt;00:00, 10.14it/s]\nModel 0 test auc = 0.872689\nEnsemble test auc = 0.872689\n1-fold cross validation\nSeed 0 ==&gt; test auc = 0.872689\nOverall test auc = 0.872689 +/- 0.000000\nOut[35]: (0.8726888202188564, 0.0)\n</div>"]}}],"execution_count":48},{"cell_type":"code","source":["#LEO Pharma 0.8548336983537269\nparser = ArgumentParser()\nadd_train_args(parser)\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-1460_bin76.csv'),\n                          '--dataset_type','classification',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_int'),\n                          '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'),\n                          '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                          '--log_frequency','1',\n                          '--depth','5',\n                          '--dropout','0.05',\n                          '--hidden_size','1500',\n                          '--ffn_num_layers','2',\n                          '--epochs','50'\n                        #,'--atom_messages'\n                        #,'--ensemble_size','3'\n                        #,'--features_generator','rdkit_2d'\n                         ,'--seed','13',\n                          ])\nmodify_train_args(args)\nlogger = create_logger(name='train', save_dir=args.save_dir, quiet=args.quiet)\n\ncross_validate(args, logger)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 5,\n &#39;dropout&#39;: 0.05,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 50,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 1500,\n &#39;ffn_num_layers&#39;: 2,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1500,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_int/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 5,\n &#39;dropout&#39;: 0.05,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 50,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 1500,\n &#39;ffn_num_layers&#39;: 2,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1500,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_int/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\n\r  0%|          | 0/1460 [00:00&lt;?, ?it/s]\r 20%|█▉        | 288/1460 [00:00&lt;00:00, 2873.47it/s]\r 43%|████▎     | 625/1460 [00:00&lt;00:00, 3005.34it/s]\r 66%|██████▌   | 964/1460 [00:00&lt;00:00, 3109.86it/s]\r 89%|████████▉ | 1305/1460 [00:00&lt;00:00, 3192.94it/s]\r100%|██████████| 1460/1460 [00:00&lt;00:00, 3266.24it/s]\nNumber of tasks = 4\nNumber of tasks = 4\nSplitting data with seed 13\nSplitting data with seed 13\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 3353.48it/s]\n\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\r100%|██████████| 182/182 [00:00&lt;00:00, 3332.23it/s]\nClass sizes\nClass sizes\nJAK1 0: 50.89%, 1: 49.11%\nJAK1 0: 50.89%, 1: 49.11%\nJAK2 0: 55.27%, 1: 44.73%\nJAK2 0: 55.27%, 1: 44.73%\nJAK3 0: 83.56%, 1: 16.44%\nJAK3 0: 83.56%, 1: 16.44%\nTYK2 0: 93.56%, 1: 6.44%\nTYK2 0: 93.56%, 1: 6.44%\nTotal size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nTotal size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nBuilding model 0\nBuilding model 0\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.05)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1500, bias=False)\n      (W_h): Linear(in_features=1500, out_features=1500, bias=False)\n      (W_o): Linear(in_features=1633, out_features=1500, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.05)\n    (1): Linear(in_features=1500, out_features=1500, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.05)\n    (4): Linear(in_features=1500, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.05)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1500, bias=False)\n      (W_h): Linear(in_features=1500, out_features=1500, bias=False)\n      (W_o): Linear(in_features=1633, out_features=1500, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.05)\n    (1): Linear(in_features=1500, out_features=1500, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.05)\n    (4): Linear(in_features=1500, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 7,179,004\nNumber of parameters = 7,179,004\nMoving model to cuda\nMoving model to cuda\n\r  0%|          | 0/50 [00:00&lt;?, ?it/s]Epoch 0\nEpoch 0\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 1.3301e-02, PNorm = 69.5978, GNorm = 1.7899, lr_0 = 1.1552e-04\nLoss = 1.3301e-02, PNorm = 69.5978, GNorm = 1.7899, lr_0 = 1.1552e-04\nLoss = 1.0383e-02, PNorm = 69.5989, GNorm = 0.9751, lr_0 = 1.3103e-04\nLoss = 1.0383e-02, PNorm = 69.5989, GNorm = 0.9751, lr_0 = 1.3103e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:02, 13.43it/s]Loss = 1.1082e-02, PNorm = 69.5999, GNorm = 0.9529, lr_0 = 1.4655e-04\nLoss = 1.1082e-02, PNorm = 69.5999, GNorm = 0.9529, lr_0 = 1.4655e-04\nLoss = 1.1120e-02, PNorm = 69.6007, GNorm = 0.8497, lr_0 = 1.6207e-04\nLoss = 1.1120e-02, PNorm = 69.6007, GNorm = 0.8497, lr_0 = 1.6207e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:01, 13.56it/s]Loss = 1.0323e-02, PNorm = 69.6016, GNorm = 1.4493, lr_0 = 1.7759e-04\nLoss = 1.0323e-02, PNorm = 69.6016, GNorm = 1.4493, lr_0 = 1.7759e-04\nLoss = 9.4507e-03, PNorm = 69.6027, GNorm = 0.5195, lr_0 = 1.9310e-04\nLoss = 9.4507e-03, PNorm = 69.6027, GNorm = 0.5195, lr_0 = 1.9310e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:01, 13.66it/s]Loss = 1.0965e-02, PNorm = 69.6036, GNorm = 1.0538, lr_0 = 2.0862e-04\nLoss = 1.0965e-02, PNorm = 69.6036, GNorm = 1.0538, lr_0 = 2.0862e-04\nLoss = 1.0342e-02, PNorm = 69.6047, GNorm = 1.0745, lr_0 = 2.2414e-04\nLoss = 1.0342e-02, PNorm = 69.6047, GNorm = 1.0745, lr_0 = 2.2414e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:01, 13.81it/s]Loss = 1.1679e-02, PNorm = 69.6059, GNorm = 0.5870, lr_0 = 2.3966e-04\nLoss = 1.1679e-02, PNorm = 69.6059, GNorm = 0.5870, lr_0 = 2.3966e-04\nLoss = 1.0045e-02, PNorm = 69.6074, GNorm = 0.5594, lr_0 = 2.5517e-04\nLoss = 1.0045e-02, PNorm = 69.6074, GNorm = 0.5594, lr_0 = 2.5517e-04\n\n\r 34%|███▍      | 10/29 [00:00&lt;00:01, 13.88it/s]Loss = 8.3923e-03, PNorm = 69.6094, GNorm = 0.6319, lr_0 = 2.7069e-04\nLoss = 8.3923e-03, PNorm = 69.6094, GNorm = 0.6319, lr_0 = 2.7069e-04\nLoss = 9.8405e-03, PNorm = 69.6118, GNorm = 0.2550, lr_0 = 2.8621e-04\nLoss = 9.8405e-03, PNorm = 69.6118, GNorm = 0.2550, lr_0 = 2.8621e-04\n\n\r 41%|████▏     | 12/29 [00:00&lt;00:01, 13.93it/s]Loss = 1.1482e-02, PNorm = 69.6141, GNorm = 0.6943, lr_0 = 3.0172e-04\nLoss = 1.1482e-02, PNorm = 69.6141, GNorm = 0.6943, lr_0 = 3.0172e-04\nLoss = 1.1718e-02, PNorm = 69.6165, GNorm = 0.5939, lr_0 = 3.1724e-04\nLoss = 1.1718e-02, PNorm = 69.6165, GNorm = 0.5939, lr_0 = 3.1724e-04\n\n\r 48%|████▊     | 14/29 [00:01&lt;00:01, 13.91it/s]Loss = 9.4846e-03, PNorm = 69.6193, GNorm = 0.4082, lr_0 = 3.3276e-04\nLoss = 9.4846e-03, PNorm = 69.6193, GNorm = 0.4082, lr_0 = 3.3276e-04\nLoss = 9.0944e-03, PNorm = 69.6226, GNorm = 0.4681, lr_0 = 3.4828e-04\nLoss = 9.0944e-03, PNorm = 69.6226, GNorm = 0.4681, lr_0 = 3.4828e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:00, 14.04it/s]Loss = 1.0108e-02, PNorm = 69.6260, GNorm = 0.3525, lr_0 = 3.6379e-04\nLoss = 1.0108e-02, PNorm = 69.6260, GNorm = 0.3525, lr_0 = 3.6379e-04\nLoss = 1.0342e-02, PNorm = 69.6296, GNorm = 0.2248, lr_0 = 3.7931e-04\nLoss = 1.0342e-02, PNorm = 69.6296, GNorm = 0.2248, lr_0 = 3.7931e-04\n\n\r 62%|██████▏   | 18/29 [00:01&lt;00:00, 14.03it/s]Loss = 1.0656e-02, PNorm = 69.6335, GNorm = 0.3506, lr_0 = 3.9483e-04\nLoss = 1.0656e-02, PNorm = 69.6335, GNorm = 0.3506, lr_0 = 3.9483e-04\nLoss = 9.6004e-03, PNorm = 69.6377, GNorm = 0.2823, lr_0 = 4.1034e-04\nLoss = 9.6004e-03, PNorm = 69.6377, GNorm = 0.2823, lr_0 = 4.1034e-04\n\n\r 69%|██████▉   | 20/29 [00:01&lt;00:00, 14.01it/s]Loss = 9.0712e-03, PNorm = 69.6421, GNorm = 0.3979, lr_0 = 4.2586e-04\nLoss = 9.0712e-03, PNorm = 69.6421, GNorm = 0.3979, lr_0 = 4.2586e-04\nLoss = 8.9537e-03, PNorm = 69.6469, GNorm = 0.3206, lr_0 = 4.4138e-04\nLoss = 8.9537e-03, PNorm = 69.6469, GNorm = 0.3206, lr_0 = 4.4138e-04\n\n\r 76%|███████▌  | 22/29 [00:01&lt;00:00, 14.03it/s]Loss = 1.1226e-02, PNorm = 69.6515, GNorm = 0.9409, lr_0 = 4.5690e-04\nLoss = 1.1226e-02, PNorm = 69.6515, GNorm = 0.9409, lr_0 = 4.5690e-04\nLoss = 9.8933e-03, PNorm = 69.6563, GNorm = 0.2926, lr_0 = 4.7241e-04\nLoss = 9.8933e-03, PNorm = 69.6563, GNorm = 0.2926, lr_0 = 4.7241e-04\n\n\r 83%|████████▎ | 24/29 [00:01&lt;00:00, 14.06it/s]Loss = 1.0817e-02, PNorm = 69.6602, GNorm = 0.8773, lr_0 = 4.8793e-04\nLoss = 1.0817e-02, PNorm = 69.6602, GNorm = 0.8773, lr_0 = 4.8793e-04\nLoss = 1.0232e-02, PNorm = 69.6643, GNorm = 0.5815, lr_0 = 5.0345e-04\nLoss = 1.0232e-02, PNorm = 69.6643, GNorm = 0.5815, lr_0 = 5.0345e-04\n\n\r 90%|████████▉ | 26/29 [00:01&lt;00:00, 14.08it/s]Loss = 9.7028e-03, PNorm = 69.6695, GNorm = 0.3955, lr_0 = 5.1897e-04\nLoss = 9.7028e-03, PNorm = 69.6695, GNorm = 0.3955, lr_0 = 5.1897e-04\nLoss = 1.1550e-02, PNorm = 69.6750, GNorm = 1.0927, lr_0 = 5.3448e-04\nLoss = 1.1550e-02, PNorm = 69.6750, GNorm = 1.0927, lr_0 = 5.3448e-04\n\n\r 97%|█████████▋| 28/29 [00:01&lt;00:00, 14.09it/s]Loss = 9.9707e-03, PNorm = 69.6818, GNorm = 0.4365, lr_0 = 5.5000e-04\nLoss = 9.9707e-03, PNorm = 69.6818, GNorm = 0.4365, lr_0 = 5.5000e-04\n\n\r100%|██████████| 29/29 [00:02&lt;00:00, 14.02it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 46.49it/s]Validation auc = 0.733357\nValidation auc = 0.733357\n\r  2%|▏         | 1/50 [00:06&lt;05:34,  6.83s/it]Epoch 1\nEpoch 1\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 1.1120e-02, PNorm = 69.6871, GNorm = 1.4379, lr_0 = 5.6552e-04\nLoss = 1.1120e-02, PNorm = 69.6871, GNorm = 1.4379, lr_0 = 5.6552e-04\nLoss = 9.0310e-03, PNorm = 69.6933, GNorm = 0.6990, lr_0 = 5.8103e-04\nLoss = 9.0310e-03, PNorm = 69.6933, GNorm = 0.6990, lr_0 = 5.8103e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:01, 13.66it/s]Loss = 9.1156e-03, PNorm = 69.7004, GNorm = 0.3525, lr_0 = 5.9655e-04\nLoss = 9.1156e-03, PNorm = 69.7004, GNorm = 0.3525, lr_0 = 5.9655e-04\nLoss = 1.1102e-02, PNorm = 69.7078, GNorm = 1.2830, lr_0 = 6.1207e-04\nLoss = 1.1102e-02, PNorm = 69.7078, GNorm = 1.2830, lr_0 = 6.1207e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:01, 13.72it/s]Loss = 9.1973e-03, PNorm = 69.7158, GNorm = 0.4010, lr_0 = 6.2759e-04\nLoss = 9.1973e-03, PNorm = 69.7158, GNorm = 0.4010, lr_0 = 6.2759e-04\nLoss = 1.1357e-02, PNorm = 69.7234, GNorm = 0.6791, lr_0 = 6.4310e-04\nLoss = 1.1357e-02, PNorm = 69.7234, GNorm = 0.6791, lr_0 = 6.4310e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:01, 13.88it/s]Loss = 7.9498e-03, PNorm = 69.7314, GNorm = 0.6444, lr_0 = 6.5862e-04\nLoss = 7.9498e-03, PNorm = 69.7314, GNorm = 0.6444, lr_0 = 6.5862e-04\nLoss = 9.7114e-03, PNorm = 69.7400, GNorm = 0.4233, lr_0 = 6.7414e-04\nLoss = 9.7114e-03, PNorm = 69.7400, GNorm = 0.4233, lr_0 = 6.7414e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:01, 13.92it/s]Loss = 9.4590e-03, PNorm = 69.7493, GNorm = 0.2697, lr_0 = 6.8966e-04\nLoss = 9.4590e-03, PNorm = 69.7493, GNorm = 0.2697, lr_0 = 6.8966e-04\nLoss = 1.0339e-02, PNorm = 69.7590, GNorm = 0.4691, lr_0 = 7.0517e-04\nLoss = 1.0339e-02, PNorm = 69.7590, GNorm = 0.4691, lr_0 = 7.0517e-04\n\n\r 34%|███▍      | 10/29 [00:00&lt;00:01, 13.93it/s]Loss = 9.9646e-03, PNorm = 69.7694, GNorm = 0.5267, lr_0 = 7.2069e-04\nLoss = 9.9646e-03, PNorm = 69.7694, GNorm = 0.5267, lr_0 = 7.2069e-04\nLoss = 1.0038e-02, PNorm = 69.7785, GNorm = 0.7016, lr_0 = 7.3621e-04\nLoss = 1.0038e-02, PNorm = 69.7785, GNorm = 0.7016, lr_0 = 7.3621e-04\n\n\r 41%|████▏     | 12/29 [00:00&lt;00:01, 13.89it/s]Loss = 9.8371e-03, PNorm = 69.7869, GNorm = 0.9314, lr_0 = 7.5172e-04\nLoss = 9.8371e-03, PNorm = 69.7869, GNorm = 0.9314, lr_0 = 7.5172e-04\nLoss = 1.0092e-02, PNorm = 69.7963, GNorm = 1.1630, lr_0 = 7.6724e-04\nLoss = 1.0092e-02, PNorm = 69.7963, GNorm = 1.1630, lr_0 = 7.6724e-04\n\n\r 48%|████▊     | 14/29 [00:01&lt;00:01, 13.93it/s]Loss = 9.3124e-03, PNorm = 69.8071, GNorm = 0.6556, lr_0 = 7.8276e-04\nLoss = 9.3124e-03, PNorm = 69.8071, GNorm = 0.6556, lr_0 = 7.8276e-04\nLoss = 9.0759e-03, PNorm = 69.8193, GNorm = 0.2932, lr_0 = 7.9828e-04\nLoss = 9.0759e-03, PNorm = 69.8193, GNorm = 0.2932, lr_0 = 7.9828e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:00, 13.96it/s]Loss = 9.3961e-03, PNorm = 69.8299, GNorm = 1.3442, lr_0 = 8.1379e-04\nLoss = 9.3961e-03, PNorm = 69.8299, GNorm = 1.3442, lr_0 = 8.1379e-04\nLoss = 1.0251e-02, PNorm = 69.8403, GNorm = 0.7736, lr_0 = 8.2931e-04\nLoss = 1.0251e-02, PNorm = 69.8403, GNorm = 0.7736, lr_0 = 8.2931e-04\n\n\r 62%|██████▏   | 18/29 [00:01&lt;00:00, 14.00it/s]Loss = 9.2330e-03, PNorm = 69.8520, GNorm = 0.3840, lr_0 = 8.4483e-04\nLoss = 9.2330e-03, PNorm = 69.8520, GNorm = 0.3840, lr_0 = 8.4483e-04\nLoss = 7.4436e-03, PNorm = 69.8647, GNorm = 0.1473, lr_0 = 8.6034e-04\nLoss = 7.4436e-03, PNorm = 69.8647, GNorm = 0.1473, lr_0 = 8.6034e-04\n\n\r 69%|██████▉   | 20/29 [00:01&lt;00:00, 14.14it/s]Loss = 1.0415e-02, PNorm = 69.8751, GNorm = 1.2431, lr_0 = 8.7586e-04\nLoss = 1.0415e-02, PNorm = 69.8751, GNorm = 1.2431, lr_0 = 8.7586e-04\nLoss = 1.2384e-02, PNorm = 69.8853, GNorm = 1.3000, lr_0 = 8.9138e-04\nLoss = 1.2384e-02, PNorm = 69.8853, GNorm = 1.3000, lr_0 = 8.9138e-04\n\n\r 76%|███████▌  | 22/29 [00:01&lt;00:00, 14.17it/s]Loss = 1.0792e-02, PNorm = 69.8978, GNorm = 0.6602, lr_0 = 9.0690e-04\nLoss = 1.0792e-02, PNorm = 69.8978, GNorm = 0.6602, lr_0 = 9.0690e-04\nLoss = 1.0771e-02, PNorm = 69.9127, GNorm = 0.1534, lr_0 = 9.2241e-04\nLoss = 1.0771e-02, PNorm = 69.9127, GNorm = 0.1534, lr_0 = 9.2241e-04\n\n\r 83%|████████▎ | 24/29 [00:01&lt;00:00, 14.11it/s]Loss = 1.0729e-02, PNorm = 69.9286, GNorm = 0.4181, lr_0 = 9.3793e-04\nLoss = 1.0729e-02, PNorm = 69.9286, GNorm = 0.4181, lr_0 = 9.3793e-04\nLoss = 1.1158e-02, PNorm = 69.9451, GNorm = 0.3613, lr_0 = 9.5345e-04\nLoss = 1.1158e-02, PNorm = 69.9451, GNorm = 0.3613, lr_0 = 9.5345e-04\n\n\r 90%|████████▉ | 26/29 [00:01&lt;00:00, 14.22it/s]Loss = 1.1085e-02, PNorm = 69.9625, GNorm = 0.3798, lr_0 = 9.6897e-04\nLoss = 1.1085e-02, PNorm = 69.9625, GNorm = 0.3798, lr_0 = 9.6897e-04\nLoss = 1.1526e-02, PNorm = 69.9804, GNorm = 0.2232, lr_0 = 9.8448e-04\nLoss = 1.1526e-02, PNorm = 69.9804, GNorm = 0.2232, lr_0 = 9.8448e-04\n\n\r 97%|█████████▋| 28/29 [00:01&lt;00:00, 14.22it/s]Loss = 1.0623e-02, PNorm = 69.9993, GNorm = 0.3217, lr_0 = 1.0000e-03\nLoss = 1.0623e-02, PNorm = 69.9993, GNorm = 0.3217, lr_0 = 1.0000e-03\n\n\r100%|██████████| 29/29 [00:02&lt;00:00, 14.08it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 46.82it/s]Validation auc = 0.808647\nValidation auc = 0.808647\n\r  4%|▍         | 2/50 [00:13&lt;05:28,  6.85s/it]Epoch 2\nEpoch 2\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 9.8468e-03, PNorm = 70.0201, GNorm = 0.2908, lr_0 = 9.9835e-04\nLoss = 9.8468e-03, PNorm = 70.0201, GNorm = 0.2908, lr_0 = 9.9835e-04\nLoss = 1.0902e-02, PNorm = 70.0414, GNorm = 0.2719, lr_0 = 9.9670e-04\nLoss = 1.0902e-02, PNorm = 70.0414, GNorm = 0.2719, lr_0 = 9.9670e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:01, 14.27it/s]Loss = 1.1795e-02, PNorm = 70.0603, GNorm = 0.5256, lr_0 = 9.9505e-04\nLoss = 1.1795e-02, PNorm = 70.0603, GNorm = 0.5256, lr_0 = 9.9505e-04\nLoss = 9.2817e-03, PNorm = 70.0800, GNorm = 0.1393, lr_0 = 9.9341e-04\nLoss = 9.2817e-03, PNorm = 70.0800, GNorm = 0.1393, lr_0 = 9.9341e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:01, 14.16it/s]Loss = 1.0255e-02, PNorm = 70.1006, GNorm = 0.1719, lr_0 = 9.9176e-04\nLoss = 1.0255e-02, PNorm = 70.1006, GNorm = 0.1719, lr_0 = 9.9176e-04\nLoss = 8.7316e-03, PNorm = 70.1219, GNorm = 0.2429, lr_0 = 9.9012e-04\nLoss = 8.7316e-03, PNorm = 70.1219, GNorm = 0.2429, lr_0 = 9.9012e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:01, 14.18it/s]Loss = 1.1169e-02, PNorm = 70.1436, GNorm = 0.2244, lr_0 = 9.8849e-04\nLoss = 1.1169e-02, PNorm = 70.1436, GNorm = 0.2244, lr_0 = 9.8849e-04\nLoss = 8.5201e-03, PNorm = 70.1662, GNorm = 0.2551, lr_0 = 9.8685e-04\nLoss = 8.5201e-03, PNorm = 70.1662, GNorm = 0.2551, lr_0 = 9.8685e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:01, 14.12it/s]Loss = 8.4133e-03, PNorm = 70.1904, GNorm = 0.2242, lr_0 = 9.8522e-04\nLoss = 8.4133e-03, PNorm = 70.1904, GNorm = 0.2242, lr_0 = 9.8522e-04\nLoss = 7.8474e-03, PNorm = 70.2163, GNorm = 0.2578, lr_0 = 9.8359e-04\nLoss = 7.8474e-03, PNorm = 70.2163, GNorm = 0.2578, lr_0 = 9.8359e-04\n\n\r 34%|███▍      | 10/29 [00:00&lt;00:01, 13.22it/s]Loss = 1.0545e-02, PNorm = 70.2415, GNorm = 0.8202, lr_0 = 9.8197e-04\nLoss = 1.0545e-02, PNorm = 70.2415, GNorm = 0.8202, lr_0 = 9.8197e-04\nLoss = 8.6489e-03, PNorm = 70.2660, GNorm = 0.2437, lr_0 = 9.8035e-04\nLoss = 8.6489e-03, PNorm = 70.2660, GNorm = 0.2437, lr_0 = 9.8035e-04\n\n\r 41%|████▏     | 12/29 [00:00&lt;00:01, 13.51it/s]Loss = 8.7824e-03, PNorm = 70.2875, GNorm = 0.7462, lr_0 = 9.7873e-04\nLoss = 8.7824e-03, PNorm = 70.2875, GNorm = 0.7462, lr_0 = 9.7873e-04\nLoss = 9.1967e-03, PNorm = 70.3100, GNorm = 0.3140, lr_0 = 9.7711e-04\nLoss = 9.1967e-03, PNorm = 70.3100, GNorm = 0.3140, lr_0 = 9.7711e-04\n\n\r 48%|████▊     | 14/29 [00:01&lt;00:01, 13.76it/s]Loss = 8.7119e-03, PNorm = 70.3353, GNorm = 0.2608, lr_0 = 9.7549e-04\nLoss = 8.7119e-03, PNorm = 70.3353, GNorm = 0.2608, lr_0 = 9.7549e-04\nLoss = 8.3469e-03, PNorm = 70.3611, GNorm = 0.3529, lr_0 = 9.7388e-04\nLoss = 8.3469e-03, PNorm = 70.3611, GNorm = 0.3529, lr_0 = 9.7388e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:00, 13.87it/s]Loss = 9.1695e-03, PNorm = 70.3880, GNorm = 0.1803, lr_0 = 9.7227e-04\nLoss = 9.1695e-03, PNorm = 70.3880, GNorm = 0.1803, lr_0 = 9.7227e-04\nLoss = 1.1242e-02, PNorm = 70.4110, GNorm = 1.9293, lr_0 = 9.7066e-04\nLoss = 1.1242e-02, PNorm = 70.4110, GNorm = 1.9293, lr_0 = 9.7066e-04\n\n\r 62%|██████▏   | 18/29 [00:01&lt;00:00, 13.93it/s]Loss = 1.0457e-02, PNorm = 70.4320, GNorm = 1.1994, lr_0 = 9.6906e-04\nLoss = 1.0457e-02, PNorm = 70.4320, GNorm = 1.1994, lr_0 = 9.6906e-04\nLoss = 1.0682e-02, PNorm = 70.4514, GNorm = 1.4852, lr_0 = 9.6746e-04\nLoss = 1.0682e-02, PNorm = 70.4514, GNorm = 1.4852, lr_0 = 9.6746e-04\n\n\r 69%|██████▉   | 20/29 [00:01&lt;00:00, 14.01it/s]Loss = 7.8800e-03, PNorm = 70.4741, GNorm = 0.3664, lr_0 = 9.6586e-04\nLoss = 7.8800e-03, PNorm = 70.4741, GNorm = 0.3664, lr_0 = 9.6586e-04\nLoss = 8.7060e-03, PNorm = 70.5000, GNorm = 0.4355, lr_0 = 9.6426e-04\nLoss = 8.7060e-03, PNorm = 70.5000, GNorm = 0.4355, lr_0 = 9.6426e-04\n\n\r 76%|███████▌  | 22/29 [00:01&lt;00:00, 14.01it/s]Loss = 8.2584e-03, PNorm = 70.5266, GNorm = 0.4090, lr_0 = 9.6267e-04\nLoss = 8.2584e-03, PNorm = 70.5266, GNorm = 0.4090, lr_0 = 9.6267e-04\nLoss = 8.5228e-03, PNorm = 70.5543, GNorm = 0.5186, lr_0 = 9.6108e-04\nLoss = 8.5228e-03, PNorm = 70.5543, GNorm = 0.5186, lr_0 = 9.6108e-04\n\n\r 83%|████████▎ | 24/29 [00:01&lt;00:00, 14.04it/s]Loss = 8.8701e-03, PNorm = 70.5827, GNorm = 0.4773, lr_0 = 9.5949e-04\nLoss = 8.8701e-03, PNorm = 70.5827, GNorm = 0.4773, lr_0 = 9.5949e-04\nLoss = 7.1510e-03, PNorm = 70.6124, GNorm = 0.3609, lr_0 = 9.5790e-04\nLoss = 7.1510e-03, PNorm = 70.6124, GNorm = 0.3609, lr_0 = 9.5790e-04\n\n\r 90%|████████▉ | 26/29 [00:01&lt;00:00, 14.15it/s]Loss = 8.3007e-03, PNorm = 70.6428, GNorm = 0.2187, lr_0 = 9.5632e-04\nLoss = 8.3007e-03, PNorm = 70.6428, GNorm = 0.2187, lr_0 = 9.5632e-04\nLoss = 1.0330e-02, PNorm = 70.6711, GNorm = 0.2100, lr_0 = 9.5474e-04\nLoss = 1.0330e-02, PNorm = 70.6711, GNorm = 0.2100, lr_0 = 9.5474e-04\n\n\r 97%|█████████▋| 28/29 [00:02&lt;00:00, 14.20it/s]Loss = 9.7587e-03, PNorm = 70.6996, GNorm = 0.3303, lr_0 = 9.5316e-04\nLoss = 9.7587e-03, PNorm = 70.6996, GNorm = 0.3303, lr_0 = 9.5316e-04\n\n\r100%|██████████| 29/29 [00:02&lt;00:00, 13.95it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 47.00it/s]Validation auc = 0.848588\nValidation auc = 0.848588\n\r  6%|▌         | 3/50 [00:20&lt;05:25,  6.92s/it]Epoch 3\nEpoch 3\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 8.5883e-03, PNorm = 70.7279, GNorm = 0.5830, lr_0 = 9.5159e-04\nLoss = 8.5883e-03, PNorm = 70.7279, GNorm = 0.5830, lr_0 = 9.5159e-04\nLoss = 9.2897e-03, PNorm = 70.7555, GNorm = 0.2016, lr_0 = 9.5001e-04\nLoss = 9.2897e-03, PNorm = 70.7555, GNorm = 0.2016, lr_0 = 9.5001e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:01, 14.16it/s]Loss = 7.3759e-03, PNorm = 70.7845, GNorm = 0.1966, lr_0 = 9.4844e-04\nLoss = 7.3759e-03, PNorm = 70.7845, GNorm = 0.1966, lr_0 = 9.4844e-04\nLoss = 9.4769e-03, PNorm = 70.8088, GNorm = 0.4018, lr_0 = 9.4688e-04\nLoss = 9.4769e-03, PNorm = 70.8088, GNorm = 0.4018, lr_0 = 9.4688e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:01, 14.07it/s]Loss = 8.6456e-03, PNorm = 70.8325, GNorm = 1.1220, lr_0 = 9.4531e-04\nLoss = 8.6456e-03, PNorm = 70.8325, GNorm = 1.1220, lr_0 = 9.4531e-04\nLoss = 8.4545e-03, PNorm = 70.8569, GNorm = 0.6538, lr_0 = 9.4375e-04\nLoss = 8.4545e-03, PNorm = 70.8569, GNorm = 0.6538, lr_0 = 9.4375e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:01, 14.06it/s]Loss = 8.5811e-03, PNorm = 70.8812, GNorm = 0.3619, lr_0 = 9.4219e-04\nLoss = 8.5811e-03, PNorm = 70.8812, GNorm = 0.3619, lr_0 = 9.4219e-04\nLoss = 8.1831e-03, PNorm = 70.9057, GNorm = 0.3570, lr_0 = 9.4063e-04\nLoss = 8.1831e-03, PNorm = 70.9057, GNorm = 0.3570, lr_0 = 9.4063e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:01, 14.09it/s]Loss = 8.8298e-03, PNorm = 70.9294, GNorm = 0.4330, lr_0 = 9.3908e-04\nLoss = 8.8298e-03, PNorm = 70.9294, GNorm = 0.4330, lr_0 = 9.3908e-04\nLoss = 9.1087e-03, PNorm = 70.9527, GNorm = 0.3686, lr_0 = 9.3752e-04\nLoss = 9.1087e-03, PNorm = 70.9527, GNorm = 0.3686, lr_0 = 9.3752e-04\n\n\r 34%|███▍      | 10/29 [00:00&lt;00:01, 14.02it/s]Loss = 6.4064e-03, PNorm = 70.9772, GNorm = 0.4986, lr_0 = 9.3598e-04\nLoss = 6.4064e-03, PNorm = 70.9772, GNorm = 0.4986, lr_0 = 9.3598e-04\nLoss = 9.3846e-03, PNorm = 70.9992, GNorm = 0.2227, lr_0 = 9.3443e-04\nLoss = 9.3846e-03, PNorm = 70.9992, GNorm = 0.2227, lr_0 = 9.3443e-04\n\n\r 41%|████▏     | 12/29 [00:00&lt;00:01, 14.06it/s]Loss = 9.6767e-03, PNorm = 71.0199, GNorm = 0.4591, lr_0 = 9.3288e-04\nLoss = 9.6767e-03, PNorm = 71.0199, GNorm = 0.4591, lr_0 = 9.3288e-04\nLoss = 8.2985e-03, PNorm = 71.0400, GNorm = 0.4245, lr_0 = 9.3134e-04\nLoss = 8.2985e-03, PNorm = 71.0400, GNorm = 0.4245, lr_0 = 9.3134e-04\n\n\r 48%|████▊     | 14/29 [00:00&lt;00:01, 14.13it/s]Loss = 8.8543e-03, PNorm = 71.0621, GNorm = 0.3986, lr_0 = 9.2980e-04\nLoss = 8.8543e-03, PNorm = 71.0621, GNorm = 0.3986, lr_0 = 9.2980e-04\nLoss = 7.8510e-03, PNorm = 71.0860, GNorm = 0.2182, lr_0 = 9.2827e-04\nLoss = 7.8510e-03, PNorm = 71.0860, GNorm = 0.2182, lr_0 = 9.2827e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:00, 13.93it/s]Loss = 8.3434e-03, PNorm = 71.1109, GNorm = 0.2307, lr_0 = 9.2673e-04\nLoss = 8.3434e-03, PNorm = 71.1109, GNorm = 0.2307, lr_0 = 9.2673e-04\nLoss = 7.8179e-03, PNorm = 71.1378, GNorm = 0.3701, lr_0 = 9.2520e-04\nLoss = 7.8179e-03, PNorm = 71.1378, GNorm = 0.3701, lr_0 = 9.2520e-04\n\n\r 62%|██████▏   | 18/29 [00:01&lt;00:00, 13.98it/s]Loss = 8.9422e-03, PNorm = 71.1660, GNorm = 0.1859, lr_0 = 9.2367e-04\nLoss = 8.9422e-03, PNorm = 71.1660, GNorm = 0.1859, lr_0 = 9.2367e-04\nLoss = 7.2047e-03, PNorm = 71.1940, GNorm = 0.4163, lr_0 = 9.2214e-04\nLoss = 7.2047e-03, PNorm = 71.1940, GNorm = 0.4163, lr_0 = 9.2214e-04\n\n\r 69%|██████▉   | 20/29 [00:01&lt;00:00, 13.95it/s]Loss = 7.6080e-03, PNorm = 71.2227, GNorm = 0.5470, lr_0 = 9.2062e-04\nLoss = 7.6080e-03, PNorm = 71.2227, GNorm = 0.5470, lr_0 = 9.2062e-04\nLoss = 1.0113e-02, PNorm = 71.2462, GNorm = 0.4601, lr_0 = 9.1910e-04\nLoss = 1.0113e-02, PNorm = 71.2462, GNorm = 0.4601, lr_0 = 9.1910e-04\n\n\r 76%|███████▌  | 22/29 [00:01&lt;00:00, 13.95it/s]Loss = 8.9241e-03, PNorm = 71.2680, GNorm = 0.8609, lr_0 = 9.1758e-04\nLoss = 8.9241e-03, PNorm = 71.2680, GNorm = 0.8609, lr_0 = 9.1758e-04\nLoss = 7.0783e-03, PNorm = 71.2872, GNorm = 0.3245, lr_0 = 9.1606e-04\nLoss = 7.0783e-03, PNorm = 71.2872, GNorm = 0.3245, lr_0 = 9.1606e-04\n\n\r 83%|████████▎ | 24/29 [00:01&lt;00:00, 14.01it/s]Loss = 7.6494e-03, PNorm = 71.3095, GNorm = 0.9394, lr_0 = 9.1455e-04\nLoss = 7.6494e-03, PNorm = 71.3095, GNorm = 0.9394, lr_0 = 9.1455e-04\nLoss = 7.7655e-03, PNorm = 71.3308, GNorm = 0.2765, lr_0 = 9.1304e-04\nLoss = 7.7655e-03, PNorm = 71.3308, GNorm = 0.2765, lr_0 = 9.1304e-04\n\n\r 90%|████████▉ | 26/29 [00:01&lt;00:00, 14.06it/s]Loss = 8.2190e-03, PNorm = 71.3517, GNorm = 0.3564, lr_0 = 9.1153e-04\nLoss = 8.2190e-03, PNorm = 71.3517, GNorm = 0.3564, lr_0 = 9.1153e-04\nLoss = 8.3512e-03, PNorm = 71.3724, GNorm = 0.4299, lr_0 = 9.1002e-04\nLoss = 8.3512e-03, PNorm = 71.3724, GNorm = 0.4299, lr_0 = 9.1002e-04\n\n*** WARNING: skipped 211064 bytes of output ***\n\n\n\r 41%|████▏     | 12/29 [00:00&lt;00:01, 14.25it/s]Loss = 7.9368e-04, PNorm = 85.9292, GNorm = 0.5032, lr_0 = 1.2440e-04\nLoss = 7.9368e-04, PNorm = 85.9292, GNorm = 0.5032, lr_0 = 1.2440e-04\nLoss = 2.8444e-04, PNorm = 85.9314, GNorm = 0.1895, lr_0 = 1.2420e-04\nLoss = 2.8444e-04, PNorm = 85.9314, GNorm = 0.1895, lr_0 = 1.2420e-04\n\n\r 48%|████▊     | 14/29 [00:00&lt;00:01, 14.27it/s]Loss = 3.5893e-04, PNorm = 85.9337, GNorm = 0.2361, lr_0 = 1.2399e-04\nLoss = 3.5893e-04, PNorm = 85.9337, GNorm = 0.2361, lr_0 = 1.2399e-04\nLoss = 1.1085e-03, PNorm = 85.9355, GNorm = 0.5893, lr_0 = 1.2379e-04\nLoss = 1.1085e-03, PNorm = 85.9355, GNorm = 0.5893, lr_0 = 1.2379e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:00, 14.25it/s]Loss = 4.9962e-04, PNorm = 85.9374, GNorm = 0.3454, lr_0 = 1.2358e-04\nLoss = 4.9962e-04, PNorm = 85.9374, GNorm = 0.3454, lr_0 = 1.2358e-04\nLoss = 7.2950e-04, PNorm = 85.9397, GNorm = 0.4899, lr_0 = 1.2338e-04\nLoss = 7.2950e-04, PNorm = 85.9397, GNorm = 0.4899, lr_0 = 1.2338e-04\n\n\r 62%|██████▏   | 18/29 [00:01&lt;00:00, 14.26it/s]Loss = 3.1762e-04, PNorm = 85.9420, GNorm = 0.2704, lr_0 = 1.2317e-04\nLoss = 3.1762e-04, PNorm = 85.9420, GNorm = 0.2704, lr_0 = 1.2317e-04\nLoss = 1.0277e-03, PNorm = 85.9448, GNorm = 0.5891, lr_0 = 1.2297e-04\nLoss = 1.0277e-03, PNorm = 85.9448, GNorm = 0.5891, lr_0 = 1.2297e-04\n\n\r 69%|██████▉   | 20/29 [00:01&lt;00:00, 14.27it/s]Loss = 2.8893e-04, PNorm = 85.9474, GNorm = 0.2509, lr_0 = 1.2277e-04\nLoss = 2.8893e-04, PNorm = 85.9474, GNorm = 0.2509, lr_0 = 1.2277e-04\nLoss = 3.9718e-04, PNorm = 85.9501, GNorm = 0.2555, lr_0 = 1.2256e-04\nLoss = 3.9718e-04, PNorm = 85.9501, GNorm = 0.2555, lr_0 = 1.2256e-04\n\n\r 76%|███████▌  | 22/29 [00:01&lt;00:00, 14.20it/s]Loss = 6.4979e-04, PNorm = 85.9527, GNorm = 0.4738, lr_0 = 1.2236e-04\nLoss = 6.4979e-04, PNorm = 85.9527, GNorm = 0.4738, lr_0 = 1.2236e-04\nLoss = 8.2989e-04, PNorm = 85.9555, GNorm = 0.5098, lr_0 = 1.2216e-04\nLoss = 8.2989e-04, PNorm = 85.9555, GNorm = 0.5098, lr_0 = 1.2216e-04\n\n\r 83%|████████▎ | 24/29 [00:01&lt;00:00, 14.23it/s]Loss = 3.8088e-04, PNorm = 85.9582, GNorm = 0.2261, lr_0 = 1.2196e-04\nLoss = 3.8088e-04, PNorm = 85.9582, GNorm = 0.2261, lr_0 = 1.2196e-04\nLoss = 2.9089e-04, PNorm = 85.9609, GNorm = 0.2985, lr_0 = 1.2176e-04\nLoss = 2.9089e-04, PNorm = 85.9609, GNorm = 0.2985, lr_0 = 1.2176e-04\n\n\r 90%|████████▉ | 26/29 [00:01&lt;00:00, 14.19it/s]Loss = 3.7583e-04, PNorm = 85.9637, GNorm = 0.2323, lr_0 = 1.2155e-04\nLoss = 3.7583e-04, PNorm = 85.9637, GNorm = 0.2323, lr_0 = 1.2155e-04\nLoss = 1.5666e-03, PNorm = 85.9666, GNorm = 0.6662, lr_0 = 1.2135e-04\nLoss = 1.5666e-03, PNorm = 85.9666, GNorm = 0.6662, lr_0 = 1.2135e-04\n\n\r 97%|█████████▋| 28/29 [00:01&lt;00:00, 14.19it/s]Loss = 1.4881e-03, PNorm = 85.9696, GNorm = 0.6626, lr_0 = 1.2115e-04\nLoss = 1.4881e-03, PNorm = 85.9696, GNorm = 0.6626, lr_0 = 1.2115e-04\n\n\r100%|██████████| 29/29 [00:02&lt;00:00, 14.22it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 46.91it/s]Validation auc = 0.908952\nValidation auc = 0.908952\n\r 92%|█████████▏| 46/50 [02:12&lt;00:08,  2.14s/it]Epoch 46\nEpoch 46\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 4.1488e-04, PNorm = 85.9723, GNorm = 0.3274, lr_0 = 1.2095e-04\nLoss = 4.1488e-04, PNorm = 85.9723, GNorm = 0.3274, lr_0 = 1.2095e-04\nLoss = 4.8094e-04, PNorm = 85.9750, GNorm = 0.2600, lr_0 = 1.2075e-04\nLoss = 4.8094e-04, PNorm = 85.9750, GNorm = 0.2600, lr_0 = 1.2075e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:01, 14.13it/s]Loss = 3.6758e-04, PNorm = 85.9776, GNorm = 0.2567, lr_0 = 1.2055e-04\nLoss = 3.6758e-04, PNorm = 85.9776, GNorm = 0.2567, lr_0 = 1.2055e-04\nLoss = 4.3316e-04, PNorm = 85.9804, GNorm = 0.3354, lr_0 = 1.2035e-04\nLoss = 4.3316e-04, PNorm = 85.9804, GNorm = 0.3354, lr_0 = 1.2035e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:01, 14.13it/s]Loss = 7.6703e-04, PNorm = 85.9831, GNorm = 0.2335, lr_0 = 1.2015e-04\nLoss = 7.6703e-04, PNorm = 85.9831, GNorm = 0.2335, lr_0 = 1.2015e-04\nLoss = 4.6581e-04, PNorm = 85.9859, GNorm = 0.4401, lr_0 = 1.1996e-04\nLoss = 4.6581e-04, PNorm = 85.9859, GNorm = 0.4401, lr_0 = 1.1996e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:01, 14.18it/s]Loss = 5.5192e-04, PNorm = 85.9889, GNorm = 0.3366, lr_0 = 1.1976e-04\nLoss = 5.5192e-04, PNorm = 85.9889, GNorm = 0.3366, lr_0 = 1.1976e-04\nLoss = 6.6180e-04, PNorm = 85.9917, GNorm = 0.4122, lr_0 = 1.1956e-04\nLoss = 6.6180e-04, PNorm = 85.9917, GNorm = 0.4122, lr_0 = 1.1956e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:01, 14.26it/s]Loss = 5.8065e-04, PNorm = 85.9944, GNorm = 0.2025, lr_0 = 1.1936e-04\nLoss = 5.8065e-04, PNorm = 85.9944, GNorm = 0.2025, lr_0 = 1.1936e-04\nLoss = 3.9253e-04, PNorm = 85.9970, GNorm = 0.3425, lr_0 = 1.1917e-04\nLoss = 3.9253e-04, PNorm = 85.9970, GNorm = 0.3425, lr_0 = 1.1917e-04\n\n\r 34%|███▍      | 10/29 [00:00&lt;00:01, 14.32it/s]Loss = 4.4251e-04, PNorm = 85.9997, GNorm = 0.2999, lr_0 = 1.1897e-04\nLoss = 4.4251e-04, PNorm = 85.9997, GNorm = 0.2999, lr_0 = 1.1897e-04\nLoss = 5.4667e-04, PNorm = 86.0023, GNorm = 0.2670, lr_0 = 1.1877e-04\nLoss = 5.4667e-04, PNorm = 86.0023, GNorm = 0.2670, lr_0 = 1.1877e-04\n\n\r 41%|████▏     | 12/29 [00:00&lt;00:01, 14.34it/s]Loss = 5.2926e-04, PNorm = 86.0051, GNorm = 0.2626, lr_0 = 1.1858e-04\nLoss = 5.2926e-04, PNorm = 86.0051, GNorm = 0.2626, lr_0 = 1.1858e-04\nLoss = 5.7427e-04, PNorm = 86.0080, GNorm = 0.3672, lr_0 = 1.1838e-04\nLoss = 5.7427e-04, PNorm = 86.0080, GNorm = 0.3672, lr_0 = 1.1838e-04\n\n\r 48%|████▊     | 14/29 [00:00&lt;00:01, 14.15it/s]Loss = 4.4175e-04, PNorm = 86.0105, GNorm = 0.3925, lr_0 = 1.1818e-04\nLoss = 4.4175e-04, PNorm = 86.0105, GNorm = 0.3925, lr_0 = 1.1818e-04\nLoss = 4.2774e-04, PNorm = 86.0128, GNorm = 0.3168, lr_0 = 1.1799e-04\nLoss = 4.2774e-04, PNorm = 86.0128, GNorm = 0.3168, lr_0 = 1.1799e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:00, 14.16it/s]Loss = 3.7384e-04, PNorm = 86.0151, GNorm = 0.2422, lr_0 = 1.1779e-04\nLoss = 3.7384e-04, PNorm = 86.0151, GNorm = 0.2422, lr_0 = 1.1779e-04\nLoss = 4.5118e-04, PNorm = 86.0174, GNorm = 0.2729, lr_0 = 1.1760e-04\nLoss = 4.5118e-04, PNorm = 86.0174, GNorm = 0.2729, lr_0 = 1.1760e-04\n\n\r 62%|██████▏   | 18/29 [00:01&lt;00:00, 14.14it/s]Loss = 1.1547e-03, PNorm = 86.0196, GNorm = 0.4176, lr_0 = 1.1740e-04\nLoss = 1.1547e-03, PNorm = 86.0196, GNorm = 0.4176, lr_0 = 1.1740e-04\nLoss = 3.6361e-04, PNorm = 86.0220, GNorm = 0.3212, lr_0 = 1.1721e-04\nLoss = 3.6361e-04, PNorm = 86.0220, GNorm = 0.3212, lr_0 = 1.1721e-04\n\n\r 69%|██████▉   | 20/29 [00:01&lt;00:00, 14.13it/s]Loss = 3.0109e-04, PNorm = 86.0244, GNorm = 0.1617, lr_0 = 1.1702e-04\nLoss = 3.0109e-04, PNorm = 86.0244, GNorm = 0.1617, lr_0 = 1.1702e-04\nLoss = 3.4278e-04, PNorm = 86.0270, GNorm = 0.2786, lr_0 = 1.1682e-04\nLoss = 3.4278e-04, PNorm = 86.0270, GNorm = 0.2786, lr_0 = 1.1682e-04\n\n\r 76%|███████▌  | 22/29 [00:01&lt;00:00, 14.11it/s]Loss = 7.2773e-04, PNorm = 86.0292, GNorm = 0.4649, lr_0 = 1.1663e-04\nLoss = 7.2773e-04, PNorm = 86.0292, GNorm = 0.4649, lr_0 = 1.1663e-04\nLoss = 3.3456e-04, PNorm = 86.0313, GNorm = 0.2749, lr_0 = 1.1644e-04\nLoss = 3.3456e-04, PNorm = 86.0313, GNorm = 0.2749, lr_0 = 1.1644e-04\n\n\r 83%|████████▎ | 24/29 [00:01&lt;00:00, 14.20it/s]Loss = 2.8580e-04, PNorm = 86.0335, GNorm = 0.1772, lr_0 = 1.1624e-04\nLoss = 2.8580e-04, PNorm = 86.0335, GNorm = 0.1772, lr_0 = 1.1624e-04\nLoss = 3.1196e-04, PNorm = 86.0355, GNorm = 0.2236, lr_0 = 1.1605e-04\nLoss = 3.1196e-04, PNorm = 86.0355, GNorm = 0.2236, lr_0 = 1.1605e-04\n\n\r 90%|████████▉ | 26/29 [00:01&lt;00:00, 14.15it/s]Loss = 8.0422e-04, PNorm = 86.0370, GNorm = 0.5366, lr_0 = 1.1586e-04\nLoss = 8.0422e-04, PNorm = 86.0370, GNorm = 0.5366, lr_0 = 1.1586e-04\nLoss = 4.4877e-04, PNorm = 86.0384, GNorm = 0.2983, lr_0 = 1.1567e-04\nLoss = 4.4877e-04, PNorm = 86.0384, GNorm = 0.2983, lr_0 = 1.1567e-04\n\n\r 97%|█████████▋| 28/29 [00:01&lt;00:00, 14.23it/s]Loss = 1.4577e-03, PNorm = 86.0394, GNorm = 0.7389, lr_0 = 1.1548e-04\nLoss = 1.4577e-03, PNorm = 86.0394, GNorm = 0.7389, lr_0 = 1.1548e-04\n\n\r100%|██████████| 29/29 [00:02&lt;00:00, 14.21it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 47.01it/s]Validation auc = 0.906145\nValidation auc = 0.906145\n\r 94%|█████████▍| 47/50 [02:14&lt;00:06,  2.13s/it]Epoch 47\nEpoch 47\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 4.1388e-04, PNorm = 86.0407, GNorm = 0.2242, lr_0 = 1.1529e-04\nLoss = 4.1388e-04, PNorm = 86.0407, GNorm = 0.2242, lr_0 = 1.1529e-04\nLoss = 2.8994e-04, PNorm = 86.0421, GNorm = 0.2683, lr_0 = 1.1510e-04\nLoss = 2.8994e-04, PNorm = 86.0421, GNorm = 0.2683, lr_0 = 1.1510e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:01, 14.39it/s]Loss = 3.4168e-04, PNorm = 86.0437, GNorm = 0.2943, lr_0 = 1.1491e-04\nLoss = 3.4168e-04, PNorm = 86.0437, GNorm = 0.2943, lr_0 = 1.1491e-04\nLoss = 5.1367e-04, PNorm = 86.0454, GNorm = 0.3151, lr_0 = 1.1472e-04\nLoss = 5.1367e-04, PNorm = 86.0454, GNorm = 0.3151, lr_0 = 1.1472e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:01, 14.28it/s]Loss = 5.5178e-04, PNorm = 86.0472, GNorm = 0.3475, lr_0 = 1.1453e-04\nLoss = 5.5178e-04, PNorm = 86.0472, GNorm = 0.3475, lr_0 = 1.1453e-04\nLoss = 2.0060e-04, PNorm = 86.0489, GNorm = 0.1587, lr_0 = 1.1434e-04\nLoss = 2.0060e-04, PNorm = 86.0489, GNorm = 0.1587, lr_0 = 1.1434e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:01, 14.35it/s]Loss = 5.9556e-04, PNorm = 86.0508, GNorm = 0.4283, lr_0 = 1.1415e-04\nLoss = 5.9556e-04, PNorm = 86.0508, GNorm = 0.4283, lr_0 = 1.1415e-04\nLoss = 4.4502e-04, PNorm = 86.0528, GNorm = 0.3089, lr_0 = 1.1396e-04\nLoss = 4.4502e-04, PNorm = 86.0528, GNorm = 0.3089, lr_0 = 1.1396e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:01, 14.33it/s]Loss = 3.2658e-04, PNorm = 86.0549, GNorm = 0.2332, lr_0 = 1.1377e-04\nLoss = 3.2658e-04, PNorm = 86.0549, GNorm = 0.2332, lr_0 = 1.1377e-04\nLoss = 8.6016e-04, PNorm = 86.0574, GNorm = 0.3786, lr_0 = 1.1358e-04\nLoss = 8.6016e-04, PNorm = 86.0574, GNorm = 0.3786, lr_0 = 1.1358e-04\n\n\r 34%|███▍      | 10/29 [00:00&lt;00:01, 14.32it/s]Loss = 2.6674e-04, PNorm = 86.0598, GNorm = 0.2591, lr_0 = 1.1340e-04\nLoss = 2.6674e-04, PNorm = 86.0598, GNorm = 0.2591, lr_0 = 1.1340e-04\nLoss = 4.3867e-04, PNorm = 86.0620, GNorm = 0.3400, lr_0 = 1.1321e-04\nLoss = 4.3867e-04, PNorm = 86.0620, GNorm = 0.3400, lr_0 = 1.1321e-04\n\n\r 41%|████▏     | 12/29 [00:00&lt;00:01, 14.32it/s]Loss = 5.5303e-04, PNorm = 86.0639, GNorm = 0.4986, lr_0 = 1.1302e-04\nLoss = 5.5303e-04, PNorm = 86.0639, GNorm = 0.4986, lr_0 = 1.1302e-04\nLoss = 6.5493e-04, PNorm = 86.0658, GNorm = 0.3501, lr_0 = 1.1283e-04\nLoss = 6.5493e-04, PNorm = 86.0658, GNorm = 0.3501, lr_0 = 1.1283e-04\n\n\r 48%|████▊     | 14/29 [00:00&lt;00:01, 14.34it/s]Loss = 7.4181e-04, PNorm = 86.0680, GNorm = 0.5450, lr_0 = 1.1265e-04\nLoss = 7.4181e-04, PNorm = 86.0680, GNorm = 0.5450, lr_0 = 1.1265e-04\nLoss = 7.9312e-04, PNorm = 86.0705, GNorm = 0.5940, lr_0 = 1.1246e-04\nLoss = 7.9312e-04, PNorm = 86.0705, GNorm = 0.5940, lr_0 = 1.1246e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:00, 14.34it/s]Loss = 7.6699e-04, PNorm = 86.0734, GNorm = 0.5304, lr_0 = 1.1228e-04\nLoss = 7.6699e-04, PNorm = 86.0734, GNorm = 0.5304, lr_0 = 1.1228e-04\nLoss = 8.8327e-04, PNorm = 86.0765, GNorm = 0.4792, lr_0 = 1.1209e-04\nLoss = 8.8327e-04, PNorm = 86.0765, GNorm = 0.4792, lr_0 = 1.1209e-04\n\n\r 62%|██████▏   | 18/29 [00:01&lt;00:00, 14.30it/s]Loss = 1.7099e-04, PNorm = 86.0796, GNorm = 0.1407, lr_0 = 1.1191e-04\nLoss = 1.7099e-04, PNorm = 86.0796, GNorm = 0.1407, lr_0 = 1.1191e-04\nLoss = 2.8338e-04, PNorm = 86.0825, GNorm = 0.1784, lr_0 = 1.1172e-04\nLoss = 2.8338e-04, PNorm = 86.0825, GNorm = 0.1784, lr_0 = 1.1172e-04\n\n\r 69%|██████▉   | 20/29 [00:01&lt;00:00, 14.32it/s]Loss = 7.1970e-04, PNorm = 86.0848, GNorm = 0.5683, lr_0 = 1.1154e-04\nLoss = 7.1970e-04, PNorm = 86.0848, GNorm = 0.5683, lr_0 = 1.1154e-04\nLoss = 1.0140e-03, PNorm = 86.0869, GNorm = 0.7331, lr_0 = 1.1135e-04\nLoss = 1.0140e-03, PNorm = 86.0869, GNorm = 0.7331, lr_0 = 1.1135e-04\n\n\r 76%|███████▌  | 22/29 [00:01&lt;00:00, 14.28it/s]Loss = 7.0410e-04, PNorm = 86.0893, GNorm = 0.5130, lr_0 = 1.1117e-04\nLoss = 7.0410e-04, PNorm = 86.0893, GNorm = 0.5130, lr_0 = 1.1117e-04\nLoss = 2.7423e-04, PNorm = 86.0916, GNorm = 0.2352, lr_0 = 1.1098e-04\nLoss = 2.7423e-04, PNorm = 86.0916, GNorm = 0.2352, lr_0 = 1.1098e-04\n\n\r 83%|████████▎ | 24/29 [00:01&lt;00:00, 14.23it/s]Loss = 1.5795e-04, PNorm = 86.0939, GNorm = 0.1224, lr_0 = 1.1080e-04\nLoss = 1.5795e-04, PNorm = 86.0939, GNorm = 0.1224, lr_0 = 1.1080e-04\nLoss = 4.7282e-04, PNorm = 86.0962, GNorm = 0.3055, lr_0 = 1.1062e-04\nLoss = 4.7282e-04, PNorm = 86.0962, GNorm = 0.3055, lr_0 = 1.1062e-04\n\n\r 90%|████████▉ | 26/29 [00:01&lt;00:00, 14.06it/s]Loss = 2.6608e-04, PNorm = 86.0985, GNorm = 0.2947, lr_0 = 1.1043e-04\nLoss = 2.6608e-04, PNorm = 86.0985, GNorm = 0.2947, lr_0 = 1.1043e-04\nLoss = 3.0961e-04, PNorm = 86.1009, GNorm = 0.2285, lr_0 = 1.1025e-04\nLoss = 3.0961e-04, PNorm = 86.1009, GNorm = 0.2285, lr_0 = 1.1025e-04\n\n\r 97%|█████████▋| 28/29 [00:01&lt;00:00, 14.19it/s]Loss = 7.3839e-04, PNorm = 86.1035, GNorm = 0.5372, lr_0 = 1.1007e-04\nLoss = 7.3839e-04, PNorm = 86.1035, GNorm = 0.5372, lr_0 = 1.1007e-04\n\n\r100%|██████████| 29/29 [00:02&lt;00:00, 14.24it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 46.74it/s]Validation auc = 0.910983\nValidation auc = 0.910983\n\r 96%|█████████▌| 48/50 [02:16&lt;00:04,  2.13s/it]Epoch 48\nEpoch 48\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 3.7843e-04, PNorm = 86.1060, GNorm = 0.2788, lr_0 = 1.0989e-04\nLoss = 3.7843e-04, PNorm = 86.1060, GNorm = 0.2788, lr_0 = 1.0989e-04\nLoss = 2.3625e-04, PNorm = 86.1084, GNorm = 0.1784, lr_0 = 1.0971e-04\nLoss = 2.3625e-04, PNorm = 86.1084, GNorm = 0.1784, lr_0 = 1.0971e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:01, 14.01it/s]Loss = 4.6163e-04, PNorm = 86.1108, GNorm = 0.2675, lr_0 = 1.0952e-04\nLoss = 4.6163e-04, PNorm = 86.1108, GNorm = 0.2675, lr_0 = 1.0952e-04\nLoss = 5.4280e-04, PNorm = 86.1131, GNorm = 0.3447, lr_0 = 1.0934e-04\nLoss = 5.4280e-04, PNorm = 86.1131, GNorm = 0.3447, lr_0 = 1.0934e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:01, 14.07it/s]Loss = 4.6259e-04, PNorm = 86.1154, GNorm = 0.3741, lr_0 = 1.0916e-04\nLoss = 4.6259e-04, PNorm = 86.1154, GNorm = 0.3741, lr_0 = 1.0916e-04\nLoss = 3.7603e-04, PNorm = 86.1176, GNorm = 0.2731, lr_0 = 1.0898e-04\nLoss = 3.7603e-04, PNorm = 86.1176, GNorm = 0.2731, lr_0 = 1.0898e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:01, 14.11it/s]Loss = 3.0036e-04, PNorm = 86.1196, GNorm = 0.2984, lr_0 = 1.0880e-04\nLoss = 3.0036e-04, PNorm = 86.1196, GNorm = 0.2984, lr_0 = 1.0880e-04\nLoss = 4.5516e-04, PNorm = 86.1215, GNorm = 0.2026, lr_0 = 1.0862e-04\nLoss = 4.5516e-04, PNorm = 86.1215, GNorm = 0.2026, lr_0 = 1.0862e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:01, 14.11it/s]Loss = 5.9320e-04, PNorm = 86.1238, GNorm = 0.4570, lr_0 = 1.0844e-04\nLoss = 5.9320e-04, PNorm = 86.1238, GNorm = 0.4570, lr_0 = 1.0844e-04\nLoss = 5.0500e-04, PNorm = 86.1264, GNorm = 0.4609, lr_0 = 1.0826e-04\nLoss = 5.0500e-04, PNorm = 86.1264, GNorm = 0.4609, lr_0 = 1.0826e-04\n\n\r 34%|███▍      | 10/29 [00:00&lt;00:01, 14.19it/s]Loss = 6.4035e-04, PNorm = 86.1292, GNorm = 0.4097, lr_0 = 1.0808e-04\nLoss = 6.4035e-04, PNorm = 86.1292, GNorm = 0.4097, lr_0 = 1.0808e-04\nLoss = 2.0495e-04, PNorm = 86.1319, GNorm = 0.1721, lr_0 = 1.0791e-04\nLoss = 2.0495e-04, PNorm = 86.1319, GNorm = 0.1721, lr_0 = 1.0791e-04\n\n\r 41%|████▏     | 12/29 [00:00&lt;00:01, 14.22it/s]Loss = 2.5434e-04, PNorm = 86.1344, GNorm = 0.2877, lr_0 = 1.0773e-04\nLoss = 2.5434e-04, PNorm = 86.1344, GNorm = 0.2877, lr_0 = 1.0773e-04\nLoss = 3.4473e-04, PNorm = 86.1368, GNorm = 0.2478, lr_0 = 1.0755e-04\nLoss = 3.4473e-04, PNorm = 86.1368, GNorm = 0.2478, lr_0 = 1.0755e-04\n\n\r 48%|████▊     | 14/29 [00:00&lt;00:01, 14.20it/s]Loss = 3.8540e-04, PNorm = 86.1389, GNorm = 0.4878, lr_0 = 1.0737e-04\nLoss = 3.8540e-04, PNorm = 86.1389, GNorm = 0.4878, lr_0 = 1.0737e-04\nLoss = 7.0634e-04, PNorm = 86.1408, GNorm = 0.5255, lr_0 = 1.0719e-04\nLoss = 7.0634e-04, PNorm = 86.1408, GNorm = 0.5255, lr_0 = 1.0719e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:00, 14.21it/s]Loss = 1.2603e-03, PNorm = 86.1420, GNorm = 0.9146, lr_0 = 1.0702e-04\nLoss = 1.2603e-03, PNorm = 86.1420, GNorm = 0.9146, lr_0 = 1.0702e-04\nLoss = 3.8142e-04, PNorm = 86.1431, GNorm = 0.3028, lr_0 = 1.0684e-04\nLoss = 3.8142e-04, PNorm = 86.1431, GNorm = 0.3028, lr_0 = 1.0684e-04\n\n\r 62%|██████▏   | 18/29 [00:01&lt;00:00, 14.22it/s]Loss = 5.5433e-04, PNorm = 86.1445, GNorm = 0.4618, lr_0 = 1.0666e-04\nLoss = 5.5433e-04, PNorm = 86.1445, GNorm = 0.4618, lr_0 = 1.0666e-04\nLoss = 7.6726e-04, PNorm = 86.1463, GNorm = 0.5951, lr_0 = 1.0649e-04\nLoss = 7.6726e-04, PNorm = 86.1463, GNorm = 0.5951, lr_0 = 1.0649e-04\n\n\r 69%|██████▉   | 20/29 [00:01&lt;00:00, 14.31it/s]Loss = 5.5331e-04, PNorm = 86.1483, GNorm = 0.4666, lr_0 = 1.0631e-04\nLoss = 5.5331e-04, PNorm = 86.1483, GNorm = 0.4666, lr_0 = 1.0631e-04\nLoss = 3.6159e-04, PNorm = 86.1504, GNorm = 0.2463, lr_0 = 1.0614e-04\nLoss = 3.6159e-04, PNorm = 86.1504, GNorm = 0.2463, lr_0 = 1.0614e-04\n\n\r 76%|███████▌  | 22/29 [00:01&lt;00:00, 14.13it/s]Loss = 4.6176e-04, PNorm = 86.1521, GNorm = 0.4011, lr_0 = 1.0596e-04\nLoss = 4.6176e-04, PNorm = 86.1521, GNorm = 0.4011, lr_0 = 1.0596e-04\nLoss = 9.8334e-04, PNorm = 86.1540, GNorm = 0.4849, lr_0 = 1.0579e-04\nLoss = 9.8334e-04, PNorm = 86.1540, GNorm = 0.4849, lr_0 = 1.0579e-04\n\n\r 83%|████████▎ | 24/29 [00:01&lt;00:00, 14.13it/s]Loss = 5.2585e-04, PNorm = 86.1561, GNorm = 0.3179, lr_0 = 1.0561e-04\nLoss = 5.2585e-04, PNorm = 86.1561, GNorm = 0.3179, lr_0 = 1.0561e-04\nLoss = 1.2019e-03, PNorm = 86.1585, GNorm = 0.6957, lr_0 = 1.0544e-04\nLoss = 1.2019e-03, PNorm = 86.1585, GNorm = 0.6957, lr_0 = 1.0544e-04\n\n\r 90%|████████▉ | 26/29 [00:01&lt;00:00, 14.08it/s]Loss = 3.1660e-04, PNorm = 86.1607, GNorm = 0.1744, lr_0 = 1.0526e-04\nLoss = 3.1660e-04, PNorm = 86.1607, GNorm = 0.1744, lr_0 = 1.0526e-04\nLoss = 7.8901e-04, PNorm = 86.1633, GNorm = 0.3707, lr_0 = 1.0509e-04\nLoss = 7.8901e-04, PNorm = 86.1633, GNorm = 0.3707, lr_0 = 1.0509e-04\n\n\r 97%|█████████▋| 28/29 [00:01&lt;00:00, 14.20it/s]Loss = 5.3607e-04, PNorm = 86.1656, GNorm = 0.5228, lr_0 = 1.0491e-04\nLoss = 5.3607e-04, PNorm = 86.1656, GNorm = 0.5228, lr_0 = 1.0491e-04\n\n\r100%|██████████| 29/29 [00:02&lt;00:00, 14.19it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 45.07it/s]Validation auc = 0.907055\nValidation auc = 0.907055\n\r 98%|█████████▊| 49/50 [02:19&lt;00:02,  2.13s/it]Epoch 49\nEpoch 49\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 3.6507e-04, PNorm = 86.1679, GNorm = 0.3055, lr_0 = 1.0474e-04\nLoss = 3.6507e-04, PNorm = 86.1679, GNorm = 0.3055, lr_0 = 1.0474e-04\nLoss = 4.5969e-04, PNorm = 86.1701, GNorm = 0.1844, lr_0 = 1.0457e-04\nLoss = 4.5969e-04, PNorm = 86.1701, GNorm = 0.1844, lr_0 = 1.0457e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:01, 14.47it/s]Loss = 2.6157e-04, PNorm = 86.1721, GNorm = 0.3345, lr_0 = 1.0439e-04\nLoss = 2.6157e-04, PNorm = 86.1721, GNorm = 0.3345, lr_0 = 1.0439e-04\nLoss = 5.4812e-04, PNorm = 86.1742, GNorm = 0.4060, lr_0 = 1.0422e-04\nLoss = 5.4812e-04, PNorm = 86.1742, GNorm = 0.4060, lr_0 = 1.0422e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:01, 14.41it/s]Loss = 6.7098e-04, PNorm = 86.1762, GNorm = 0.4370, lr_0 = 1.0405e-04\nLoss = 6.7098e-04, PNorm = 86.1762, GNorm = 0.4370, lr_0 = 1.0405e-04\nLoss = 3.5352e-04, PNorm = 86.1784, GNorm = 0.1916, lr_0 = 1.0388e-04\nLoss = 3.5352e-04, PNorm = 86.1784, GNorm = 0.1916, lr_0 = 1.0388e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:01, 14.40it/s]Loss = 2.4283e-04, PNorm = 86.1808, GNorm = 0.2340, lr_0 = 1.0371e-04\nLoss = 2.4283e-04, PNorm = 86.1808, GNorm = 0.2340, lr_0 = 1.0371e-04\nLoss = 2.0668e-04, PNorm = 86.1831, GNorm = 0.1939, lr_0 = 1.0353e-04\nLoss = 2.0668e-04, PNorm = 86.1831, GNorm = 0.1939, lr_0 = 1.0353e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:01, 14.22it/s]Loss = 2.9198e-04, PNorm = 86.1854, GNorm = 0.2522, lr_0 = 1.0336e-04\nLoss = 2.9198e-04, PNorm = 86.1854, GNorm = 0.2522, lr_0 = 1.0336e-04\nLoss = 7.8083e-04, PNorm = 86.1882, GNorm = 0.5092, lr_0 = 1.0319e-04\nLoss = 7.8083e-04, PNorm = 86.1882, GNorm = 0.5092, lr_0 = 1.0319e-04\n\n\r 34%|███▍      | 10/29 [00:00&lt;00:01, 14.23it/s]Loss = 6.5555e-04, PNorm = 86.1909, GNorm = 0.5583, lr_0 = 1.0302e-04\nLoss = 6.5555e-04, PNorm = 86.1909, GNorm = 0.5583, lr_0 = 1.0302e-04\nLoss = 2.6812e-04, PNorm = 86.1935, GNorm = 0.2950, lr_0 = 1.0285e-04\nLoss = 2.6812e-04, PNorm = 86.1935, GNorm = 0.2950, lr_0 = 1.0285e-04\n\n\r 41%|████▏     | 12/29 [00:00&lt;00:01, 14.21it/s]Loss = 2.8271e-04, PNorm = 86.1959, GNorm = 0.2376, lr_0 = 1.0268e-04\nLoss = 2.8271e-04, PNorm = 86.1959, GNorm = 0.2376, lr_0 = 1.0268e-04\nLoss = 3.7010e-04, PNorm = 86.1985, GNorm = 0.3274, lr_0 = 1.0251e-04\nLoss = 3.7010e-04, PNorm = 86.1985, GNorm = 0.3274, lr_0 = 1.0251e-04\n\n\r 48%|████▊     | 14/29 [00:00&lt;00:01, 14.27it/s]Loss = 2.9240e-04, PNorm = 86.2008, GNorm = 0.2102, lr_0 = 1.0234e-04\nLoss = 2.9240e-04, PNorm = 86.2008, GNorm = 0.2102, lr_0 = 1.0234e-04\nLoss = 4.6026e-04, PNorm = 86.2029, GNorm = 0.3586, lr_0 = 1.0217e-04\nLoss = 4.6026e-04, PNorm = 86.2029, GNorm = 0.3586, lr_0 = 1.0217e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:00, 14.32it/s]Loss = 5.3947e-04, PNorm = 86.2052, GNorm = 0.3640, lr_0 = 1.0200e-04\nLoss = 5.3947e-04, PNorm = 86.2052, GNorm = 0.3640, lr_0 = 1.0200e-04\nLoss = 6.5802e-04, PNorm = 86.2070, GNorm = 0.5176, lr_0 = 1.0184e-04\nLoss = 6.5802e-04, PNorm = 86.2070, GNorm = 0.5176, lr_0 = 1.0184e-04\n\n\r 62%|██████▏   | 18/29 [00:01&lt;00:00, 14.26it/s]Loss = 6.1893e-04, PNorm = 86.2088, GNorm = 0.3977, lr_0 = 1.0167e-04\nLoss = 6.1893e-04, PNorm = 86.2088, GNorm = 0.3977, lr_0 = 1.0167e-04\nLoss = 3.0461e-04, PNorm = 86.2107, GNorm = 0.1932, lr_0 = 1.0150e-04\nLoss = 3.0461e-04, PNorm = 86.2107, GNorm = 0.1932, lr_0 = 1.0150e-04\n\n\r 69%|██████▉   | 20/29 [00:01&lt;00:00, 14.25it/s]Loss = 8.7737e-04, PNorm = 86.2124, GNorm = 0.5505, lr_0 = 1.0133e-04\nLoss = 8.7737e-04, PNorm = 86.2124, GNorm = 0.5505, lr_0 = 1.0133e-04\nLoss = 5.9355e-04, PNorm = 86.2144, GNorm = 0.3562, lr_0 = 1.0116e-04\nLoss = 5.9355e-04, PNorm = 86.2144, GNorm = 0.3562, lr_0 = 1.0116e-04\n\n\r 76%|███████▌  | 22/29 [00:01&lt;00:00, 14.25it/s]Loss = 7.1599e-04, PNorm = 86.2163, GNorm = 0.4260, lr_0 = 1.0100e-04\nLoss = 7.1599e-04, PNorm = 86.2163, GNorm = 0.4260, lr_0 = 1.0100e-04\nLoss = 2.9581e-04, PNorm = 86.2181, GNorm = 0.3210, lr_0 = 1.0083e-04\nLoss = 2.9581e-04, PNorm = 86.2181, GNorm = 0.3210, lr_0 = 1.0083e-04\n\n\r 83%|████████▎ | 24/29 [00:01&lt;00:00, 14.27it/s]Loss = 3.8390e-04, PNorm = 86.2198, GNorm = 0.2582, lr_0 = 1.0066e-04\nLoss = 3.8390e-04, PNorm = 86.2198, GNorm = 0.2582, lr_0 = 1.0066e-04\nLoss = 3.3218e-04, PNorm = 86.2216, GNorm = 0.2284, lr_0 = 1.0050e-04\nLoss = 3.3218e-04, PNorm = 86.2216, GNorm = 0.2284, lr_0 = 1.0050e-04\n\n\r 90%|████████▉ | 26/29 [00:01&lt;00:00, 14.32it/s]Loss = 3.8273e-04, PNorm = 86.2235, GNorm = 0.1924, lr_0 = 1.0033e-04\nLoss = 3.8273e-04, PNorm = 86.2235, GNorm = 0.1924, lr_0 = 1.0033e-04\nLoss = 5.6117e-04, PNorm = 86.2255, GNorm = 0.4606, lr_0 = 1.0017e-04\nLoss = 5.6117e-04, PNorm = 86.2255, GNorm = 0.4606, lr_0 = 1.0017e-04\n\n\r 97%|█████████▋| 28/29 [00:01&lt;00:00, 14.26it/s]Loss = 3.6707e-04, PNorm = 86.2276, GNorm = 0.2119, lr_0 = 1.0000e-04\nLoss = 3.6707e-04, PNorm = 86.2276, GNorm = 0.2119, lr_0 = 1.0000e-04\n\n\r100%|██████████| 29/29 [00:02&lt;00:00, 14.26it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 46.74it/s]Validation auc = 0.909702\nValidation auc = 0.909702\n\r100%|██████████| 50/50 [02:21&lt;00:00,  2.13s/it]\nModel 0 best validation auc = 0.918501 on epoch 11\nModel 0 best validation auc = 0.918501 on epoch 11\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\nMoving model to cuda\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\r100%|██████████| 4/4 [00:00&lt;00:00, 45.62it/s]\nModel 0 test auc = 0.854834\nModel 0 test auc = 0.854834\nEnsemble test auc = 0.854834\nEnsemble test auc = 0.854834\n1-fold cross validation\n1-fold cross validation\nSeed 13 ==&gt; test auc = 0.854834\nSeed 13 ==&gt; test auc = 0.854834\nOverall test auc = 0.854834 +/- 0.000000\nOverall test auc = 0.854834 +/- 0.000000\nOut[14]: (0.8548336983537269, 0.0)\n</div>"]}}],"execution_count":49},{"cell_type":"markdown","source":["#### Classification external vs internal pEC 7.6 w/ feat SLogP"],"metadata":{}},{"cell_type":"code","source":["%sh cat /dbfs/FileStore/chemprop/JAK/configs/bin76_ext_Feat_SLogP.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{\n    &#34;depth&#34;: 5,\n    &#34;dropout&#34;: 0.15000000000000002,\n    &#34;ffn_num_layers&#34;: 1,\n    &#34;hidden_size&#34;: 2300\n}</div>"]}}],"execution_count":51},{"cell_type":"code","source":["#LEO + PubCHem 0.8718238387132906\nparser = ArgumentParser()\nadd_train_args(parser)\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-8396_bin76.csv'),\n                          '--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtrain-8396.csv'),\n                          '--dataset_type','classification',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext_Feat_SLogP'),\n                          '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'),\n                          '--separate_val_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPval-182.csv'),\n                          '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                          '--separate_test_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtest-183.csv'),\n                          '--log_frequency','1',\n                          '--depth','5',\n                          '--dropout','0.15',\n                          '--ffn_num_layers','1',\n                          '--hidden_size','2300',\n                          '--epochs','50'\n                         ,'--seed','13'\n                          ])\nmodify_train_args(args)\nlogger = create_logger(name='train', save_dir=args.save_dir, quiet=args.quiet)\n\ncross_validate(args, logger)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-8396_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 5,\n &#39;dropout&#39;: 0.15,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 50,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2300,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2300,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_ext_Feat_SLogP/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-8396_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 5,\n &#39;dropout&#39;: 0.15,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 50,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2300,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2300,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_ext_Feat_SLogP/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-8396.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\n\r  0%|          | 0/8396 [00:00&lt;?, ?it/s]\r  4%|▍         | 323/8396 [00:00&lt;00:02, 3227.97it/s]\r  8%|▊         | 649/8396 [00:00&lt;00:02, 3236.07it/s]\r 12%|█▏        | 966/8396 [00:00&lt;00:02, 3215.55it/s]\r 15%|█▌        | 1283/8396 [00:00&lt;00:02, 3200.43it/s]\r 19%|█▉        | 1595/8396 [00:00&lt;00:02, 3175.43it/s]\r 22%|██▏       | 1876/8396 [00:00&lt;00:02, 3050.28it/s]\r 26%|██▌       | 2145/8396 [00:00&lt;00:02, 2898.02it/s]\r 29%|██▉       | 2426/8396 [00:00&lt;00:02, 2869.59it/s]\r 32%|███▏      | 2714/8396 [00:00&lt;00:01, 2872.07it/s]\r 36%|███▌      | 3037/8396 [00:01&lt;00:01, 2968.42it/s]\r 40%|███▉      | 3327/8396 [00:01&lt;00:01, 2880.72it/s]\r 43%|████▎     | 3611/8396 [00:01&lt;00:01, 2724.28it/s]\r 47%|████▋     | 3930/8396 [00:01&lt;00:01, 2846.31it/s]\r 51%|█████     | 4259/8396 [00:01&lt;00:01, 2966.27it/s]\r 54%|█████▍    | 4558/8396 [00:01&lt;00:02, 1827.09it/s]\r 58%|█████▊    | 4889/8396 [00:01&lt;00:01, 2110.46it/s]\r 62%|██████▏   | 5210/8396 [00:01&lt;00:01, 2351.15it/s]\r 66%|██████▌   | 5549/8396 [00:02&lt;00:01, 2588.00it/s]\r 70%|██████▉   | 5859/8396 [00:02&lt;00:00, 2721.15it/s]\r 73%|███████▎  | 6162/8396 [00:02&lt;00:00, 2777.67it/s]\r 77%|███████▋  | 6461/8396 [00:02&lt;00:00, 2797.19it/s]\r 81%|████████  | 6768/8396 [00:02&lt;00:00, 2873.15it/s]\r 84%|████████▍ | 7082/8396 [00:02&lt;00:00, 2947.04it/s]\r 88%|████████▊ | 7420/8396 [00:02&lt;00:00, 3061.85it/s]\r 92%|█████████▏| 7757/8396 [00:02&lt;00:00, 3143.76it/s]\r 96%|█████████▋| 8089/8396 [00:02&lt;00:00, 3194.63it/s]\r100%|██████████| 8396/8396 [00:02&lt;00:00, 2840.61it/s]\nNumber of tasks = 4\nNumber of tasks = 4\nSplitting data with seed 13\nSplitting data with seed 13\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 3025.18it/s]\n\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\r100%|██████████| 182/182 [00:00&lt;00:00, 3077.12it/s]\nClass sizes\nClass sizes\nJAK1 0: 33.55%, 1: 66.45%\nJAK1 0: 33.55%, 1: 66.45%\nJAK2 0: 60.19%, 1: 39.81%\nJAK2 0: 60.19%, 1: 39.81%\nJAK3 0: 77.42%, 1: 22.58%\nJAK3 0: 77.42%, 1: 22.58%\nTYK2 0: 92.62%, 1: 7.38%\nTYK2 0: 92.62%, 1: 7.38%\nTotal size = 8,396 | train size = 8,396 | val size = 182 | test size = 183\nTotal size = 8,396 | train size = 8,396 | val size = 182 | test size = 183\nBuilding model 0\nBuilding model 0\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=2300, bias=False)\n      (W_h): Linear(in_features=2300, out_features=2300, bias=False)\n      (W_o): Linear(in_features=2433, out_features=2300, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15)\n    (1): Linear(in_features=2301, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=2300, bias=False)\n      (W_h): Linear(in_features=2300, out_features=2300, bias=False)\n      (W_o): Linear(in_features=2433, out_features=2300, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15)\n    (1): Linear(in_features=2301, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 11,235,508\nNumber of parameters = 11,235,508\nMoving model to cuda\nMoving model to cuda\n\r  0%|          | 0/50 [00:00&lt;?, ?it/s]Epoch 0\nEpoch 0\n\n\r  0%|          | 0/167 [00:00&lt;?, ?it/s]Loss = 1.3589e-02, PNorm = 70.3377, GNorm = 2.0700, lr_0 = 1.0269e-04\nLoss = 1.3589e-02, PNorm = 70.3377, GNorm = 2.0700, lr_0 = 1.0269e-04\n\n\r  1%|          | 1/167 [00:00&lt;00:29,  5.64it/s]Loss = 1.1235e-02, PNorm = 70.3386, GNorm = 1.1596, lr_0 = 1.0539e-04\nLoss = 1.1235e-02, PNorm = 70.3386, GNorm = 1.1596, lr_0 = 1.0539e-04\n\n\r  1%|          | 2/167 [00:00&lt;00:29,  5.66it/s]Loss = 1.5616e-02, PNorm = 70.3390, GNorm = 3.3504, lr_0 = 1.0808e-04\nLoss = 1.5616e-02, PNorm = 70.3390, GNorm = 3.3504, lr_0 = 1.0808e-04\n\n\r  2%|▏         | 3/167 [00:00&lt;00:29,  5.48it/s]Loss = 1.3781e-02, PNorm = 70.3392, GNorm = 2.1913, lr_0 = 1.1078e-04\nLoss = 1.3781e-02, PNorm = 70.3392, GNorm = 2.1913, lr_0 = 1.1078e-04\n\n\r  2%|▏         | 4/167 [00:00&lt;00:29,  5.55it/s]Loss = 1.2346e-02, PNorm = 70.3396, GNorm = 1.2379, lr_0 = 1.1347e-04\nLoss = 1.2346e-02, PNorm = 70.3396, GNorm = 1.2379, lr_0 = 1.1347e-04\n\n\r  3%|▎         | 5/167 [00:00&lt;00:28,  5.64it/s]Loss = 1.1681e-02, PNorm = 70.3402, GNorm = 0.8859, lr_0 = 1.1617e-04\nLoss = 1.1681e-02, PNorm = 70.3402, GNorm = 0.8859, lr_0 = 1.1617e-04\n\n\r  4%|▎         | 6/167 [00:01&lt;00:28,  5.67it/s]Loss = 1.1839e-02, PNorm = 70.3410, GNorm = 1.1676, lr_0 = 1.1886e-04\nLoss = 1.1839e-02, PNorm = 70.3410, GNorm = 1.1676, lr_0 = 1.1886e-04\n\n\r  4%|▍         | 7/167 [00:01&lt;00:28,  5.68it/s]Loss = 1.1707e-02, PNorm = 70.3419, GNorm = 1.5411, lr_0 = 1.2156e-04\nLoss = 1.1707e-02, PNorm = 70.3419, GNorm = 1.5411, lr_0 = 1.2156e-04\n\n\r  5%|▍         | 8/167 [00:01&lt;00:28,  5.63it/s]Loss = 1.2846e-02, PNorm = 70.3428, GNorm = 0.9452, lr_0 = 1.2425e-04\nLoss = 1.2846e-02, PNorm = 70.3428, GNorm = 0.9452, lr_0 = 1.2425e-04\n\n\r  5%|▌         | 9/167 [00:01&lt;00:28,  5.52it/s]Loss = 1.1439e-02, PNorm = 70.3438, GNorm = 1.0132, lr_0 = 1.2695e-04\nLoss = 1.1439e-02, PNorm = 70.3438, GNorm = 1.0132, lr_0 = 1.2695e-04\n\n\r  6%|▌         | 10/167 [00:01&lt;00:27,  5.62it/s]Loss = 1.0672e-02, PNorm = 70.3450, GNorm = 1.0004, lr_0 = 1.2964e-04\nLoss = 1.0672e-02, PNorm = 70.3450, GNorm = 1.0004, lr_0 = 1.2964e-04\n\n\r  7%|▋         | 11/167 [00:01&lt;00:27,  5.63it/s]Loss = 1.1047e-02, PNorm = 70.3463, GNorm = 0.6100, lr_0 = 1.3234e-04\nLoss = 1.1047e-02, PNorm = 70.3463, GNorm = 0.6100, lr_0 = 1.3234e-04\n\n\r  7%|▋         | 12/167 [00:02&lt;00:27,  5.58it/s]Loss = 1.1871e-02, PNorm = 70.3475, GNorm = 0.8359, lr_0 = 1.3503e-04\nLoss = 1.1871e-02, PNorm = 70.3475, GNorm = 0.8359, lr_0 = 1.3503e-04\n\n\r  8%|▊         | 13/167 [00:02&lt;00:26,  5.73it/s]Loss = 1.1445e-02, PNorm = 70.3488, GNorm = 0.5986, lr_0 = 1.3772e-04\nLoss = 1.1445e-02, PNorm = 70.3488, GNorm = 0.5986, lr_0 = 1.3772e-04\n\n\r  8%|▊         | 14/167 [00:02&lt;00:26,  5.74it/s]Loss = 1.0362e-02, PNorm = 70.3501, GNorm = 0.3416, lr_0 = 1.4042e-04\nLoss = 1.0362e-02, PNorm = 70.3501, GNorm = 0.3416, lr_0 = 1.4042e-04\n\n\r  9%|▉         | 15/167 [00:02&lt;00:26,  5.68it/s]Loss = 1.5034e-02, PNorm = 70.3513, GNorm = 1.5940, lr_0 = 1.4311e-04\nLoss = 1.5034e-02, PNorm = 70.3513, GNorm = 1.5940, lr_0 = 1.4311e-04\n\n\r 10%|▉         | 16/167 [00:02&lt;00:27,  5.55it/s]Loss = 9.7782e-03, PNorm = 70.3525, GNorm = 0.5412, lr_0 = 1.4581e-04\nLoss = 9.7782e-03, PNorm = 70.3525, GNorm = 0.5412, lr_0 = 1.4581e-04\n\n\r 10%|█         | 17/167 [00:03&lt;00:27,  5.51it/s]Loss = 1.0948e-02, PNorm = 70.3538, GNorm = 0.3698, lr_0 = 1.4850e-04\nLoss = 1.0948e-02, PNorm = 70.3538, GNorm = 0.3698, lr_0 = 1.4850e-04\n\n\r 11%|█         | 18/167 [00:03&lt;00:40,  3.70it/s]Loss = 1.1608e-02, PNorm = 70.3550, GNorm = 0.4366, lr_0 = 1.5120e-04\nLoss = 1.1608e-02, PNorm = 70.3550, GNorm = 0.4366, lr_0 = 1.5120e-04\n\n\r 11%|█▏        | 19/167 [00:03&lt;00:35,  4.15it/s]Loss = 1.0314e-02, PNorm = 70.3562, GNorm = 0.5374, lr_0 = 1.5389e-04\nLoss = 1.0314e-02, PNorm = 70.3562, GNorm = 0.5374, lr_0 = 1.5389e-04\n\n\r 12%|█▏        | 20/167 [00:03&lt;00:32,  4.50it/s]Loss = 9.9419e-03, PNorm = 70.3575, GNorm = 0.4174, lr_0 = 1.5659e-04\nLoss = 9.9419e-03, PNorm = 70.3575, GNorm = 0.4174, lr_0 = 1.5659e-04\n\n\r 13%|█▎        | 21/167 [00:04&lt;00:30,  4.78it/s]Loss = 1.1728e-02, PNorm = 70.3588, GNorm = 0.5793, lr_0 = 1.5928e-04\nLoss = 1.1728e-02, PNorm = 70.3588, GNorm = 0.5793, lr_0 = 1.5928e-04\n\n\r 13%|█▎        | 22/167 [00:04&lt;00:29,  4.95it/s]Loss = 1.1217e-02, PNorm = 70.3602, GNorm = 0.4389, lr_0 = 1.6198e-04\nLoss = 1.1217e-02, PNorm = 70.3602, GNorm = 0.4389, lr_0 = 1.6198e-04\n\n\r 14%|█▍        | 23/167 [00:04&lt;00:27,  5.16it/s]Loss = 1.1545e-02, PNorm = 70.3614, GNorm = 0.5517, lr_0 = 1.6467e-04\nLoss = 1.1545e-02, PNorm = 70.3614, GNorm = 0.5517, lr_0 = 1.6467e-04\n\n\r 14%|█▍        | 24/167 [00:04&lt;00:27,  5.28it/s]Loss = 1.1484e-02, PNorm = 70.3626, GNorm = 0.6715, lr_0 = 1.6737e-04\nLoss = 1.1484e-02, PNorm = 70.3626, GNorm = 0.6715, lr_0 = 1.6737e-04\n\n\r 15%|█▍        | 25/167 [00:04&lt;00:26,  5.39it/s]Loss = 1.0231e-02, PNorm = 70.3640, GNorm = 0.2685, lr_0 = 1.7006e-04\nLoss = 1.0231e-02, PNorm = 70.3640, GNorm = 0.2685, lr_0 = 1.7006e-04\n\n\r 16%|█▌        | 26/167 [00:04&lt;00:25,  5.43it/s]Loss = 1.0551e-02, PNorm = 70.3654, GNorm = 0.5255, lr_0 = 1.7275e-04\nLoss = 1.0551e-02, PNorm = 70.3654, GNorm = 0.5255, lr_0 = 1.7275e-04\n\n\r 16%|█▌        | 27/167 [00:05&lt;00:25,  5.45it/s]Loss = 1.1268e-02, PNorm = 70.3670, GNorm = 0.4731, lr_0 = 1.7545e-04\nLoss = 1.1268e-02, PNorm = 70.3670, GNorm = 0.4731, lr_0 = 1.7545e-04\n\n\r 17%|█▋        | 28/167 [00:05&lt;00:25,  5.47it/s]Loss = 1.0175e-02, PNorm = 70.3686, GNorm = 0.3507, lr_0 = 1.7814e-04\nLoss = 1.0175e-02, PNorm = 70.3686, GNorm = 0.3507, lr_0 = 1.7814e-04\n\n\r 17%|█▋        | 29/167 [00:05&lt;00:24,  5.58it/s]Loss = 9.1636e-03, PNorm = 70.3702, GNorm = 0.6460, lr_0 = 1.8084e-04\nLoss = 9.1636e-03, PNorm = 70.3702, GNorm = 0.6460, lr_0 = 1.8084e-04\n\n\r 18%|█▊        | 30/167 [00:05&lt;00:24,  5.61it/s]Loss = 1.1618e-02, PNorm = 70.3718, GNorm = 0.7287, lr_0 = 1.8353e-04\nLoss = 1.1618e-02, PNorm = 70.3718, GNorm = 0.7287, lr_0 = 1.8353e-04\n\n\r 19%|█▊        | 31/167 [00:05&lt;00:24,  5.61it/s]Loss = 1.1004e-02, PNorm = 70.3733, GNorm = 0.4638, lr_0 = 1.8623e-04\nLoss = 1.1004e-02, PNorm = 70.3733, GNorm = 0.4638, lr_0 = 1.8623e-04\n\n\r 19%|█▉        | 32/167 [00:06&lt;00:24,  5.60it/s]Loss = 1.1296e-02, PNorm = 70.3750, GNorm = 0.7352, lr_0 = 1.8892e-04\nLoss = 1.1296e-02, PNorm = 70.3750, GNorm = 0.7352, lr_0 = 1.8892e-04\n\n\r 20%|█▉        | 33/167 [00:06&lt;00:23,  5.60it/s]Loss = 1.2762e-02, PNorm = 70.3766, GNorm = 0.5921, lr_0 = 1.9162e-04\nLoss = 1.2762e-02, PNorm = 70.3766, GNorm = 0.5921, lr_0 = 1.9162e-04\n\n\r 20%|██        | 34/167 [00:06&lt;00:24,  5.39it/s]Loss = 1.0632e-02, PNorm = 70.3782, GNorm = 0.2791, lr_0 = 1.9431e-04\nLoss = 1.0632e-02, PNorm = 70.3782, GNorm = 0.2791, lr_0 = 1.9431e-04\n\n\r 21%|██        | 35/167 [00:06&lt;00:24,  5.46it/s]Loss = 1.0158e-02, PNorm = 70.3799, GNorm = 0.3530, lr_0 = 1.9701e-04\nLoss = 1.0158e-02, PNorm = 70.3799, GNorm = 0.3530, lr_0 = 1.9701e-04\n\n\r 22%|██▏       | 36/167 [00:06&lt;00:24,  5.44it/s]Loss = 1.2524e-02, PNorm = 70.3816, GNorm = 0.4468, lr_0 = 1.9970e-04\nLoss = 1.2524e-02, PNorm = 70.3816, GNorm = 0.4468, lr_0 = 1.9970e-04\n\n\r 22%|██▏       | 37/167 [00:06&lt;00:24,  5.21it/s]Loss = 1.1792e-02, PNorm = 70.3832, GNorm = 0.4716, lr_0 = 2.0240e-04\nLoss = 1.1792e-02, PNorm = 70.3832, GNorm = 0.4716, lr_0 = 2.0240e-04\n\n\r 23%|██▎       | 38/167 [00:07&lt;00:23,  5.42it/s]Loss = 1.0683e-02, PNorm = 70.3847, GNorm = 0.4782, lr_0 = 2.0509e-04\nLoss = 1.0683e-02, PNorm = 70.3847, GNorm = 0.4782, lr_0 = 2.0509e-04\n\n\r 23%|██▎       | 39/167 [00:07&lt;00:37,  3.40it/s]Loss = 1.2117e-02, PNorm = 70.3861, GNorm = 0.5245, lr_0 = 2.0778e-04\nLoss = 1.2117e-02, PNorm = 70.3861, GNorm = 0.5245, lr_0 = 2.0778e-04\n\n\r 24%|██▍       | 40/167 [00:07&lt;00:33,  3.83it/s]Loss = 1.1851e-02, PNorm = 70.3874, GNorm = 0.5611, lr_0 = 2.1048e-04\nLoss = 1.1851e-02, PNorm = 70.3874, GNorm = 0.5611, lr_0 = 2.1048e-04\n\n\r 25%|██▍       | 41/167 [00:08&lt;00:29,  4.29it/s]Loss = 1.0672e-02, PNorm = 70.3889, GNorm = 0.3043, lr_0 = 2.1317e-04\nLoss = 1.0672e-02, PNorm = 70.3889, GNorm = 0.3043, lr_0 = 2.1317e-04\n\n\r 25%|██▌       | 42/167 [00:08&lt;00:26,  4.70it/s]Loss = 1.1130e-02, PNorm = 70.3908, GNorm = 0.5182, lr_0 = 2.1587e-04\nLoss = 1.1130e-02, PNorm = 70.3908, GNorm = 0.5182, lr_0 = 2.1587e-04\n\n\r 26%|██▌       | 43/167 [00:08&lt;00:25,  4.90it/s]Loss = 1.2254e-02, PNorm = 70.3930, GNorm = 0.9528, lr_0 = 2.1856e-04\nLoss = 1.2254e-02, PNorm = 70.3930, GNorm = 0.9528, lr_0 = 2.1856e-04\n\n\r 26%|██▋       | 44/167 [00:08&lt;00:24,  5.01it/s]Loss = 1.0420e-02, PNorm = 70.3957, GNorm = 0.9068, lr_0 = 2.2126e-04\nLoss = 1.0420e-02, PNorm = 70.3957, GNorm = 0.9068, lr_0 = 2.2126e-04\n\n\r 27%|██▋       | 45/167 [00:08&lt;00:24,  5.05it/s]Loss = 1.3286e-02, PNorm = 70.3985, GNorm = 0.3842, lr_0 = 2.2395e-04\nLoss = 1.3286e-02, PNorm = 70.3985, GNorm = 0.3842, lr_0 = 2.2395e-04\n\n\r 28%|██▊       | 46/167 [00:08&lt;00:23,  5.11it/s]Loss = 1.2051e-02, PNorm = 70.4016, GNorm = 0.6037, lr_0 = 2.2665e-04\nLoss = 1.2051e-02, PNorm = 70.4016, GNorm = 0.6037, lr_0 = 2.2665e-04\n\n\r 28%|██▊       | 47/167 [00:09&lt;00:23,  5.20it/s]Loss = 1.0927e-02, PNorm = 70.4050, GNorm = 0.2476, lr_0 = 2.2934e-04\nLoss = 1.0927e-02, PNorm = 70.4050, GNorm = 0.2476, lr_0 = 2.2934e-04\n\n\r 29%|██▊       | 48/167 [00:09&lt;00:22,  5.29it/s]Loss = 1.1179e-02, PNorm = 70.4079, GNorm = 0.4800, lr_0 = 2.3204e-04\nLoss = 1.1179e-02, PNorm = 70.4079, GNorm = 0.4800, lr_0 = 2.3204e-04\n\n\r 29%|██▉       | 49/167 [00:09&lt;00:22,  5.35it/s]Loss = 1.0678e-02, PNorm = 70.4108, GNorm = 0.4077, lr_0 = 2.3473e-04\nLoss = 1.0678e-02, PNorm = 70.4108, GNorm = 0.4077, lr_0 = 2.3473e-04\n\n\r 30%|██▉       | 50/167 [00:09&lt;00:22,  5.30it/s]Loss = 1.2126e-02, PNorm = 70.4132, GNorm = 0.6834, lr_0 = 2.3743e-04\nLoss = 1.2126e-02, PNorm = 70.4132, GNorm = 0.6834, lr_0 = 2.3743e-04\n\n\r 31%|███       | 51/167 [00:09&lt;00:21,  5.40it/s]Loss = 1.2605e-02, PNorm = 70.4151, GNorm = 0.8918, lr_0 = 2.4012e-04\nLoss = 1.2605e-02, PNorm = 70.4151, GNorm = 0.8918, lr_0 = 2.4012e-04\n\n\r 31%|███       | 52/167 [00:10&lt;00:21,  5.43it/s]Loss = 1.1660e-02, PNorm = 70.4170, GNorm = 0.7140, lr_0 = 2.4281e-04\nLoss = 1.1660e-02, PNorm = 70.4170, GNorm = 0.7140, lr_0 = 2.4281e-04\n\n\r 32%|███▏      | 53/167 [00:10&lt;00:20,  5.60it/s]Loss = 9.7596e-03, PNorm = 70.4194, GNorm = 0.3596, lr_0 = 2.4551e-04\nLoss = 9.7596e-03, PNorm = 70.4194, GNorm = 0.3596, lr_0 = 2.4551e-04\n\n\r 32%|███▏      | 54/167 [00:10&lt;00:20,  5.57it/s]Loss = 1.0597e-02, PNorm = 70.4219, GNorm = 0.2852, lr_0 = 2.4820e-04\nLoss = 1.0597e-02, PNorm = 70.4219, GNorm = 0.2852, lr_0 = 2.4820e-04\n\n\r 33%|███▎      | 55/167 [00:10&lt;00:19,  5.68it/s]Loss = 1.0525e-02, PNorm = 70.4248, GNorm = 0.2786, lr_0 = 2.5090e-04\nLoss = 1.0525e-02, PNorm = 70.4248, GNorm = 0.2786, lr_0 = 2.5090e-04\n\n\r 34%|███▎      | 56/167 [00:10&lt;00:19,  5.65it/s]Loss = 1.1538e-02, PNorm = 70.4281, GNorm = 0.4047, lr_0 = 2.5359e-04\nLoss = 1.1538e-02, PNorm = 70.4281, GNorm = 0.4047, lr_0 = 2.5359e-04\n\n\r 34%|███▍      | 57/167 [00:10&lt;00:20,  5.45it/s]Loss = 1.0028e-02, PNorm = 70.4316, GNorm = 0.4773, lr_0 = 2.5629e-04\nLoss = 1.0028e-02, PNorm = 70.4316, GNorm = 0.4773, lr_0 = 2.5629e-04\n\n\r 35%|███▍      | 58/167 [00:11&lt;00:19,  5.54it/s]Loss = 1.2306e-02, PNorm = 70.4353, GNorm = 0.5586, lr_0 = 2.5898e-04\nLoss = 1.2306e-02, PNorm = 70.4353, GNorm = 0.5586, lr_0 = 2.5898e-04\n\n\r 35%|███▌      | 59/167 [00:11&lt;00:19,  5.54it/s]Loss = 1.1246e-02, PNorm = 70.4393, GNorm = 0.5793, lr_0 = 2.6168e-04\nLoss = 1.1246e-02, PNorm = 70.4393, GNorm = 0.5793, lr_0 = 2.6168e-04\n\n\r 36%|███▌      | 60/167 [00:11&lt;00:19,  5.47it/s]Loss = 1.0085e-02, PNorm = 70.4434, GNorm = 0.2196, lr_0 = 2.6437e-04\nLoss = 1.0085e-02, PNorm = 70.4434, GNorm = 0.2196, lr_0 = 2.6437e-04\n\n\r 37%|███▋      | 61/167 [00:12&lt;00:33,  3.13it/s]Loss = 1.1420e-02, PNorm = 70.4477, GNorm = 0.3338, lr_0 = 2.6707e-04\nLoss = 1.1420e-02, PNorm = 70.4477, GNorm = 0.3338, lr_0 = 2.6707e-04\n\n\r 37%|███▋      | 62/167 [00:12&lt;00:28,  3.63it/s]Loss = 1.0303e-02, PNorm = 70.4523, GNorm = 0.3310, lr_0 = 2.6976e-04\nLoss = 1.0303e-02, PNorm = 70.4523, GNorm = 0.3310, lr_0 = 2.6976e-04\n\n\r 38%|███▊      | 63/167 [00:12&lt;00:25,  4.02it/s]Loss = 1.1794e-02, PNorm = 70.4568, GNorm = 0.4234, lr_0 = 2.7246e-04\nLoss = 1.1794e-02, PNorm = 70.4568, GNorm = 0.4234, lr_0 = 2.7246e-04\n\n\r 38%|███▊      | 64/167 [00:12&lt;00:23,  4.40it/s]Loss = 1.1624e-02, PNorm = 70.4612, GNorm = 0.4737, lr_0 = 2.7515e-04\nLoss = 1.1624e-02, PNorm = 70.4612, GNorm = 0.4737, lr_0 = 2.7515e-04\n\n\r 39%|███▉      | 65/167 [00:12&lt;00:21,  4.68it/s]Loss = 1.0484e-02, PNorm = 70.4662, GNorm = 0.4387, lr_0 = 2.7784e-04\nLoss = 1.0484e-02, PNorm = 70.4662, GNorm = 0.4387, lr_0 = 2.7784e-04\n\n\r 40%|███▉      | 66/167 [00:13&lt;00:20,  4.86it/s]Loss = 1.0172e-02, PNorm = 70.4713, GNorm = 0.2993, lr_0 = 2.8054e-04\nLoss = 1.0172e-02, PNorm = 70.4713, GNorm = 0.2993, lr_0 = 2.8054e-04\n\n\r 40%|████      | 67/167 [00:13&lt;00:19,  5.05it/s]Loss = 1.0377e-02, PNorm = 70.4763, GNorm = 0.3323, lr_0 = 2.8323e-04\nLoss = 1.0377e-02, PNorm = 70.4763, GNorm = 0.3323, lr_0 = 2.8323e-04\n\n\r 41%|████      | 68/167 [00:13&lt;00:19,  5.09it/s]Loss = 1.0793e-02, PNorm = 70.4809, GNorm = 0.3847, lr_0 = 2.8593e-04\nLoss = 1.0793e-02, PNorm = 70.4809, GNorm = 0.3847, lr_0 = 2.8593e-04\n\n\r 41%|████▏     | 69/167 [00:13&lt;00:19,  5.13it/s]Loss = 1.0947e-02, PNorm = 70.4855, GNorm = 0.2624, lr_0 = 2.8862e-04\nLoss = 1.0947e-02, PNorm = 70.4855, GNorm = 0.2624, lr_0 = 2.8862e-04\n\n\r 42%|████▏     | 70/167 [00:13&lt;00:18,  5.24it/s]Loss = 1.1178e-02, PNorm = 70.4900, GNorm = 0.6476, lr_0 = 2.9132e-04\nLoss = 1.1178e-02, PNorm = 70.4900, GNorm = 0.6476, lr_0 = 2.9132e-04\n\n\r 43%|████▎     | 71/167 [00:13&lt;00:17,  5.43it/s]Loss = 1.2010e-02, PNorm = 70.4944, GNorm = 0.3626, lr_0 = 2.9401e-04\nLoss = 1.2010e-02, PNorm = 70.4944, GNorm = 0.3626, lr_0 = 2.9401e-04\n\n\r 43%|████▎     | 72/167 [00:14&lt;00:17,  5.49it/s]Loss = 1.0989e-02, PNorm = 70.4992, GNorm = 0.3749, lr_0 = 2.9671e-04\nLoss = 1.0989e-02, PNorm = 70.4992, GNorm = 0.3749, lr_0 = 2.9671e-04\n\n\r 44%|████▎     | 73/167 [00:14&lt;00:16,  5.67it/s]Loss = 1.1581e-02, PNorm = 70.5046, GNorm = 0.3008, lr_0 = 2.9940e-04\nLoss = 1.1581e-02, PNorm = 70.5046, GNorm = 0.3008, lr_0 = 2.9940e-04\n\n\r 44%|████▍     | 74/167 [00:14&lt;00:16,  5.48it/s]Loss = 1.0222e-02, PNorm = 70.5103, GNorm = 0.2717, lr_0 = 3.0210e-04\nLoss = 1.0222e-02, PNorm = 70.5103, GNorm = 0.2717, lr_0 = 3.0210e-04\n\n\r 45%|████▍     | 75/167 [00:14&lt;00:16,  5.52it/s]Loss = 1.2094e-02, PNorm = 70.5168, GNorm = 0.5021, lr_0 = 3.0479e-04\nLoss = 1.2094e-02, PNorm = 70.5168, GNorm = 0.5021, lr_0 = 3.0479e-04\n\n\r 46%|████▌     | 76/167 [00:14&lt;00:16,  5.51it/s]Loss = 1.2084e-02, PNorm = 70.5222, GNorm = 0.5719, lr_0 = 3.0749e-04\nLoss = 1.2084e-02, PNorm = 70.5222, GNorm = 0.5719, lr_0 = 3.0749e-04\n\n\r 46%|████▌     | 77/167 [00:15&lt;00:16,  5.48it/s]Loss = 1.1377e-02, PNorm = 70.5267, GNorm = 0.5636, lr_0 = 3.1018e-04\nLoss = 1.1377e-02, PNorm = 70.5267, GNorm = 0.5636, lr_0 = 3.1018e-04\n\n\r 47%|████▋     | 78/167 [00:15&lt;00:16,  5.50it/s]Loss = 9.5347e-03, PNorm = 70.5318, GNorm = 0.3513, lr_0 = 3.1287e-04\nLoss = 9.5347e-03, PNorm = 70.5318, GNorm = 0.3513, lr_0 = 3.1287e-04\n\n\r 47%|████▋     | 79/167 [00:15&lt;00:15,  5.63it/s]Loss = 1.1759e-02, PNorm = 70.5372, GNorm = 0.3396, lr_0 = 3.1557e-04\nLoss = 1.1759e-02, PNorm = 70.5372, GNorm = 0.3396, lr_0 = 3.1557e-04\n\n\r 48%|████▊     | 80/167 [00:15&lt;00:15,  5.51it/s]Loss = 9.9189e-03, PNorm = 70.5421, GNorm = 0.6319, lr_0 = 3.1826e-04\nLoss = 9.9189e-03, PNorm = 70.5421, GNorm = 0.6319, lr_0 = 3.1826e-04\n\n\r 49%|████▊     | 81/167 [00:15&lt;00:15,  5.49it/s]Loss = 8.4932e-03, PNorm = 70.5481, GNorm = 0.4140, lr_0 = 3.2096e-04\nLoss = 8.4932e-03, PNorm = 70.5481, GNorm = 0.4140, lr_0 = 3.2096e-04\n\n\r 49%|████▉     | 82/167 [00:15&lt;00:15,  5.62it/s]Loss = 1.1379e-02, PNorm = 70.5539, GNorm = 0.2087, lr_0 = 3.2365e-04\nLoss = 1.1379e-02, PNorm = 70.5539, GNorm = 0.2087, lr_0 = 3.2365e-04\n\n\r 50%|████▉     | 83/167 [00:16&lt;00:14,  5.62it/s]Loss = 1.1988e-02, PNorm = 70.5594, GNorm = 0.5864, lr_0 = 3.2635e-04\nLoss = 1.1988e-02, PNorm = 70.5594, GNorm = 0.5864, lr_0 = 3.2635e-04\n\n\r 50%|█████     | 84/167 [00:16&lt;00:15,  5.48it/s]Loss = 1.1304e-02, PNorm = 70.5650, GNorm = 0.5282, lr_0 = 3.2904e-04\nLoss = 1.1304e-02, PNorm = 70.5650, GNorm = 0.5282, lr_0 = 3.2904e-04\n\n\r 51%|█████     | 85/167 [00:16&lt;00:15,  5.43it/s]Loss = 9.7671e-03, PNorm = 70.5711, GNorm = 0.3325, lr_0 = 3.3174e-04\nLoss = 9.7671e-03, PNorm = 70.5711, GNorm = 0.3325, lr_0 = 3.3174e-04\n\n\r 51%|█████▏    | 86/167 [00:16&lt;00:14,  5.44it/s]Loss = 1.0086e-02, PNorm = 70.5772, GNorm = 0.2144, lr_0 = 3.3443e-04\nLoss = 1.0086e-02, PNorm = 70.5772, GNorm = 0.2144, lr_0 = 3.3443e-04\n\n\r 52%|█████▏    | 87/167 [00:16&lt;00:14,  5.38it/s]Loss = 1.3580e-02, PNorm = 70.5811, GNorm = 0.7249, lr_0 = 3.3713e-04\nLoss = 1.3580e-02, PNorm = 70.5811, GNorm = 0.7249, lr_0 = 3.3713e-04\n\n\r 53%|█████▎    | 88/167 [00:17&lt;00:27,  2.83it/s]Loss = 9.9144e-03, PNorm = 70.5856, GNorm = 0.3714, lr_0 = 3.3982e-04\nLoss = 9.9144e-03, PNorm = 70.5856, GNorm = 0.3714, lr_0 = 3.3982e-04\n\n\r 53%|█████▎    | 89/167 [00:17&lt;00:23,  3.30it/s]Loss = 9.3755e-03, PNorm = 70.5909, GNorm = 0.3372, lr_0 = 3.4251e-04\nLoss = 9.3755e-03, PNorm = 70.5909, GNorm = 0.3372, lr_0 = 3.4251e-04\n\n\r 54%|█████▍    | 90/167 [00:17&lt;00:20,  3.74it/s]Loss = 8.9886e-03, PNorm = 70.5970, GNorm = 0.3226, lr_0 = 3.4521e-04\nLoss = 8.9886e-03, PNorm = 70.5970, GNorm = 0.3226, lr_0 = 3.4521e-04\n\n\r 54%|█████▍    | 91/167 [00:18&lt;00:18,  4.19it/s]Loss = 9.0560e-03, PNorm = 70.6039, GNorm = 0.3618, lr_0 = 3.4790e-04\nLoss = 9.0560e-03, PNorm = 70.6039, GNorm = 0.3618, lr_0 = 3.4790e-04\n\n\r 55%|█████▌    | 92/167 [00:18&lt;00:16,  4.46it/s]Loss = 1.0970e-02, PNorm = 70.6112, GNorm = 0.3336, lr_0 = 3.5060e-04\nLoss = 1.0970e-02, PNorm = 70.6112, GNorm = 0.3336, lr_0 = 3.5060e-04\n\n\r 56%|█████▌    | 93/167 [00:18&lt;00:15,  4.68it/s]Loss = 8.9633e-03, PNorm = 70.6189, GNorm = 0.4065, lr_0 = 3.5329e-04\nLoss = 8.9633e-03, PNorm = 70.6189, GNorm = 0.4065, lr_0 = 3.5329e-04\n\n\r 56%|█████▋    | 94/167 [00:18&lt;00:14,  4.92it/s]Loss = 1.0837e-02, PNorm = 70.6269, GNorm = 0.3625, lr_0 = 3.5599e-04\nLoss = 1.0837e-02, PNorm = 70.6269, GNorm = 0.3625, lr_0 = 3.5599e-04\n\n\r 57%|█████▋    | 95/167 [00:18&lt;00:14,  5.07it/s]Loss = 9.5767e-03, PNorm = 70.6346, GNorm = 0.4286, lr_0 = 3.5868e-04\nLoss = 9.5767e-03, PNorm = 70.6346, GNorm = 0.4286, lr_0 = 3.5868e-04\n\n*** WARNING: skipped 1601483 bytes of output ***\n\n\n\r 29%|██▊       | 48/167 [00:06&lt;00:16,  7.11it/s]Loss = 2.6469e-05, PNorm = 148.1620, GNorm = 0.0291, lr_0 = 1.0345e-04\nLoss = 2.6469e-05, PNorm = 148.1620, GNorm = 0.0291, lr_0 = 1.0345e-04\n\n\r 29%|██▉       | 49/167 [00:06&lt;00:16,  7.14it/s]Loss = 1.3699e-04, PNorm = 148.1629, GNorm = 0.1412, lr_0 = 1.0342e-04\nLoss = 1.3699e-04, PNorm = 148.1629, GNorm = 0.1412, lr_0 = 1.0342e-04\n\n\r 30%|██▉       | 50/167 [00:06&lt;00:16,  7.13it/s]Loss = 2.4345e-05, PNorm = 148.1637, GNorm = 0.0324, lr_0 = 1.0339e-04\nLoss = 2.4345e-05, PNorm = 148.1637, GNorm = 0.0324, lr_0 = 1.0339e-04\n\n\r 31%|███       | 51/167 [00:07&lt;00:16,  7.17it/s]Loss = 5.2903e-05, PNorm = 148.1644, GNorm = 0.0547, lr_0 = 1.0336e-04\nLoss = 5.2903e-05, PNorm = 148.1644, GNorm = 0.0547, lr_0 = 1.0336e-04\n\n\r 31%|███       | 52/167 [00:07&lt;00:15,  7.23it/s]Loss = 4.0824e-05, PNorm = 148.1652, GNorm = 0.0999, lr_0 = 1.0333e-04\nLoss = 4.0824e-05, PNorm = 148.1652, GNorm = 0.0999, lr_0 = 1.0333e-04\n\n\r 32%|███▏      | 53/167 [00:07&lt;00:15,  7.21it/s]Loss = 4.7395e-04, PNorm = 148.1654, GNorm = 0.4955, lr_0 = 1.0330e-04\nLoss = 4.7395e-04, PNorm = 148.1654, GNorm = 0.4955, lr_0 = 1.0330e-04\n\n\r 32%|███▏      | 54/167 [00:07&lt;00:15,  7.25it/s]Loss = 1.9489e-04, PNorm = 148.1657, GNorm = 0.2890, lr_0 = 1.0327e-04\nLoss = 1.9489e-04, PNorm = 148.1657, GNorm = 0.2890, lr_0 = 1.0327e-04\n\n\r 33%|███▎      | 55/167 [00:07&lt;00:15,  7.26it/s]Loss = 2.8491e-04, PNorm = 148.1660, GNorm = 0.3335, lr_0 = 1.0324e-04\nLoss = 2.8491e-04, PNorm = 148.1660, GNorm = 0.3335, lr_0 = 1.0324e-04\n\n\r 34%|███▎      | 56/167 [00:07&lt;00:15,  7.00it/s]Loss = 1.0383e-05, PNorm = 148.1664, GNorm = 0.0196, lr_0 = 1.0321e-04\nLoss = 1.0383e-05, PNorm = 148.1664, GNorm = 0.0196, lr_0 = 1.0321e-04\n\n\r 34%|███▍      | 57/167 [00:07&lt;00:15,  7.11it/s]Loss = 7.7115e-04, PNorm = 148.1668, GNorm = 0.3231, lr_0 = 1.0318e-04\nLoss = 7.7115e-04, PNorm = 148.1668, GNorm = 0.3231, lr_0 = 1.0318e-04\n\n\r 35%|███▍      | 58/167 [00:08&lt;00:15,  7.13it/s]Loss = 5.4080e-05, PNorm = 148.1673, GNorm = 0.1073, lr_0 = 1.0315e-04\nLoss = 5.4080e-05, PNorm = 148.1673, GNorm = 0.1073, lr_0 = 1.0315e-04\n\n\r 35%|███▌      | 59/167 [00:08&lt;00:15,  7.11it/s]Loss = 3.7200e-05, PNorm = 148.1678, GNorm = 0.0444, lr_0 = 1.0312e-04\nLoss = 3.7200e-05, PNorm = 148.1678, GNorm = 0.0444, lr_0 = 1.0312e-04\n\n\r 36%|███▌      | 60/167 [00:08&lt;00:15,  7.07it/s]Loss = 1.6531e-05, PNorm = 148.1683, GNorm = 0.0266, lr_0 = 1.0309e-04\nLoss = 1.6531e-05, PNorm = 148.1683, GNorm = 0.0266, lr_0 = 1.0309e-04\n\n\r 37%|███▋      | 61/167 [00:08&lt;00:14,  7.12it/s]Loss = 2.7222e-04, PNorm = 148.1689, GNorm = 0.2404, lr_0 = 1.0306e-04\nLoss = 2.7222e-04, PNorm = 148.1689, GNorm = 0.2404, lr_0 = 1.0306e-04\n\n\r 37%|███▋      | 62/167 [00:08&lt;00:14,  7.19it/s]Loss = 2.8442e-04, PNorm = 148.1697, GNorm = 0.2761, lr_0 = 1.0303e-04\nLoss = 2.8442e-04, PNorm = 148.1697, GNorm = 0.2761, lr_0 = 1.0303e-04\n\n\r 38%|███▊      | 63/167 [00:08&lt;00:14,  7.20it/s]Loss = 3.8545e-04, PNorm = 148.1707, GNorm = 0.4536, lr_0 = 1.0300e-04\nLoss = 3.8545e-04, PNorm = 148.1707, GNorm = 0.4536, lr_0 = 1.0300e-04\n\n\r 38%|███▊      | 64/167 [00:08&lt;00:14,  7.20it/s]Loss = 1.2975e-04, PNorm = 148.1717, GNorm = 0.1820, lr_0 = 1.0297e-04\nLoss = 1.2975e-04, PNorm = 148.1717, GNorm = 0.1820, lr_0 = 1.0297e-04\n\n\r 39%|███▉      | 65/167 [00:09&lt;00:14,  7.10it/s]Loss = 2.9273e-05, PNorm = 148.1727, GNorm = 0.0368, lr_0 = 1.0294e-04\nLoss = 2.9273e-05, PNorm = 148.1727, GNorm = 0.0368, lr_0 = 1.0294e-04\n\n\r 40%|███▉      | 66/167 [00:09&lt;00:14,  6.85it/s]Loss = 1.5550e-04, PNorm = 148.1738, GNorm = 0.2025, lr_0 = 1.0291e-04\nLoss = 1.5550e-04, PNorm = 148.1738, GNorm = 0.2025, lr_0 = 1.0291e-04\n\n\r 40%|████      | 67/167 [00:09&lt;00:14,  6.98it/s]Loss = 2.0450e-05, PNorm = 148.1748, GNorm = 0.0258, lr_0 = 1.0288e-04\nLoss = 2.0450e-05, PNorm = 148.1748, GNorm = 0.0258, lr_0 = 1.0288e-04\n\n\r 41%|████      | 68/167 [00:09&lt;00:13,  7.09it/s]Loss = 7.5526e-05, PNorm = 148.1757, GNorm = 0.1184, lr_0 = 1.0286e-04\nLoss = 7.5526e-05, PNorm = 148.1757, GNorm = 0.1184, lr_0 = 1.0286e-04\n\n\r 41%|████▏     | 69/167 [00:09&lt;00:13,  7.17it/s]Loss = 1.2302e-04, PNorm = 148.1769, GNorm = 0.1601, lr_0 = 1.0283e-04\nLoss = 1.2302e-04, PNorm = 148.1769, GNorm = 0.1601, lr_0 = 1.0283e-04\n\n\r 42%|████▏     | 70/167 [00:09&lt;00:13,  7.22it/s]Loss = 2.2577e-05, PNorm = 148.1779, GNorm = 0.0327, lr_0 = 1.0280e-04\nLoss = 2.2577e-05, PNorm = 148.1779, GNorm = 0.0327, lr_0 = 1.0280e-04\n\n\r 43%|████▎     | 71/167 [00:09&lt;00:13,  7.21it/s]Loss = 3.8184e-05, PNorm = 148.1790, GNorm = 0.0483, lr_0 = 1.0277e-04\nLoss = 3.8184e-05, PNorm = 148.1790, GNorm = 0.0483, lr_0 = 1.0277e-04\n\n\r 43%|████▎     | 72/167 [00:10&lt;00:13,  7.26it/s]Loss = 1.2508e-05, PNorm = 148.1799, GNorm = 0.0152, lr_0 = 1.0274e-04\nLoss = 1.2508e-05, PNorm = 148.1799, GNorm = 0.0152, lr_0 = 1.0274e-04\n\n\r 44%|████▎     | 73/167 [00:10&lt;00:13,  7.02it/s]Loss = 1.8235e-05, PNorm = 148.1808, GNorm = 0.0232, lr_0 = 1.0271e-04\nLoss = 1.8235e-05, PNorm = 148.1808, GNorm = 0.0232, lr_0 = 1.0271e-04\n\n\r 44%|████▍     | 74/167 [00:10&lt;00:13,  6.99it/s]Loss = 1.1344e-04, PNorm = 148.1816, GNorm = 0.1547, lr_0 = 1.0268e-04\nLoss = 1.1344e-04, PNorm = 148.1816, GNorm = 0.1547, lr_0 = 1.0268e-04\n\n\r 45%|████▍     | 75/167 [00:10&lt;00:12,  7.08it/s]Loss = 1.8998e-04, PNorm = 148.1823, GNorm = 0.1502, lr_0 = 1.0265e-04\nLoss = 1.8998e-04, PNorm = 148.1823, GNorm = 0.1502, lr_0 = 1.0265e-04\n\n\r 46%|████▌     | 76/167 [00:10&lt;00:12,  7.09it/s]Loss = 3.2615e-05, PNorm = 148.1830, GNorm = 0.0518, lr_0 = 1.0262e-04\nLoss = 3.2615e-05, PNorm = 148.1830, GNorm = 0.0518, lr_0 = 1.0262e-04\n\n\r 46%|████▌     | 77/167 [00:10&lt;00:12,  7.11it/s]Loss = 1.9354e-04, PNorm = 148.1837, GNorm = 0.2727, lr_0 = 1.0259e-04\nLoss = 1.9354e-04, PNorm = 148.1837, GNorm = 0.2727, lr_0 = 1.0259e-04\n\n\r 47%|████▋     | 78/167 [00:10&lt;00:12,  7.12it/s]Loss = 3.1099e-04, PNorm = 148.1847, GNorm = 0.2305, lr_0 = 1.0256e-04\nLoss = 3.1099e-04, PNorm = 148.1847, GNorm = 0.2305, lr_0 = 1.0256e-04\n\n\r 47%|████▋     | 79/167 [00:11&lt;00:12,  7.14it/s]Loss = 1.4858e-04, PNorm = 148.1857, GNorm = 0.1243, lr_0 = 1.0253e-04\nLoss = 1.4858e-04, PNorm = 148.1857, GNorm = 0.1243, lr_0 = 1.0253e-04\n\n\r 48%|████▊     | 80/167 [00:11&lt;00:12,  7.21it/s]Loss = 8.5703e-04, PNorm = 148.1869, GNorm = 0.4852, lr_0 = 1.0250e-04\nLoss = 8.5703e-04, PNorm = 148.1869, GNorm = 0.4852, lr_0 = 1.0250e-04\n\n\r 49%|████▊     | 81/167 [00:11&lt;00:11,  7.22it/s]Loss = 5.3341e-05, PNorm = 148.1880, GNorm = 0.0830, lr_0 = 1.0247e-04\nLoss = 5.3341e-05, PNorm = 148.1880, GNorm = 0.0830, lr_0 = 1.0247e-04\n\n\r 49%|████▉     | 82/167 [00:11&lt;00:11,  7.27it/s]Loss = 2.9165e-05, PNorm = 148.1891, GNorm = 0.0425, lr_0 = 1.0244e-04\nLoss = 2.9165e-05, PNorm = 148.1891, GNorm = 0.0425, lr_0 = 1.0244e-04\n\n\r 50%|████▉     | 83/167 [00:11&lt;00:11,  7.29it/s]Loss = 2.1926e-04, PNorm = 148.1902, GNorm = 0.1766, lr_0 = 1.0241e-04\nLoss = 2.1926e-04, PNorm = 148.1902, GNorm = 0.1766, lr_0 = 1.0241e-04\n\n\r 50%|█████     | 84/167 [00:11&lt;00:11,  7.26it/s]Loss = 1.6575e-05, PNorm = 148.1911, GNorm = 0.0192, lr_0 = 1.0238e-04\nLoss = 1.6575e-05, PNorm = 148.1911, GNorm = 0.0192, lr_0 = 1.0238e-04\n\n\r 51%|█████     | 85/167 [00:11&lt;00:11,  7.29it/s]Loss = 1.1183e-04, PNorm = 148.1921, GNorm = 0.1157, lr_0 = 1.0235e-04\nLoss = 1.1183e-04, PNorm = 148.1921, GNorm = 0.1157, lr_0 = 1.0235e-04\n\n\r 51%|█████▏    | 86/167 [00:12&lt;00:11,  7.26it/s]Loss = 1.4835e-05, PNorm = 148.1931, GNorm = 0.0355, lr_0 = 1.0232e-04\nLoss = 1.4835e-05, PNorm = 148.1931, GNorm = 0.0355, lr_0 = 1.0232e-04\n\n\r 52%|█████▏    | 87/167 [00:12&lt;00:10,  7.29it/s]Loss = 6.2237e-04, PNorm = 148.1937, GNorm = 0.4043, lr_0 = 1.0230e-04\nLoss = 6.2237e-04, PNorm = 148.1937, GNorm = 0.4043, lr_0 = 1.0230e-04\n\n\r 53%|█████▎    | 88/167 [00:12&lt;00:10,  7.26it/s]Loss = 3.4419e-05, PNorm = 148.1943, GNorm = 0.0445, lr_0 = 1.0227e-04\nLoss = 3.4419e-05, PNorm = 148.1943, GNorm = 0.0445, lr_0 = 1.0227e-04\n\n\r 53%|█████▎    | 89/167 [00:12&lt;00:10,  7.22it/s]Loss = 1.9049e-05, PNorm = 148.1949, GNorm = 0.0176, lr_0 = 1.0224e-04\nLoss = 1.9049e-05, PNorm = 148.1949, GNorm = 0.0176, lr_0 = 1.0224e-04\n\n\r 54%|█████▍    | 90/167 [00:12&lt;00:10,  7.19it/s]Loss = 1.2649e-04, PNorm = 148.1954, GNorm = 0.1496, lr_0 = 1.0221e-04\nLoss = 1.2649e-04, PNorm = 148.1954, GNorm = 0.1496, lr_0 = 1.0221e-04\n\n\r 54%|█████▍    | 91/167 [00:12&lt;00:10,  7.10it/s]Loss = 1.2522e-04, PNorm = 148.1959, GNorm = 0.0905, lr_0 = 1.0218e-04\nLoss = 1.2522e-04, PNorm = 148.1959, GNorm = 0.0905, lr_0 = 1.0218e-04\n\n\r 55%|█████▌    | 92/167 [00:12&lt;00:10,  7.10it/s]Loss = 6.5815e-05, PNorm = 148.1965, GNorm = 0.1043, lr_0 = 1.0215e-04\nLoss = 6.5815e-05, PNorm = 148.1965, GNorm = 0.1043, lr_0 = 1.0215e-04\n\n\r 56%|█████▌    | 93/167 [00:13&lt;00:10,  6.86it/s]Loss = 8.7821e-05, PNorm = 148.1971, GNorm = 0.0841, lr_0 = 1.0212e-04\nLoss = 8.7821e-05, PNorm = 148.1971, GNorm = 0.0841, lr_0 = 1.0212e-04\n\n\r 56%|█████▋    | 94/167 [00:13&lt;00:10,  6.90it/s]Loss = 3.7249e-05, PNorm = 148.1977, GNorm = 0.0432, lr_0 = 1.0209e-04\nLoss = 3.7249e-05, PNorm = 148.1977, GNorm = 0.0432, lr_0 = 1.0209e-04\n\n\r 57%|█████▋    | 95/167 [00:13&lt;00:10,  7.00it/s]Loss = 5.2381e-04, PNorm = 148.1978, GNorm = 0.4347, lr_0 = 1.0206e-04\nLoss = 5.2381e-04, PNorm = 148.1978, GNorm = 0.4347, lr_0 = 1.0206e-04\n\n\r 57%|█████▋    | 96/167 [00:13&lt;00:10,  7.09it/s]Loss = 2.3800e-05, PNorm = 148.1980, GNorm = 0.0334, lr_0 = 1.0203e-04\nLoss = 2.3800e-05, PNorm = 148.1980, GNorm = 0.0334, lr_0 = 1.0203e-04\n\n\r 58%|█████▊    | 97/167 [00:13&lt;00:09,  7.13it/s]Loss = 4.5697e-05, PNorm = 148.1982, GNorm = 0.0669, lr_0 = 1.0200e-04\nLoss = 4.5697e-05, PNorm = 148.1982, GNorm = 0.0669, lr_0 = 1.0200e-04\n\n\r 59%|█████▊    | 98/167 [00:13&lt;00:09,  7.13it/s]Loss = 7.8160e-05, PNorm = 148.1986, GNorm = 0.1097, lr_0 = 1.0197e-04\nLoss = 7.8160e-05, PNorm = 148.1986, GNorm = 0.1097, lr_0 = 1.0197e-04\n\n\r 59%|█████▉    | 99/167 [00:13&lt;00:09,  7.15it/s]Loss = 1.0139e-04, PNorm = 148.1990, GNorm = 0.1183, lr_0 = 1.0194e-04\nLoss = 1.0139e-04, PNorm = 148.1990, GNorm = 0.1183, lr_0 = 1.0194e-04\n\n\r 60%|█████▉    | 100/167 [00:13&lt;00:09,  7.21it/s]Loss = 1.6666e-05, PNorm = 148.1994, GNorm = 0.0315, lr_0 = 1.0191e-04\nLoss = 1.6666e-05, PNorm = 148.1994, GNorm = 0.0315, lr_0 = 1.0191e-04\n\n\r 60%|██████    | 101/167 [00:14&lt;00:09,  7.16it/s]Loss = 2.9920e-04, PNorm = 148.1999, GNorm = 0.2556, lr_0 = 1.0188e-04\nLoss = 2.9920e-04, PNorm = 148.1999, GNorm = 0.2556, lr_0 = 1.0188e-04\n\n\r 61%|██████    | 102/167 [00:14&lt;00:09,  7.22it/s]Loss = 3.0653e-05, PNorm = 148.2003, GNorm = 0.0376, lr_0 = 1.0186e-04\nLoss = 3.0653e-05, PNorm = 148.2003, GNorm = 0.0376, lr_0 = 1.0186e-04\n\n\r 62%|██████▏   | 103/167 [00:14&lt;00:08,  7.12it/s]Loss = 3.0149e-05, PNorm = 148.2007, GNorm = 0.0411, lr_0 = 1.0183e-04\nLoss = 3.0149e-05, PNorm = 148.2007, GNorm = 0.0411, lr_0 = 1.0183e-04\n\n\r 62%|██████▏   | 104/167 [00:14&lt;00:08,  7.13it/s]Loss = 1.5512e-04, PNorm = 148.2013, GNorm = 0.3936, lr_0 = 1.0180e-04\nLoss = 1.5512e-04, PNorm = 148.2013, GNorm = 0.3936, lr_0 = 1.0180e-04\n\n\r 63%|██████▎   | 105/167 [00:14&lt;00:08,  7.09it/s]Loss = 2.9603e-05, PNorm = 148.2018, GNorm = 0.0412, lr_0 = 1.0177e-04\nLoss = 2.9603e-05, PNorm = 148.2018, GNorm = 0.0412, lr_0 = 1.0177e-04\n\n\r 63%|██████▎   | 106/167 [00:14&lt;00:08,  6.90it/s]Loss = 8.5530e-06, PNorm = 148.2024, GNorm = 0.0076, lr_0 = 1.0174e-04\nLoss = 8.5530e-06, PNorm = 148.2024, GNorm = 0.0076, lr_0 = 1.0174e-04\n\n\r 64%|██████▍   | 107/167 [00:14&lt;00:08,  7.03it/s]Loss = 4.1792e-05, PNorm = 148.2028, GNorm = 0.0747, lr_0 = 1.0171e-04\nLoss = 4.1792e-05, PNorm = 148.2028, GNorm = 0.0747, lr_0 = 1.0171e-04\n\n\r 65%|██████▍   | 108/167 [00:15&lt;00:08,  6.99it/s]Loss = 3.7857e-05, PNorm = 148.2033, GNorm = 0.0516, lr_0 = 1.0168e-04\nLoss = 3.7857e-05, PNorm = 148.2033, GNorm = 0.0516, lr_0 = 1.0168e-04\n\n\r 65%|██████▌   | 109/167 [00:15&lt;00:08,  7.11it/s]Loss = 1.6512e-05, PNorm = 148.2037, GNorm = 0.0238, lr_0 = 1.0165e-04\nLoss = 1.6512e-05, PNorm = 148.2037, GNorm = 0.0238, lr_0 = 1.0165e-04\n\n\r 66%|██████▌   | 110/167 [00:15&lt;00:07,  7.15it/s]Loss = 2.0415e-05, PNorm = 148.2042, GNorm = 0.0143, lr_0 = 1.0162e-04\nLoss = 2.0415e-05, PNorm = 148.2042, GNorm = 0.0143, lr_0 = 1.0162e-04\n\n\r 66%|██████▋   | 111/167 [00:15&lt;00:07,  7.11it/s]Loss = 6.0180e-05, PNorm = 148.2045, GNorm = 0.0861, lr_0 = 1.0159e-04\nLoss = 6.0180e-05, PNorm = 148.2045, GNorm = 0.0861, lr_0 = 1.0159e-04\n\n\r 67%|██████▋   | 112/167 [00:15&lt;00:07,  7.05it/s]Loss = 3.4177e-06, PNorm = 148.2048, GNorm = 0.0153, lr_0 = 1.0156e-04\nLoss = 3.4177e-06, PNorm = 148.2048, GNorm = 0.0153, lr_0 = 1.0156e-04\n\n\r 68%|██████▊   | 113/167 [00:15&lt;00:07,  7.10it/s]Loss = 9.4955e-05, PNorm = 148.2052, GNorm = 0.0970, lr_0 = 1.0153e-04\nLoss = 9.4955e-05, PNorm = 148.2052, GNorm = 0.0970, lr_0 = 1.0153e-04\n\n\r 68%|██████▊   | 114/167 [00:15&lt;00:07,  7.11it/s]Loss = 4.2521e-05, PNorm = 148.2056, GNorm = 0.1114, lr_0 = 1.0150e-04\nLoss = 4.2521e-05, PNorm = 148.2056, GNorm = 0.1114, lr_0 = 1.0150e-04\n\n\r 69%|██████▉   | 115/167 [00:16&lt;00:07,  7.17it/s]Loss = 1.1526e-04, PNorm = 148.2059, GNorm = 0.1138, lr_0 = 1.0148e-04\nLoss = 1.1526e-04, PNorm = 148.2059, GNorm = 0.1138, lr_0 = 1.0148e-04\n\n\r 69%|██████▉   | 116/167 [00:16&lt;00:07,  7.09it/s]Loss = 2.8434e-04, PNorm = 148.2063, GNorm = 0.1896, lr_0 = 1.0145e-04\nLoss = 2.8434e-04, PNorm = 148.2063, GNorm = 0.1896, lr_0 = 1.0145e-04\n\n\r 70%|███████   | 117/167 [00:16&lt;00:07,  7.11it/s]Loss = 7.7159e-05, PNorm = 148.2068, GNorm = 0.0770, lr_0 = 1.0142e-04\nLoss = 7.7159e-05, PNorm = 148.2068, GNorm = 0.0770, lr_0 = 1.0142e-04\n\n\r 71%|███████   | 118/167 [00:16&lt;00:06,  7.18it/s]Loss = 3.4372e-05, PNorm = 148.2072, GNorm = 0.0585, lr_0 = 1.0139e-04\nLoss = 3.4372e-05, PNorm = 148.2072, GNorm = 0.0585, lr_0 = 1.0139e-04\n\n\r 71%|███████▏  | 119/167 [00:16&lt;00:06,  7.23it/s]Loss = 9.7733e-04, PNorm = 148.2073, GNorm = 0.5033, lr_0 = 1.0136e-04\nLoss = 9.7733e-04, PNorm = 148.2073, GNorm = 0.5033, lr_0 = 1.0136e-04\n\n\r 72%|███████▏  | 120/167 [00:16&lt;00:06,  7.21it/s]Loss = 2.9364e-04, PNorm = 148.2076, GNorm = 0.2932, lr_0 = 1.0133e-04\nLoss = 2.9364e-04, PNorm = 148.2076, GNorm = 0.2932, lr_0 = 1.0133e-04\n\n\r 72%|███████▏  | 121/167 [00:16&lt;00:06,  7.25it/s]Loss = 1.7514e-05, PNorm = 148.2080, GNorm = 0.0255, lr_0 = 1.0130e-04\nLoss = 1.7514e-05, PNorm = 148.2080, GNorm = 0.0255, lr_0 = 1.0130e-04\n\n\r 73%|███████▎  | 122/167 [00:17&lt;00:06,  7.29it/s]Loss = 1.3049e-05, PNorm = 148.2083, GNorm = 0.0141, lr_0 = 1.0127e-04\nLoss = 1.3049e-05, PNorm = 148.2083, GNorm = 0.0141, lr_0 = 1.0127e-04\n\n\r 74%|███████▎  | 123/167 [00:17&lt;00:06,  7.23it/s]Loss = 4.9198e-05, PNorm = 148.2086, GNorm = 0.0407, lr_0 = 1.0124e-04\nLoss = 4.9198e-05, PNorm = 148.2086, GNorm = 0.0407, lr_0 = 1.0124e-04\n\n\r 74%|███████▍  | 124/167 [00:17&lt;00:05,  7.27it/s]Loss = 1.8420e-05, PNorm = 148.2090, GNorm = 0.0229, lr_0 = 1.0121e-04\nLoss = 1.8420e-05, PNorm = 148.2090, GNorm = 0.0229, lr_0 = 1.0121e-04\n\n\r 75%|███████▍  | 125/167 [00:17&lt;00:05,  7.26it/s]Loss = 1.0503e-04, PNorm = 148.2094, GNorm = 0.1119, lr_0 = 1.0118e-04\nLoss = 1.0503e-04, PNorm = 148.2094, GNorm = 0.1119, lr_0 = 1.0118e-04\n\n\r 75%|███████▌  | 126/167 [00:17&lt;00:05,  7.23it/s]Loss = 3.6218e-05, PNorm = 148.2097, GNorm = 0.0457, lr_0 = 1.0116e-04\nLoss = 3.6218e-05, PNorm = 148.2097, GNorm = 0.0457, lr_0 = 1.0116e-04\n\n\r 76%|███████▌  | 127/167 [00:17&lt;00:05,  7.18it/s]Loss = 1.2612e-04, PNorm = 148.2102, GNorm = 0.1218, lr_0 = 1.0113e-04\nLoss = 1.2612e-04, PNorm = 148.2102, GNorm = 0.1218, lr_0 = 1.0113e-04\n\n\r 77%|███████▋  | 128/167 [00:17&lt;00:05,  7.23it/s]Loss = 1.1268e-04, PNorm = 148.2107, GNorm = 0.1338, lr_0 = 1.0110e-04\nLoss = 1.1268e-04, PNorm = 148.2107, GNorm = 0.1338, lr_0 = 1.0110e-04\n\n\r 77%|███████▋  | 129/167 [00:18&lt;00:05,  7.18it/s]Loss = 2.7378e-05, PNorm = 148.2112, GNorm = 0.0234, lr_0 = 1.0107e-04\nLoss = 2.7378e-05, PNorm = 148.2112, GNorm = 0.0234, lr_0 = 1.0107e-04\n\n\r 78%|███████▊  | 130/167 [00:18&lt;00:05,  7.14it/s]Loss = 2.3691e-05, PNorm = 148.2116, GNorm = 0.0362, lr_0 = 1.0104e-04\nLoss = 2.3691e-05, PNorm = 148.2116, GNorm = 0.0362, lr_0 = 1.0104e-04\n\n\r 78%|███████▊  | 131/167 [00:18&lt;00:04,  7.23it/s]Loss = 3.7650e-05, PNorm = 148.2121, GNorm = 0.0650, lr_0 = 1.0101e-04\nLoss = 3.7650e-05, PNorm = 148.2121, GNorm = 0.0650, lr_0 = 1.0101e-04\n\n\r 79%|███████▉  | 132/167 [00:18&lt;00:04,  7.27it/s]Loss = 1.4665e-04, PNorm = 148.2127, GNorm = 0.1322, lr_0 = 1.0098e-04\nLoss = 1.4665e-04, PNorm = 148.2127, GNorm = 0.1322, lr_0 = 1.0098e-04\n\n\r 80%|███████▉  | 133/167 [00:18&lt;00:04,  7.24it/s]Loss = 9.5187e-05, PNorm = 148.2134, GNorm = 0.1931, lr_0 = 1.0095e-04\nLoss = 9.5187e-05, PNorm = 148.2134, GNorm = 0.1931, lr_0 = 1.0095e-04\n\n\r 80%|████████  | 134/167 [00:18&lt;00:04,  7.20it/s]Loss = 5.9569e-05, PNorm = 148.2140, GNorm = 0.0840, lr_0 = 1.0092e-04\nLoss = 5.9569e-05, PNorm = 148.2140, GNorm = 0.0840, lr_0 = 1.0092e-04\n\n\r 81%|████████  | 135/167 [00:18&lt;00:04,  7.20it/s]Loss = 6.2435e-04, PNorm = 148.2146, GNorm = 0.3223, lr_0 = 1.0089e-04\nLoss = 6.2435e-04, PNorm = 148.2146, GNorm = 0.3223, lr_0 = 1.0089e-04\n\n\r 81%|████████▏ | 136/167 [00:19&lt;00:04,  7.23it/s]Loss = 1.7479e-05, PNorm = 148.2151, GNorm = 0.0216, lr_0 = 1.0087e-04\nLoss = 1.7479e-05, PNorm = 148.2151, GNorm = 0.0216, lr_0 = 1.0087e-04\n\n\r 82%|████████▏ | 137/167 [00:19&lt;00:04,  7.21it/s]Loss = 7.2858e-05, PNorm = 148.2157, GNorm = 0.1267, lr_0 = 1.0084e-04\nLoss = 7.2858e-05, PNorm = 148.2157, GNorm = 0.1267, lr_0 = 1.0084e-04\n\n\r 83%|████████▎ | 138/167 [00:19&lt;00:04,  7.21it/s]Loss = 1.8107e-04, PNorm = 148.2161, GNorm = 0.1978, lr_0 = 1.0081e-04\nLoss = 1.8107e-04, PNorm = 148.2161, GNorm = 0.1978, lr_0 = 1.0081e-04\n\n\r 83%|████████▎ | 139/167 [00:19&lt;00:03,  7.20it/s]Loss = 1.4380e-04, PNorm = 148.2166, GNorm = 0.1688, lr_0 = 1.0078e-04\nLoss = 1.4380e-04, PNorm = 148.2166, GNorm = 0.1688, lr_0 = 1.0078e-04\n\n\r 84%|████████▍ | 140/167 [00:19&lt;00:03,  7.18it/s]Loss = 1.1189e-04, PNorm = 148.2171, GNorm = 0.0961, lr_0 = 1.0075e-04\nLoss = 1.1189e-04, PNorm = 148.2171, GNorm = 0.0961, lr_0 = 1.0075e-04\n\n\r 84%|████████▍ | 141/167 [00:19&lt;00:03,  7.13it/s]Loss = 1.4607e-04, PNorm = 148.2176, GNorm = 0.1629, lr_0 = 1.0072e-04\nLoss = 1.4607e-04, PNorm = 148.2176, GNorm = 0.1629, lr_0 = 1.0072e-04\n\n\r 85%|████████▌ | 142/167 [00:19&lt;00:03,  7.06it/s]Loss = 1.7267e-04, PNorm = 148.2181, GNorm = 0.2328, lr_0 = 1.0069e-04\nLoss = 1.7267e-04, PNorm = 148.2181, GNorm = 0.2328, lr_0 = 1.0069e-04\n\n\r 86%|████████▌ | 143/167 [00:19&lt;00:03,  7.10it/s]Loss = 1.2331e-04, PNorm = 148.2187, GNorm = 0.2589, lr_0 = 1.0066e-04\nLoss = 1.2331e-04, PNorm = 148.2187, GNorm = 0.2589, lr_0 = 1.0066e-04\n\n\r 86%|████████▌ | 144/167 [00:20&lt;00:03,  7.12it/s]Loss = 4.0957e-04, PNorm = 148.2191, GNorm = 0.2255, lr_0 = 1.0063e-04\nLoss = 4.0957e-04, PNorm = 148.2191, GNorm = 0.2255, lr_0 = 1.0063e-04\n\n\r 87%|████████▋ | 145/167 [00:20&lt;00:03,  7.18it/s]Loss = 6.6491e-05, PNorm = 148.2196, GNorm = 0.1006, lr_0 = 1.0061e-04\nLoss = 6.6491e-05, PNorm = 148.2196, GNorm = 0.1006, lr_0 = 1.0061e-04\n\n\r 87%|████████▋ | 146/167 [00:20&lt;00:02,  7.19it/s]Loss = 8.9668e-05, PNorm = 148.2200, GNorm = 0.1269, lr_0 = 1.0058e-04\nLoss = 8.9668e-05, PNorm = 148.2200, GNorm = 0.1269, lr_0 = 1.0058e-04\n\n\r 88%|████████▊ | 147/167 [00:20&lt;00:02,  7.12it/s]Loss = 1.7763e-05, PNorm = 148.2205, GNorm = 0.0656, lr_0 = 1.0055e-04\nLoss = 1.7763e-05, PNorm = 148.2205, GNorm = 0.0656, lr_0 = 1.0055e-04\n\n\r 89%|████████▊ | 148/167 [00:20&lt;00:02,  7.20it/s]Loss = 4.8851e-04, PNorm = 148.2207, GNorm = 0.2751, lr_0 = 1.0052e-04\nLoss = 4.8851e-04, PNorm = 148.2207, GNorm = 0.2751, lr_0 = 1.0052e-04\n\n\r 89%|████████▉ | 149/167 [00:20&lt;00:02,  7.20it/s]Loss = 5.6219e-05, PNorm = 148.2210, GNorm = 0.0651, lr_0 = 1.0049e-04\nLoss = 5.6219e-05, PNorm = 148.2210, GNorm = 0.0651, lr_0 = 1.0049e-04\n\n\r 90%|████████▉ | 150/167 [00:20&lt;00:02,  7.26it/s]Loss = 3.3276e-04, PNorm = 148.2210, GNorm = 0.2961, lr_0 = 1.0046e-04\nLoss = 3.3276e-04, PNorm = 148.2210, GNorm = 0.2961, lr_0 = 1.0046e-04\n\n\r 90%|█████████ | 151/167 [00:21&lt;00:02,  7.24it/s]Loss = 2.6945e-04, PNorm = 148.2210, GNorm = 0.3711, lr_0 = 1.0043e-04\nLoss = 2.6945e-04, PNorm = 148.2210, GNorm = 0.3711, lr_0 = 1.0043e-04\n\n\r 91%|█████████ | 152/167 [00:21&lt;00:02,  7.18it/s]Loss = 1.1009e-04, PNorm = 148.2211, GNorm = 0.1035, lr_0 = 1.0040e-04\nLoss = 1.1009e-04, PNorm = 148.2211, GNorm = 0.1035, lr_0 = 1.0040e-04\n\n\r 92%|█████████▏| 153/167 [00:21&lt;00:01,  7.22it/s]Loss = 2.2069e-05, PNorm = 148.2212, GNorm = 0.0446, lr_0 = 1.0037e-04\nLoss = 2.2069e-05, PNorm = 148.2212, GNorm = 0.0446, lr_0 = 1.0037e-04\n\n\r 92%|█████████▏| 154/167 [00:21&lt;00:01,  7.09it/s]Loss = 4.1738e-05, PNorm = 148.2214, GNorm = 0.0590, lr_0 = 1.0035e-04\nLoss = 4.1738e-05, PNorm = 148.2214, GNorm = 0.0590, lr_0 = 1.0035e-04\n\n\r 93%|█████████▎| 155/167 [00:21&lt;00:01,  7.11it/s]Loss = 2.7238e-05, PNorm = 148.2216, GNorm = 0.0473, lr_0 = 1.0032e-04\nLoss = 2.7238e-05, PNorm = 148.2216, GNorm = 0.0473, lr_0 = 1.0032e-04\n\n\r 93%|█████████▎| 156/167 [00:21&lt;00:01,  7.18it/s]Loss = 3.5382e-06, PNorm = 148.2218, GNorm = 0.0053, lr_0 = 1.0029e-04\nLoss = 3.5382e-06, PNorm = 148.2218, GNorm = 0.0053, lr_0 = 1.0029e-04\n\n\r 94%|█████████▍| 157/167 [00:21&lt;00:01,  7.07it/s]Loss = 7.0601e-05, PNorm = 148.2220, GNorm = 0.1268, lr_0 = 1.0026e-04\nLoss = 7.0601e-05, PNorm = 148.2220, GNorm = 0.1268, lr_0 = 1.0026e-04\n\n\r 95%|█████████▍| 158/167 [00:22&lt;00:01,  7.09it/s]Loss = 6.0343e-05, PNorm = 148.2223, GNorm = 0.0982, lr_0 = 1.0023e-04\nLoss = 6.0343e-05, PNorm = 148.2223, GNorm = 0.0982, lr_0 = 1.0023e-04\n\n\r 95%|█████████▌| 159/167 [00:22&lt;00:01,  7.13it/s]Loss = 6.8296e-04, PNorm = 148.2228, GNorm = 0.4166, lr_0 = 1.0020e-04\nLoss = 6.8296e-04, PNorm = 148.2228, GNorm = 0.4166, lr_0 = 1.0020e-04\n\n\r 96%|█████████▌| 160/167 [00:22&lt;00:00,  7.13it/s]Loss = 5.3950e-04, PNorm = 148.2235, GNorm = 0.2943, lr_0 = 1.0017e-04\nLoss = 5.3950e-04, PNorm = 148.2235, GNorm = 0.2943, lr_0 = 1.0017e-04\n\n\r 96%|█████████▋| 161/167 [00:22&lt;00:00,  7.21it/s]Loss = 2.4828e-04, PNorm = 148.2243, GNorm = 0.2092, lr_0 = 1.0014e-04\nLoss = 2.4828e-04, PNorm = 148.2243, GNorm = 0.2092, lr_0 = 1.0014e-04\n\n\r 97%|█████████▋| 162/167 [00:22&lt;00:00,  7.22it/s]Loss = 1.8798e-04, PNorm = 148.2252, GNorm = 0.1837, lr_0 = 1.0011e-04\nLoss = 1.8798e-04, PNorm = 148.2252, GNorm = 0.1837, lr_0 = 1.0011e-04\n\n\r 98%|█████████▊| 163/167 [00:22&lt;00:00,  7.26it/s]Loss = 6.7289e-04, PNorm = 148.2259, GNorm = 0.3118, lr_0 = 1.0009e-04\nLoss = 6.7289e-04, PNorm = 148.2259, GNorm = 0.3118, lr_0 = 1.0009e-04\n\n\r 98%|█████████▊| 164/167 [00:22&lt;00:00,  7.10it/s]Loss = 6.4387e-04, PNorm = 148.2268, GNorm = 0.3573, lr_0 = 1.0006e-04\nLoss = 6.4387e-04, PNorm = 148.2268, GNorm = 0.3573, lr_0 = 1.0006e-04\n\n\r 99%|█████████▉| 165/167 [00:23&lt;00:00,  7.20it/s]Loss = 2.2186e-04, PNorm = 148.2278, GNorm = 0.2922, lr_0 = 1.0003e-04\nLoss = 2.2186e-04, PNorm = 148.2278, GNorm = 0.2922, lr_0 = 1.0003e-04\n\n\r 99%|█████████▉| 166/167 [00:23&lt;00:00,  7.28it/s]Loss = 1.1350e-05, PNorm = 148.2287, GNorm = 0.0148, lr_0 = 1.0000e-04\nLoss = 1.1350e-05, PNorm = 148.2287, GNorm = 0.0148, lr_0 = 1.0000e-04\n\n\r100%|██████████| 167/167 [00:23&lt;00:00,  7.16it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 24.53it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 26.42it/s]Validation auc = 0.903092\nValidation auc = 0.903092\n\r100%|██████████| 50/50 [19:38&lt;00:00, 23.13s/it]\nModel 0 best validation auc = 0.915659 on epoch 9\nModel 0 best validation auc = 0.915659 on epoch 9\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\nMoving model to cuda\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 24.90it/s]\r100%|██████████| 4/4 [00:00&lt;00:00, 26.59it/s]\nModel 0 test auc = 0.871824\nModel 0 test auc = 0.871824\nEnsemble test auc = 0.871824\nEnsemble test auc = 0.871824\n1-fold cross validation\n1-fold cross validation\nSeed 13 ==&gt; test auc = 0.871824\nSeed 13 ==&gt; test auc = 0.871824\nOverall test auc = 0.871824 +/- 0.000000\nOverall test auc = 0.871824 +/- 0.000000\nOut[12]: (0.8718238387132906, 0.0)\n</div>"]}}],"execution_count":52},{"cell_type":"code","source":["%sh cat /dbfs/FileStore/chemprop/JAK/configs/bin76_int_Feat_SLogP.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{\n    &#34;depth&#34;: 6,\n    &#34;dropout&#34;: 0.25,\n    &#34;ffn_num_layers&#34;: 1,\n    &#34;hidden_size&#34;: 2200\n}</div>"]}}],"execution_count":53},{"cell_type":"code","source":["#LEO Pharma 0.8683418723401743\nparser = ArgumentParser()\nadd_train_args(parser)\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-1460_bin76.csv'),\n                          '--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtrain-1460.csv'),\n                          '--dataset_type','classification',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_int_Feat_SLogP'),\n                          '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'),\n                          '--separate_val_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPval-182.csv'),\n                          '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                          '--separate_test_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtest-183.csv'),\n                          '--log_frequency','1',\n                          '--depth','6',\n                          '--dropout','0.25',\n                          '--ffn_num_layers','1',\n                          '--hidden_size','2200',\n                          '--epochs','50'\n                         ,'--seed','13'\n                          ])\nmodify_train_args(args)\nlogger = create_logger(name='train', save_dir=args.save_dir, quiet=args.quiet)\n\ncross_validate(args, logger)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fold 0\nFold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 50,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_int_Feat_SLogP/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 50,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_int_Feat_SLogP/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_bin76.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 50,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_int_Feat_SLogP/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_bin76.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_bin76.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\nLoading data\n\r  0%|          | 0/1460 [00:00&lt;?, ?it/s]\r 18%|█▊        | 266/1460 [00:00&lt;00:00, 2650.56it/s]\r 40%|███▉      | 578/1460 [00:00&lt;00:00, 2773.94it/s]\r 61%|██████    | 887/1460 [00:00&lt;00:00, 2861.37it/s]\r 81%|████████▏ | 1189/1460 [00:00&lt;00:00, 2907.13it/s]\r100%|██████████| 1460/1460 [00:00&lt;00:00, 2975.12it/s]\nNumber of tasks = 4\nNumber of tasks = 4\nNumber of tasks = 4\nSplitting data with seed 13\nSplitting data with seed 13\nSplitting data with seed 13\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 3097.41it/s]\n\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\r100%|██████████| 182/182 [00:00&lt;00:00, 3090.54it/s]\nClass sizes\nClass sizes\nClass sizes\nJAK1 0: 50.89%, 1: 49.11%\nJAK1 0: 50.89%, 1: 49.11%\nJAK1 0: 50.89%, 1: 49.11%\nJAK2 0: 55.27%, 1: 44.73%\nJAK2 0: 55.27%, 1: 44.73%\nJAK2 0: 55.27%, 1: 44.73%\nJAK3 0: 83.56%, 1: 16.44%\nJAK3 0: 83.56%, 1: 16.44%\nJAK3 0: 83.56%, 1: 16.44%\nTYK2 0: 93.56%, 1: 6.44%\nTYK2 0: 93.56%, 1: 6.44%\nTYK2 0: 93.56%, 1: 6.44%\nTotal size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nTotal size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nTotal size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nBuilding model 0\nBuilding model 0\nBuilding model 0\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.25)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=2200, bias=False)\n      (W_h): Linear(in_features=2200, out_features=2200, bias=False)\n      (W_o): Linear(in_features=2333, out_features=2200, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.25)\n    (1): Linear(in_features=2201, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.25)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=2200, bias=False)\n      (W_h): Linear(in_features=2200, out_features=2200, bias=False)\n      (W_o): Linear(in_features=2333, out_features=2200, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.25)\n    (1): Linear(in_features=2201, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.25)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=2200, bias=False)\n      (W_h): Linear(in_features=2200, out_features=2200, bias=False)\n      (W_o): Linear(in_features=2333, out_features=2200, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.25)\n    (1): Linear(in_features=2201, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 10,307,008\nNumber of parameters = 10,307,008\nNumber of parameters = 10,307,008\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\r  0%|          | 0/50 [00:00&lt;?, ?it/s]Epoch 0\nEpoch 0\nEpoch 0\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 1.4936e-02, PNorm = 68.9104, GNorm = 4.9198, lr_0 = 1.1552e-04\nLoss = 1.4936e-02, PNorm = 68.9104, GNorm = 4.9198, lr_0 = 1.1552e-04\nLoss = 1.4936e-02, PNorm = 68.9104, GNorm = 4.9198, lr_0 = 1.1552e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:03,  7.99it/s]Loss = 8.9005e-03, PNorm = 68.9121, GNorm = 0.9983, lr_0 = 1.3103e-04\nLoss = 8.9005e-03, PNorm = 68.9121, GNorm = 0.9983, lr_0 = 1.3103e-04\nLoss = 8.9005e-03, PNorm = 68.9121, GNorm = 0.9983, lr_0 = 1.3103e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:03,  7.88it/s]Loss = 1.5420e-02, PNorm = 68.9130, GNorm = 4.2195, lr_0 = 1.4655e-04\nLoss = 1.5420e-02, PNorm = 68.9130, GNorm = 4.2195, lr_0 = 1.4655e-04\nLoss = 1.5420e-02, PNorm = 68.9130, GNorm = 4.2195, lr_0 = 1.4655e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:03,  7.89it/s]Loss = 1.2267e-02, PNorm = 68.9142, GNorm = 2.5862, lr_0 = 1.6207e-04\nLoss = 1.2267e-02, PNorm = 68.9142, GNorm = 2.5862, lr_0 = 1.6207e-04\nLoss = 1.2267e-02, PNorm = 68.9142, GNorm = 2.5862, lr_0 = 1.6207e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  7.87it/s]Loss = 1.2837e-02, PNorm = 68.9153, GNorm = 2.8714, lr_0 = 1.7759e-04\nLoss = 1.2837e-02, PNorm = 68.9153, GNorm = 2.8714, lr_0 = 1.7759e-04\nLoss = 1.2837e-02, PNorm = 68.9153, GNorm = 2.8714, lr_0 = 1.7759e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:03,  7.92it/s]Loss = 1.1681e-02, PNorm = 68.9166, GNorm = 1.4505, lr_0 = 1.9310e-04\nLoss = 1.1681e-02, PNorm = 68.9166, GNorm = 1.4505, lr_0 = 1.9310e-04\nLoss = 1.1681e-02, PNorm = 68.9166, GNorm = 1.4505, lr_0 = 1.9310e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:02,  7.96it/s]Loss = 1.0204e-02, PNorm = 68.9183, GNorm = 0.6358, lr_0 = 2.0862e-04\nLoss = 1.0204e-02, PNorm = 68.9183, GNorm = 0.6358, lr_0 = 2.0862e-04\nLoss = 1.0204e-02, PNorm = 68.9183, GNorm = 0.6358, lr_0 = 2.0862e-04\n\n\r 24%|██▍       | 7/29 [00:00&lt;00:02,  7.97it/s]Loss = 1.0625e-02, PNorm = 68.9206, GNorm = 0.4136, lr_0 = 2.2414e-04\nLoss = 1.0625e-02, PNorm = 68.9206, GNorm = 0.4136, lr_0 = 2.2414e-04\nLoss = 1.0625e-02, PNorm = 68.9206, GNorm = 0.4136, lr_0 = 2.2414e-04\n\n\r 28%|██▊       | 8/29 [00:01&lt;00:02,  7.92it/s]Loss = 1.0597e-02, PNorm = 68.9232, GNorm = 0.6020, lr_0 = 2.3966e-04\nLoss = 1.0597e-02, PNorm = 68.9232, GNorm = 0.6020, lr_0 = 2.3966e-04\nLoss = 1.0597e-02, PNorm = 68.9232, GNorm = 0.6020, lr_0 = 2.3966e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:02,  7.87it/s]Loss = 1.0429e-02, PNorm = 68.9261, GNorm = 0.4658, lr_0 = 2.5517e-04\nLoss = 1.0429e-02, PNorm = 68.9261, GNorm = 0.4658, lr_0 = 2.5517e-04\nLoss = 1.0429e-02, PNorm = 68.9261, GNorm = 0.4658, lr_0 = 2.5517e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  7.93it/s]Loss = 1.0898e-02, PNorm = 68.9295, GNorm = 0.2963, lr_0 = 2.7069e-04\nLoss = 1.0898e-02, PNorm = 68.9295, GNorm = 0.2963, lr_0 = 2.7069e-04\nLoss = 1.0898e-02, PNorm = 68.9295, GNorm = 0.2963, lr_0 = 2.7069e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  7.92it/s]Loss = 1.0133e-02, PNorm = 68.9332, GNorm = 0.3276, lr_0 = 2.8621e-04\nLoss = 1.0133e-02, PNorm = 68.9332, GNorm = 0.3276, lr_0 = 2.8621e-04\nLoss = 1.0133e-02, PNorm = 68.9332, GNorm = 0.3276, lr_0 = 2.8621e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  7.98it/s]Loss = 1.0496e-02, PNorm = 68.9372, GNorm = 0.5533, lr_0 = 3.0172e-04\nLoss = 1.0496e-02, PNorm = 68.9372, GNorm = 0.5533, lr_0 = 3.0172e-04\nLoss = 1.0496e-02, PNorm = 68.9372, GNorm = 0.5533, lr_0 = 3.0172e-04\n\n\r 45%|████▍     | 13/29 [00:01&lt;00:02,  7.97it/s]Loss = 9.6778e-03, PNorm = 68.9416, GNorm = 0.3257, lr_0 = 3.1724e-04\nLoss = 9.6778e-03, PNorm = 68.9416, GNorm = 0.3257, lr_0 = 3.1724e-04\nLoss = 9.6778e-03, PNorm = 68.9416, GNorm = 0.3257, lr_0 = 3.1724e-04\n\n\r 48%|████▊     | 14/29 [00:01&lt;00:01,  8.01it/s]Loss = 1.0109e-02, PNorm = 68.9461, GNorm = 0.2630, lr_0 = 3.3276e-04\nLoss = 1.0109e-02, PNorm = 68.9461, GNorm = 0.2630, lr_0 = 3.3276e-04\nLoss = 1.0109e-02, PNorm = 68.9461, GNorm = 0.2630, lr_0 = 3.3276e-04\n\n\r 52%|█████▏    | 15/29 [00:01&lt;00:01,  7.90it/s]Loss = 1.0304e-02, PNorm = 68.9507, GNorm = 0.5175, lr_0 = 3.4828e-04\nLoss = 1.0304e-02, PNorm = 68.9507, GNorm = 0.5175, lr_0 = 3.4828e-04\nLoss = 1.0304e-02, PNorm = 68.9507, GNorm = 0.5175, lr_0 = 3.4828e-04\n\n\r 55%|█████▌    | 16/29 [00:02&lt;00:01,  7.95it/s]Loss = 1.0655e-02, PNorm = 68.9550, GNorm = 0.4533, lr_0 = 3.6379e-04\nLoss = 1.0655e-02, PNorm = 68.9550, GNorm = 0.4533, lr_0 = 3.6379e-04\nLoss = 1.0655e-02, PNorm = 68.9550, GNorm = 0.4533, lr_0 = 3.6379e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  7.97it/s]Loss = 1.1297e-02, PNorm = 68.9592, GNorm = 0.6811, lr_0 = 3.7931e-04\nLoss = 1.1297e-02, PNorm = 68.9592, GNorm = 0.6811, lr_0 = 3.7931e-04\nLoss = 1.1297e-02, PNorm = 68.9592, GNorm = 0.6811, lr_0 = 3.7931e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  7.96it/s]Loss = 1.0915e-02, PNorm = 68.9633, GNorm = 0.6821, lr_0 = 3.9483e-04\nLoss = 1.0915e-02, PNorm = 68.9633, GNorm = 0.6821, lr_0 = 3.9483e-04\nLoss = 1.0915e-02, PNorm = 68.9633, GNorm = 0.6821, lr_0 = 3.9483e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  7.94it/s]Loss = 1.0045e-02, PNorm = 68.9677, GNorm = 0.3385, lr_0 = 4.1034e-04\nLoss = 1.0045e-02, PNorm = 68.9677, GNorm = 0.3385, lr_0 = 4.1034e-04\nLoss = 1.0045e-02, PNorm = 68.9677, GNorm = 0.3385, lr_0 = 4.1034e-04\n\n\r 69%|██████▉   | 20/29 [00:02&lt;00:01,  7.96it/s]Loss = 1.1580e-02, PNorm = 68.9723, GNorm = 0.4997, lr_0 = 4.2586e-04\nLoss = 1.1580e-02, PNorm = 68.9723, GNorm = 0.4997, lr_0 = 4.2586e-04\nLoss = 1.1580e-02, PNorm = 68.9723, GNorm = 0.4997, lr_0 = 4.2586e-04\n\n\r 72%|███████▏  | 21/29 [00:02&lt;00:01,  7.88it/s]Loss = 1.0053e-02, PNorm = 68.9771, GNorm = 0.2434, lr_0 = 4.4138e-04\nLoss = 1.0053e-02, PNorm = 68.9771, GNorm = 0.2434, lr_0 = 4.4138e-04\nLoss = 1.0053e-02, PNorm = 68.9771, GNorm = 0.2434, lr_0 = 4.4138e-04\n\n\r 76%|███████▌  | 22/29 [00:02&lt;00:00,  7.87it/s]Loss = 9.9873e-03, PNorm = 68.9821, GNorm = 0.3938, lr_0 = 4.5690e-04\nLoss = 9.9873e-03, PNorm = 68.9821, GNorm = 0.3938, lr_0 = 4.5690e-04\nLoss = 9.9873e-03, PNorm = 68.9821, GNorm = 0.3938, lr_0 = 4.5690e-04\n\n\r 79%|███████▉  | 23/29 [00:02&lt;00:00,  7.93it/s]Loss = 9.9477e-03, PNorm = 68.9873, GNorm = 0.2755, lr_0 = 4.7241e-04\nLoss = 9.9477e-03, PNorm = 68.9873, GNorm = 0.2755, lr_0 = 4.7241e-04\nLoss = 9.9477e-03, PNorm = 68.9873, GNorm = 0.2755, lr_0 = 4.7241e-04\n\n\r 83%|████████▎ | 24/29 [00:03&lt;00:00,  8.00it/s]Loss = 1.0090e-02, PNorm = 68.9927, GNorm = 0.2531, lr_0 = 4.8793e-04\nLoss = 1.0090e-02, PNorm = 68.9927, GNorm = 0.2531, lr_0 = 4.8793e-04\nLoss = 1.0090e-02, PNorm = 68.9927, GNorm = 0.2531, lr_0 = 4.8793e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  7.96it/s]Loss = 9.9170e-03, PNorm = 68.9982, GNorm = 0.2135, lr_0 = 5.0345e-04\nLoss = 9.9170e-03, PNorm = 68.9982, GNorm = 0.2135, lr_0 = 5.0345e-04\nLoss = 9.9170e-03, PNorm = 68.9982, GNorm = 0.2135, lr_0 = 5.0345e-04\n\n\r 90%|████████▉ | 26/29 [00:03&lt;00:00,  8.02it/s]Loss = 1.0515e-02, PNorm = 69.0037, GNorm = 0.3047, lr_0 = 5.1897e-04\nLoss = 1.0515e-02, PNorm = 69.0037, GNorm = 0.3047, lr_0 = 5.1897e-04\nLoss = 1.0515e-02, PNorm = 69.0037, GNorm = 0.3047, lr_0 = 5.1897e-04\n\n\r 93%|█████████▎| 27/29 [00:03&lt;00:00,  8.09it/s]Loss = 1.0906e-02, PNorm = 69.0091, GNorm = 0.5310, lr_0 = 5.3448e-04\nLoss = 1.0906e-02, PNorm = 69.0091, GNorm = 0.5310, lr_0 = 5.3448e-04\nLoss = 1.0906e-02, PNorm = 69.0091, GNorm = 0.5310, lr_0 = 5.3448e-04\n\n\r 97%|█████████▋| 28/29 [00:03&lt;00:00,  8.03it/s]Loss = 1.0766e-02, PNorm = 69.0146, GNorm = 0.2661, lr_0 = 5.5000e-04\nLoss = 1.0766e-02, PNorm = 69.0146, GNorm = 0.2661, lr_0 = 5.5000e-04\nLoss = 1.0766e-02, PNorm = 69.0146, GNorm = 0.2661, lr_0 = 5.5000e-04\n\n\r100%|██████████| 29/29 [00:03&lt;00:00,  8.04it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 28.69it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 31.23it/s]Validation auc = 0.702918\nValidation auc = 0.702918\nValidation auc = 0.702918\n\r  2%|▏         | 1/50 [00:09&lt;07:42,  9.44s/it]Epoch 1\nEpoch 1\nEpoch 1\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 1.0746e-02, PNorm = 69.0203, GNorm = 0.2366, lr_0 = 5.6552e-04\nLoss = 1.0746e-02, PNorm = 69.0203, GNorm = 0.2366, lr_0 = 5.6552e-04\nLoss = 1.0746e-02, PNorm = 69.0203, GNorm = 0.2366, lr_0 = 5.6552e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:04,  6.40it/s]Loss = 1.0718e-02, PNorm = 69.0260, GNorm = 0.2601, lr_0 = 5.8103e-04\nLoss = 1.0718e-02, PNorm = 69.0260, GNorm = 0.2601, lr_0 = 5.8103e-04\nLoss = 1.0718e-02, PNorm = 69.0260, GNorm = 0.2601, lr_0 = 5.8103e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:04,  6.37it/s]Loss = 1.0521e-02, PNorm = 69.0315, GNorm = 0.4050, lr_0 = 5.9655e-04\nLoss = 1.0521e-02, PNorm = 69.0315, GNorm = 0.4050, lr_0 = 5.9655e-04\nLoss = 1.0521e-02, PNorm = 69.0315, GNorm = 0.4050, lr_0 = 5.9655e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:04,  6.40it/s]Loss = 1.0238e-02, PNorm = 69.0367, GNorm = 0.4009, lr_0 = 6.1207e-04\nLoss = 1.0238e-02, PNorm = 69.0367, GNorm = 0.4009, lr_0 = 6.1207e-04\nLoss = 1.0238e-02, PNorm = 69.0367, GNorm = 0.4009, lr_0 = 6.1207e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  6.42it/s]Loss = 1.1010e-02, PNorm = 69.0419, GNorm = 0.4650, lr_0 = 6.2759e-04\nLoss = 1.1010e-02, PNorm = 69.0419, GNorm = 0.4650, lr_0 = 6.2759e-04\nLoss = 1.1010e-02, PNorm = 69.0419, GNorm = 0.4650, lr_0 = 6.2759e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:03,  6.40it/s]Loss = 9.3661e-03, PNorm = 69.0474, GNorm = 0.2359, lr_0 = 6.4310e-04\nLoss = 9.3661e-03, PNorm = 69.0474, GNorm = 0.2359, lr_0 = 6.4310e-04\nLoss = 9.3661e-03, PNorm = 69.0474, GNorm = 0.2359, lr_0 = 6.4310e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:03,  6.44it/s]Loss = 9.0611e-03, PNorm = 69.0535, GNorm = 0.3267, lr_0 = 6.5862e-04\nLoss = 9.0611e-03, PNorm = 69.0535, GNorm = 0.3267, lr_0 = 6.5862e-04\nLoss = 9.0611e-03, PNorm = 69.0535, GNorm = 0.3267, lr_0 = 6.5862e-04\n\n\r 24%|██▍       | 7/29 [00:01&lt;00:03,  6.42it/s]Loss = 1.1149e-02, PNorm = 69.0594, GNorm = 0.5833, lr_0 = 6.7414e-04\nLoss = 1.1149e-02, PNorm = 69.0594, GNorm = 0.5833, lr_0 = 6.7414e-04\nLoss = 1.1149e-02, PNorm = 69.0594, GNorm = 0.5833, lr_0 = 6.7414e-04\n\n\r 28%|██▊       | 8/29 [00:01&lt;00:03,  6.41it/s]Loss = 9.7798e-03, PNorm = 69.0653, GNorm = 0.2816, lr_0 = 6.8966e-04\nLoss = 9.7798e-03, PNorm = 69.0653, GNorm = 0.2816, lr_0 = 6.8966e-04\nLoss = 9.7798e-03, PNorm = 69.0653, GNorm = 0.2816, lr_0 = 6.8966e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:03,  6.42it/s]Loss = 9.3417e-03, PNorm = 69.0715, GNorm = 0.2717, lr_0 = 7.0517e-04\nLoss = 9.3417e-03, PNorm = 69.0715, GNorm = 0.2717, lr_0 = 7.0517e-04\nLoss = 9.3417e-03, PNorm = 69.0715, GNorm = 0.2717, lr_0 = 7.0517e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  6.39it/s]Loss = 1.1959e-02, PNorm = 69.0779, GNorm = 0.7099, lr_0 = 7.2069e-04\nLoss = 1.1959e-02, PNorm = 69.0779, GNorm = 0.7099, lr_0 = 7.2069e-04\nLoss = 1.1959e-02, PNorm = 69.0779, GNorm = 0.7099, lr_0 = 7.2069e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  6.43it/s]Loss = 8.7454e-03, PNorm = 69.0849, GNorm = 0.3292, lr_0 = 7.3621e-04\nLoss = 8.7454e-03, PNorm = 69.0849, GNorm = 0.3292, lr_0 = 7.3621e-04\nLoss = 8.7454e-03, PNorm = 69.0849, GNorm = 0.3292, lr_0 = 7.3621e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  6.44it/s]Loss = 9.7632e-03, PNorm = 69.0929, GNorm = 0.2516, lr_0 = 7.5172e-04\nLoss = 9.7632e-03, PNorm = 69.0929, GNorm = 0.2516, lr_0 = 7.5172e-04\nLoss = 9.7632e-03, PNorm = 69.0929, GNorm = 0.2516, lr_0 = 7.5172e-04\n\n\r 45%|████▍     | 13/29 [00:02&lt;00:02,  6.46it/s]Loss = 1.0139e-02, PNorm = 69.1006, GNorm = 0.4514, lr_0 = 7.6724e-04\nLoss = 1.0139e-02, PNorm = 69.1006, GNorm = 0.4514, lr_0 = 7.6724e-04\nLoss = 1.0139e-02, PNorm = 69.1006, GNorm = 0.4514, lr_0 = 7.6724e-04\n\n\r 48%|████▊     | 14/29 [00:02&lt;00:02,  6.42it/s]Loss = 1.0971e-02, PNorm = 69.1057, GNorm = 1.1708, lr_0 = 7.8276e-04\nLoss = 1.0971e-02, PNorm = 69.1057, GNorm = 1.1708, lr_0 = 7.8276e-04\nLoss = 1.0971e-02, PNorm = 69.1057, GNorm = 1.1708, lr_0 = 7.8276e-04\n\n\r 52%|█████▏    | 15/29 [00:02&lt;00:02,  6.42it/s]Loss = 8.6729e-03, PNorm = 69.1116, GNorm = 0.5056, lr_0 = 7.9828e-04\nLoss = 8.6729e-03, PNorm = 69.1116, GNorm = 0.5056, lr_0 = 7.9828e-04\nLoss = 8.6729e-03, PNorm = 69.1116, GNorm = 0.5056, lr_0 = 7.9828e-04\n\n\r 55%|█████▌    | 16/29 [00:02&lt;00:02,  6.39it/s]Loss = 9.9782e-03, PNorm = 69.1183, GNorm = 0.1837, lr_0 = 8.1379e-04\nLoss = 9.9782e-03, PNorm = 69.1183, GNorm = 0.1837, lr_0 = 8.1379e-04\nLoss = 9.9782e-03, PNorm = 69.1183, GNorm = 0.1837, lr_0 = 8.1379e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  6.41it/s]Loss = 1.1765e-02, PNorm = 69.1251, GNorm = 0.7035, lr_0 = 8.2931e-04\nLoss = 1.1765e-02, PNorm = 69.1251, GNorm = 0.7035, lr_0 = 8.2931e-04\nLoss = 1.1765e-02, PNorm = 69.1251, GNorm = 0.7035, lr_0 = 8.2931e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  6.38it/s]Loss = 9.0689e-03, PNorm = 69.1322, GNorm = 0.2532, lr_0 = 8.4483e-04\nLoss = 9.0689e-03, PNorm = 69.1322, GNorm = 0.2532, lr_0 = 8.4483e-04\nLoss = 9.0689e-03, PNorm = 69.1322, GNorm = 0.2532, lr_0 = 8.4483e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  6.41it/s]Loss = 1.1758e-02, PNorm = 69.1392, GNorm = 0.4877, lr_0 = 8.6034e-04\nLoss = 1.1758e-02, PNorm = 69.1392, GNorm = 0.4877, lr_0 = 8.6034e-04\nLoss = 1.1758e-02, PNorm = 69.1392, GNorm = 0.4877, lr_0 = 8.6034e-04\n\n\r 69%|██████▉   | 20/29 [00:03&lt;00:01,  6.38it/s]Loss = 1.0461e-02, PNorm = 69.1465, GNorm = 0.4357, lr_0 = 8.7586e-04\nLoss = 1.0461e-02, PNorm = 69.1465, GNorm = 0.4357, lr_0 = 8.7586e-04\nLoss = 1.0461e-02, PNorm = 69.1465, GNorm = 0.4357, lr_0 = 8.7586e-04\n\n\r 72%|███████▏  | 21/29 [00:03&lt;00:01,  6.38it/s]Loss = 9.5427e-03, PNorm = 69.1543, GNorm = 0.1755, lr_0 = 8.9138e-04\nLoss = 9.5427e-03, PNorm = 69.1543, GNorm = 0.1755, lr_0 = 8.9138e-04\nLoss = 9.5427e-03, PNorm = 69.1543, GNorm = 0.1755, lr_0 = 8.9138e-04\n\n\r 76%|███████▌  | 22/29 [00:03&lt;00:01,  6.41it/s]Loss = 9.5288e-03, PNorm = 69.1624, GNorm = 0.2537, lr_0 = 9.0690e-04\nLoss = 9.5288e-03, PNorm = 69.1624, GNorm = 0.2537, lr_0 = 9.0690e-04\nLoss = 9.5288e-03, PNorm = 69.1624, GNorm = 0.2537, lr_0 = 9.0690e-04\n\n\r 79%|███████▉  | 23/29 [00:03&lt;00:00,  6.40it/s]Loss = 9.5979e-03, PNorm = 69.1705, GNorm = 0.3423, lr_0 = 9.2241e-04\nLoss = 9.5979e-03, PNorm = 69.1705, GNorm = 0.3423, lr_0 = 9.2241e-04\nLoss = 9.5979e-03, PNorm = 69.1705, GNorm = 0.3423, lr_0 = 9.2241e-04\n\n\r 83%|████████▎ | 24/29 [00:03&lt;00:00,  6.38it/s]Loss = 1.0431e-02, PNorm = 69.1783, GNorm = 0.1494, lr_0 = 9.3793e-04\nLoss = 1.0431e-02, PNorm = 69.1783, GNorm = 0.1494, lr_0 = 9.3793e-04\nLoss = 1.0431e-02, PNorm = 69.1783, GNorm = 0.1494, lr_0 = 9.3793e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  6.35it/s]Loss = 1.0295e-02, PNorm = 69.1866, GNorm = 0.2880, lr_0 = 9.5345e-04\nLoss = 1.0295e-02, PNorm = 69.1866, GNorm = 0.2880, lr_0 = 9.5345e-04\nLoss = 1.0295e-02, PNorm = 69.1866, GNorm = 0.2880, lr_0 = 9.5345e-04\n\n\r 90%|████████▉ | 26/29 [00:04&lt;00:00,  6.36it/s]Loss = 9.7893e-03, PNorm = 69.1954, GNorm = 0.1891, lr_0 = 9.6897e-04\nLoss = 9.7893e-03, PNorm = 69.1954, GNorm = 0.1891, lr_0 = 9.6897e-04\nLoss = 9.7893e-03, PNorm = 69.1954, GNorm = 0.1891, lr_0 = 9.6897e-04\n\n\r 93%|█████████▎| 27/29 [00:04&lt;00:00,  6.32it/s]Loss = 9.1934e-03, PNorm = 69.2043, GNorm = 0.2875, lr_0 = 9.8448e-04\nLoss = 9.1934e-03, PNorm = 69.2043, GNorm = 0.2875, lr_0 = 9.8448e-04\nLoss = 9.1934e-03, PNorm = 69.2043, GNorm = 0.2875, lr_0 = 9.8448e-04\n\n\r 97%|█████████▋| 28/29 [00:04&lt;00:00,  6.31it/s]Loss = 1.0826e-02, PNorm = 69.2134, GNorm = 0.1555, lr_0 = 1.0000e-03\nLoss = 1.0826e-02, PNorm = 69.2134, GNorm = 0.1555, lr_0 = 1.0000e-03\nLoss = 1.0826e-02, PNorm = 69.2134, GNorm = 0.1555, lr_0 = 1.0000e-03\n\n\r100%|██████████| 29/29 [00:04&lt;00:00,  6.30it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 22.41it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 24.25it/s]Validation auc = 0.752919\nValidation auc = 0.752919\nValidation auc = 0.752919\n\r  4%|▍         | 2/50 [00:20&lt;08:02, 10.05s/it]Epoch 2\nEpoch 2\nEpoch 2\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 1.0952e-02, PNorm = 69.2228, GNorm = 0.1804, lr_0 = 9.9835e-04\nLoss = 1.0952e-02, PNorm = 69.2228, GNorm = 0.1804, lr_0 = 9.9835e-04\nLoss = 1.0952e-02, PNorm = 69.2228, GNorm = 0.1804, lr_0 = 9.9835e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:04,  5.80it/s]Loss = 1.0828e-02, PNorm = 69.2326, GNorm = 0.1667, lr_0 = 9.9670e-04\nLoss = 1.0828e-02, PNorm = 69.2326, GNorm = 0.1667, lr_0 = 9.9670e-04\nLoss = 1.0828e-02, PNorm = 69.2326, GNorm = 0.1667, lr_0 = 9.9670e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:04,  5.92it/s]Loss = 1.0142e-02, PNorm = 69.2434, GNorm = 0.1906, lr_0 = 9.9505e-04\nLoss = 1.0142e-02, PNorm = 69.2434, GNorm = 0.1906, lr_0 = 9.9505e-04\nLoss = 1.0142e-02, PNorm = 69.2434, GNorm = 0.1906, lr_0 = 9.9505e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:04,  6.07it/s]Loss = 9.8303e-03, PNorm = 69.2533, GNorm = 0.2860, lr_0 = 9.9341e-04\nLoss = 9.8303e-03, PNorm = 69.2533, GNorm = 0.2860, lr_0 = 9.9341e-04\nLoss = 9.8303e-03, PNorm = 69.2533, GNorm = 0.2860, lr_0 = 9.9341e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:04,  6.10it/s]Loss = 9.5751e-03, PNorm = 69.2639, GNorm = 0.2308, lr_0 = 9.9176e-04\nLoss = 9.5751e-03, PNorm = 69.2639, GNorm = 0.2308, lr_0 = 9.9176e-04\nLoss = 9.5751e-03, PNorm = 69.2639, GNorm = 0.2308, lr_0 = 9.9176e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:03,  6.20it/s]Loss = 8.8862e-03, PNorm = 69.2739, GNorm = 0.3762, lr_0 = 9.9012e-04\n\n*** WARNING: skipped 356044 bytes of output ***\n\n\n\r 10%|█         | 3/29 [00:00&lt;00:04,  6.48it/s]Loss = 2.1326e-03, PNorm = 82.6430, GNorm = 0.5989, lr_0 = 1.1472e-04\nLoss = 2.1326e-03, PNorm = 82.6430, GNorm = 0.5989, lr_0 = 1.1472e-04\nLoss = 2.1326e-03, PNorm = 82.6430, GNorm = 0.5989, lr_0 = 1.1472e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  6.43it/s]Loss = 1.9511e-03, PNorm = 82.6456, GNorm = 0.5605, lr_0 = 1.1453e-04\nLoss = 1.9511e-03, PNorm = 82.6456, GNorm = 0.5605, lr_0 = 1.1453e-04\nLoss = 1.9511e-03, PNorm = 82.6456, GNorm = 0.5605, lr_0 = 1.1453e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:03,  6.43it/s]Loss = 2.0371e-03, PNorm = 82.6483, GNorm = 0.7739, lr_0 = 1.1434e-04\nLoss = 2.0371e-03, PNorm = 82.6483, GNorm = 0.7739, lr_0 = 1.1434e-04\nLoss = 2.0371e-03, PNorm = 82.6483, GNorm = 0.7739, lr_0 = 1.1434e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:03,  6.45it/s]Loss = 1.8532e-03, PNorm = 82.6512, GNorm = 0.8640, lr_0 = 1.1415e-04\nLoss = 1.8532e-03, PNorm = 82.6512, GNorm = 0.8640, lr_0 = 1.1415e-04\nLoss = 1.8532e-03, PNorm = 82.6512, GNorm = 0.8640, lr_0 = 1.1415e-04\n\n\r 24%|██▍       | 7/29 [00:01&lt;00:03,  6.42it/s]Loss = 1.6277e-03, PNorm = 82.6542, GNorm = 0.6284, lr_0 = 1.1396e-04\nLoss = 1.6277e-03, PNorm = 82.6542, GNorm = 0.6284, lr_0 = 1.1396e-04\nLoss = 1.6277e-03, PNorm = 82.6542, GNorm = 0.6284, lr_0 = 1.1396e-04\n\n\r 28%|██▊       | 8/29 [00:01&lt;00:03,  6.40it/s]Loss = 1.8551e-03, PNorm = 82.6573, GNorm = 0.6080, lr_0 = 1.1377e-04\nLoss = 1.8551e-03, PNorm = 82.6573, GNorm = 0.6080, lr_0 = 1.1377e-04\nLoss = 1.8551e-03, PNorm = 82.6573, GNorm = 0.6080, lr_0 = 1.1377e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:03,  6.40it/s]Loss = 1.1741e-03, PNorm = 82.6604, GNorm = 0.3589, lr_0 = 1.1358e-04\nLoss = 1.1741e-03, PNorm = 82.6604, GNorm = 0.3589, lr_0 = 1.1358e-04\nLoss = 1.1741e-03, PNorm = 82.6604, GNorm = 0.3589, lr_0 = 1.1358e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  6.38it/s]Loss = 2.5041e-03, PNorm = 82.6631, GNorm = 0.9260, lr_0 = 1.1340e-04\nLoss = 2.5041e-03, PNorm = 82.6631, GNorm = 0.9260, lr_0 = 1.1340e-04\nLoss = 2.5041e-03, PNorm = 82.6631, GNorm = 0.9260, lr_0 = 1.1340e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  6.37it/s]Loss = 1.4930e-03, PNorm = 82.6659, GNorm = 0.4857, lr_0 = 1.1321e-04\nLoss = 1.4930e-03, PNorm = 82.6659, GNorm = 0.4857, lr_0 = 1.1321e-04\nLoss = 1.4930e-03, PNorm = 82.6659, GNorm = 0.4857, lr_0 = 1.1321e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  6.30it/s]Loss = 2.4689e-03, PNorm = 82.6686, GNorm = 0.6977, lr_0 = 1.1302e-04\nLoss = 2.4689e-03, PNorm = 82.6686, GNorm = 0.6977, lr_0 = 1.1302e-04\nLoss = 2.4689e-03, PNorm = 82.6686, GNorm = 0.6977, lr_0 = 1.1302e-04\n\n\r 45%|████▍     | 13/29 [00:02&lt;00:02,  6.36it/s]Loss = 1.9627e-03, PNorm = 82.6713, GNorm = 0.5820, lr_0 = 1.1283e-04\nLoss = 1.9627e-03, PNorm = 82.6713, GNorm = 0.5820, lr_0 = 1.1283e-04\nLoss = 1.9627e-03, PNorm = 82.6713, GNorm = 0.5820, lr_0 = 1.1283e-04\n\n\r 48%|████▊     | 14/29 [00:02&lt;00:02,  6.35it/s]Loss = 8.0081e-04, PNorm = 82.6740, GNorm = 0.3377, lr_0 = 1.1265e-04\nLoss = 8.0081e-04, PNorm = 82.6740, GNorm = 0.3377, lr_0 = 1.1265e-04\nLoss = 8.0081e-04, PNorm = 82.6740, GNorm = 0.3377, lr_0 = 1.1265e-04\n\n\r 52%|█████▏    | 15/29 [00:02&lt;00:02,  6.38it/s]Loss = 2.5316e-03, PNorm = 82.6768, GNorm = 0.6453, lr_0 = 1.1246e-04\nLoss = 2.5316e-03, PNorm = 82.6768, GNorm = 0.6453, lr_0 = 1.1246e-04\nLoss = 2.5316e-03, PNorm = 82.6768, GNorm = 0.6453, lr_0 = 1.1246e-04\n\n\r 55%|█████▌    | 16/29 [00:02&lt;00:02,  6.37it/s]Loss = 1.7277e-03, PNorm = 82.6796, GNorm = 0.4737, lr_0 = 1.1228e-04\nLoss = 1.7277e-03, PNorm = 82.6796, GNorm = 0.4737, lr_0 = 1.1228e-04\nLoss = 1.7277e-03, PNorm = 82.6796, GNorm = 0.4737, lr_0 = 1.1228e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  6.39it/s]Loss = 1.5523e-03, PNorm = 82.6822, GNorm = 0.6392, lr_0 = 1.1209e-04\nLoss = 1.5523e-03, PNorm = 82.6822, GNorm = 0.6392, lr_0 = 1.1209e-04\nLoss = 1.5523e-03, PNorm = 82.6822, GNorm = 0.6392, lr_0 = 1.1209e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  6.43it/s]Loss = 2.4654e-03, PNorm = 82.6848, GNorm = 0.8690, lr_0 = 1.1191e-04\nLoss = 2.4654e-03, PNorm = 82.6848, GNorm = 0.8690, lr_0 = 1.1191e-04\nLoss = 2.4654e-03, PNorm = 82.6848, GNorm = 0.8690, lr_0 = 1.1191e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  6.36it/s]Loss = 1.9527e-03, PNorm = 82.6873, GNorm = 0.9755, lr_0 = 1.1172e-04\nLoss = 1.9527e-03, PNorm = 82.6873, GNorm = 0.9755, lr_0 = 1.1172e-04\nLoss = 1.9527e-03, PNorm = 82.6873, GNorm = 0.9755, lr_0 = 1.1172e-04\n\n\r 69%|██████▉   | 20/29 [00:03&lt;00:01,  6.40it/s]Loss = 1.2457e-03, PNorm = 82.6898, GNorm = 0.4526, lr_0 = 1.1154e-04\nLoss = 1.2457e-03, PNorm = 82.6898, GNorm = 0.4526, lr_0 = 1.1154e-04\nLoss = 1.2457e-03, PNorm = 82.6898, GNorm = 0.4526, lr_0 = 1.1154e-04\n\n\r 72%|███████▏  | 21/29 [00:03&lt;00:01,  6.42it/s]Loss = 1.4147e-03, PNorm = 82.6924, GNorm = 0.5204, lr_0 = 1.1135e-04\nLoss = 1.4147e-03, PNorm = 82.6924, GNorm = 0.5204, lr_0 = 1.1135e-04\nLoss = 1.4147e-03, PNorm = 82.6924, GNorm = 0.5204, lr_0 = 1.1135e-04\n\n\r 76%|███████▌  | 22/29 [00:03&lt;00:01,  6.39it/s]Loss = 1.7928e-03, PNorm = 82.6950, GNorm = 0.9074, lr_0 = 1.1117e-04\nLoss = 1.7928e-03, PNorm = 82.6950, GNorm = 0.9074, lr_0 = 1.1117e-04\nLoss = 1.7928e-03, PNorm = 82.6950, GNorm = 0.9074, lr_0 = 1.1117e-04\n\n\r 79%|███████▉  | 23/29 [00:03&lt;00:00,  6.42it/s]Loss = 1.8353e-03, PNorm = 82.6977, GNorm = 0.7394, lr_0 = 1.1098e-04\nLoss = 1.8353e-03, PNorm = 82.6977, GNorm = 0.7394, lr_0 = 1.1098e-04\nLoss = 1.8353e-03, PNorm = 82.6977, GNorm = 0.7394, lr_0 = 1.1098e-04\n\n\r 83%|████████▎ | 24/29 [00:03&lt;00:00,  6.44it/s]Loss = 2.4108e-03, PNorm = 82.7002, GNorm = 0.6773, lr_0 = 1.1080e-04\nLoss = 2.4108e-03, PNorm = 82.7002, GNorm = 0.6773, lr_0 = 1.1080e-04\nLoss = 2.4108e-03, PNorm = 82.7002, GNorm = 0.6773, lr_0 = 1.1080e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  6.40it/s]Loss = 2.6626e-03, PNorm = 82.7029, GNorm = 0.7747, lr_0 = 1.1062e-04\nLoss = 2.6626e-03, PNorm = 82.7029, GNorm = 0.7747, lr_0 = 1.1062e-04\nLoss = 2.6626e-03, PNorm = 82.7029, GNorm = 0.7747, lr_0 = 1.1062e-04\n\n\r 90%|████████▉ | 26/29 [00:04&lt;00:00,  6.38it/s]Loss = 1.8357e-03, PNorm = 82.7055, GNorm = 0.6802, lr_0 = 1.1043e-04\nLoss = 1.8357e-03, PNorm = 82.7055, GNorm = 0.6802, lr_0 = 1.1043e-04\nLoss = 1.8357e-03, PNorm = 82.7055, GNorm = 0.6802, lr_0 = 1.1043e-04\n\n\r 93%|█████████▎| 27/29 [00:04&lt;00:00,  6.34it/s]Loss = 1.9090e-03, PNorm = 82.7081, GNorm = 0.6432, lr_0 = 1.1025e-04\nLoss = 1.9090e-03, PNorm = 82.7081, GNorm = 0.6432, lr_0 = 1.1025e-04\nLoss = 1.9090e-03, PNorm = 82.7081, GNorm = 0.6432, lr_0 = 1.1025e-04\n\n\r 97%|█████████▋| 28/29 [00:04&lt;00:00,  6.35it/s]Loss = 2.4986e-03, PNorm = 82.7108, GNorm = 0.7881, lr_0 = 1.1007e-04\nLoss = 2.4986e-03, PNorm = 82.7108, GNorm = 0.7881, lr_0 = 1.1007e-04\nLoss = 2.4986e-03, PNorm = 82.7108, GNorm = 0.7881, lr_0 = 1.1007e-04\n\n\r100%|██████████| 29/29 [00:04&lt;00:00,  6.34it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 22.35it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 24.10it/s]Validation auc = 0.905464\nValidation auc = 0.905464\nValidation auc = 0.905464\n\r 96%|█████████▌| 48/50 [05:00&lt;00:09,  4.71s/it]Epoch 48\nEpoch 48\nEpoch 48\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 2.2633e-03, PNorm = 82.7132, GNorm = 0.7872, lr_0 = 1.0989e-04\nLoss = 2.2633e-03, PNorm = 82.7132, GNorm = 0.7872, lr_0 = 1.0989e-04\nLoss = 2.2633e-03, PNorm = 82.7132, GNorm = 0.7872, lr_0 = 1.0989e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:04,  6.38it/s]Loss = 2.1328e-03, PNorm = 82.7157, GNorm = 0.6114, lr_0 = 1.0971e-04\nLoss = 2.1328e-03, PNorm = 82.7157, GNorm = 0.6114, lr_0 = 1.0971e-04\nLoss = 2.1328e-03, PNorm = 82.7157, GNorm = 0.6114, lr_0 = 1.0971e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:04,  6.36it/s]Loss = 1.8841e-03, PNorm = 82.7182, GNorm = 0.9593, lr_0 = 1.0952e-04\nLoss = 1.8841e-03, PNorm = 82.7182, GNorm = 0.9593, lr_0 = 1.0952e-04\nLoss = 1.8841e-03, PNorm = 82.7182, GNorm = 0.9593, lr_0 = 1.0952e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:04,  6.32it/s]Loss = 1.1746e-03, PNorm = 82.7208, GNorm = 0.3914, lr_0 = 1.0934e-04\nLoss = 1.1746e-03, PNorm = 82.7208, GNorm = 0.3914, lr_0 = 1.0934e-04\nLoss = 1.1746e-03, PNorm = 82.7208, GNorm = 0.3914, lr_0 = 1.0934e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  6.35it/s]Loss = 2.2599e-03, PNorm = 82.7233, GNorm = 0.6893, lr_0 = 1.0916e-04\nLoss = 2.2599e-03, PNorm = 82.7233, GNorm = 0.6893, lr_0 = 1.0916e-04\nLoss = 2.2599e-03, PNorm = 82.7233, GNorm = 0.6893, lr_0 = 1.0916e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:03,  6.39it/s]Loss = 1.7795e-03, PNorm = 82.7260, GNorm = 0.6861, lr_0 = 1.0898e-04\nLoss = 1.7795e-03, PNorm = 82.7260, GNorm = 0.6861, lr_0 = 1.0898e-04\nLoss = 1.7795e-03, PNorm = 82.7260, GNorm = 0.6861, lr_0 = 1.0898e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:03,  6.39it/s]Loss = 1.4684e-03, PNorm = 82.7288, GNorm = 0.6438, lr_0 = 1.0880e-04\nLoss = 1.4684e-03, PNorm = 82.7288, GNorm = 0.6438, lr_0 = 1.0880e-04\nLoss = 1.4684e-03, PNorm = 82.7288, GNorm = 0.6438, lr_0 = 1.0880e-04\n\n\r 24%|██▍       | 7/29 [00:01&lt;00:03,  6.39it/s]Loss = 2.9331e-03, PNorm = 82.7315, GNorm = 1.1000, lr_0 = 1.0862e-04\nLoss = 2.9331e-03, PNorm = 82.7315, GNorm = 1.1000, lr_0 = 1.0862e-04\nLoss = 2.9331e-03, PNorm = 82.7315, GNorm = 1.1000, lr_0 = 1.0862e-04\n\n\r 28%|██▊       | 8/29 [00:01&lt;00:03,  6.41it/s]Loss = 1.8687e-03, PNorm = 82.7340, GNorm = 0.6085, lr_0 = 1.0844e-04\nLoss = 1.8687e-03, PNorm = 82.7340, GNorm = 0.6085, lr_0 = 1.0844e-04\nLoss = 1.8687e-03, PNorm = 82.7340, GNorm = 0.6085, lr_0 = 1.0844e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:03,  6.44it/s]Loss = 2.0374e-03, PNorm = 82.7368, GNorm = 0.6918, lr_0 = 1.0826e-04\nLoss = 2.0374e-03, PNorm = 82.7368, GNorm = 0.6918, lr_0 = 1.0826e-04\nLoss = 2.0374e-03, PNorm = 82.7368, GNorm = 0.6918, lr_0 = 1.0826e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  6.46it/s]Loss = 1.5534e-03, PNorm = 82.7395, GNorm = 0.6113, lr_0 = 1.0808e-04\nLoss = 1.5534e-03, PNorm = 82.7395, GNorm = 0.6113, lr_0 = 1.0808e-04\nLoss = 1.5534e-03, PNorm = 82.7395, GNorm = 0.6113, lr_0 = 1.0808e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  6.41it/s]Loss = 1.4415e-03, PNorm = 82.7421, GNorm = 0.5270, lr_0 = 1.0791e-04\nLoss = 1.4415e-03, PNorm = 82.7421, GNorm = 0.5270, lr_0 = 1.0791e-04\nLoss = 1.4415e-03, PNorm = 82.7421, GNorm = 0.5270, lr_0 = 1.0791e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  6.40it/s]Loss = 1.4704e-03, PNorm = 82.7446, GNorm = 0.4161, lr_0 = 1.0773e-04\nLoss = 1.4704e-03, PNorm = 82.7446, GNorm = 0.4161, lr_0 = 1.0773e-04\nLoss = 1.4704e-03, PNorm = 82.7446, GNorm = 0.4161, lr_0 = 1.0773e-04\n\n\r 45%|████▍     | 13/29 [00:02&lt;00:02,  6.40it/s]Loss = 1.6910e-03, PNorm = 82.7470, GNorm = 0.7784, lr_0 = 1.0755e-04\nLoss = 1.6910e-03, PNorm = 82.7470, GNorm = 0.7784, lr_0 = 1.0755e-04\nLoss = 1.6910e-03, PNorm = 82.7470, GNorm = 0.7784, lr_0 = 1.0755e-04\n\n\r 48%|████▊     | 14/29 [00:02&lt;00:02,  6.35it/s]Loss = 9.0919e-04, PNorm = 82.7493, GNorm = 0.3389, lr_0 = 1.0737e-04\nLoss = 9.0919e-04, PNorm = 82.7493, GNorm = 0.3389, lr_0 = 1.0737e-04\nLoss = 9.0919e-04, PNorm = 82.7493, GNorm = 0.3389, lr_0 = 1.0737e-04\n\n\r 52%|█████▏    | 15/29 [00:02&lt;00:02,  6.42it/s]Loss = 2.4277e-03, PNorm = 82.7517, GNorm = 0.6920, lr_0 = 1.0719e-04\nLoss = 2.4277e-03, PNorm = 82.7517, GNorm = 0.6920, lr_0 = 1.0719e-04\nLoss = 2.4277e-03, PNorm = 82.7517, GNorm = 0.6920, lr_0 = 1.0719e-04\n\n\r 55%|█████▌    | 16/29 [00:02&lt;00:02,  6.43it/s]Loss = 1.7023e-03, PNorm = 82.7541, GNorm = 0.5676, lr_0 = 1.0702e-04\nLoss = 1.7023e-03, PNorm = 82.7541, GNorm = 0.5676, lr_0 = 1.0702e-04\nLoss = 1.7023e-03, PNorm = 82.7541, GNorm = 0.5676, lr_0 = 1.0702e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  6.38it/s]Loss = 1.4727e-03, PNorm = 82.7565, GNorm = 0.5437, lr_0 = 1.0684e-04\nLoss = 1.4727e-03, PNorm = 82.7565, GNorm = 0.5437, lr_0 = 1.0684e-04\nLoss = 1.4727e-03, PNorm = 82.7565, GNorm = 0.5437, lr_0 = 1.0684e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  6.40it/s]Loss = 1.8559e-03, PNorm = 82.7588, GNorm = 0.6436, lr_0 = 1.0666e-04\nLoss = 1.8559e-03, PNorm = 82.7588, GNorm = 0.6436, lr_0 = 1.0666e-04\nLoss = 1.8559e-03, PNorm = 82.7588, GNorm = 0.6436, lr_0 = 1.0666e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  6.37it/s]Loss = 2.8056e-03, PNorm = 82.7610, GNorm = 1.0303, lr_0 = 1.0649e-04\nLoss = 2.8056e-03, PNorm = 82.7610, GNorm = 1.0303, lr_0 = 1.0649e-04\nLoss = 2.8056e-03, PNorm = 82.7610, GNorm = 1.0303, lr_0 = 1.0649e-04\n\n\r 69%|██████▉   | 20/29 [00:03&lt;00:01,  6.36it/s]Loss = 1.8209e-03, PNorm = 82.7633, GNorm = 0.5945, lr_0 = 1.0631e-04\nLoss = 1.8209e-03, PNorm = 82.7633, GNorm = 0.5945, lr_0 = 1.0631e-04\nLoss = 1.8209e-03, PNorm = 82.7633, GNorm = 0.5945, lr_0 = 1.0631e-04\n\n\r 72%|███████▏  | 21/29 [00:03&lt;00:01,  6.38it/s]Loss = 1.0734e-03, PNorm = 82.7657, GNorm = 0.3714, lr_0 = 1.0614e-04\nLoss = 1.0734e-03, PNorm = 82.7657, GNorm = 0.3714, lr_0 = 1.0614e-04\nLoss = 1.0734e-03, PNorm = 82.7657, GNorm = 0.3714, lr_0 = 1.0614e-04\n\n\r 76%|███████▌  | 22/29 [00:03&lt;00:01,  6.36it/s]Loss = 1.7193e-03, PNorm = 82.7681, GNorm = 0.5895, lr_0 = 1.0596e-04\nLoss = 1.7193e-03, PNorm = 82.7681, GNorm = 0.5895, lr_0 = 1.0596e-04\nLoss = 1.7193e-03, PNorm = 82.7681, GNorm = 0.5895, lr_0 = 1.0596e-04\n\n\r 79%|███████▉  | 23/29 [00:03&lt;00:00,  6.40it/s]Loss = 1.1759e-03, PNorm = 82.7705, GNorm = 0.4308, lr_0 = 1.0579e-04\nLoss = 1.1759e-03, PNorm = 82.7705, GNorm = 0.4308, lr_0 = 1.0579e-04\nLoss = 1.1759e-03, PNorm = 82.7705, GNorm = 0.4308, lr_0 = 1.0579e-04\n\n\r 83%|████████▎ | 24/29 [00:03&lt;00:00,  6.38it/s]Loss = 2.5134e-03, PNorm = 82.7731, GNorm = 0.8428, lr_0 = 1.0561e-04\nLoss = 2.5134e-03, PNorm = 82.7731, GNorm = 0.8428, lr_0 = 1.0561e-04\nLoss = 2.5134e-03, PNorm = 82.7731, GNorm = 0.8428, lr_0 = 1.0561e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  6.36it/s]Loss = 1.0381e-03, PNorm = 82.7758, GNorm = 0.5802, lr_0 = 1.0544e-04\nLoss = 1.0381e-03, PNorm = 82.7758, GNorm = 0.5802, lr_0 = 1.0544e-04\nLoss = 1.0381e-03, PNorm = 82.7758, GNorm = 0.5802, lr_0 = 1.0544e-04\n\n\r 90%|████████▉ | 26/29 [00:04&lt;00:00,  6.41it/s]Loss = 2.2497e-03, PNorm = 82.7785, GNorm = 0.8592, lr_0 = 1.0526e-04\nLoss = 2.2497e-03, PNorm = 82.7785, GNorm = 0.8592, lr_0 = 1.0526e-04\nLoss = 2.2497e-03, PNorm = 82.7785, GNorm = 0.8592, lr_0 = 1.0526e-04\n\n\r 93%|█████████▎| 27/29 [00:04&lt;00:00,  6.43it/s]Loss = 1.7732e-03, PNorm = 82.7812, GNorm = 0.5756, lr_0 = 1.0509e-04\nLoss = 1.7732e-03, PNorm = 82.7812, GNorm = 0.5756, lr_0 = 1.0509e-04\nLoss = 1.7732e-03, PNorm = 82.7812, GNorm = 0.5756, lr_0 = 1.0509e-04\n\n\r 97%|█████████▋| 28/29 [00:04&lt;00:00,  6.45it/s]Loss = 1.9639e-03, PNorm = 82.7837, GNorm = 0.7002, lr_0 = 1.0491e-04\nLoss = 1.9639e-03, PNorm = 82.7837, GNorm = 0.7002, lr_0 = 1.0491e-04\nLoss = 1.9639e-03, PNorm = 82.7837, GNorm = 0.7002, lr_0 = 1.0491e-04\n\n\r100%|██████████| 29/29 [00:04&lt;00:00,  6.43it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 22.58it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 24.26it/s]Validation auc = 0.905744\nValidation auc = 0.905744\nValidation auc = 0.905744\n\r 98%|█████████▊| 49/50 [05:05&lt;00:04,  4.71s/it]Epoch 49\nEpoch 49\nEpoch 49\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 1.1844e-03, PNorm = 82.7862, GNorm = 0.6686, lr_0 = 1.0474e-04\nLoss = 1.1844e-03, PNorm = 82.7862, GNorm = 0.6686, lr_0 = 1.0474e-04\nLoss = 1.1844e-03, PNorm = 82.7862, GNorm = 0.6686, lr_0 = 1.0474e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:04,  6.50it/s]Loss = 1.2301e-03, PNorm = 82.7887, GNorm = 0.5340, lr_0 = 1.0457e-04\nLoss = 1.2301e-03, PNorm = 82.7887, GNorm = 0.5340, lr_0 = 1.0457e-04\nLoss = 1.2301e-03, PNorm = 82.7887, GNorm = 0.5340, lr_0 = 1.0457e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:04,  6.43it/s]Loss = 1.7430e-03, PNorm = 82.7912, GNorm = 0.6952, lr_0 = 1.0439e-04\nLoss = 1.7430e-03, PNorm = 82.7912, GNorm = 0.6952, lr_0 = 1.0439e-04\nLoss = 1.7430e-03, PNorm = 82.7912, GNorm = 0.6952, lr_0 = 1.0439e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:04,  6.39it/s]Loss = 1.2891e-03, PNorm = 82.7937, GNorm = 0.5992, lr_0 = 1.0422e-04\nLoss = 1.2891e-03, PNorm = 82.7937, GNorm = 0.5992, lr_0 = 1.0422e-04\nLoss = 1.2891e-03, PNorm = 82.7937, GNorm = 0.5992, lr_0 = 1.0422e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  6.42it/s]Loss = 1.1837e-03, PNorm = 82.7960, GNorm = 0.5630, lr_0 = 1.0405e-04\nLoss = 1.1837e-03, PNorm = 82.7960, GNorm = 0.5630, lr_0 = 1.0405e-04\nLoss = 1.1837e-03, PNorm = 82.7960, GNorm = 0.5630, lr_0 = 1.0405e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:03,  6.39it/s]Loss = 1.4405e-03, PNorm = 82.7983, GNorm = 0.5195, lr_0 = 1.0388e-04\nLoss = 1.4405e-03, PNorm = 82.7983, GNorm = 0.5195, lr_0 = 1.0388e-04\nLoss = 1.4405e-03, PNorm = 82.7983, GNorm = 0.5195, lr_0 = 1.0388e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:03,  6.39it/s]Loss = 1.7704e-03, PNorm = 82.8006, GNorm = 0.6061, lr_0 = 1.0371e-04\nLoss = 1.7704e-03, PNorm = 82.8006, GNorm = 0.6061, lr_0 = 1.0371e-04\nLoss = 1.7704e-03, PNorm = 82.8006, GNorm = 0.6061, lr_0 = 1.0371e-04\n\n\r 24%|██▍       | 7/29 [00:01&lt;00:03,  6.36it/s]Loss = 1.6179e-03, PNorm = 82.8029, GNorm = 0.5556, lr_0 = 1.0353e-04\nLoss = 1.6179e-03, PNorm = 82.8029, GNorm = 0.5556, lr_0 = 1.0353e-04\nLoss = 1.6179e-03, PNorm = 82.8029, GNorm = 0.5556, lr_0 = 1.0353e-04\n\n\r 28%|██▊       | 8/29 [00:01&lt;00:03,  6.40it/s]Loss = 1.1949e-03, PNorm = 82.8051, GNorm = 0.5280, lr_0 = 1.0336e-04\nLoss = 1.1949e-03, PNorm = 82.8051, GNorm = 0.5280, lr_0 = 1.0336e-04\nLoss = 1.1949e-03, PNorm = 82.8051, GNorm = 0.5280, lr_0 = 1.0336e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:03,  6.42it/s]Loss = 1.8991e-03, PNorm = 82.8075, GNorm = 0.8478, lr_0 = 1.0319e-04\nLoss = 1.8991e-03, PNorm = 82.8075, GNorm = 0.8478, lr_0 = 1.0319e-04\nLoss = 1.8991e-03, PNorm = 82.8075, GNorm = 0.8478, lr_0 = 1.0319e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  6.43it/s]Loss = 1.5357e-03, PNorm = 82.8102, GNorm = 0.7949, lr_0 = 1.0302e-04\nLoss = 1.5357e-03, PNorm = 82.8102, GNorm = 0.7949, lr_0 = 1.0302e-04\nLoss = 1.5357e-03, PNorm = 82.8102, GNorm = 0.7949, lr_0 = 1.0302e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  6.43it/s]Loss = 1.8797e-03, PNorm = 82.8129, GNorm = 0.5356, lr_0 = 1.0285e-04\nLoss = 1.8797e-03, PNorm = 82.8129, GNorm = 0.5356, lr_0 = 1.0285e-04\nLoss = 1.8797e-03, PNorm = 82.8129, GNorm = 0.5356, lr_0 = 1.0285e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  6.44it/s]Loss = 1.8742e-03, PNorm = 82.8154, GNorm = 0.7045, lr_0 = 1.0268e-04\nLoss = 1.8742e-03, PNorm = 82.8154, GNorm = 0.7045, lr_0 = 1.0268e-04\nLoss = 1.8742e-03, PNorm = 82.8154, GNorm = 0.7045, lr_0 = 1.0268e-04\n\n\r 45%|████▍     | 13/29 [00:02&lt;00:02,  6.41it/s]Loss = 2.3856e-03, PNorm = 82.8183, GNorm = 0.7701, lr_0 = 1.0251e-04\nLoss = 2.3856e-03, PNorm = 82.8183, GNorm = 0.7701, lr_0 = 1.0251e-04\nLoss = 2.3856e-03, PNorm = 82.8183, GNorm = 0.7701, lr_0 = 1.0251e-04\n\n\r 48%|████▊     | 14/29 [00:02&lt;00:02,  6.39it/s]Loss = 1.3823e-03, PNorm = 82.8211, GNorm = 0.4251, lr_0 = 1.0234e-04\nLoss = 1.3823e-03, PNorm = 82.8211, GNorm = 0.4251, lr_0 = 1.0234e-04\nLoss = 1.3823e-03, PNorm = 82.8211, GNorm = 0.4251, lr_0 = 1.0234e-04\n\n\r 52%|█████▏    | 15/29 [00:02&lt;00:02,  6.47it/s]Loss = 1.7471e-03, PNorm = 82.8238, GNorm = 0.5482, lr_0 = 1.0217e-04\nLoss = 1.7471e-03, PNorm = 82.8238, GNorm = 0.5482, lr_0 = 1.0217e-04\nLoss = 1.7471e-03, PNorm = 82.8238, GNorm = 0.5482, lr_0 = 1.0217e-04\n\n\r 55%|█████▌    | 16/29 [00:02&lt;00:02,  6.46it/s]Loss = 2.0449e-03, PNorm = 82.8265, GNorm = 0.6148, lr_0 = 1.0200e-04\nLoss = 2.0449e-03, PNorm = 82.8265, GNorm = 0.6148, lr_0 = 1.0200e-04\nLoss = 2.0449e-03, PNorm = 82.8265, GNorm = 0.6148, lr_0 = 1.0200e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  6.38it/s]Loss = 1.6680e-03, PNorm = 82.8290, GNorm = 0.6268, lr_0 = 1.0184e-04\nLoss = 1.6680e-03, PNorm = 82.8290, GNorm = 0.6268, lr_0 = 1.0184e-04\nLoss = 1.6680e-03, PNorm = 82.8290, GNorm = 0.6268, lr_0 = 1.0184e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  6.40it/s]Loss = 1.6766e-03, PNorm = 82.8316, GNorm = 0.5857, lr_0 = 1.0167e-04\nLoss = 1.6766e-03, PNorm = 82.8316, GNorm = 0.5857, lr_0 = 1.0167e-04\nLoss = 1.6766e-03, PNorm = 82.8316, GNorm = 0.5857, lr_0 = 1.0167e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  6.35it/s]Loss = 1.3425e-03, PNorm = 82.8339, GNorm = 0.5221, lr_0 = 1.0150e-04\nLoss = 1.3425e-03, PNorm = 82.8339, GNorm = 0.5221, lr_0 = 1.0150e-04\nLoss = 1.3425e-03, PNorm = 82.8339, GNorm = 0.5221, lr_0 = 1.0150e-04\n\n\r 69%|██████▉   | 20/29 [00:03&lt;00:01,  6.35it/s]Loss = 1.7411e-03, PNorm = 82.8360, GNorm = 0.9297, lr_0 = 1.0133e-04\nLoss = 1.7411e-03, PNorm = 82.8360, GNorm = 0.9297, lr_0 = 1.0133e-04\nLoss = 1.7411e-03, PNorm = 82.8360, GNorm = 0.9297, lr_0 = 1.0133e-04\n\n\r 72%|███████▏  | 21/29 [00:03&lt;00:01,  6.34it/s]Loss = 2.0829e-03, PNorm = 82.8380, GNorm = 0.7026, lr_0 = 1.0116e-04\nLoss = 2.0829e-03, PNorm = 82.8380, GNorm = 0.7026, lr_0 = 1.0116e-04\nLoss = 2.0829e-03, PNorm = 82.8380, GNorm = 0.7026, lr_0 = 1.0116e-04\n\n\r 76%|███████▌  | 22/29 [00:03&lt;00:01,  6.30it/s]Loss = 2.4043e-03, PNorm = 82.8398, GNorm = 0.8317, lr_0 = 1.0100e-04\nLoss = 2.4043e-03, PNorm = 82.8398, GNorm = 0.8317, lr_0 = 1.0100e-04\nLoss = 2.4043e-03, PNorm = 82.8398, GNorm = 0.8317, lr_0 = 1.0100e-04\n\n\r 79%|███████▉  | 23/29 [00:03&lt;00:00,  6.31it/s]Loss = 1.2662e-03, PNorm = 82.8417, GNorm = 0.4069, lr_0 = 1.0083e-04\nLoss = 1.2662e-03, PNorm = 82.8417, GNorm = 0.4069, lr_0 = 1.0083e-04\nLoss = 1.2662e-03, PNorm = 82.8417, GNorm = 0.4069, lr_0 = 1.0083e-04\n\n\r 83%|████████▎ | 24/29 [00:03&lt;00:00,  6.35it/s]Loss = 1.4226e-03, PNorm = 82.8435, GNorm = 0.4589, lr_0 = 1.0066e-04\nLoss = 1.4226e-03, PNorm = 82.8435, GNorm = 0.4589, lr_0 = 1.0066e-04\nLoss = 1.4226e-03, PNorm = 82.8435, GNorm = 0.4589, lr_0 = 1.0066e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  6.39it/s]Loss = 1.4894e-03, PNorm = 82.8454, GNorm = 0.5409, lr_0 = 1.0050e-04\nLoss = 1.4894e-03, PNorm = 82.8454, GNorm = 0.5409, lr_0 = 1.0050e-04\nLoss = 1.4894e-03, PNorm = 82.8454, GNorm = 0.5409, lr_0 = 1.0050e-04\n\n\r 90%|████████▉ | 26/29 [00:04&lt;00:00,  6.41it/s]Loss = 2.6309e-03, PNorm = 82.8471, GNorm = 0.7476, lr_0 = 1.0033e-04\nLoss = 2.6309e-03, PNorm = 82.8471, GNorm = 0.7476, lr_0 = 1.0033e-04\nLoss = 2.6309e-03, PNorm = 82.8471, GNorm = 0.7476, lr_0 = 1.0033e-04\n\n\r 93%|█████████▎| 27/29 [00:04&lt;00:00,  6.39it/s]Loss = 2.0429e-03, PNorm = 82.8489, GNorm = 0.5953, lr_0 = 1.0017e-04\nLoss = 2.0429e-03, PNorm = 82.8489, GNorm = 0.5953, lr_0 = 1.0017e-04\nLoss = 2.0429e-03, PNorm = 82.8489, GNorm = 0.5953, lr_0 = 1.0017e-04\n\n\r 97%|█████████▋| 28/29 [00:04&lt;00:00,  6.39it/s]Loss = 1.6191e-03, PNorm = 82.8508, GNorm = 0.4759, lr_0 = 1.0000e-04\nLoss = 1.6191e-03, PNorm = 82.8508, GNorm = 0.4759, lr_0 = 1.0000e-04\nLoss = 1.6191e-03, PNorm = 82.8508, GNorm = 0.4759, lr_0 = 1.0000e-04\n\n\r100%|██████████| 29/29 [00:04&lt;00:00,  6.42it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 22.43it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 24.18it/s]Validation auc = 0.901110\nValidation auc = 0.901110\nValidation auc = 0.901110\n\r100%|██████████| 50/50 [05:10&lt;00:00,  4.71s/it]\nModel 0 best validation auc = 0.928952 on epoch 16\nModel 0 best validation auc = 0.928952 on epoch 16\nModel 0 best validation auc = 0.928952 on epoch 16\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 21.97it/s]\r100%|██████████| 4/4 [00:00&lt;00:00, 23.56it/s]\nModel 0 test auc = 0.868342\nModel 0 test auc = 0.868342\nModel 0 test auc = 0.868342\nEnsemble test auc = 0.868342\nEnsemble test auc = 0.868342\nEnsemble test auc = 0.868342\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\nSeed 13 ==&gt; test auc = 0.868342\nSeed 13 ==&gt; test auc = 0.868342\nSeed 13 ==&gt; test auc = 0.868342\nOverall test auc = 0.868342 +/- 0.000000\nOverall test auc = 0.868342 +/- 0.000000\nOverall test auc = 0.868342 +/- 0.000000\nOut[14]: (0.8683418723401743, 0.0)\n</div>"]}}],"execution_count":54},{"cell_type":"markdown","source":["# Predicting"],"metadata":{}},{"cell_type":"code","source":["from chemprop.parsing import parse_predict_args\nfrom chemprop.train import make_predictions\nfrom chemprop.parsing import add_predict_args\nfrom chemprop.parsing import modify_predict_args"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":56},{"cell_type":"code","source":["parser = ArgumentParser()\nadd_predict_args(parser)\nargs = parser.parse_args(['-h'])\nargs = modify_predict_args(args)\nmake_predictions(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">An exception has occurred, use %tb to see the full traceback.\n\n<span class=\"ansi-red-fg\">SystemExit</span><span class=\"ansi-red-fg\">:</span> 0\n</div>"]}}],"execution_count":57},{"cell_type":"code","source":["args = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183.csv'),\n                          '--checkpoint_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x','fold_0/model_0/model.pt'),\n                          '--preds_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x','fold_0/model_0/test_preds.csv')])\nmodify_predict_args(args)\nmake_predictions(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading training args\nLoading data\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 2918.70it/s]\nValidating SMILES\nTest size = 183\nPredicting with an ensemble of 1 models\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 41.35it/s]\r100%|██████████| 1/1 [00:02&lt;00:00,  2.65s/it]\nSaving predictions to /dbfs/FileStore/chemprop/JAK/hyperopt_4x/fold_0/model_0/test_preds.csv\nOut[18]: \n[[6.899637586169948, 6.531157096847112, 6.175900392628288, 5.320111577712392],\n [7.593240553468566, 7.312949170868578, 6.859105123723975, 6.276921745976],\n [8.020992975285782, 7.857408510215259, 7.442176224274623, 6.682392337185232],\n [8.658362812268239, 8.546459274857964, 7.767360477703561, 7.266105693319486],\n [6.864499169382916, 6.645517846372046, 6.011358890164527, 5.472122091479891],\n [7.1040683856700415,\n  7.469376671921825,\n  6.9059594085544544,\n  5.9740965175293335],\n [7.30754328585698, 7.357443363033125, 6.917917163479255, 6.000812272560142],\n [8.009960102787538, 7.956842218289083, 6.984640418591329, 6.709087645729637],\n [8.584225309550554, 8.341514940194493, 7.869507884637914, 7.455692008005942],\n [8.12609317495107, 7.897917402231123, 7.014885601852481, 6.7066224487450254],\n [6.476417210574423, 6.204789671863807, 5.801053758475284, 5.0406507671277],\n [8.338812591161643, 8.224016302414533, 7.397551459203281, 6.903509639881448],\n [7.650031316874766, 7.778541806582597, 7.297534495063218, 6.397647590669345],\n [7.672467871347743, 7.323727732781512, 7.089780923105578, 6.2001155022589565],\n [6.766955175070212, 6.557115935170149, 6.039251893368728, 5.35170020317129],\n [4.950023638578006, 5.086581352722986, 4.931853690920167, 4.382439185456414],\n [7.6207134435795005, 7.548837129888205, 6.953088713780863, 6.180198121195923],\n [8.360827679290793, 8.194650829344278, 7.264586568687507, 6.927689119334243],\n [7.678875753744483, 7.461450493219322, 6.838029098031936, 6.201838884057814],\n [7.103889214926498, 7.383565278559359, 6.446064534127553, 5.925109156652351],\n [8.308746200764306, 8.2779788134243, 7.947611176561811, 7.075904275420901],\n [6.822823360091342, 6.776854240480904, 6.244417084216265, 5.340652379941437],\n [7.411881564576869, 7.237566742783765, 6.452107530308637, 6.0505619912191575],\n [8.561261024029912, 8.237095981919294, 7.661343148792008, 7.3807019277152115],\n [7.443728556691487, 7.460667850117051, 6.734979631675984, 6.211487137462194],\n [8.4924418282286, 8.246636898065288, 7.772974585724306, 7.338037343749908],\n [5.863901299133064, 5.953659515003183, 5.446369942442336, 4.694003884348164],\n [7.230360454086937, 7.098380570681725, 6.319552259242483, 5.808197799064974],\n [7.16277900057464, 7.021496489010715, 6.441216418645131, 5.763310015665831],\n [6.937983989459417, 6.681725187243604, 6.227975774874055, 5.456763818583041],\n [7.768479601084319, 7.39774500738674, 7.01236478380439, 6.2825145648130345],\n [8.144314250887188, 7.996885113087711, 7.086142397261297, 6.744770979099277],\n [7.4815368807662415, 7.445972418233734, 6.637195239400001, 6.113930799800478],\n [7.3619230480395235, 7.214039203928581, 6.712841966826436, 5.952983384543003],\n [7.689779387834453, 7.314106605589771, 6.86130658005233, 6.005052426751254],\n [8.342002162473927, 7.8959037710588325, 7.616065110688559, 6.651953214331002],\n [8.343047299987237, 8.376731427325595, 7.743831567813293, 7.167461591190321],\n [7.4069035009402135, 7.292952975607953, 6.686847654817569, 5.961316609217326],\n [8.355093551342986, 8.213387652992438, 7.838122930171352, 7.144623834095983],\n [7.427022533921191, 7.026064181762678, 6.807551924151561, 5.8055416637915975],\n [8.335623261360052, 8.10460152265176, 7.2406546828852845, 6.938958835352471],\n [7.994335930929118, 8.006019667245505, 7.2275247480584195, 6.684688209924504],\n [8.942526947374652, 8.75275637594369, 7.706005666808015, 7.5836946447260285],\n [7.345954670616622, 6.971777473473867, 6.501731177492072, 5.697590281183902],\n [7.672053423908359, 7.326810502788137, 6.8069575267140845, 6.073205435500165],\n [7.6824938404938745, 7.489532596485503, 6.873968372236618, 6.230199548952809],\n [5.408195260175395, 5.483836442878369, 5.10330952262491, 4.541314080209271],\n [7.269795255492159, 7.093520382649795, 6.69433857448617, 5.847219889888451],\n [8.013383697958954, 7.934677366586632, 7.021653331996646, 6.609017102165344],\n [8.583590740211163, 8.279059452546761, 7.876758999563191, 7.4112660646019055],\n [6.6036501744528255, 6.533965384073247, 6.126540525759529, 5.231323080707571],\n [8.559461527874703, 8.54708090007068, 7.79255443954297, 7.354202256164664],\n [7.90528209399815, 7.583238897812199, 7.412804727503687, 6.724423535704045],\n [6.814331361190591,\n  6.6387548456069165,\n  6.1620060342166445,\n  5.361940931251226],\n [7.71680031332967, 7.594481330081855, 6.814144259385513, 6.295953829596043],\n [8.596515909347456, 8.392763992859983, 8.081664038301126, 7.538667039254128],\n [8.960263975509257, 8.831901635952876, 7.911455524664291, 7.626360367771355],\n [7.987977920489975, 7.871731440145451, 7.560816176690374, 6.656831808636203],\n [8.985335924826238, 8.73959816194121, 7.879774234097004, 7.566021362545529],\n [7.441162407490786, 7.097423220190297, 6.717200483000815, 5.814790025355966],\n [6.560510629243564, 6.739008260964627, 5.949776531975226, 5.240163252945891],\n [7.021134459826182, 6.658811625216742, 6.498805765548683, 5.734485880467447],\n [6.8661420458553115, 6.639298213437289, 6.189048419450713, 5.428190565759604],\n [7.868703941418303, 8.091273215231144, 7.392230255197382, 6.879149274521085],\n [7.159312265556126, 6.86097510707636, 6.536797019548616, 5.53382036089652],\n [6.8068416920332515,\n  7.123656208755372,\n  6.4823274141938825,\n  5.589597179770384],\n [8.256456177756537, 7.842560585204159, 7.533626407882715, 6.843271671053614],\n [7.738937651151487, 7.761927160199728, 6.840060583371512, 6.45033354437926],\n [8.208238206124282, 7.7988921067992845, 7.485673990403003, 6.768749695698114],\n [8.647189320170426, 8.220652121743004, 7.747406526524121, 7.457497677657548],\n [8.82393807375437, 8.441991655122642, 7.902872760418086, 7.774112715644885],\n [7.706366160927691, 7.7821786687846695, 6.925406612253908, 6.493593354610301],\n [7.179836900647447, 6.876329405015062, 6.405361098776894, 5.471640032814385],\n [7.9072226323966355, 7.794960393836544, 6.965751504947291, 6.463890476128589],\n [7.291193102673962, 7.390920982351747, 6.50253916405261, 6.119356793248974],\n [8.859589398870435, 8.75059797969868, 7.849003328823937, 7.53037465059861],\n [8.140264961894257, 7.813013241312261, 7.51359763723112, 6.6132112802390495],\n [9.001092565298418, 8.626461043073046, 8.156118728757198, 7.975170249605543],\n [6.476220258606289, 6.242002719860004, 5.820037998304603, 5.07832048519451],\n [7.915496909409865, 7.795502819474637, 6.826256409145056, 6.512758603804221],\n [6.230770649459398, 6.060803847583848, 5.750686721490528, 5.046529331307619],\n [8.455398677002956, 8.372978398357667, 7.497829016721131, 7.165496336427568],\n [7.367224064299894, 7.261124501070764, 6.8946052104463185, 6.313195599787035],\n [8.2766194222883, 8.294745014206612, 7.318006985389128, 7.009313258629791],\n [8.557941459588447, 8.563965207426488, 7.616580070659755, 7.311046842617682],\n [6.917988172213868, 6.660918699694012, 6.212120567740327, 5.432585819934275],\n [8.550209011809443, 8.233497029100738, 7.900196714028683, 7.353570978016215],\n [8.387123785920446, 8.14851677229947, 7.387156896821722, 6.902785868435176],\n [6.791799440183687, 7.306109596197017, 6.428887753296442, 5.748988476451424],\n [5.103053023467335, 5.653069916578942, 5.203408268468481, 4.5595821899775],\n [7.685934930095936, 7.640583020050681, 6.841199012386194, 6.27127339698787],\n [8.070753053054805, 7.842790507832096, 7.434609749686893, 6.680261631049216],\n [6.765364344151628, 6.396945464437141, 6.044169925343478, 5.214298732415868],\n [8.492358024018984, 8.271130516546261, 7.781858883229852, 7.3508445899824535],\n [7.8604166832054565, 7.911821333711946, 6.947295401847453, 6.558415066879522],\n [6.957932168721256, 7.549427219371068, 6.597032431119931, 6.046766562345815],\n [7.395978847872957, 7.000439101199179, 6.977402897177622, 5.793109573564449],\n [6.835083529541237, 6.6846555160815795, 6.403385001787548, 5.422011797039765],\n [7.853717629482435, 7.824731924068085, 7.162500939298971, 6.506391402771666],\n [7.212119815784885,\n  7.1814996097678145,\n  6.7534881506945315,\n  5.739404228665432],\n [7.041066971081972, 7.079651672533334, 6.51167086592021, 5.78252951354582],\n [6.97685090866467, 7.247993375096344, 6.923272198045958, 5.995030359320128],\n [6.577332211105679, 6.2814386765608745, 5.838411130728324, 5.147308638938888],\n [8.734436868458014, 8.346683363659778, 7.8274158028258425, 7.564089368919431],\n [5.700149322605162, 5.900229342631805, 5.407067460032454, 4.684510107992956],\n [8.606574346587047, 8.330006892836398, 7.895865323590194, 7.292410354370188],\n [8.27524933214087, 8.286816964975317, 7.246409751576046, 6.999525200950781],\n [8.07774901398301, 7.791625753664157, 7.376645153222306, 6.7442318525246465],\n [5.977291775894829, 6.1268128411283795, 5.908973541619186, 4.978270074869614],\n [7.455748468287549,\n  7.497062292362983,\n  7.2943519218279205,\n  6.2710033495915445],\n [7.534709147043557, 7.818214170411217, 6.817837680217678, 6.562116564366511],\n [8.448549735707617, 8.067902357409872, 7.5761239661264534, 6.924716974785635],\n [7.60878483811888, 7.499191570709799, 6.841163995917144, 6.233373634591274],\n [7.54644884586092, 7.414840722599387, 6.59044111018993, 6.099441032705254],\n [7.202210662340884, 6.9708311521744255, 6.425679022663768, 5.61969685677792],\n [8.857628090552563, 8.645787512819467, 7.849338888763275, 7.506189475745702],\n [4.815240144848249, 5.4418572320975125, 4.873171298208953, 4.435714299830947],\n [7.766871864417397, 7.644628599721652, 6.712164160509022, 6.337329829285938],\n [6.826490035772786, 6.441475688528527, 5.931484985135821, 5.290656050457558],\n [8.211836715413321, 8.098513020713197, 7.611906697378355, 6.892145038497349],\n [7.907229907906188, 7.821013451387614, 6.91370848236838, 6.456744429130676],\n [6.326803511381396, 6.660940647231835, 6.106669729117948, 5.3023524658522],\n [6.698711740755146, 6.424424446869876, 6.10529655155789, 5.263314257046406],\n [6.330991789776732, 6.008974071935222, 5.750719571455366, 4.886589282825759],\n [7.097361241338886, 6.966493714588854, 6.197910947725829, 5.7713313032760025],\n [4.615558139867398,\n  4.8111789925672666,\n  4.686788246362278,\n  4.1604191618423965],\n [7.687573051826611, 7.4556064555380805, 6.874601536922711, 6.238842770695619],\n [8.151076308711605, 8.276861484225979, 7.435724638269682, 6.945635894628136],\n [7.76300346693168, 7.612951250055074, 6.979202143684619, 6.3321650553635935],\n [8.357750893470993, 8.016025749262605, 7.609889611477659, 6.9806753060610784],\n [8.829865953653663, 8.429665341480618, 7.8517189912903405, 7.633500235260145],\n [7.9056396203866575, 7.512264675652251, 6.952172444910852, 6.221634934410423],\n [6.568136389675022, 6.293630201283736, 5.882340751319839, 5.107530597658505],\n [8.115015743730421, 8.45530871165897, 7.825367435913851, 7.065404349681852],\n [8.698382341244091, 8.634633175528023, 7.722740321433256, 7.431498403960868],\n [8.341781059436737, 8.238507552224455, 7.878085353665906, 7.0874170711159215],\n [7.30178958044882, 7.1936506617899285, 6.624688865547494, 5.761311699105426],\n [6.973272444762946, 7.355589946115797, 6.481726455956526, 6.124398845537398],\n [6.929078463878789, 6.572551039952658, 6.092249231941104, 5.383486117290077],\n [7.1509947279404535, 6.946544317187447, 6.259596758789473, 5.748310951654064],\n [7.565571986867831, 7.210619530902894, 6.961785080069651, 6.065504770439315],\n [7.353040707485188, 7.20966568592098, 6.588529144176717, 5.92492348660869],\n [6.062847662550531, 5.795535153364599, 5.427927874122252, 4.691008673429101],\n [7.7984730552531145, 7.467028451643868, 7.351352543428783, 6.279307492129044],\n [8.572345640193605, 8.32928982908799, 8.031247873458506, 7.435777585881249],\n [7.079657239791213, 7.1433903429322605, 6.722433519171981, 5.816456072273819],\n [8.186337654440122, 8.079426754575184, 7.534424514938903, 6.9138462784992765],\n [6.495916662973241, 6.174486883743849, 5.784784396635596, 5.00645991335749],\n [5.74993131746935, 6.163837894053112, 6.137404352340299, 4.944670175815057],\n [7.495411236916617, 7.449230037651147, 6.675197610906691, 6.2517615755778655],\n [8.233697478211216, 8.164907980554212, 7.369083287435613, 6.907060152311461],\n [8.347690463768204, 8.441521999982434, 7.592340620933005, 7.12690476089858],\n [6.6954920409842185, 6.478085788888388, 6.041069084781891, 5.232443422863663],\n [7.714618611412263, 7.8882795511264066, 6.917486314126985, 6.597419245669646],\n [8.689163274409472, 8.383688907662, 7.7954220943840005, 7.632449433939424],\n [7.2666359635827025,\n  7.537284300685256,\n  6.9842843151665175,\n  6.212835010852772],\n [8.671425280232105, 8.378153029207386, 7.3382480060374204, 7.192605643606137],\n [8.403036019655007, 8.10070400572649, 7.802388346181347, 6.9239282188240825],\n [7.6118653220711305,\n  7.267092263839759,\n  6.7757141703049175,\n  5.9274292348421005],\n [7.557593047240295, 7.299755513809434, 6.769308169754916, 6.061566956563165],\n [8.288488223664176, 7.787091979834287, 7.433954907701241, 6.70091953070457],\n [6.357414508385901, 5.900862274151885, 5.695936812779102, 4.787715542164066],\n [7.071922829737848, 6.711581043684234, 6.380496396547746, 5.465309139957546],\n [6.393774425868009, 6.325438723513811, 5.932515395376196, 5.051770694122974],\n [7.608253061767149, 7.562326773881059, 7.111865562228299, 6.128265114509262],\n [6.485265316988375, 6.389928570564, 5.919702928493109, 5.195055570148411],\n [6.5134781128358235,\n  6.343178430614007,\n  5.961260585504072,\n  5.1134092757464265],\n [7.974156686314105, 8.128970882597882, 7.428416599748617, 6.748789311694515],\n [7.093184676212041, 6.783712735204799, 6.453096780033367, 5.558038796886084],\n [7.14612010635144, 6.78257257169945, 6.728798371463787, 5.858561367948094],\n [6.891086719038378, 6.679238464700438, 6.083868225314768, 5.518731651286986],\n [6.361454861483282, 6.803419739786322, 6.007274756998699, 5.292639188776664],\n [6.177809528620292, 6.563215133762249, 6.501129606382237, 5.236901611209574],\n [7.060573729179178, 7.139968688531631, 6.398654778455058, 5.7743457931244455],\n [6.844909875007218, 6.546230288947823, 6.112179599787578, 5.339452245229772],\n [7.904615494303425, 8.10774173867595, 7.241545456260196, 6.676991787936738],\n [8.667512203268638, 8.220501537247378, 7.759554541581096, 7.182183517032623],\n [7.683863357053415, 7.990662043922285, 6.950656811271929, 6.473796343158924],\n [8.70590660680785, 8.702239240410789, 8.035623292655837, 7.617443193723516],\n [9.01430271704955, 8.671885906446143, 8.230842886984545, 7.892363118752668],\n [8.174887753356987, 8.214427279039167, 7.4935343005717465, 7.041052299605654],\n [7.525675884949261, 7.768255865746702, 6.832066421853501, 6.446868178180926],\n [7.663480639681622, 7.698873297319375, 6.845289062835801, 6.275055605413701]]\n</div>"]}}],"execution_count":58},{"cell_type":"markdown","source":["# Similarity versus accuracy"],"metadata":{}},{"cell_type":"code","source":["%sh \ncp /dbfs/FileStore/chemprop/JAK/train-1460_bin76.csv /dbfs/FileStore/chemprop/JAK/all-1825_bin76.csv\nsed 1d /dbfs/FileStore/chemprop/JAK/val-182_bin76.csv >> /dbfs/FileStore/chemprop/JAK/all-1825_bin76.csv\nsed 1d /dbfs/FileStore/chemprop/JAK/test-183_bin76.csv >> /dbfs/FileStore/chemprop/JAK/all-1825_bin76.csv"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":60},{"cell_type":"code","source":["for name in names:\n  args = parser.parse_args(['--test_path','/dbfs/FileStore/tables/'+name+'-smiles.txt',\n                          #'--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPall-1825.csv'),\n                            '--checkpoint_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext','fold_0/model_0/model.pt'),\n                            '--preds_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext',name+'-preds.csv')])\n  modify_predict_args(args)\n  make_predictions(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading training args\nLoading data\n\r  0%|          | 0/2107 [00:00&lt;?, ?it/s]\r 14%|█▍        | 300/2107 [00:00&lt;00:00, 2998.88it/s]\r 30%|██▉       | 632/2107 [00:00&lt;00:00, 3088.39it/s]\r 45%|████▌     | 953/2107 [00:00&lt;00:00, 3123.33it/s]\r 60%|█████▉    | 1260/2107 [00:00&lt;00:00, 3104.09it/s]\r 74%|███████▎  | 1553/2107 [00:00&lt;00:00, 3048.95it/s]\r 88%|████████▊ | 1853/2107 [00:00&lt;00:00, 3031.06it/s]\r100%|██████████| 2107/2107 [00:00&lt;00:00, 3092.96it/s]\nValidating SMILES\nTest size = 2,107\nPredicting with an ensemble of 1 models\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\r  0%|          | 0/43 [00:00&lt;?, ?it/s]\n\r  7%|▋         | 3/43 [00:00&lt;00:02, 16.76it/s]\n\r 12%|█▏        | 5/43 [00:00&lt;00:02, 15.32it/s]\n\r 23%|██▎       | 10/43 [00:00&lt;00:01, 19.00it/s]\n\r 35%|███▍      | 15/43 [00:00&lt;00:01, 21.82it/s]\n\r 42%|████▏     | 18/43 [00:00&lt;00:01, 21.94it/s]\n\r 53%|█████▎    | 23/43 [00:00&lt;00:00, 25.92it/s]\n\r 60%|██████    | 26/43 [00:01&lt;00:02,  8.13it/s]\n\r 70%|██████▉   | 30/43 [00:01&lt;00:01, 10.42it/s]\n\r 81%|████████▏ | 35/43 [00:02&lt;00:00, 13.39it/s]\n\r 91%|█████████ | 39/43 [00:02&lt;00:00, 16.64it/s]\n\r100%|██████████| 43/43 [00:02&lt;00:00, 19.12it/s]\r100%|██████████| 1/1 [00:03&lt;00:00,  3.93s/it]\nSaving predictions to /dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_ext/JAK1 EC50 nM 1027-preds.csv\nLoading training args\nLoading data\n\r  0%|          | 0/2145 [00:00&lt;?, ?it/s]\r 14%|█▍        | 306/2145 [00:00&lt;00:00, 3056.79it/s]\r 30%|██▉       | 637/2145 [00:00&lt;00:00, 3128.30it/s]\r 45%|████▌     | 966/2145 [00:00&lt;00:00, 3173.54it/s]\r 60%|█████▉    | 1278/2145 [00:00&lt;00:00, 3157.20it/s]\r 73%|███████▎  | 1567/2145 [00:00&lt;00:00, 3070.24it/s]\r 87%|████████▋ | 1874/2145 [00:00&lt;00:00, 3067.50it/s]\r100%|██████████| 2145/2145 [00:00&lt;00:00, 3124.76it/s]\nValidating SMILES\nTest size = 2,145\nPredicting with an ensemble of 1 models\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\r  0%|          | 0/43 [00:00&lt;?, ?it/s]\n\r 12%|█▏        | 5/43 [00:00&lt;00:00, 46.13it/s]\n\r 21%|██        | 9/43 [00:00&lt;00:00, 41.34it/s]\n\r 33%|███▎      | 14/43 [00:00&lt;00:00, 41.41it/s]\n\r 44%|████▍     | 19/43 [00:00&lt;00:00, 42.37it/s]\n\r 56%|█████▌    | 24/43 [00:00&lt;00:00, 43.26it/s]\n\r 67%|██████▋   | 29/43 [00:00&lt;00:00, 43.58it/s]\n\r 79%|███████▉  | 34/43 [00:00&lt;00:00, 43.14it/s]\n\r 88%|████████▊ | 38/43 [00:00&lt;00:00, 41.06it/s]\n\r100%|██████████| 43/43 [00:01&lt;00:00, 41.65it/s]\r100%|██████████| 1/1 [00:02&lt;00:00,  2.48s/it]\nSaving predictions to /dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_ext/JAK2 EC50 nM 1024-preds.csv\nLoading training args\nLoading data\n\r  0%|          | 0/2142 [00:00&lt;?, ?it/s]\r 10%|█         | 224/2142 [00:00&lt;00:00, 2233.42it/s]\r 25%|██▌       | 544/2142 [00:00&lt;00:00, 2454.74it/s]\r 41%|████      | 879/2142 [00:00&lt;00:00, 2666.48it/s]\r 55%|█████▌    | 1185/2142 [00:00&lt;00:00, 2771.15it/s]\r 69%|██████▉   | 1483/2142 [00:00&lt;00:00, 2830.62it/s]\r 83%|████████▎ | 1772/2142 [00:00&lt;00:00, 2845.93it/s]\r 96%|█████████▋| 2063/2142 [00:00&lt;00:00, 2864.40it/s]\r100%|██████████| 2142/2142 [00:00&lt;00:00, 2945.15it/s]\nValidating SMILES\nTest size = 2,142\nPredicting with an ensemble of 1 models\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\r  0%|          | 0/43 [00:00&lt;?, ?it/s]\n\r  9%|▉         | 4/43 [00:00&lt;00:01, 29.54it/s]\n\r 16%|█▋        | 7/43 [00:00&lt;00:01, 28.42it/s]\n\r 23%|██▎       | 10/43 [00:00&lt;00:01, 28.38it/s]\n\r 33%|███▎      | 14/43 [00:00&lt;00:00, 29.10it/s]\n\r 40%|███▉      | 17/43 [00:00&lt;00:00, 29.17it/s]\n\r 47%|████▋     | 20/43 [00:00&lt;00:00, 28.09it/s]\n\r 56%|█████▌    | 24/43 [00:00&lt;00:00, 28.60it/s]\n\r 65%|██████▌   | 28/43 [00:00&lt;00:00, 28.54it/s]\n\r 74%|███████▍  | 32/43 [00:01&lt;00:00, 29.69it/s]\n\r 81%|████████▏ | 35/43 [00:01&lt;00:00, 28.54it/s]\n\r 88%|████████▊ | 38/43 [00:01&lt;00:00, 25.98it/s]\n\r 95%|█████████▌| 41/43 [00:01&lt;00:00, 26.16it/s]\n\r100%|██████████| 43/43 [00:01&lt;00:00, 27.88it/s]\r100%|██████████| 1/1 [00:03&lt;00:00,  3.12s/it]\nSaving predictions to /dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_ext/JAK3 EC50 nM 1026-preds.csv\nLoading training args\nLoading data\n\r  0%|          | 0/1865 [00:00&lt;?, ?it/s]\r 16%|█▋        | 307/1865 [00:00&lt;00:00, 3061.51it/s]\r 34%|███▍      | 642/1865 [00:00&lt;00:00, 3141.52it/s]\r 51%|█████     | 949/1865 [00:00&lt;00:00, 3117.97it/s]\r 67%|██████▋   | 1244/1865 [00:00&lt;00:00, 3065.35it/s]\r 83%|████████▎ | 1556/1865 [00:00&lt;00:00, 3078.71it/s]\r100%|██████████| 1865/1865 [00:00&lt;00:00, 3104.65it/s]\nValidating SMILES\nTest size = 1,865\nPredicting with an ensemble of 1 models\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\r  0%|          | 0/38 [00:00&lt;?, ?it/s]\n\r  8%|▊         | 3/38 [00:00&lt;00:01, 28.51it/s]\n\r 18%|█▊        | 7/38 [00:00&lt;00:01, 28.60it/s]\n\r 29%|██▉       | 11/38 [00:00&lt;00:00, 29.62it/s]\n\r 37%|███▋      | 14/38 [00:00&lt;00:00, 29.57it/s]\n\r 45%|████▍     | 17/38 [00:00&lt;00:00, 28.32it/s]\n\r 55%|█████▌    | 21/38 [00:00&lt;00:00, 29.79it/s]\n\r 63%|██████▎   | 24/38 [00:00&lt;00:00, 29.34it/s]\n\r 71%|███████   | 27/38 [00:00&lt;00:00, 28.29it/s]\n\r 79%|███████▉  | 30/38 [00:01&lt;00:00, 28.64it/s]\n\r 87%|████████▋ | 33/38 [00:01&lt;00:00, 27.89it/s]\n\r 95%|█████████▍| 36/38 [00:01&lt;00:00, 27.64it/s]\n\r100%|██████████| 38/38 [00:01&lt;00:00, 28.86it/s]\r100%|██████████| 1/1 [00:03&lt;00:00,  3.29s/it]\nSaving predictions to /dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_ext/TYK2 EC50 nM 1025-preds.csv\n</div>"]}}],"execution_count":61},{"cell_type":"code","source":["targets=['JAK1','JAK2','JAK3','TYK2']\nfrom sklearn.metrics import roc_auc_score\n\nplt.close()\n\nf, axes = plt.subplots(1, 5, sharex = True, sharey=True, figsize=(16,4))\n\n\n#colors = [plt.cm.Blues(np.linspace(0.2,1,len(idx))),plt.cm.Reds(np.linspace(0.2,1,len(idx)))]\nfor i,ax in enumerate(axes.flat):\n  mean_ratios = []\n  for name,target in zip(names,targets):\n    trues = pd.read_csv('/dbfs/FileStore/tables/'+name+'-trues_bin76.csv')\n    chemprop_preds = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext',name+'-preds.csv'))\n    df = trues.filter(['smiles',name]).merge(chemprop_preds.filter(['smiles',target]))\n    #diffs.append(abs(np.array(df[name])-np.array(df[targets[i]])))\n    fps = get_fps([Chem.MolFromSmiles(smi) for smi in df.smiles], bit=True)\n    dists = fps_distances(fps)\n    dists[dists==0]=1\n    ratio = []\n    for thresh in range(50):\n      n_neighbors = np.sum(dists<(i+1)*0.1,axis=1)\n      #n_true = diff_cp[(diff_cp.JAK1<0.5) & (n_neighbors>thresh)]\n      #n_false = diff_cp[(diff_cp.JAK1>0.5) & (n_neighbors>thresh)]\n      preds = df[target][n_neighbors>thresh]\n      truevals = df[name][n_neighbors>thresh]\n      try:\n        ratio.append(roc_auc_score(truevals,preds)-roc_auc_score(df[name],df[target]))\n      except:\n        continue#ratio.append(0)\n    ax.plot(range(len(ratio)),ratio,label=target)\n    mean_ratios.append(ratio)\n    #ax.set_xlim([0,55])\n    #ax.set_ylim([-0.3,0.3])\n  ax.plot(range(len(ratio)),np.mean(np.array(mean_ratios),axis=0),color='black',ls='--',label='mean')\n  ax.plot(range(55),np.zeros(55),color='grey',ls='--')\n  ax.set_title('Threshold: {:02.1f}'.format((i+1)*0.1))\n  ax.set_xlabel('Number of neighbors')\n  ax.set_ylabel('ROC/AUC score increase')\n  #plt.axis([0,55,0.7,1.001])\n  ax.legend()\nplt.tight_layout()\ndisplay(plt.show())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"image/png":["iVBORw0KGgoAAAANSUhEUgAABkAAAAGQCAYAAAD2lawGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XmcW1X5x/HPSTJrp810X4GylE4Kl012QTYBoaCAIori4IoiKAhqQdGLGwUVFHEDBaL+QBAEhYKsRZBVZAtthqVQoJSWrulMO2tyfn+cm046zNZOpslkvu/X677u5N5z7z1hyjPJfe55jrHWIiIiIiIiIiIiIiIiUkpChe6AiIiIiIiIiIiIiIhIvikBIiIiIiIiIiIiIiIiJUcJEBERERERERERERERKTlKgIiIiIiIiIiIiIiISMlRAkREREREREREREREREqOEiAiIiIiIiIiIiIiIlJylAAREREREREREREREZGSowSIiIiIiIiIiIiIiIiUHCVARERERERERERERESk5CgBIiIiIiIiIiIiIiIiJUcJEBERERERERERERERKTlKgIiIiIiIiIiIiIiISMlRAkREREREREREREREREqOEiAiIiIiIiIiIiIiIlJylACRIcUYc6gxxhpjPlbovmQZYxYbY+7M4/mmB+/x9H60vd4Yszhf1xaRrUOx7D1tFctEhhjFsfe0VRwTGYIUy97TVrFMZIhRHHtPW8UxeQ8lQKTggiDWn+XQQve1FBljPmyMecYY02KMedMYc7ExJtLPY79jjPmnMWZ58DvyB7m7IkVLsaywtjSWGWPqjDGXGWOeM8Y0GmPeMcbMM8bsvTX6LVJMFMcKawBxbIox5i/GmJeCOLbWGPOUMabeGGO2Rt9FioliWWEN5Ptll/N8Kvg9NQ1GP0WKmeJYYQ3gM9n0Xn5Xn9gafZfubfYfIZFBcFqX158BjuxmexKIbZUeDRPGmGOA24GHgLMBD/guMAH4Sj9O8SNgGfAscPTg9FJkyFAsK5ABxrIvAJ8HbgV+A0SBM4AnjDEfstbeP0jdFilGimMFMsA4Ng6YBtwCvAmU4X5v1wMzgQsHo88iRUyxrEDy8P0ye54a4DJgff57KTIkKI4VSJ7i2I3AXV22PZ6nLsoWUAJECs5a+5fc18aY/YEju24P9g0osBtjqq21GwZyjhLzM+AF4ChrbQeAMWYdcKEx5pfW2oY+jt/eWrvYGDMOWDHIfRUpaoplBTWQWHYj4FtrNz5daIy5FvdlwgeUAJFhQ3GsoLY4jllrXwAO7bL5KmPMHcDXjDEXWWvTg9RvkaKjWFZQA/1+mfVdoBGYD5wwKD0VKWKKYwWVjzj2THe/KykclcCSoSpkXPmlJcGQtAeMMTvlNjDGPGSMedEY8z5jzMPGmA3AT3L2H2OMecQYsz4oFzDPGLNLl3NMMsZcF1yn1bjSKP8wxkzv2iFjzEFBuYEWY8xrxpjPdNNmB2PM34wxq40xG4wxTxhjZvfnDRtjTgjeT0uwPrGHdpONK+lS1sf5ZgGzgKuzQT3wG8AAfdaPtNYu7k/fRaRHimUFjmXW2v/lJj+CbauAR9DTVCL9oThWBJ/JerAYqAbKt/B4keFEsaxIYpkxZgZwLvANoKOP5iLSSXGsSOJYcK4Rxhh9BisSSoDIUDUHOBGXmb0E2B/4v27ajQXuBp4DzsE9QYIx5jRgHtAEfBv4IS7I/adL0L41uM51wJnAlcBIYNsu19kJV3bgPuA8YA1wfe4fCmPMROAxXKmo3wDfASqBf/YUpHOOPSroiwUuwA3Huw7orkb9Jbgnl6f2dk5gz2D9dO5Ga+1SYEnOfhEZPIplxRvLJgErt/BYkeFEcaxI4pgxpsoYM864+tP1wGeBx621zf05XmSYUywrklgG/AKYb63tWj5GRHqnOFY8cez7uP+OLcaY/wZ9lUKy1mrRUlQLcJX7p9ntvkNxwW0hUJ6z/WvB9l1ztj0UbDujyzlqcIH36i7bJwJrs9uB2uD48/vo7+Kg3cE528YDLcDPcrZdEbQ7qEtfXgNeB0LBtulBu9Nz2j0LLAWiOduODNot7tKf64Pt0/vo9/lBu2262fcU7gtzf39n44Jz+YX+96NFS7EsimVDL5blHHcwkAF+UOh/R1q0FHJRHBtacQx348PmLPd3d04tWobbolg2dGIZMBtoB2blXLup0P+GtGgp9KI4NjTiGC4JdA/wZeB44OvAG0AamF3of0fDedEIEBmqrrPWtuW8fiRY79ClXSsuA5zrSFzQvjF4Sm6ccXNYpIEngcOCds1AG3CoMWZ0H/1ZaK3N9gFr7QrgpS79ORZ4ylr7n5x2TcDVuGA+q7sTG2MmA3sAcWttKufY+3B/4DZhrT3dWmts3+WpqoJ1azf7WnL2i8jgUSwrslhmjJkA3ID7wH3Z5hwrMkwpjhVPHLsR99/0VFwcyz23iPROsazAsSwoFXMF8Dtr7Xv6ISJ9UhwrcByz1r5prT3aWvs7a+0d1tpf4kaNrAB+3se1ZRApASJD1ZtdXq8J1l0D8Ntd/gAAzAjWD+KCUO5yFDABwFrbihv2dwywPKiP+C1jzKR+9Cfbp9z+bIcL9l0lc/Z3J7v9lW72dXe+/sqWQ6joZl9lzn4RGTyKZU5RxDJjzAjgTtwQ7o/YLnODiEi3FMecgscxa+0b1tr7rbU3Wms/hXt68n5jjJIgIn1TLHMKGcvOxVUW+P4A+iAynCmOOQX/TJbLWrsal3CaaYyZNoC+yQBECt0BkS2U7mG76fK6u+CUTfydBizrZv/GiY6stb8wxtwBnICrSfhD4AJjzOHW2me3oD/F5J1gPRl4q8u+ybjhfSIyuBTLBi4vsSx46vDvwG7A0dbaF/PWQ5HSpjg2cIP1mewW4IvAB3DlGESkZ4plA7fFscwYEwW+i5sDYJQxZlSwq8btNtOBDdbad/PZYZESozg2cIP1mSx7rjG4uURkK1MCRIajRcH6XWvt/X01ttYuwg1V+7kxZgZuoqjzgE9v5nXfAGZ2s70uZ39Px0FnRj5Xd+frr+eC9d7kBHFjzBRgGm7IoYgUL8UyZ8CxzBgTAv4EHAF83Fr77wH0R0T6T3HMGazPZNmRH9Et75qI9INimTOQWDYal+z4VrB09TrwD9wNVxHJP8UxZ7A+k2XLfq3Y8q7JQKgElgxH9wDrgAuNMWVddxpjxgframNMZZfdi4BGuh8O15e7gH2NMQfkXGsE8CXcBFHd1jm11r6DC8L1wZMx2WOPpJt6iMaYycaYuu7eW5fzLgAagC8ZY8I5u76Cm/TplpxzRoNz6gu0SPFQLCNvsexXwCnAmdbav/d2PRHJK8UxBh7Hsv+duvH54Phneru+iAyYYhkDjmXvAid2s8zH1d0/Ebikt+uLyIAojjE4n8mMMVOBzwEvBP2WAtAIEBl2rLXrjDFfAf4MPGOM+SsuC7stMBt4FDgL2Bl4wBhzMy7oduA+eE0E/roFl54LfBK42xhzJbAaqAe2Bz5qrc30cuwFwDzgP8aYa3HD5s4GFuCelMl1Sc55F/fRp28C/wTuDf477Ip773+w1iZz2p2Iq1n4WeD67EZjzGm42ovVwaYPGGO+G/z8Z2ttT9l6ERkgxbJNbHEsM8acA5wJPA5sMMZ0fWrpNmvt+j6uLyJbQHFsEwP5TPYdY8z7gX/h6m2PAT4K7AP8ylr7ah/XFpEBUCzbxBbFMmvtBuD2riczxpwA7Gutfc8+EckfxbFNDOQz2WXGmB2BB4CluInczwBGAF/v47oyiJQAkWHJWnuDMWYpMAcX3CqAt4FHcAEMXI2+G3ElUU7DBfYGXHmUW7fgmsuNMQcCl+KCciXwAnC8tXZeH8f+yxhzMvAjXOBehAuyHwEO3dy+5Jz3TmPMSbiJ5n6F+wP3E+AH/TzF54FDcl4fFiwA/6Hn4YoikgeKZRvPO5BYtkewPiBYutoeUAJEZJAojm0870Di2DxgR9zTheNxT0u/EPQrvqV9EpH+UyzbeN6Bfr8UkQJRHNt43oHEsXuBLwNfxZX2Wws8DPzIWqsRuQVkrLWF7oOIiIiIiIiIiIiIiEheaQ4QEREREREREREREREpOUqAiIiIiIiIiIiIiIhIyVECRERERERERERERERESo4SICIiIiIiIiIiIiIiUnKUABERERERERERERERkZKjBIiIiIiIiIiIiIiIiJScSKE7UKqMMQaYAjQWui8iMiSMBJZaa22hO5KlOCYiW0CxTESGOsUxESkFimUiUgryEsuUABk8U4Alhe6EiAwp04C3C92JHIpjIrIlFMtEZKhTHBORUqBYJiKlYMCxTAmQwZPNaE9D2W0R6d1I3AfBYosVimMisjkUy0RkqFMcE5FSoFgmIqUgb7FMCZDB12itXVfoTohI8XIjgYua4piI9EmxTESGOsUxESkFimUiUgryGcs0CbqIiIiIiIiIiIiIiJQcJUBERERERERERERERKTkKAEiIiIiIiIiIiIiIiIlRwkQEREREREREREREREpOUqAiIiIiIiIiIiIiIhIyVECRERERERERERERERESo4SICIiIiIiIiIiIiIiUnKUABERERERERERERERkZKjBIiIiIiIiIiIiIiIiJQcJUBERERERERERERERKTkRArdAeknP1oG3ALstoVnuBI/dUUeeyQiIqXGjx4K/BqoLnBP+uNY/FSy0J0QERERERERkeKlBMjQsR/w4QEcX5uvjoiISMk6DZhV6E70U3mhOyAiIiIiIiIixU0JkKFjn2D9IHDBFhy/NI99ERGR0pRNll8G3FrIjvTDK4XugIiIiIiIiIgUNyVAho69g/V8/NRTBe2JiIiUqmwC5Hn9rRERERERERGRoU6ToA8d2REg/y1oL0REpJRlEyBrC9oLEREREREREZE8UAJkKPCjtcCM4NX/CtkVEREpaUqAiIiIiIiIiEjJUAJkaHhfsH4dP7WyoD0REZFSpgSIiIiIiIiIiJQMJUCGhuz8H08XtBciIlK6/KhBCRARERERERERKSFKgAwNmv9DREQGWw2dnwvWFLIjIiIiIiIiIiL5oATI0KARICIiMtiyoz/agJZCdkREREREREREJB8ihe6A9MGPjge2C15pAnQRkWLgR68DPgrcAlwNPImfsoXt1IB1lr8a+u9FREREREREREQjQIaA7OiPl/BT6wraExERAT+6G3A6MBL4LPA48AJ+9Gz8aE0huzZAo4O15v8QERERERERkZKgBEjx0/wfIiLF5RvB+kHgeqAZ2BW4EpiPH60sUL8GShOgi4iIiIiIiEhJUQKk+GVHgCgBIiIyUH50O/zoIfjRLSsB6UenAqcGry7ET30WmAKcBazGxeyr8tHVAlACRERERERERERKiuYAKWZ+1NA5AkQToIuIDIQfDeFGbewALA3m8bgWP/XaZpzlbKAMeAQ/9aQ7b2ot8Gv86MvAv4DP40efxE9d0+X6o4GvAgZ4FngOeLuI5ttQAkRERERERERESopGgBS3qcAkII27USYiIltuN1zyA9yoje8Ai/Cj9+NHP44fLe/1aD86Evhy8Opn792fug/4bvDqKvzovjnHHgkkgB8CPwDuAN4C3sWP3oofnbSF7ymflAARERERERERkZKiBEhxy5a/WoCf2lDQnoiIDH1HB+t7gJODtQWOAG4C3sSP/hg/Or2H4z8PRIGXgDt7aDMXuB0oB24JSm5dBdyLS2q/AvwFeBGX3B4HnATcXgRzhygBIiIiIiIiIiIlRQmQ4qYJ0EVE8iebALkTP3ULfupDuBEhPwLeASYCFwKv4Uf/iR+dtfFIN2fIucGrn+OnMt1ewZWzOh14GdgGeBVX9grg18Ce+KnT8FMeUAMcAqwB9gN+H5Q+LBQlQERERERERESkpCgBUtyyI0A0/4eIyED40RrgoODVPZ3bU4vxUxcB2wEfBe7DzdFxPPA8fvRy/GgUN2JkW+Bd4M+9XyuVwo3qWI+ba+tt4Gj81Fn4qfU57VrwUw8H504DnwHOG+hbHQAlQERERERERESkpCgBUqzcU8DZBIhGgIiIDMyhuMnLX8eNytiUn2rHT/0dP3UUMBP4By55cS6u5NWPg5ZX4ada+ryan1oAHImbE8TDT93bS9sH6Bxdchl+9Nh+vaP8UwJEREREREREREpKpNAdkB5tD4wB2nAT5/abMWYikFtLfq21NpXHvomIDDWd83+4MlU981MvAyfgRz8EXAnMCPY0A7/t9xX91OPA4/1sfRVukvYvADfiR/fHTyX7fa38qAV4aHGHPcyY7brsW2Gt3QBgjKkBxvZynpXW2vVB2xG4eU56sspa2xS0rQIm9NJ2jbV2XR/vQURERERERERkIyVAild2/o/n8VNt3TWYPmfeUcDP2DTZQcW0WVNblyyszr6Ovv+TK6fPmffDxXNnXzlovRURKW65E6Azfc48A+wOnAIcA1S/95AbqKDNnBn558pPhh+s/b+OI9b/Mv3Rx5gzbxC6dwPltHNz+cXNe4ReG/VGZsLzx8y59Y0NVPaerMmjxyrGTr/zmXc4486WG7vuG/eRby+dPmfeeoAxR3915Op7fj2pp/OMPebry6bPmdcIMPa482pW3fnzyT21Hf3BM5ZPnzNvHcD4E79TveK2H0/tqW3tIaevmD5nXu7olI8snjt7ayeJRERERERERGQIUQKkePWn/NWZgNd1oymvwkTKN74OlVePo/endUVESpcf3R43iiN9dttZr9wxZ94PcYmPGb0fCK2Uc0XHx7ii42PgRjL0NpphQNoo4/Nt32RexYVsF3q37MLI/+303Y7PD9bl3qOaFuYv7nAvQmFMKLxxnymrnJL9OdTlb0xXprxqEjAJIFRW0WvbUHnVRNzk85i+244Hxuds6rmxiIiIiIiIiAhKgBSzXYP1c720mRWsz8ptN/Hki7tr+1Z+uiUiUqT86DeB6cA5+Kn2nD3Z0R+P35E58FbcHB8ALcBdwN8okhi5iij/Tu+2zymRf1/x6cgDjDQbvv319rMfHezrVtBmoqz/99JGGwKo2mHv70346EUPdtd2xKxDGTHr0H6dt3rnA9n2vL/3q23V9nv1u23gvXO5iIiIiIiIiIjkUAKkeO0QrLu9wTN9zrxKYMfg5a2L585eZow5DEhaa5dtjQ6KiBQNPzoVuBQwwLtAbib4aICMNfcCfrDtS8BfF8+d3bgVe9lPsx/Fj24LnPuR8OPnfyT8eBw/tXxQL+lHRwGhurEhHnkj/Wzzq0/eu3ju7CcH9ZoiIiIiIiIiIoMsVOgOSDf8aAj3FDPAaz20mon7/a0BlhtjKoC7gXeMMTsNeh9FRIrLybjkB8B38aN7AuBHy4AjAJ62Oz9B59+964sz+bHRhcALuJJP1+JHTR/tB6oW4PfHV7VmrN3LWqvkh4iIiIiIiIgMeSWRADHGfNUYs9gY02KMedIYs28f7U82xjQE7RPGmGNz9pUZYy4Ntq83xiw1xvzJGDOlt3Pm2RRcbfMOYEkPbbLlrxYsnjvb4uYMqQBWAIsGvYciIsXllGC9Eje6MY4fLQf2B0YCq77Qdn52dNzqxXNnt3dzjuLhp1qAU4FW4FjcnE+DqTZYr+21lYiIiIiIiIjIEDLkEyDGmFOAy3HlTvYCngfuMcZM6KH9gcCNwB+BPYHbgduNMdk5N6qD8/wwWJ+EG23xz0F8G11ly1+9gZ9K99Bml2C9MFgfHKwfsdbaQeuZiEix8aPb4RIdFjfaYyXgARfROf/HfesYkZ1Ae3DLSeWLn1oAfDN49TP86C/wox8MEjv5Njr406EEiIiIiIiIiIiUjCGfAAG+AVxjrb3OWrsQ+DKwAfhcD+2/DvzLWvtTa23SWnsR8AxuInGstSlr7ZHW2puttS9Za58I9r3PGLPt4L8doDMB0lP5K+gcAfKeBMig9EhEpHh9PFj/Gz/1Ap2jJS4A6oOf7wGyifF3t2LfBuoq4E6gEvf36z5gBX70ZvzoQXm8Tu2Dr6ep/vG6GcaYu/J4XhERERERERGRghnSCRBjTDnwPuD+7DZrbSZ4fUAPhx2Q2z5wTy/tAaK4J4u31pOxm5MAWWCMCQPvD14rASIiw022/NVNAPipvwU/h4Fpwb57gYnBz0NjBAiAn7K4kYgnAdfi+j4KN+fJLcGcUflQu7QxQ3MHIaAsT+cUERERERERESmoSKE7MEDjcDe4ut7MWg7U9XDMpB7aT+qusTGmErgUuNFau66njgSTkFfkbBrZc7f71GsCZPqceRVAdqLzhcCuuCRNE64EmIjIZstzHNs6/OhOuER4Brg1Z89ZwGG4UR8J/NRS5szL2wgQY8xxQNpae/dAz9UnP9UO3AbcFiQ89gbm4xI6uwCJPFyldmnjxuqJ7+ThfCIFMyRjmYhIDsUxESkFimUiUiyGegJkUBljyoCbAQN8pY/mFwDfz9Oltw/WGxMgXtwLNb/RfHNkVKSuZmZ1JNM2Lgw2E6pYdtfEj00ct/yW5VTvXG3/vM22q5/xYiO6nvDF6eaOz9yx8IQ89U9ESlM+41i/JetiBgjHGpIdXbZX4cr7fQg4Ctim67EmNLnChC02Q8amQ6/y11iwZwoYGwmFrbUZM8P+NZa6PVRWtdpabLr9i8nbz//Mlvb3+ebmcARG3LTd9KZkXSwD8GZbm4kYw5SyskGeg2kKACacKTchsGnzlP1rrG2gZzWhyRUL3l0OtFI/bsynn/FinxroOQfbw5454ZwbFt5R6H5IUSpILBORTsm62FRgNnAccCD9G134+1hD8luD2rGhQ3FMREqBYpmIFIWhngBZCaTpLGuSNRFY1sMxy/rTPif5sR1weG+jPwKX4CZjzxoJLOnjmJ5kR4C8nt2Qeip1wJI/LPlouCbMjB/PIFz1DrgSZrvXHlRL2ZgyxhAaGft39yfsCFO7hX0RkeEjn3GsX5J1sRDwKLB/si62BhfXVwDtwH64uS96ZDMGmzHg/p6N2nSnIdNhCM5RWZ5u48w3FnPBhAkVU6pHVAC0ZDJUGIMxpl/9TaXTnP/OUgAebGqqqauowBjDr1et5M516zhu1Ci+NX4CYyOD++fVpkPYNBC8twGfL2NY3upOOCUUMVXt9O8/SAG1RQb+vqVkbfVYJsNXsi42CXeDPwN05CzNwHrcCO31uDkK24E2IB1rSA5ywrx/knWxWcDHgBOA8bj3kc5ZtwGtwTq7dODeS3vwczpnnQH2BPbagu4orndSHBORUqBYJiJFYUgnQKy1bcaY/wFHALcDGGNCweurejjs8WD/L3K2HRlsJzhHNvkxAzjMWruqH31pxX05yJ5js97LRn60ms5yXBtHgDQvbt7Htlk6Vnew5JpIYvwJn/NMJHV31ZRbriirLaP2wFqOeyKzI2R+2x5m7TUfCp2Te9qM4cWeZoUXEYE8xrHNsx2wf/Dz6GCZkbN/CW6epntwJf4y2R1jZzVuX7v9hnuBjuXPRvdvWlqZ6u1Cp+1+yh0vL/pJ3fkb7MPXhlu/slNFRdtRry26aG06fWhFKLQoAuvKjFlXZsy60eHI67+ZNvXOCZGydPb4tkyGI19b9LumTOaICLy5c0XFCcaYxrZMhgebmq6xcOgd69Yxb926tTMrKn5y47bb3VYeGpyptibskdpl5NSW262l6Y0Hxu2dbg2n+z6qZ9OPXDG38e8bPkoT3HFMOcmdwzdMW2XvzVd/B0NHiAcL3QcpTgWKZTJ8zWMLbvYn62LZZEJrztJGZ+Ihm0wI40ZPlAfrENCCS7BklxY2TVC04hIu2WU9LlmRyTn/FNz8Utmhk/lmgSeBO3F/w/szl2Kvf8eHE8UxESkFimUiUiyGdAIkcDkQN8Y8DTwFnAOMAK4DMMb8CXjbWntB0P6XwL+NMefhvrB8AldP/UtB+zLgFtwXmeOAsDEmm5BYba0dcKmRPmTLX63FT63Jbkw3p3fM/tz4XHKXUfvVEhk1477EBd+/L7s9eUmsHaAszbJf/XxBfJD7KSKSDzsH65fpfPp0HO7poKeBhT0+JetHTw1+um+bB1//X18Xeu3Qz06w6Q5Wb0hVHb9y3ULjPoEfCGzbkclsm9v2nY4ODl206NPAodbaFEDwd+MIoK0DTvza20uezbZvgsOMMfsA12Rg92Rr62V7vPLy0cCZ1tqX+/MfwhhjrLU2+HkG8HXg+90m4f3o67ibSbU7n7g8ip96qj/X6JEfDb+zweWWNkwr59Wp5uHbLnxRf0dERHqRrItFgN2Dl/8N1pFgqQJqcN9LRuASF7nKg6Vm8HvaqzbgXtz3nwW40r8hXOIlTGc/s3Xcy3Dvryzn53CX9VvA3bGG5IDn3BIRERERGaghnwCx1t5kjBkP/AA3cuI54EPW2uxE59uS88SwtfYxY8ypwI+AnwCvACdYa18MmkwFPhz8/FyXyx0GPDQY7yNHtxOgZ1o6b86Fyips+6olREaNX2CM+SAuWXP3wpl10aCJnp4SkaEimwBZGGtIJoHkZhx7SrC+qa+G0+fMMy1vJWoBQhUjHgCw1lpjzJ7AobhSiLW4EShjg3O/kZP8OBCYG5zuHGvtM12vYa39b5AE+Qbg45IlLxljdrPWJoLzHATsGLzPBmvtOmPM5ODcz9M5RPznwPHAqcaY7wG/s9Z2zpHip9L40YdwSaMjcA8ADETt3lPCrKgONUfGRKpwTwuLiEjvtsHd9G8F9o81JDPdNQrmusqO4CjvslR0WWeTD9l1ms7SWe24kRWVuARLdul6ruz+6mAZgfvel3vuFtzIjDtiDUl9dxARERGRkjXkEyAA1tqr6KHklbX20G62/Q34Ww/tF0NBa593mwCJRCPlFdMqCNdEkmOP/uOMUGUNwELgh8DpuJt22RuH/RliLiJSDHJHgPSfH90VmIW7IXR7X82bEvePbn3rxRCAiVT8I7vdWtuEK8+xiSDpEAl+jgD1weu/Ar/r6TrW2nbgUmPMrcCvcZO3v5HT5FTgKznXWYqbu6QGSBljrrHWNuISIdsBuwG/Ar4anHM+8Li1dgPwIC4Bcjiuvu5A1P7t5GoO2nbq4lQ4HEMJEBGR/tg4b19PyQ+AYCTjJmVARERERERk6xic4uQyENkSWJskQCYoRF32AAAgAElEQVR/YvLyGT+awXbn7vuvUGVNBGgE3gYODpo8AmgEiIgMNdn5Pl7ZzOM+Haz/hZ/qM+atb3jkGNveSqi61ravfOPJvtpba1dYa98Jfu7APUn7MvClbJmqPo5/1Vp7NFBrrV2Xs2shLomxLHg9BZf8eAo4Kkh+YK19CHgfLlmyCqgDvgPcT2cC/4FgfRB+tKKvPvWhFqDNlYEEJUBERPpjYwKkoL0QEREREZEeKQFSfHr6IjURINMypQLAWrvwjUuPOxNXSsUCjxLcwEIJEBEZOjZ/BIgfLQc+F7y6rj+HpFPvHgVQuc2u6/uTwMgVzBVyG3BwNkHRX9kSWjmvr7LWHm6tnYwbuXcAcAhwgLX2qS5tO6y1v8MliT4P/AWX+H44aJIEluNKnezPAKQzttZaS4cx5cEmJUBERPrW7chtEREREREpHkqAFJ+evkhNAki3TI0CrH34T2k6y34tDG6yaQSIiAwZybpYBTA9eLk5JbBOxE2WvpRuyld1J70htT9A5ba7Lu+rbVfWuc1am9fJXK21a621T1hrH7bW9lg6xVq7xlp7rbX2NFy9eTdPiJ+yq5ttNhlyxBZ3xI+G/vRCe3TkJY28fvWSCcFWJUBERPrW7chtEREREREpHkqAFBM/augmAWKMMcmzk7u9cuErtC2vnAYwcq/Z83KOTATrbAJEc4CIyFCwI27OpUbcSIb++nKw/gN+qqPXloHJp1951fiPfZ+qGQdsbqmtohIkY9qNMRONMbdte0XjB1s7LLh5QLbUyHcarVnfDhjCwTYlQERE+qYSWCIiIiIiRU4JkOIyEVdnPgO8md0YrgmPTzemQ61LWzHl0ycBREaOexaYDTwO/CBoqhJYIjKUbJz/I5ggtm9+tA44FBcn/9DfC0VGjYtW77gPkZFjl2xuJ4vUGmDf9e2MvmlBO8B++NGaLTxX7dJGNwClbEx5JNimBIiISN9UAktEREREpMgpAVJcsl+i3sJPtWU3Vk6rnAUQGRUBOzbbZoG19i5r7YHW2mSwTSWwRGQo2fz5P+CMYH0nfuqt3B3GCXdzDATzKAF5LWNVKNbaNuBXAJc+2tZmrY0AB23h6WqXNrr8U2R0xATbNgy4kyIiJSxZFxsFjA1eagSIiIiIiEiRUgKkuHQ7jN6UGZcAiUbSEC4DmoC3eC+VwBKRoWTzEiB+tAo4PXj1u9xdwUTl9wOLjDE75G43xvxr5bzLj0i3NMHmldoqdlcDGxauyJTPX5yGLZ8HZHQ2AVJWW5bdphEgIiK9y87/sTLWkFxX0J6IiIiIiEiPlAApLt1PpGjZCSAyqrw92JJcPHd2d+ViVAJLRIaSzR0BcjIuzi0G7u2yrw43D8Z2wN3GmMpg+y7A0euTj8wMRcqhREaAAFhrVwPXAVz+eBts+TwgG0tgRUZHANoT9Yn2Xo8QERHN/yEiIiIiMgQoAVJcuq0jbDvsdIBwtCpbFmtBD8erBJaIDCUb5wDpZ/vs5OfX4KfSXfbNzvn5cmttS/Dz0QAVU+qajUuAlNIIEIBfAnbeKx0kV6b3xI+O2dwTtKXt6HeaNhkBotEfIiJ96/7BJRERERERKSpKgBSXbhMgmbbMFIDIqGgm2LSwh+NVAktEhoRkXWwkMDl42XcCxI/uDhwAdADXdtMimwD5lrX29znbjwKo2nHv7NwWJTMCBMBa+wrwT4Df/rfdAMdt7jlWN9vxx86IsO348LrIqAgoASIi0h+aAF1EREREZAhQAqS4dPtFqmxsWbpyWgVferV11Jz//gW6SYAk62IVQLbki0aAiEixy47+WBFrSK7pR/vs5Oe34aeW5e4wxtQCBwcvb83ZPo1sAmSHvauDzaU2AgTgkjPeV3b3T46oAPg5fnRyXwfkmlQTqv7HJ6r5zTfHzzduDnQlQERE+qYSWCIiIiIiQ0Ck0B2QgB+tBKYGr16bPmfeTFy5l9CU0w8ZNX3Da5z2h3SoY+kLlKXbuyuBFc35uXGwuysiMkDZBEjf83/40Sjw6eDV77ppcRQQBhqsta/BxqTIW9kGZeO2BUgDq7e4x0XKWvskfvRE4Elgd+B6/Ogx+KlMH4dm1QKsDYeyZcOUABER6ZtGgIiIiIiIDAEaAVI8tgMM7sbTSuAS4Bzga9jyXSuD2T8iNsMf7r+0u6elswmQxlhDsmttfBGRYpOdAL0/8398ARgJJIH53ex/EvgW8IvsBmvtWuBKgPCo8X81xgCsWDx3dn+TAkOLn2oFTk212Bb/oZajmtrsuf09tLXDjrbWsiYUzs4zpQSIiEgvknWxEDA9eKkEiIiIiIhIEVMCpHh0TqTopyywX/D6j6GK5csr2u3GhhOa13Y3yW1tsNb8HyIyFGQTIL2PAPGjEeDrwavLg/i4CWvtG9ban3aZ+wNcEnnfSZ+67IbgdUnN/9GVuXhdcrffNr118b/bOOOO5svWXzhqz/4cd+49LfvWXNLINf+3evdg04ZB7KaISCmYDFTgRha+1UdbEREREREpICVAisfGYfTT58ybAkwBMkuvPevKhq89NuGBqzcpLzyum+OzI0A0/4eIDAX9S4DAx4BtgBXAXzbnAtb5b2TU+GyCuBTn/9jIWmuXNNrzwgZ7w4sdoS/Pa3lg2fkjq40xM4wxH+jpuFXNNrqhHdrDpiPYpBEgIiK9y35ufzPWkOzotaWIiIiIiBSUEiDFI7eO8PuCnxe0r148PtOcMR0tm1S1GtvN8UqAiEjBJetiu/SjjaE/CRA/aoDzgle/xk+1dG1ijDnZGPMZY8xYL+6d4sW9c7241/Vv28RgXdIjQADSGXvHdrXmDAP85YX20Ttd2bQG99/4jz0ds3KDHQEQqY1kR34oASKyFQUxUYaWzpHbIiIiIiJS1DQJevHITYDsHfz8v7Jo2c7tq9upqd7kV9XbCBCVwBKRgkjWxS4AfpSsi9XHGpK9jdYYS2fZvle9uBcDQon6xIIu7Q7CxcOWW0aOiF8c98Ym6hOrurSZA+xVvVP1RcDFuMT+aOB7OW0mBOuSHgGStWh15prjZ5bV3flyxzfWt1MeCZGxluXGmCrAAkcA/7LWpgFWbrCVAKEx5euDVLsSICJbSbIu9mfg48m62GJcsvKVYP0U8Hx/5nVL1sXKgE8C+wCrcLEuu6zCPRyzFmiJNSTfU0ZQtkj2c/vrvbYaZF7cKwO+AVQCrwKLgvWqRH1Cv2sREREREZQAKSa5CZBjgp+fNhVmBkBtVVkaCAfbuxsBkr2ZqBEgIrLVBU8wT8clIP6UrIuFYw3JeA/Ns6M/3vr4BZFIx8q2/2WgYrfrdz3zhdNfzJ3H4zyAW2tGPHjxuLHPARkv7u2bqE+8CmCMmQzsBTDtK9OOonNU40Ve3GtI1Ceyc38MmxEgWXe81H7e5UdXNu45Kfyd/aeFI1Vlpmzlhkzt+J82PQVMwyWXHgV4d70tA2B8eXNwuBIgIltBsi42CjgVF7t2pjM2ZqWSdbFHgIeA/wCJWENyQ87xlcBngW8D2/Xjkm3Jutg6oAn3/3l22QA05yxrgBeAZ4BX+pOEGYZyP7cX0mW4+a66Wu3FvQeBe4B7E/WJN7dut0REREREiocSIMXAlXnZAaDdhl+ncwTI07gndRldHm6lg+pgu0pgiUhRiTUkbbIu9hUgA3wZuC5ZFwvFGpLXddN8Y/mr/Zas++r1c9+usu2WHb63w+/2vW6XXZpDoXMSr7+5o4UP/2XUSC4bU/shOpMbf/Li3gcS9YkO4FiAcE34lfKx5QcDrcCNwOnAtV7cez1Rn3icYTYCJOsb97T4+NEHgduAfcdVhx4fX22eW7HBTgNOBB49eVZZ2bvrrSu/M6kiW2JMCRCRrWN/XGx7A/g8MCNYZgEH4j7bHRcsAJlkXewVXHLiTVzyZHKw713gBqAaF/MmBsvo4DwhoBw3iri7kcQ9WZ+siz0HLGTTEQaLYg3Jps1+x6Wj4CWwvLh3Kp3Jj78Ck4CdcEnuMbg5tD4WtG0A5gNPBMsrGiEiIiIiIsNF3hIgxpha3IfsHYGfWmtXG2P2ApZba9/O13VK1FhgJMCn2i5sx31x7QBeIM00gLGRSG4CRCWwRKRftmZsjjUkM8m62Jm4JMiZwB+DJEjX+SdmuJV95d2GDRekm9zDxW/+6k3Kvr/j2RPK0t674fCrvx4dNX8fWQMQWnHXiseAvcYcNuaAcFX4W8BPCG4Kjj5kdDYp/HPgItyIuBOA2724ty/MHXYjQDbyUw/jR/cH5gEzrjymcvwnb20GONEY881TvciOGQshA3ZCRXtwlBIgUnRK9HPmQcH64VhD8gHggeyOZF0sDOwBHAocBuwLjAdmBkvWEuBS4I+xhmQz3QhG6NXgYmMUGBG8HhEs1UBVzjIxuPYewf73B0sum6yLvYQr1fXfYFkwjJIiBR0B4sW93YA/BC9/nKhPfDdnXxWwO3AUcDQu0VYXLF8Jmq324t5jwF+A2xP1idat1ffhrETjmIgMM4plIjIU5SUBYozZDbgfN/pgOnANsBo4CdgW+Ew+rlPCsk+RLX3Kxrzg5xcXz53dXHZ1ZiLAxHAkd/JflcASkT4VIjYHI0HOAtLA2cAfgnJYV+c02xlg1HbNE/67oGVUdmPbsjaW/+Udwl+cdujsaZMPbQmFsJlM5o3L33yo6cWmwwnTPObwMQAXz/zZzAeAIwGi+0THAMuAuYn6RMaLe6cBj+Bu3t1JqHkMmSoYZiNANvJTr+BHDwDuPG7nyP6REOmODDsA3k6jQyM+MjNCS4fNvB0JVQZHKAEiRaWEP2dmEyD/6bojKDv1v2D5eZDEmIi7sb0bLo4+Afw51pBs6+0iwbwfjcHyVn87FyRhdsaVGtwZd6Njp2A9js6b6p/JOWYVbkTLG7j5MZ7FjWh+OdaQzPT32sUsWRerAqYELwd1DhAv7tUAHYn6REvOtjG4kX1VuBJX3889JlGfaKZzpMcPvLhXCxyOG1W0P26k+Rg6Rxet9OLedcA1ifrEK4P5foazEo5jIjKMKJaJyFCVrxEglwPXW2u/ZYxpzNl+F244vvRuCe5p6RCblr+ifHx5KlITYbtIee5TfSqBJSL9UZDYHCRBvo5LgpwD9nfJurpVsYaGW4MmOwOkpjYfs3y+u9ceHhk+n6b0Zy/eI7Tu5nT6gDXhMFXt6cwLFy26r3VZ29EApLk1XBmuBD7WurT1JmBEZFTEVm5baYALE/WJRoBEfaLJi3vH455M3rVmp8toT+2FCTWPgtmD9baLm59ahR/9SU25+efh08Ot976WrgZOvPiwyn8ELZZ77klvUAJEik/Jfc4MJi7fL3j5aF/tgyTGsmC5ZxC7lnvNNJAMlk0k62LjcZOu75uzHof7jDqWYH6mHE3JutgzuLj8GPB4rCG5bPB6P6imB+tG3CTzeefFvSjwA+AsoMWLe/Nx/97vAa7CjUB5HTg1UZ/odY6WRH1iLfD3YMGLe+W4JNqHcaXXpgDfBL7pxb0EkAAWAC8CLyTqE4vz/f6GqZKLYyIyLCmWiciQlK8EyD7AGd1sfxtXj1Z646feAX4LwJx52S+1TwPscOEOTwPeHn/syE2A9FYCSwkQEckqWGyONSRt5qLor5Y/Gz1n7asjDMbevGifGce2NUbuIyiB9e8KU1U5rZKOdR0t7avaLweuOGtch91txarz5kcqzv7NJUuXt65KH40rqfVFa+21XtwbCxxkys12AKP2HmVMyDwDbDLheqI+scSLe8dZa/5uws3blY95FOBBL+49AlyeqE/cPpD358W9j+GeqL0gUZ8YKnF3PtD+iV3Lqu99LQ2uTNjDwb61dCZANnRzrEghleLnzD1wpafW0E2CodjFGpIrcDc77spuS9bForjJ2LPLzsD7gD1xJbc+ECzZ9q/hkj93A3fHGpJDpYzrxvk/gsRU3nhxzwCn4G4wZed3qcZl73Mz+C3ASYn6xOrNvUaiPtGG+57xtBf3foCbT+sM4BjAC5bcPiVxI05uA/6nuUO2WCnGMREZfhTLRGRIylcCpBUY1c32nYEVebpGyZs+Z56hywgQXLkDqjetzNtbCayh8uVRRAZfQWNzKMxJk/ZK0dEcountqlBHS+iuUds2f3Xdm1VVGMvrsRF2x/0nmPa17Vc2fL3BAhbgEGN+iZvYdR+gDfiEtfY2gER9YpUX9z5XObXyrm2+ug01Xg3AOYn6xHtKqyTqE89Mv/Bvx4ar3lpQPvrxjsjIhhBwMHCwF/c+lahPbNFTSl7cKwOuxk0uHPPi3oeGRP10P9WEH33k+JmRww1YC3vcvKB95smzIhhjchMgGgEixaYUP2dm59R4rFRKQ8UakincBO0v5G5P1sUiuFJZ++BGvRwI7IobxbADcBrQkZxV99i4WY3bV41tb21ZXfb7FS+OfAhr2nG//2VAKt8Jhy2Unf9jQOWvvLgXxpWimoCb32Ui8AXgg0GTV3AjQJbjkhPH4P7dhIAvJuoTzw3k+gCJ+kQH8E/gn17cm4xLWO2as+wCxILlQuBNL+7dinvKV8mQzVOKcUxEhh/FMhEZkvKVAPkn8D1jzMeD19YYsy1uUsZbez5MupiO+yLUBrxojDG7Xr/rRICKdkxOO40AEZH+KHRsPsmEYMp+a699c3740y1ryssb3678LUBZTZqHRla1AFVltWV3djnuVNwTRI3AR6y183N3JuoTd3tx77fRfaJfAW5O1Cce6bEHmepx6fUzaV4/87WRsTmHAz7uBtM1XtxLJOoTiS14X4fhkh/gJij+sxf3PtlXGZL+CJ7+PQt4MVGfmN9X+y1w97jq0OE/P6rihW/c23rKpY+2Xn367c2cd0D5KD5PKGijBIgUm0LHssHQ4/wfpSbWkOzAlVN6EbgONo4W2Q83ku54YBYZ84GVL268p/LTbk7VnIzVLQuX2UaMTaRbw9cDDwXn35r6nADdi3t7Au8k6hPvKfMVjGQ8H/gqMLKbw1uBHwM/zZn743lgbjCfx+hEfSLvc48k6hPvAHcGS7avUdzIk5NwCZhtgXOD5WUv7t0A/A1XCsziRmxmgLXdPZgwzJViHBOR4UexTESGpHwlQM4DbgHexU3I92/czavHge/k6RrDQXb0xwuL585uNZdy0sIvL9xr5O4jqWifnPu7qkzWxapjDcncMiVKgIhIV1s1NntxbwauZvnZidffLAcOAAiX24uqJ7Rd0tESfq6jOTwC4LVIa2tTS1lVuJom3EStuf6Dq3F+vbW2pydcvwbcATzUR7cmBOt3E/WJt72492VcaZYjgb97cW/vLShh9dFg/Tgubp8MvOvFvbPz8DTsB4ErgYwX976cqE9cM8DzdfUv4KfnHlAx89wDKt6su6ppfHMHlIVNI52T+ioBIsWmpD5nBhOaZ0eADDgBEowk+DAwCzfp+BNbUhppawpGi9wbLHOavjThe62NkYub3q60resi60JhO8pajM0YMu2mw6ZDEaAKa7ZPtxlwc1h8CuyqZF3sH8DtuJES7UBHsG7DlYpqDtYteRpBsrEEVnc7vbh3LDAPF8fnAzfi5t8wuH/LX8OVBMtajfu3vQI36uMnifrEou7OHcznsdVGewd/H28AbvDiXhVwFPBJ3L+3nXEPFfjdHPq2F/cuBq4LRplIicUxERm2FMtEZEjKSwLEWpsCjjTGvB/YHfeh/hlr7f35OP8w0rX81dRMS8bYtCWSpqJL27EEddqDL9IqgSUim9iasTkYuXAd7qbeIddGR/7tpOXr+N78liW/eqr9ZFh3ZTI2czcTzjxn06GRcxatbmn42pKKCSdNeHXZTcvau/R7Ea4EVo+CGyp396NrE4P18uC4tBf3TgX+B+wExL24d1J/n1QNbjSeELz8Pm7U3o24J3nfwT21OxDHBusQcLUX9ybgboblq8zIAlyN3qnAB5ra7GiAiSPMSoK5WVACRIpMCX7O3AF3syA7F8MW8eJeJfAZ3GiCGV32JXE3I14EFuPKNb2+1eYs8qMfD/p1KX6q9ydC/ehuNVO4sIZWxs5cfz5+6nL86Da4hHo9EMmkoaM5TEdz2LY1hVc3rygf2/h2JenW8Fjgc8HSp2RdLI1LjmQTJe8A9+MmF/93rCHZn/jXVwms7FOxIeCIYPktbmRHNvHxLC5xcHeiPtHe9QTFKFGfaAb+AfzDi3sjcX8LT8WN4imDTUarT8WVivymF/e+C9wy3EeElGAcE5FhSLFMRIaqfI0AAcBa+yhuMkOMMbV9NJf32iQBEqoK7ZRpzlA2uoxwI5Vd2o4D3gp+rgbCwc8aASIim9gasTlRn7Be3Psq8Fdrbd33G/jaOTeup7ExMw34BVBTt7Dhx8m62PvXptNfe37Nhs/ZDktZbdl9g9GfHBtHgOT0dWUwifl/gI8A3wLmAnhxbzTuqdb1ifrEi92c76DgnGuAhxL1iXYv7k0Efgn8yIt7SxP1iesG0N8PBev5uFJbPwImeHHv3LzcPPJTFj96N/CF3/+v7Yy3G+0kgKmjzDI0B4gUuRL6nJktf/V0rCHZ0mvLLoKkx1640WJn0pnkXYO7kb87LoZl523oevy7uJFz9wP3D0YpJfzodFxCvBq4BT/6a+B8/NR736sfHQH8FajATaj+C7c99RbwWfzoL4ATQ2GWlNekE+U16QXVv0411frR2ZMyqd9vWFE+tfGtKhqXVK7taA01YU0I9/0mEpyzCjaW9wP3eTkMGz9Xj8aNnPka0Jasi/0HlyBvwE1O3xBrSK7JHhw8dNRjCSwv7oXojOOfwyW6PoEbsVKGK2XlA/8YyvNnJOoTjcCfg2Wj4GGISuCLwHdxibmbgGe9uHcNcHtQamvYKqE4JiLDmGKZiAw1eUmAGGO+DSy21t4UvL4Z+KgxZhlwrLX2+Xxcp5RNnzMvhJt4EIIEiImY7QHCo8LNpvOJsQ24L5S5E6Fny1+l0Y0rEQls7dicqE88X7lN5SdCcHfzktbJAJGR4Y6OxnQEuMgY8xdrbSI8IvxQpsN+oWx8GaP2HXVtvq4/fc68K3E3/k5aPHd2tkTgJiNAcvr6Xy/unYV7QvXHXtw7HnejZnzQJOPFvfcn6hNdy3Nly1/9I/vUbqI+caUX9yYBFwC/9+Jespvj+uTFvem4iYLTwIm4J59/ibsxN96Le5/LqQc/EP8CvvDq6swHshu2Gx1aSucNQf0dkaJSgp8zs+WvHu2rYXBD+VBcTNgf2AN3Iz3rTeBy4I+J+kRTcMw4XAnC/XFxbTqubNM4XAL348GCF/cW4ZIhDwAPJuoTqwb0zvyowcXVamAJMA03Qu5A/Ogp+KlXuhzxC1yi5h3gdPzUpoleP/U8LmnQ5TqpecaP7jJiYttlIya2fWnS3qla3Gjox3BzUtyCn1oSJCxc+Sy3ZJMjZcEyC1fW6WhcecTDg2WjZF1sEW4Exx+DY7OfyRd3819gD9zfnfXADYn6RCtwiRf3ZgX9e6KUR0IESZ1m4Eov7l2HmyvkfGBP4DfAb7y49wRwG3Bjoj7xVo8nKzElGMdEZBhSLBORoSpfI0C+DHwKwBhzJK62+jG4L1c/xX2xkN7tiEtktAALAbBsA1BWE1mLe4IM4A3cF8XcidCzGfdUnmobi0hp2Kqx2RgTwdVhn1wegQnHj8+MOnZ8ZOW8lc1tK9rOXvOfNW8AhKvCZ2Q2ZBi156jGUCT0Uj6uPX3OvHLc5OEGOAO4Itj1nhEgOf6Au0n4WeDAnO3rcaMh5npx77DsU7rBk73ZBEjXki7fAWbiJoq9xYt770vUJ5azeY4O1o8HZWqu9OLeSiCOq7m+qxf3Tu1hZMrmeABIf3LXsrE/e6wNgNrxZStz9isBIsWm1D5n9jkBepD4OBL4Hp0Jk6zluLmTbgFu6lpCKVGfWImbI+mOLueswd2g/2Cw7I/7/LkjLm5aL+49C9wH3Aw829cohaAU0im4309ZbMqkjtlN6w/Zvr297Ymqyi+OS6d3qmtt/1FzyOy5LhR6YeRlk+46fEPzhpAbeTEONxG6BT6Nn1rR27Xew0+lgDPwozcBF+P+ux4YLFfgR++MfYJP4KfW40perevmLEng1iBRsjNu5N0uuGR0HS6BsyPwM1xJrmyJj6U9jN45Jlg/ECQ/AEjUJxZu1nsrAcEokR94ce+3uL+z2SRedvmuF/cOTtQnhsvNslKLYyIyPCmWiciQlK8EyCQ6yzEdB9xsrb3XGLMYeDJP1yh12fJXzy2eO7sdwHbYiQDVNZGVwORgfzYB0t0IEJW/EpFcWzU2W2s7jDEX7D81/Mv/+2jVhA0T7RWfLAsdPOGECfsCl3px76kXT3/xNcJucvSaXWvm57EEyBQ6649/e/qceb8PRoF0OwIENpbt+gquTn4T8BLwMu7G3CvAIbgP8fcEh+wXXKcRd4Ow67k+i3uauA64yYt7H9zMyV+zZVP+lXPeG7y4twL4P8ADnvbi3reBX23xU8R+ai1+9LG9JocP/vvHq6itNKysiDTntMjHKBORfCqZz5nJuthYOktTPZbd7sW9CtycQmNwydRv4WIOuLkj/oxLXj4BvLElsTMYIfKfYPG9uDcKF+eOwCVEdsGV19oL+Dbwkhf3bsQ9qf9ykAQuD5YY8AVceaeNE3onK8pJVpQTtOk6T1MlcNKs1lY+ta6JDzWtp9xt/wF+6sHNfT8buWMfxI9OxSWpT8YljY4DbsaPfgQ/1WssDh4geilYNkrWxUbhbup8HdgVN/k39DABOp0JkP7MUTUsJOoTK4DLgMu8uDcFV3ryDFy5tnle3NsvUZ94u5B93EpKJo6JyLCmWCYiQ1K+EiBrgG1wgfBDuJqv4G5GhXs6SDaxyfwfxhhDyI3yqK2KZJ9ctnT+sVECRET6stVj88pv1swbXWWuD5n/Z++8w+Oorjb+uzNb1FfNvck2trW21wUwpncwjsDGdAL5BCQQEiAQSECkaZ2GAgGSEAKBUAQEDKGDIPQOMWBjs/Av8gsAACAASURBVMYrNyz3qrJWXe3O3O+POyut5V1ZsuQ+7/PMs7szZ+7c2XL23vue8x4Bra2PAr8D3gSOAN5x9XM93bqp1eHIcpAxPmNOL156SNzzfsBVKGmVzjJAsCJ0H+ywu8FX7rsXuBElXfKWRTbEsj9ejY/sjWtrm6/cNwv4HLWo+Cfgpq503lfuc6IWISGOALHafctX7vMBD6OKpP8FmO4r913eAy31/wLHzfIqJZ1XHI7Y/TQdyPIsNvZbHEjjzFi2WeUFtzpqKPf9AyV3l5bAthm4H7hjd9RNCBQHthGXKeIr9w1AyT/NBM5CETF+FFlikPy9Xgr86+fVtedu07Spi9yuuv8ui9RtnLNpaNqotJasKVlbM7zpGwZiZm/W9ZGL3W7HL/u4mZ2XGxoUjT6z0uV8LNAbN+QPrQP+BvwNv+doFGH0HeA+/J6r8Ie6TRp5K4PbgH8FC70PobJDrkeRHC92tLVqSB1lvbQJkAQIFAfWA/dZxNqnKCLtFV+57/iYhNsBjAPJj9mwYePghe3LbNiwsV9C27lJl/A88KQQ4i3Uwnxs0D8ZWN5L1zjQsR0BArjdA92rU4ak0DfFtdHa1wjE5AESSmDt5j7asGFj/8Ie9815adp0TQg3KoPiG0vKaRowH+jjynddBZBzQg5CE+/04qUHW4+xKN9bCkoqUukkA2QnuA2V6TEZON+So0kmf9WGQHGgErWYCXCjr9x3URevdxSQifLxXyVodxMqyupaVIbGtM760QVsR7KscjpjEjq2/JWNfREH0jgzXv7ql8CPaCc/TKAalYVwBzA8UBy4cU8VjQ4UBzYEigP/DhQHLkCRx/+Heq8TkR/NwBPACct+sWz8IWWVI1M/q5l6bV3IuH/TlpNW3b368PCGsKz9sDZt1Z2rhn7zg8XD3vpB5dvfXL+kdNVfV73Xsr5lU6smPCtdzisbFjesSB+VXuvu516subS3hRD3CSFmCCEy2FX4Q5+islNMVKbKb3a5LVSGiLcy+K63MjgTcHsrg3cmMDsVNbcKBooDq3pyvQMdgeJAHVCECk6YDMzxlfsO9IWzA8mP2bBh4+CF7cts2LCxX6K3MkB+iioEOAS4WUoZi+AZgCp4Z6MTFJRU6Ci5AYB5AFLKFl+57wNgZL8lZq2av9GAmhhD4gyQuj3QXRs2bOw/2Bu++Rzr8flYtG2gOFDnK/edBrw75LohExsCDWROyFwQKA4kzMrYRcQyQJ5HycYMQ0XqZlr7u3WtQHFgq6/cdwdK8/33wEpUIeFmOpAHCc59wVfuuw1VFP0hX7lvcaA48PVOLhmTv3ozWQaGJXlzr6/c9xGqKPBRvnJflhXF3V0sQL0nfQGqnI4YcWQTIDb2RRxI48xjAP43RtSj6lYA/BiYA4T2VgaWEEKgZC3GoYJsFgNPSykft6SyMoDW+C1QHDCFEIdrgnkvrse3tUly/jjn7fhDC6QfhBDnoTJJZlltXxPdFqX+q3oavmk4f9wD4zTgqkht5OSm5U2xIuZeVDbc1Qiieor+mTTkz8f9a1wdcETNezXnNa9snuIe6K7OOzXvD8Ihng8UB1oT3pQ/9BJ+zzWoAuZ+/J61+EMP9fS96qTeXpv8lRBiErBEStmcxHanEEKkAFEpZXekFPcbBIoDK33lvhnA+ygy5C++ct9PelEac1/DgeTHbNiwcfDC9mU2bNjYL9ErBIiUMoIqDthx/90JzG3siNHSiGRsfe0vRsuKL52UtWWA9wPIrSe2o5HOCRA7A8SGDRtt2OO+2e9JQS1igCIiKCipmAI0VZUFvvGV+07VU/T3PFM844GKXr56LAPkW1SR2gdoT8kOk7j47c5wN3AdcAhKfx/g9UBxoCskwa+Bw1A1RF72lfumWDroybBD/Y9kCBQHvvaV+1YDQ1EFjT/sQn+2hz9k4vf8FxXhTZXTGVt0tQkQG/sc9odxZrDQeyhKfmk9Kpv3S2CetzIYirNJAaYAPHWC9n2UXMT9geLAfXu+x+0QQmShFjNyOhyKCCEWAy9KKf0dzskQl4nfAtebEi0nBaYf4qhZEzJ/G2OjpZQvAi8KIX6EIjXOQZHSW2WrXBkoDswDnsl9OHdq3ml517r6uY7W3NqI5qpm6hfWE9kacZhh87iR/pEfY81ZIjURaj+sBRjQsKjhqUHfH7TVV+57EHgwUBxYucPN+UP34/cMRmXb/BO/5wja66BU7YosViJYWYJnAKx7eF0NKpNvgRDiJClltwKUhBD5qMyga4EbgKd6o4/7IgLFgbm+ct+lwLOo+10O/HXv9mr3YH/wYzZs2LCxM9i+zIYNG/sreisDBAAhxFjUgowrfr+U8uXevE6C614D/BwVXbYQuE5K+Xkn9uejdOkLUDItt0gpX4s7LlBReVeiotE+AX4kpVy2m24hpfbdh1Y0Lf5gJPCZEOJm4N7xj47vB9C3TjZZdg3AVut5vASWTYDYsGEjKfagbz4VFSW8FviyoKTiCuAhIFxQUjGpqixQ6Sv3HY8qIvt8L187tua2FihHkR9DrX2bq8qKdqlgsK/c93vUYsxoa3eXZKcCxQHDkr/6HEWgPGcVRd8hUtlX7uuPShsHVS+lK5iPur9D2RUCRKGNAFnjbBsO2ASIjX0We2ucuTMEC725KJ82zNp1fuzYVz5vnWay0WWwCSXR56pPJbohlyxUQfMbAIQQpwGFwBwpZWdkaY8hhDgSGCOlLAeQUm4TQkRR6cYrUFJ841Djy4nEyfIJIZyo2g39sPzumaMdNQ/NSMntm679EX+opeP1pJStKImMhHUxaj6omYtVONVX7huRc1zOcfISOaVlTcvxTUubvCnDUhwo6b95zlznppSClMyW1S0nNSxqcCz/zfL8wd8ffGvmpMwSn6qnUpKglsSvUSR5Mao+1FXW/g34PS8BP8cf6mn9iYmo6NfG2o9rJ1j7JqEIoJOklDv9DxJCxIiaYiDV2n0xBzABAhAoDjznK/fdjCqUPmpv92d3Y1/1YzZsHCywCOt+qKzLXc7SO9hh+zIbNmzsb+gVAkQIMQJ4AfChCnUL61BssL/bNF2FEBcCdwFXoyZPNwBvCCHGSCl3kDwRQhyNmkjcCrwKfBc1OTlUSrnIMrsZ+AlqArISRZa8IYQYK6XcYWLXU2R6S1a6csPvtG5ym+F14VGoxTb/4h8uzs47PY9+Mj9WnDaZBFasBogtgWXDho027AXfHJO/eqGg5ckLgX9Zr93AQwUlFcdXlQVqUQRFbyNGgKypKitqLSip+APwT2tfd+t/xOOfqGLow1CyL6929cRAcaDWV+47C/XfdBzwd1+574cJ5D1Otx7ndUMWbD5wNu3yibuCN1DE+aYmTYtNXmwCxMY+h705ztwZgoVeATyC8hErJDy4NYsi3WRqbgOulAgxaafC2DlfFwgHQmxuXt188fJfL4+N8X6KklC6SwjxX+Bl4B0p5bdd6YcQQgNygbp4ySRr/1Dr+oUoP30c0CCEeFlKWWuZHg2sjY1zrWCgYaiF/Y20YyztdetWn+N13PHcBWn3oPxjj317oDjwLSqTrxzAV+5zofz76kBxIBKrsGQtvDxl1BsTVv1lFbmn5Ip+5/W7Rk/Vp/vKfZcFigMftTXqD0n8nitQBPaJKBmyw1CExdXA0fg9M/CHelK3YzqAGTbfx2yTwgoBd3eR/PCgyOzh1q55wJ2ozAiEEDpgdqWt/RR/Rkkzvr23O7K7sC/7MRs2Dgb4yn19gEuAK1C/Q3zlvmpU8NQa1DpLOG5rRf0uHXGbgfLtsS2WYe4AnHF2c6w6iAccbF9mw4aN/RWiN8bRQohXUH8GP0ARBkegFujvBH4mpfyok9N7eu25wBdSymut1xrqD+weKWVZAvungXQp5Zlx+/4HLJBSXm1N+NYDd0op/2wd96AW0C6TUs7pYr+yUH+KHillp9IrvnLfTOBFzdBa1/x+zR0Naxt+hlowJP/MfP5Z169k+MbWMt0030DJsSyNOvSG588/vz/A2c8+9y/NNC+KOh2/eHnWrL8BZmlpaVs0w+zZs9M7uXxH2zTa/8Q6QpaWljbtom0qqjBkQpSWljbuom0KnfzJdscWaCotLZWWrZvOCcLu2DaXlpaalq0LNTjqDduW0tJSYxdsnXSI1OiAcGlpaXQXbB1Y39skaC0tLY3sgq0OpHRiGyktLW3dBVuN9gjLntpGS0tLw5atANLiv3s7Q3f8RXfQU9/c7X75PcUtuC67Lzrjw3ujM38J6BryaRNRBGRoyJuWls28K2beTd/Uqe0jzYevREVzHXp56pdLvo3muD6JFHwNDBHINy5NXRArYN5tP+Yr910MPOkwHc+evfrsy5J1Ipkfe33Q66e16C3PA8JpOn925toz74+3nfTIpDnAhbrUb5+xZsZvd9JuCqC/Puj1aS16y3MCsfjs1WcfEWfePT8m7+oLhA8dOvJCU5j3a2ivz1w98/wEtrYfUzio/Bhs//3bGQ4gX+aZLW6MAizLWpayJGvJd6PCOEWX2icj60c+OjY0tr60tLQFIFjo/WlU1++S0PriMe7rXzjKvEQijwXIapKNg7bKFbppjEtrQc9oAWdUZ26hMIKPb7yzbmHdd/r06XPZlVdeWfnnP//5ynA4fGkkEjks1g2n04kQYpWu62/ccsstN3bopSwtLW0SQuQC33c6ndcAwzwez6nXXnvt/wDuvPPOS1paWv4CpEYikbYTHQ5HxOFwzCkoKPBfdNFFOxDEnY3J3njjjcxgMHicYRieyZMnv/zBSQv8KLmmp2eLGy9jD47JvvjiC/e77747u7m5+VqAwVcP3pI7NbePkELqUr93dGj07MJthR2je5V/8nvSmkg5zYHxAMi+ILaEyPzuP8Rln+1gy87903PDnntdCnnc5pc231v9cvU1uq5vPOaYYyaccMIJTQnMt/NljY2NrnvuuecxwzDOEUKsyszM/OEVV1zxcXp6OkDY7/efB/hTUlJuLCkp+aCT98z2ZQr2mCwOOxk/GTFf1gVbe365C7bsg/PLhTkLc1dlrLrJFOaJdFi8NoTRJIVsApp0U2/RpBa17i+2xUiAaFSLhqWQrUBYN/Umh3Q0CSkagHqr3RQpZArgjmgRhylME0A3dU2Xui4QrUiaBKJRIJoEImpi5rZqrTmmZuYBfYQUebrU84BsicwG0oF6gagD6kxh1pjCbACimtQM3dQlEBGIBqBRIBqQNJvCPCaqRYukkM7Y3eoy+cdmChMpJLtqm92aPemkjSclKghu+7Jd6Jftx3bJ9oD2YzuxteeXCvv9mAx615f1lgTWUcDJUsqtQggTFaH0sRDiVpQe8uTOT981CCFcqAiu22L7pJSmEOJtq0/J+npXh31voCJpQUVe9ScuAklKGbKIlqNQRSoT9cXN9l/UzER2SfAy8NY5a845jWJ+2fHgfCjbsG49x334YZsE1kuzZmWgMkJ48bzYuh5/tLYPUBFuMVSxvWRWPL7E0qO2sJh2CYeOWIySRIjhC1QkYCKsQkmMxfAh7RGDHbEV6BP3+nXghCS2TaiBTwzPAd9JYgvb//E8DpzXiW0G7dHP/4RYnGFC9EVJRID6Pv24E9vhqM8A4A/AzzqxHQ98Yz3/BVDaie0RqM8AVLHn2zuxPQlVZBKU9MPfO7E9k/b6DJegIluT4QLgP9bzWcAzndheDjxqPZ9G55H01wL3Ws+PA97rxPZm4A7r+aEoyaFkmA34redeYFFyU/6MktYDFT27o7Z4O/4BXGM9z0cVl0426NmT6JZv7qEfA3+o/E+zZz+KkxO/52xTTLkw9mStkXV7QUnFi1VlRbGI5s1Yi6wJ0GU/JiXzsGomoaK4Fo9w1A4b4YgFNjMN2mopdduPBYoDT/nKfd/MWD3jkbh2OiKpH5u+bnq83Z9NzD/4yn2pgeKA9JX79GM3H3tO/5b+oL7LNydoewc/Ftfm2A596p4f84c2ARxTdt0l+eF8UFHMie7R9mMKB5sfg4PMl00umZNRUZW5Ca86f9S2UYza1qbKMxO4vdZRv3r8I4ceNuf25pEa3P7SObMwHA4XcN/Zq7drLh2ofXbEs7koicDpM1afdflJNS79pCJutiomfQnws5/9DOBLv9/vRfnNU6+55ppjs7Ozh7G9bBMALS0ta6xMkUuB1CuvvJK+fftC3Pj1pptuAqCuro6//OUv3wCVwMKSkpJzHQ7H94DvJXgLOh2TTZs2jWnTpqkXUv4VFhjWoQfYw2OyKVOmMGXKFB5++OGLVq9efXnKoJSLjtx85CcDmweORf3+rk3QrvJl/lDTHbNnH4saz4G657c62HbZl2W3Zhu17lqq36rOmzp1Kqeffnp/2n9DHbGdL0tPT/97SUlJ7Ngwtq8FdSbKT46ZNGnS3bTLMSaC7csU7DEZ8PcfHPlSXXDzGZxenHSBRqvd1PjwzEmLhGnUhltb6znywlloesL1AUck8kWw0HsxSo6uhYsurETVq0kEe37Zji6Pyz7s9+EMX7kvHdBPdp18WW5r7qnJbD/o94HfV+7bCLSe6D7x3PxwflEyWyxf5iv36cemHPvsxNqJZ02snZjQ8J0B71DrVuPnkfUjmdCm6JewD2xJVdPhgoYCJtckX/L5uO/HbExTyYSDGwczpXpKUtvP+nzGuvR1AAxsGshRW3ZY1nFjzQm+yPuCVZkqea9vc1+O3Xxs0na/yv2KFVkrvgQePn396WuyIlmvJLNdmbHyg3n58xYCxqCmQflHbTkq0f8lAMszl1ctyFuwHIjmhnNTTt548omojLZEsH1ZN33Zk+f6mplwXtKF25o1K3nuzHFGLBnFnDxL1xyJ17sbNq2TT86c0CARBgjTHDc9R3OnJHxPW2q3Go+dPXm9RErAML2nD9VS0hOSBLK5of6hGd5ndIQBRDTfWRebrrTcRLZaNLztiaKxbbXfHBPP/L+oO31AIlthRJuemj72TikwAdMx8TuXRVMyhyeyRZrhF071XiEkUSExnVPPujWSkp5UKeCjKd5TdRPTGaXVPX3mb8OpqScnsz35zbcODT41pwnAPevs28IpKbOS2R765ZeHBQu91UDUee45v4u4XJcns80MhcYEC71VgBQXXnCb1LSOwT7xsOeXCvaYrAfoLQJERzH9oAYbA4ElqEHKmF66RiLkW9fuGL22iTipgQ7on8S+f9xxdmKTCLfS+Y8wKaxFsOtRA8DO0IBivkw6YX5t2LBhw0J3ffMu+zGAgpKKUy7vhP+XCB14oKCk4rRdqcmRDAYiNrlvob1OUq8iUBz4evbs2ebOLXcOU5huYL6v3HcX8G1bNNpehCnMvd4HGzY6wR7zZVellk2s/SrdjTe5TWt93dDMJrm5Oh2Wb2oUYWl2OqAOFAe2oWqEPO/3+89GJA1KQUpZiZqEzPb7/ator2W0HbZt2zYEVasOYGFGRkYf1PuyAzwezyop5fjY69mzZ8/opLtdhoZ0oEjs5ajJ40290W53ccUVV7xbWlr6NMDs2bPfJ/niKf/r878TfOW+sYB2suvkkbmtCdcnABgpq7KTHtwROrDEaDDqdE1E6WSOtWLFis7mE4lwJ3BNS0tLZ+SHjX0fe3RMlt4SPmLriiZXZ6sMen04/agl4akAr21roPVwA5crcRBwSnX1FNRvnXWRCKKlBZmaeNCXsW3b6MWF3qcFNAMtrnNm5ba6Ewe0usLh3GCh92pUjaSo85xzPBF3Ys5GMwxXsNA7ESVN1KpdcL7T1Pee2o6v3CfOcJwxMCOakdTmpSEv3fts+bM1QP1J7pN8eeG8pLZ1rrq22gk17ho680+NjkZ/7Hm1uxoriCUhPuvz2UW+cl8AuLXGXTPJCrpJiLyWvFtr3bUbgLTs1uzpwFnJbLMiWQ9sSd2yBkjJiGYcjVpITIi0aNrrWIE06dH0MUDSxda0aNo7qIW6LX1a+owgMZkNQH44/1+rMlfNA5zZrdk+2v8Xd0BBQ8EfX7zuxV8CzJ49+8RkdgDDG4ZXPHrTo3dYtlNIHDAAwCH1h5Q/fuPjfst2HJ0vHB4I2KO+bPC6aMqy5Dwc9S1Rxi43LUcgWTBRJl0oq22OiMlLIm0EzOJCM2kIf01TWJ9S2RKTWKZyRJSUJDTMlvqmzHOXyO87LIcbHNZEWn7iGL+ahuas6cuMW9I11cvA0HrS+idORGlsCacNWhb5dR+HGlJ8MyiEa3Bi/igaibpzqyL/7muRP19v2grDkie4GDWRt/OdylbbsBFGJOZVAKLVW+fH1qwz164lfMghSW37Vq2aF3s+tKqKFaOTD12O/+DDJbHno5csYYk3+eD79NdeC3zz1BwJSK9vvAiOH5/U9tj33in/+pk524QkOsY3of+SsUmHhRz18Sezg4XeTYD0TvCNDI4bl9T20C++uDZY6D0TiHp9vjHB8cltx30duChY6J0EMG7cuMJvJviS2o6uXHJmsNA7AJC+sd7RgYmJCWqAgm+/PTFY6HUATBo9+pAFhyVXwx68evVRwULvjwAOHzliyJdHHJHUtt+GDZODhd4rAHPqsGED5x6dLJ9g/0ZvESCLUBrBK1Fa5zcLIVpRDFqXtIsPANzG9pklmahI5C4hUBwIHvbwYfcYwrhOIJamGCk3NevNrwjEsof/Ip91RiK3Ao3eyqAZLPRWz3z+hT7bPFlT35427ZtZ/3n2f0LK8c2pqTNeP+vMd1EESTwKOrl0R9uxdJKu1+H1lG7YHk/XSZvp3bA9l67rTH4PuKyT4/EyBT+knancme2NJI7ajiFefuGXtDOrO7P9I+2MbSLE16P5K4pd7YrtA7QzzIkQjnv+b9qZ653ZvoCKckqG+OLPb3TD9qOd2Ebins/vhm2wG7ard2IbjXu+dSe2exLd9c277McKSioE8JvHmycjkK8e71p56VA91Pa+vBYeM2KrmT4XOAX4Pqo+SN8kzUE3/NgL4fFHoSJ311aVFcnZs7/cJ/3YiswV7m+yv5ltCOMKVHHcx4DIZ30+Q0d/aebqmZd0od02P/bS0JeeMzGnOU3nT89ce+aD1vFd8mOf9v30fYk8Upf6/TPWzEiU3WH7MQXbj+0d7DFftqI6Uu3yOuQHD/xNZEQFOVIjXdPYZhi8vG0bLVJyqSebB/PzhDQlpVs2svn2O3A4kOeNdYoPVkabN9TLVACHw3HPLbfccguAEKIvMCw1NfXpaDR6DVAzfPjwqd/97nc3xF1+O78nhPDSwT899thjQ9avX/8rwzCmAF+joi0/TktLS+1oG9fObhmT3cw/3kVF2f0Lf8hk9ux9Ykz20ksv3RZcErzdiBgXA7j7uxl4+UBSC1IxhPFozPC9Ae+hSQ2B2KCb4sWba2oyz2qsv6RG08UXqW6+THF8dOLDY8ONQjRHhjof8bR6Dj1548lL4y+8MGdhZlVG1T8iWmQW8LqU8qd1v8iZ4DIWH+3QBGHcP7lL/PDhu++++6ympqZ/AlnRaPT+xx9/XEopn6YLvkxKGRVCPPr1119fFQwG37z11lvPSWYb99z2ZQr7ki/bY34MoN6T/nZkUsbMr5//p9swpTBMqZmmKSKmKSIRk9aoFOcVeqj0uZAmLFsR5eE7kv9d/27oAJrT0nBGYX5zE7+/q71r/RwOhrpcFDidHOJyMz49PUto2gWx42e+1GlN5P5AWzT0WS+91JntAOIi689+7vkdDCQYhobZ6iBn7otzoqZG1BRET9D1VqmJzaagWUIjggZNigbNpFGDls9fnPOwbtDsNGicqTnnS037VAoRRtAsEWEEhkREEESDT8058+1JYtwRI8Rla93/LTSFhqGBFGBqYFqPUkDfGqNYampAWel6H+kWbTaGBs1uiFirpoYw1gArgGggJyAX5SySQkWUR5FKdsraRESLaChZF9c32d+kBrODeUAfiexDhzUeQxi/ij2vzK4Mrcha8cdJ1ZMeGNI0JP53BcCk2kktL/zkhZgczEN0Ih0zuWZyy4vXvdgl6ZhDaw4Nv3TdS12SjplYOzH8/E+ej5eOKUlmO7xheOujNz0aLx3z02S2Oa05ti/rHexRX1ZZmBFY8/Zj/RtbjYTjC7cuzM+m5DSixkDaxo/mpIeaow7TlEgpNVNK3TRMYUgpnBragsNyoyA1gRTfvvmYvr6+VUhUnoeUIEFICbqAb6b2lcIqGbD0zSfEsuoWIoYkakpaTUmrIYkYkohhcuopwxAIkIJPXniCzzcnV/mZckIBqQ6JAOa/9gwfbKhPalt4zFCq3RLNFCx46wXeWp9cAeiQSUNocYOQEHj7VV7cmLxE8OCxgzFTlW3wvdf591O1SW2HFgwkI00R3kvfe4v7nn46qW2/gf05KiMdzYRl775H2X/+g0sInEKgAQ4h0K3nQ/PzOTJNkTSbP/mUOa9W4BDKxmltDgROTTAiI1McnpYmAMS8+Sx9/310BLqAHN3BQKeTgU4ng5wO8hHe2MB1fGAR4xZ9k7izgGaax8eejwsswvtN8ph0zTTPaLNdtAjv4k5t28Zq3sWLGVNZ2Zntuah1TcYEKxm1ZGlntudhZR0esmwZI1asSGorpJyFyj6h4NuVDK1KXupOSDkDmAEwdNUqBq1di9S0914479xEJPh+68d6qwbINFRdjeeFEIegUnZGowpJXSilfLfHF0l8XRdq0nOelPLFuP3lQLaUcmaCc1YDd0kp/xK3bzZwtpRyolXUaQUwWUq5IM7mA1SdkOu72Ldu65T5yn3ZwFJUuu5nqPTCj565LfoFapH9dm9l8JZgoTeIynA5yVsZfD9Y6I1FCE71VgY7S2uyYcPGPojdqNHaI9/c3X4VlFRko6J8flNVVhROcPxGVCTrNuD31mO9tX1VVVbU5cFwh3YvAZ4A3q8qK0oafbavwFfuy0UV3r2O9szCKwPFgX8lPythO78DfgU8FCgO/KCHfbobuAH4U6A4kHSiacNGZzhQfJmv3HcDKprxv8/cFk1HRP61xgAAIABJREFUyII+vvrSTQ3mefcGmrk4Jb81X3e6IpomfzhonDhk27vcfmyYAZkahil5ZEHkk6teaWmScHWsiLkQ4grgobjLzJRSdroquE/D7/GhCJgoMDgmp7cvQQgxHfWeD0Bg5p2atz5vWl6tK98VRpFNEhX40xZOKaRskEJ0Njn7BPV/MwhF6B9BO+kzLbBy9WrUAlgMdUAh/tAmIcRwFCEbH1Z3pJRybhfuZQSwDEVGHSalnL+zc2zsGg4UP9Yl+D0CyFhhDjhkg8yd1Nxq+uqbmr2RpoaRRvO2/q1NjZnrGk3WNsK5h7kZ0t9JsyZ4b0mE+95qZF21SVN4x+TY75w5hLEFmbgjsHzFNuYGqkl36WQ6HWS6dbJcOlkuBx6XzvCsVLKcOpoJmiFxmALdBIcp0Q1wmOAwQDfAaSCdUUynAQ4D4TAQutwnJIF6BFMQBqo1yVaU4kPHBZoG1Jg5tjWggk2aUcEhTUAtsCWiseWZ47XIm5NFn6xmJrsjTHQYjNVNhqWF5RdnfCn/etgKuYn2It8mqp5D/GNsM7yVQRks9AqU7xGA8FYGdyBObOx7OKh82Z6E3+MEMlqlwxMiPb9Zuvq04ugnIEtChoD0xlbT0xox0k2J05Q4TCkdhokTK6BkQJYjrGvKddU0RR1Nre1FZkyJbkrTbZq4TNN0jch3mQ5NOE2Evro2krq5wXBL03RJM5qCEXWZRtSFEXVIaWozxzhIt/L+/rc2yvwNJlEJEVMSMcGQAlNKTAmXHeYiL13DEPDGsijvLI/QagoMU2JIiJpqi5hw42mpDMh1EBXw0sIwcz5rIRyFlqgkHJGEoxCJSiJRSclVAxlUkEUYJ299WM0rr65P+lZOL57E4NH5SASVX6zj4xeTEwq+SyaSP6E/ptTY/NVGGXz6q6S+/4SzBhmHjPPomgnrqxqY+/5mHA6B06G1P+oCl1Nj7BgPA/qloUkIbWtl1ZpGdE2gCdA1gUPTcOkChy7ok+UmK8WBJiVEJdGIiVsINIuIEYoC2wGxMCRNqudCWs5Uth8XoNg3trfB2g/JIzZF3PEd2uys3XioY1KgShoJoDqTldc8szh5us8eQm/6jF4hQBI2rIoy1srddYH268wFPpdSXme91lAs1N87KYKeJqU8K27fp8DXHYqg/1lKead1PAulVbZbiqDHw1fui0VGx/DsM7dFa1CM+m+8lcHfBQu9HwPHAOd5K4PPBQu9ISALKPRWBpfs2KoNGzb2ZezJgWB3fHNv96ugpEJHLR5NTXC4BhhSVVaUqGDsztq9BSgDHq8qK/q/nvVyz8FX7nMDF6F0sW8LFAe6NaH0lftmoWR1FgSKAz2qteUr9z2Akg34TaA48LuetGXj4MUB7cvUQmEpUCpNaFif8rdoi/ZBxsjwg05h5G6RHu6PnkWJ4ymcwgC4H/gx/pC0+vAj4Dco0vMOKWVnmaP7Pvyev6FI3OfwhzrTv9+rsL4n/6C9HtU6KeXg2PGc43JyBl0+6Bihi3NR9QCzUQt/8/tFo/Ovrgv1yzPMmc9npPNBWqop1TyjI1ZIUz4f/FHwQaMk44coKbDXUJ/1ocC/8YcutfrjQH2PYhHZTcAIKeVOCSQhxBMozelnpZTnd/vNsNElHNB+rLvwe1JQQXfjUGOVocAQaxsjpXRVN0uWVZssrTb5YrPD+Hhzuj7s9EtZnDsew9FC6NPXqXkrOdc76MoJSz1TsmvRWql+c02/zS+sH6qn64Yj0yEdHodweBwOR5ZD6Jk6nsM9uPqqxAEzaiKEWqRyGuCMgm6CM0ooo4UPB9TID/uGqI5q9DE18qUgD8jVTLJ1tWXpJpmaSYompVMzcekSzWG15Ypu/6hJcBgS3VRkjC6RWU3UeBrZ6lAckIMdi4XHEweatSWy2d/QAKxDRfGvQ9XFFKh70uIeBe33DipquNV6jNCe0RJFkS+xLZ6EqQXWWNt6b2UwPovDRiewfdlBBjVOjRXQdsU9uuJe6x222P5UVIHsVFTWlxNwSokrip5uoGWaaNkSMkFkAdkCM1tHZukYGQ5hJhQHa45IQmFFkIQNiWHSRq5EDBidp+GxSrCsrzdZvMUkamJl1ijypdVQ55820kFBtiUbtsng2cURIqagydBZ2eCisk5nfV0rDfUN9Jl1K2mjjwLM+obAm9Hq1+7NSfa25Zxy3Bv5Zxw2Xzjqm6rf/OKQ6te/TlqnKfuY7KcGXzn4U8BR/Vb1mA3/3nD1DkYCEw0z98Tc9wd+b+DXgKN+Yf2AdY+uO1UIIRFINKTQhBS62jxHeVb2+U6fdQDNq5rT1z+2fpLVFtY5CE1IBCLrsKzNeafmbQVcrZtbM9Y9sm64NFSVGGmqFCbNrQktVROZkzKN3BNydSDFaDb06reqNaEJ5ZFNkIZEmhJpSFILUvEc7gHAbDWp/6oez1TP24HiwGnJ3o89hX2xCDoAFgM8EvhQSlljkQm7G3cB5UKIL1FakTegipg9YvXpMdRk51bL/q/AB0KIm1AFbC5CFU+7CkBKKYUQfwF+JYRYhkrt+x2KFGnLMtmNeAQVGRwr6LYRiAmAxgrTxjTu84OFXo32yLXkOW42bNg4aLGXfPMOqCorMgpKKi5EpafnoXxXJiqCNhdV9Py1XWg6ps+6phe6uccQKA6EgfIeNBGLAB7vK/e5rfZ2FTGR2OT52jZs7GXsVV+miAw/fo8hNH6bObjlJ8CPUWPphY9Gp/3kIeM7d26UuYff47wHTcirUfXj/ABSyvuA+4QQKVLKFks28GTU+DMH+HlVWdHCPXY/SXDWrfdkTdfnnnqZ/qaRJsKDUfKDQ1Hj4HdQRYcjtOuhP7BXOtpFSClrgIuEEC8Cv0YFSQEghPAAb9d9XPc+cMX4R8f/EFVkc2WgOKC0INSCwj0nNTVfs1nXozf0zS8PpLjHobTO3wbeCRQHVgkhJgJLJ/+zITrvqnQ0Ie5HjeHnApfg9zyCP/SOlDIK/FoI8TZqfvF0V8gPC2UoAqQw9j3q4dtjYy9gXxmTdQn+UAtKcmrHgs5+T4oQYmp+mjg+P0074aghHF0MqSop4UHC0tESMEds+mhEhvPzGSdmrmtJSdvajL61SdLY1IzZVIfRGMKMXjm6cYXSnm/d8jwy8jDRuqgWrYvuMKqLNhz5RM4J3qDm2hLZ/MKbY+s+XFOspWh1eoZer6fpDdFQtCa6LdqAJA14O6bmIIQ4FFW89kvUb7cKWCWl3G7M4yv3xeoapaHGRbEtAxVsGNscwHOB4kBVT95eK7MiCzUGzrMeO4r2a9a++Otn0r5YmWr1Nw+lItHH2heDgcq0brLacgPuFtN0r4lEdKcQpApBuqaR0fV6KhmoWg+7s9ZrIshgoXcL6n4aUffUiMpm6UimxD9GaZeYlNYWRZE3Vda2EkW2CLYnbmKLwS7rUXRov9VbGUyuX3QAY7/yZQcD1Dg1Ri72CgTtP4DOr+1xovySB+WjPEB6qlOkpzpFzJ+mYvkflP9y004cOwZmavrATM3Z4XiKdW6GlGRKSSaQ4euna75+8f4qxpcKwtEMQvJfNOn/oY70zG9Hufj64kOpbXVSG3WyLaJTH9GpjwjqWwVZw46cZmz1TqsjHWeel9RDnkGaBhgG0owioxGk0YqMRnDkX3J+ffC4aUBDpOFj1LCsAyQaBlqk7lhXffAqA6ivX/y6K1qbnIRBK3w7ZfjvnwPkti9fHt684oEzkplGG0Z87hp028fAgPD6ytGNwZ8lDYJsrS5419n3tgeAUGPwo5ytL//pyWS2GZNOiuqeH4RBSqMpxNa3HkjR82484LL9eoUAEULkoaran4T6QxmF0v97SAhRK6XcbUURpZRPCyH6AL9FRVotAM6Im0wMJU5TWUr5qRDiuyj5lT+i0snPllLGF6u6HfVDewAVCfax1eZun2gEigOmr9z3E+BTa9cm2hf3YgRItfUYW0CM/dmEdnf/bNiwsf9gb/rmZKgqK1qFIqrbUFBScT9K4/0MekaA7JKE1n6M1ajMmVzUot28zs07RaxSn02A2NjnsE/5Mn/od/g9UdQY0gG8BFz68z882HBvScUpFeaRr+RE64//vfMRgFL8nhrgnlgmyLBbXk0rKKn4McrnxVeG/LKgpOJPwO+ryor2+MJ2tDRn4gJ5yG0PuzZO6yO2JasPcgNqwWcFanxchSIB9nlYGdxzxPYZHKejMjQOBTyLLlv0wx2kpfwhid9zPTCwr2HMenLDpguAY/GHOha5nQUwzKM5NCHWA6/jD0Xxe+5FFfC9D79ngrWgjJTyA1Qtlu7cwyIrsOud2JxECJGN0mL/cHdn3dvoGfYpP9YbUN/lD6ztd1a2yCmoTKoZbhHte7hYOuzw/rSLfVpolXrjVjzVq8z+TUvl++ZqGXSukX1SV032Zq0ddXdWTbOJ0RRSJElTyHoewj246NJIzUAApAwDT2C2mDlmi5kT6bDm5/D0G1pQUrEESNEz+5xu1G+Z3fEWhBBbUIvg10kpPw4UB6JCiCEopYWauG0NsFlK2drh/EOB7wDHosZiNahF9Nh2j5Ryo2XrRX3mLbTLV7Wi5vP9gFdihIwQoghF2AjUd2Uraoz7tfW4REoZX29tOwQLvbHFxnrUAn2bb7CI32tQwUhtldM1eHPRmMJzsTJVfEsq3xEgNSEahOpzWFjbAKdz9avDR7wNDAYGfdjQUFhrGK5G09QaTVNrMk19ixFN2xKNZvV3OGpm9x/wP+syziOXLb0hLGWKIZXkjy6EFCB1IeRIl2vLnGEFH8b6MP3bFadJcA10OsVgp9M9wOHU8x2Ovk4h+uY5dI5Lb1crnNvUiCHBKQQuIcjTdfIcDlK17f/OTCn5qLGRrUaUAQ4nI10u+joc9GTt3iqgvABYSDthuNRbGTR2udF9GAecL7PRc/hDEdr95W5B2y9UBaY4aSd/M4AJKHnRo9wOcVhfWtyx0pGTsuGc7M5abq8g0DLCKcMjnDKMS7ZIF0242UaWtk2miXrS2CaXO0JsyA3JjNxabzo1o39NTcRFVGpETIhKgWFCs6nR4s4/vgnjeAOd9MLjcOYOQpommIYiWKRpES1RnH2HX4IKcCF93IloLpVQIyUgLcVWKZGmgbv/qJnATABn3hByz7gOITTQdISmEu/MSBjZ2oyr34iTUcFWpAybQMbEaaoP0gShITTdOk/HPXisQ0Y9ih+QLlJHHIOM5O2U+9rf0Fs1QB5DFbP9AUr3dqKU8ltLH/AuKeW4Hl9kP0NP03R85b77UJkgpz1zW7QENaC8xFsZfDJY6P0Tquj23aiMlirU4MbdW/23YcPGnsNu1GjtkW/eUynKBSUVZ6MKti6rKisavTP7BOfPQy1gnVVVVvRqb/dvX4av3PcWcCpwVaA48ODO7Dtp503gNOB7geLAE73VPxsHFw4qX+b3nIdauHoQf6gt0KagpCINeO4Gx7Nn3OBQBXrXybyN90fPWvWkcUqWgT6K9gCkBuBx697OtfZVAt+vKiuKBcLsPvg9OvDjZum6NlW0tvneBplClezPOpnPWtmHTTI7MkVbsuo4LZCSIiKD41r4Ff7QH3rShYKSCr2qrGivLRIJIS5D1QnRgP8Al3Zc5ATA70kF3kItjK4FjsIfWhvXzteA79GZKRRPcv0ef+jX1nke1Hd2ADAbf8jfw/46UPO3iPX6BtR84F3gYinl5p60b+Mg82O7C8q3HImSPB2EWiiPbUPYieyTKUV9AylVG2TexqAcGvrSHBOea3rN5XJQukTkA32lNEcajXVOY9sWotu2YDbVgeZA6A6E7sQ91IcjQ4koNK9aSFPwI4ymOqKhTTJat9GUrc1t4cNp3uO/12fGzc9VlRU1CyGuA/6WpGs1qN/ZmwBCiLvopOg24JVSVlq2sbptyTBOSrnYsvWjpPKS4Vgp5SeW7fnApajVPkl7faNGFFn9lJRytRWd7weuR0VngyJIQC0kviKlPMdqU8Mqtp7k+m9LKdtkUYQQ1bQrVnTE+1LKk+Js6+Ku3xEfSSmPj7PdhPrN7ACPpgU/GzX6F6igVffhS5f8uUnKHSKsnUKEBzmc614bMeIJQJhSikOXLb25Vcq2IuzpmiZHulxioNPJGLebH+a18UI8W1eHgYwVcTYyNF32dTi0fg6HlqPraMmJk2O9lcFPkh3cE7B9mY2DEn6PGxiB8knxW461xT/PQxHBybMzeohW6Yg244oY6NJAk1E0DHRapcNswWU24zabpXo02Z6wValqOlF0EUUnKh1E0Rqi6LUmWo1EbHYSbcoUTWlptKSnE053iUhqSGawSeZoG2Suc4PMTakhyx3GKSLSoYVxaq04tHqZtqqarEAY1zfAYhTRL9leurGpqqxoryts7HM1QIQQG4FpUsqFQoh62p3gCFRtjb1a6X1voDsfUrDQOw6VbbLNWxmcDuAr9wmgb6A4sClY6P0fagA501sZfDlY6L0Z+BPwGKqg8EJgs7cy2G/33ZENGzZ2F3bjALVHvnkPEiBZqMw2B3BIVVnRim6evxmV8j9pX5CQ2ZPwlftihPj9geLAj3rQzifA0cC5geLA873VPxsHFw52XxZDQUmFSyD//RP9+fOudrxCqlDr6d+a/XnQKOJ547iFYVz/AJ6qKiuqt845B7gXFSstUePCX1WVFW1Ncpmewe8RUand6xDmjwDC0sF75mRjkVnwZA2Z1z5pnDoBldUwCxgeO22kWD/vBsezn0/XPt/gEOYdsYyG7sKqCfVnVCbM3YC/qqxor6TaCyHOAeagIgr/C5zXURoHAL8nF1XHqhBYCpyEP7ReCDESWK4L2PTzTJmXKkbgD1XFnXc+Klq2FTgcfyjQi33/FfAL1ALmWuAcKeUXvdX+wQjbj+1mKDLxENTvaAxKQmdA3Nank7MbUCRxZavUF71pHr75zugF2ko5YJTVXl9r64eKCo5BogiCmOY9ZksD0dBmjMZa3APHoKVkAKxrWPROqGHBG7lmS70ww40OM9zslpGWVLCKFAvtGGkanxaUVBSEPn36hpZVC89LHTU1z+Hpl2K2NESNplCL2VjXajSHojknXr5OT8+RgAjNfbZP4zfv58hIWMpoKzLaqkkzKjRnSouWktGce9qPPkkZNiEEtNYv+G968/K5+WakpVGGm5oidRuR4cY8FJk0CDhKSrkSQAjxR+BWkmOqlPJzy/YFVIZOEPgDSoIvah3TpJRm7DlwHCrTL4sdJWmqpJT/tmwdwHKUDFVz3LYFtaAWkFK2yb1amTAGSqZHosb+MRmcsJRyeZztZOuzjCfQ+lv2lVLKG+Js37COuVBR4X2tvgJ8KqU8Js72aeveClDfvzYyzCnEhwtHjzkHi0gau6RypWWbCPMXjyk8ERgLTPrVhg1X5Dj0/pfl5OblORwD9rY0lu3LbNjoIvweB4oEyaBdWjCVdvlBj7VlW1tMsjBGpsT7MSftUor7A+pQ/roOK9vP2lqAhfhDe7026L5IgNQDh0opl3VwgocDb0gp83p8kf0M3SRAxgMBYKu3MrjDoC9Y6F2EKj53ircy+G6w0BsrlF6BIkI+BJZ5K4Pdjpy2YcPG3sduHKD2yDfvyQFqQUnFe6gaINdWlRXd243zUlATLYD8qrKi6s7sDzT4yn0XAE8DnweKA4mKy3e1nQUoGZUzAsWBN3qrfzYOLti+rB0FJRUO4Nf9qT7x586n04q0uWNTRCQNQErWC8HtqOyRprhzclCBLZdbu+pQEbv/6G1ywCjN+ZUuzN+ZUlAWvYgXjONe3UL2tZZMYfx9COAwVMHzi2mXgl6H0tOvRC2mVaJkW7ebPFWVFZl0QEFJhRuV+RJfyPsz4LtVZUVVvXibXYYQ4nRUrb9UVGb1TCnl1zsY+j3DUJI/w7BIEDF723eBO04ervPO/6W/iT80rcM5AngFKEK9T1PwhxrYGfyeTNQi5bv4Q0nrEFoLii+gFpPDwI+llA/vtH0bCWH7sb0MFb07CjX3HW89jqPDQnUHLAW+QtUq2gRs2iqzQitl/2W/iPxgyTI5OFpVViQLSio0FKHrs9r2WdcaiVrkSggpTcyWBoyGWhzZ/Ro1Z0oN7fKrewrVqPWCRdbj10Bg1Z/OHI2qpedk+4Lr2aj7uk5KWQsghJgAjBpy/dPztZT0O4ETgM0of77eelyNqoexEli1K5KM1v9GNoq86IciJvJoXxRMQ/laEzWGb7IeY8XVV6PIk7qqsqKki1UFJRWZKHmXNOuc1cCGqrKiqJXxkmld2xHLrukIIYQbRciNRRFw66WUz8Ydf8xqJ1ZQOgcYaN3Xf6WU34mzDQFZ6Zo2osEwVnb1/dpdsH2ZDRt7EaouSow8yUT56BhREqt1El9rKo0ds+40rBop1uZCkTSZcZuGmi+ErK3BssmnPcMli+1rGaVY/dqZ9t/b+EMHVBH03iJAXgPmSSl/bTnBCagCY3MATUp5Xo8vsp+hmwTIANSgwwSc3sqg2eF4FWqiNdVbGfw8WOidiZqkzUVNjF4GvvRWBqf0+o3YsGFjt2M3DlB75Jv3MAFyC6qS2KtVZUVndeO8kajIs2YgvbOJ0oEIX7nvEFQtqzCQGSgO7NIiqa/ctww1ATwuUBz4uBe7aOMggu3LOoHfk4GSi7gJFcUKauHpTuA+/KG2SNGCkorjUBIsk6xdlcD1VWVFb/ZGV8zS7B9oQj4IMDvyPfMRY/pFVWVF/9nZeQUlFQNQhd+vJk47vhO0AP8G7qwqKwpabWShFutPRkUA343KAvGgPqPvV5UVPdf9u+o5hBBHowhlBzBaSpk4etfvKQDeB4Y1tsrlubfXR1sNCu+ZnsK1R7guwB/a8b30e/qgFmgHAU8Cl8bqwiSF3/M47dI2E/GHlnbSdw8qM3yGtes+4PqYVJaNrsP2Y/so/B4XakG/EPACk4HDUVH8nWE9qp7nx8BHwCL8oWi8gbVgn4eSTSlge4mUHBTZMco6FtMoMVDE7RuozLFlqEWmWIRwjFCJl6VyoRasYoXMM+OOm1abqagsmL7W48AO142HRI2Bv0b9TyyztqVAdccxsUXM3wDMpuvRyRtQpET8tj5u22D1fSoqk/go63lmx4acRMmhnlxRT46oJ4Nm3ERIEa24ieAmgo6BEwMdA5eIRupl6sYlcsjCL80xHzSSOh9FcJ2KqpFyAjvWZzZQ2XBLrfckRtIvQZEjvTJPsDJfMqSUddZrDVVjdhBwlVRFavYqbF9mw4aNpFBSlTmo/5l81P9WxyL16zsLwNlT2BcJkPHAO8B81ITmZVSkRi5wjJSyW3ImBwK6SYC4iVXpgVxvZbC2w/GtqEHZOG9lcHGw0HssagC3HDWAeRx421sZ3OvsnA0bNrqP3ThA7ZFv3sMEyERU0cAmILeqrKhLE4eCkooTgfeApVVlRWN2Xw/3TfjKfRqq0GYWMCFQHNglaRVfuW89KvLt0EBx4Kte7KKNgwi2L+sCVHTz/6EkS2LSUluAK/CH2moYWRJRV6CKrcfIhiuryor+1ZXLFJRUjEJlkqwDnqgqKwoBmKXZM4AXNSHFvdEZ8o7oRRd2hfzo0HYqaqHLS/tiZCHq83CTOKKsAiXrNRtF7NQDs6rKit4pKKkYDjyFWjQDRRDcA8zd06S2ECINGCOl/Mp6raEkpu6RUobaDBUJ8l7F0kjBWU81k+GCymszqgdmagPxh3asIaLOORZFnOjAD/GHHkjaEb/n/4DyuD0fAyfE15tJ0HcN+CXqPY6gfhtf7vSmbWwH24/tZ/B78lFZauNR0f4xCawBKN/UcYG8CZiHCiScixp7VnUkRRKhoKTChfLb/YGFVWVFdb10Fzu7bioqQyGWteJDLTr37+S0GhQxEivMvRW1QD8xjRbudN6/8RRtnstEi7biNFtwyiaZQg2Z2kaZ69og8zK2SI+zjgzC0kkrTsLWtkVms1r2palNYSox8qnbdqb+v6bv6HNd40VVZpoI73JB3bB0slgOY4E5kvnmKL4wx7CRPFDrIetRJNUQ2mtsJUKjQC49Wlu0+WRtQbRK9pv3nHH8e02kLAfWJ8pY3J9h+zIbNmwcCNjnCBCrUx7gWpSERgbKId4rpdzQKxfYz9DdDylY6G1ApT6N8lYGl3c41oxi4Qq8lcFVwUJvISqSoQ5VTO3vwHPeyuBBl2ljw8aBgN05EOyJb97DBIhALdQNAE6tKit6p4vnfQ8V9fpuVVnRKbuxi/ssfOW+91FRcJcFigPlOzFP1kYIRaKMCRQHkkYZ27DRGWxf1g2o1PjvohasR1l77wFujq+rUVBSkQ3cgcoekcDFVWVFTydr1srS+I1lH1sIagKe+oPjoc8u0N+/3ykMxzPREyiJXnnZt2Vn7ZLP6OT6wrquG/VZ3YiqIxJPimwGpleVFc2PO88J/BYoibNbjCpQ/nhVWdGW3uxnVyGEuAR4AvgWuHA7QkHJYb3/2MLWguHZGscNc/wZf+jnnTbo98Tq+IWBI/GHFiSwGY36fqcD/0ARZhnA9fhDyYozx/f5LMAppbTrOe0CbD92AMHvSQOmAMeialocRWKpqyhK8imWPbEC9Zv/FliJP7TXo/kToaCkoi+KCJkAjEb9l4yiE3muMWJ16GnX7xqzRePAnl6/WmbKKtlfbJB5GIhtKUTWZ9K0eoCoWT1UbCrUhTyaHTNXTJSc11ZgG+01Q1qsLQJEW6UuQ2SkGWiFHhoLU0VrasfrR6S+zimMD1AESJYpRVYLzvwW3J51Mr85YA4358tRaQvMkf36ibohp2rztNP0eQwW7aW1amUGH5k+PjQnRBeZBevqZdrqGjK/bSalCiWp9S0WybK/ESS2L7Nhw8aBgH2KALHS/34BPCylXNujxg4g7AIBsgoYiiVzFbffgRoIAOR7K4PVwUJvH9TkEaAUFen1sLcy+P3evAcbNmzsGeyOgWBv+Oa9UDj4YVTE8p1VZUU/6+I5t6IipMuryoou241Gz8MnAAAgAElEQVTd22fhK/fdBfwU+FugOHD9LpwvUP8zOjA4UBxY18tdtHGQwPZluwCVEVKGkiUBpe1+Ef5Qm165RSr8AyU9FQXOriorqohvxqofchPKF8SkTd5GkcrjBovNvOz6FbmigbeNyVwb+cl1lWXn/H133lpc30ZZ/bocJU0yvaqsaHkS26koma3zUVIwoCRNVqIWoWLba1VlRct2c9cRQhyJkvUYhvKTt6ACjzKllDUWCfIuSqbGhz+U8L7a4PdoqGjZItR9HIY/tC3uuBslqzMZlS1yKnAV6vNvAibgD3Urs14IMQaok1Ju6s55BytsP3YAQ/3+xqCyzaaiamd4afc1iSBR9SgqUTJKS6zXw2mvTzIWNYaK+adl1uMqy3Yt/lDzDi239yueqBmDyuyI1c7IRREyc+O2r/GHdpS3U/c3qVm6Zm6RnkPeNg+re8Q4Q18j+44BRv5If2nlzY6nDxeCNBRpcBVKUipW8DfNul6fuC2H7SVRUlEyT12t8ToPJXtYgSIU6jrLZEsIVUdpBOrzOhI4BpVJmKwmTKeISC1SJQdsHSS29kkT4YQZIy3SST1pGHH8jYaMNkl307dywJrlctDiz83C/71nTppnoK8HNgIN+5IUr+3LbNiwcSBgnyJArA41AOOllFU9buwAwS4QIPNRk53veCuDr8ft96AyPQBSvJXBcAdS5FHgMuBub2Xwxt67Axs2bOwp7MYU5R755r1AgMQKen9TVVY0vovn/AP4EfD7qrKiX+/O/u2r8JX7LkVJIX4cKA4ctwvnx8sw5gSKA3tE0sHGgQfbl/UAfs901JiuL+r3eCNwf6xOhFW89zHgElT2wHRUMe6TUVJZs6BNj2QucEtVWdEHBSUVYrRYc+rDrjueGSy2ZgfMAopbS345v+ziP+7Bu8O6hzQgWlVWlFgiantbD6ro+vdROv8d0QwcWlVWVNm7vdwRQogc4F/AOdautajv06lSyo34PalANv5Q17Le/Z48VLTsUFR08b+Bp/CHgvg9MUK7GlX3Y521qPkOcCKKFDmlqwuIQoihwCeo9+tm1OJlNmpRMwz/z96dh8lR1Wsc//5mkpAFMgkECBihQwghwoiSqyiIwmXVBheComyRLYAsKlyxRZFWBFpQUQFBESGRTQFlsXOBRMArBFlGNAMmrLYsIRCydAKBbHPuH6c66Uxm6+nq6aqa9/M8/dTUfprlnZ7+1TmHa0tj2IunHOtn/P9f27JhD4od8PON7IB/uj0Mb+KLDkvww1Mtxv8/uDt+CK9Khodagy+wlOa4eAlfHDiYjofF+ic+t9LB+izgKLLFNzo4tmeyTSPwRaBx+IJIaWLzofjfRc8Cd5AtvtTre3R9/03xRay98L16lpW91rB+vpid8f9ulwN3A3fiJ/ZdEfTE3GONa/jkWhoPbWTt2Ebahpl1OzHwOm+64TzbNoYX3TYU3OhVL7mtFv3bbTN/oRtxzD9yR8wN8y1XSlkmIkkQxQLIncAfnHOhdqOPs14UQGYB+wHHTJw394ay7e/Bf2BZAwyaOG+uC7Yvxv/x8n/Ax4HzJ86b+/3Q34iI1FwNP6BWlc11KIBsjh8LvwHYrpBLv9yDc+4CDgVOKeTSv6xxEyOpeVrz+4CngbeB4a1TWit6sq55WvNI/B/iAIN6O5G6iLKsStmmrfHzPhwUbPkTcELpS6pgmKhbgc8Ab+H/v92u7Aqt+J7Bd6x7CtV/uXcrcFibszeya6Yc9f0LfzarD95NaFKZ/HvwX0ruGLw+hR8D/zFgr0Iu3e3Y/dUyM8MX23+Cfwp6DZB2zvVuYvps0x74iZNHlG39F/5JcoBDy+eEIdu0A/7f71DgK2SLV/Ww3Tviv+zcvpNDznfO6e+HMsoxWcf3PNgSnz8743tn7IwfYqqA/+xVeq1mfUaV8qo0L8WwHtxtPn6OzyfxE4u/ju9VUMT3MinvtTKik2uA/yw4C//9wSfw84aUOOAC4Ptki2t70KZk8L8HXemBgh4cuxn+n3ET0LjMDR3wYNtu2zzbNua9w+3tXXe0+R/Y3l4ft529MWqgre2wWPK/az80+ZMXzKrrMITKMhFJgjAzo6tJoirxv0DOzJrx3RzfLt/pnLsrpPsk2aJg2b47aempk7dKxY+y40fin24A/x+EiEi5WGVzIZdenMrkH8WP0XwQ/onb7pTGOe62WJJgz+Cf7h2G/6P7mQrPL/1hvlrFD4moWGVZr2WLr5Nt+hRwJn6eiEOAVrJNx5Etzijk0qtTmfwX8U+x7g9sOoLlxTMG3NH6xcb7txrKyi3N2BN4HIqloSnOw/dcWNVg7nPfv/Bns+vwzqpSyKVfxc8R9SBAKpO/AngK/0XgN4CLa90G558Y+4WZzQ7ueUOvix8A2eKjZJu2Az6N7+lyEOuLHz/foPjhj3+RbNO3gJ8Bl5BtyvfkyWrn3PNmthd+CK0x+F7lS/EFtOH44dekb/SPHEsS/4X5G8Hr4R6csfET/76IMgJfrN4KP8TUyGC5Gb548hDwny6+oC/gh5AqfUE/hvXFmJ3xPTHmAjOAhzaYs8QX1vfBDxl1L9nigz14H8lSyZBb/tgiZd+tDMcH9cbHNg3Gz4Gx8ztu0K6rGLDrANbuNJhV7x3Kyp789xJXyjIRiaWweoB09UvFOed6NT5jnPWiB0hpGJfvT5w39/yy7bvjf7G8MnHe3PeWbf8b/imQkuMmzpt7fUjNF5E+VMMndKrK5no8oZPK5M/DT4T7h0IuPbkHx7+JLxy/v5BLt9a6fVHVPK15Nr5wdEzrlNYbuju+3bkT8EMoFFuntHb1VKFIl5RlIco2vR8/NFLp6d2b8V9wLVnihq24Ye0BB3yq4dHtd7DXJpkxqN3Za4Ljn8T3WADfk+Q3fdH0vpDK5I/F95ZZDUyKff77YbEm478UvazDSZf9F59/wc8T8AeyxW5/R/aUmQ3EzyHzc+fcirCuG0fKMRGpSrapMQo9bJRlIpIEkesB4pxr6P4o6UZ3PUDebrf9zXbrGr9XRDYQ02y+B18A2T8Y7mUw/sGrw/FP4J1ayKXbAFKZ/BDWZ2avJuJLkL/gCyCHARUVQFjfA6T97xmRSIhpllUnW5xDtulD+Cf0v4rvJQDASHubMwbcUX70P4Fr8T3hvop/2veY4AXwsyQVPwK/xRcMPg1MS2XyexRy6fj2YMsWFwG/6uaYNrJNpwL/AA4j23Qg2WLve6Fs6EJ8z5YpZnaSc+6hkK4rgX6ZYyL9UQSKH7WkLBORuFJ4RUdp/PVOh8Bqt31Ru3UNgSUiSdCCL/AOB+7Fj4F8A/BZYCr+i72SMcHybVQEvjlYppunNVfai0MFEJEoyhbfJVv8Gn4c90vwwwLeDtwPPA5chZ8g/INki5eTLd5Btrgv8CHgFmAtftiU/6lH82spmOPkZPzn5w8C365vi/pItvgUcHmwdjnZpk1CuvIM/HwDOwN/NbNfBpO/i4iIiIjEXigFEDP7uZmd2cH2083sp2Hcox/odg6Qdtvb9wBRAURENhDHbA56d5SeaN0XGAI8hy+MABxbdvi6+T/WTfjbf7Xix5EehH8quhIqgEikxTHLQpUt/h/Z4jfJFk8iWzycbHE/ssUPky1+hWyxZaNx47PFJ8gWv4SfwPXTZIs1nyS8Hgq59ALgK8Hqd1KZ/Cfq2Z4+lMUXK3YCvh7GBZ1zD+InWi7NvTUVmGtmRwQTwEuV+n2OiUgiKMtEJK7C6gEymY4nBpuNH7ZEulcqgGzebrt6gIhIb8U1m3P4J5x/jH+6eQJ+UmCAw1OZfOkL+1IPkP4+/BWtU1odfr4AgCM7OqZ5WvOuzdOar2ye1rxlu10qgEjUxTXL6itbfLuiyV9jqJBL/w64FWgEZqUy+Uwqk092D/dssYgfrgrgvGAy9ao55xY7504CPo6fF2prfE+i88K4vijHRCQRlGUiEkth/YGwBR1/Ab8MGBXSPZKusyGwSl9MdVcA6e/Dv4jIxmKZzYVcurWQS+9XyKX/p5BLtwS9Ox4BXsBn4ueCQ9f1AKlHOyOoNAzWvs3Tmt9TvqN5WvMA/JeEXwGOb3eeCiASdbHMMukzJwC/w89teDFwXyqT36a+Taq5G4G/AkPxDwuExjn3V+AD+Pm4FuB/d0j1lGMikgTKMhGJpbAKIM8DB3ew/ZPAiyHdI+mqnQR9WegtEpG4S0w2B0WQ6cHqlGBZ6gGiAgjQOqW1gH8iy4Avttt9HH5sd4Dt2+0bGixVAJGoSkyWSfgKufRy/CTxJwArgP2AOalM/pN1bVgt+WHPTsfP83I42ab9w7y8c26lc+58YGfn3Nwwr92PKcdEJAmUZSISS2EVQH4CXGJm3zOzTwSv7+OHMbkspHskXakAstncnScOKtvekyGwVkycN3d1zVomInGVtGy+IVjul8rkx7C+B0i/HwKrzEbDYDVPax4KfK/smDFsSD1AJOqSlmUSskIu7Qq59G+AScA/8U+h3p3K5EMtDERKtjgHuDJYu5ps02Zh38I5t+4pXzPbz8wuM7NkDzFWO8oxEUkCZZmIxFIoH2Cdc78BzsY/efVA8DoaONU5d00Y9+gHlgKlSSxHlm3vySToGv5KRDaStGwu5NIv4of8MOAoNARWR24F1gC7N09rLvX4+BqwDVCaC6CzAsiK2jdPpHJJyzKpnUIuPQ/4CH7uikbg1lQmP76+raqp7wIvAeOAq8g21WTCcjPbErgD//vk57W4R9Ipx0QkCZRlIhJXoT3B45y7yjk3Bj9h3nDn3A7OuendnSfexHlz17K+kFE+DFZPeoBoAnQR6VACs3lasDwWFUA20jql9U3g3mD1yOZpzaOAbwbrPwqW6gEisZPALJMaKeTS7+KH/XsEGIHvCTKivq2qET8h+pfwQ2Edhf/dGDrn3EL8HFJtwGlmtl8t7pN0yjERSQJlmYjEUehdmJ1zC51z7b+sl57paB6QnkyCrgKIiHQpQdl8G/Au8D7W95bTEFgbuilYHgV8BxgOPAlcGmzfsnla8+Cy41UAkdhIUJZJDQVFkMPwvx8mADenMvnG+raqRrLF2fieIAC/INs0oRa3cc79lvVDbl1uZoO6Ol46pxwTkSRQlolInIRSADGzrc3st2Y238zWmNna8lcY9+gnOiqAdNgDZOK8uauA5cGqhsASkY0kMZsLuXQRPwxHyfJgm6x3J344qx2AM4Nt38T/jnknWN+27HgVQCTSkphlUnuFXHoB8Gl8Hh4MXFLfFtXUD4H7gaHALWSbBndzfG99F1gITGT97xfpAeWYiCSBskxE4mpASNe5HtgOuAB4jfVzWUhluiqAdPTF1CJgM9QDREQ6dj3JzObpwBeDn9X7o53WKa1vN09rvgM/EboBs1qntM4EaJ7W/AowHj8M1ovBKSqASNRdTzKzTGqskEs/mcrkp+DnRzorlcmPBC4v5NJP1rlp4coW15JtOgb4B/ABfLEn9AKFc26pmX0T+A1wvpnd5JybH/Z9Eup6lGMiEn/XoywTkRgKqwDyMWBv59w/Qrpef7U4WG5etq2zOUDAT4SeQgUQEelYUrN5JrAAGI3m/+jMjfgCCKyfAwR8wahUAClRAUSiLqlZJn2gkEvflsrkvwt8Hz83yHGpTP5R4BfA74PhsuIvW5xPtmkKMAM4g2zTH8kWH6jBnaYBU/GTzR9NL3rWmNn2gDnnCuE2LdKUYyKSBMoyEYmlsOYAeRn/lKlUp8dDYLU7XgUQEelIIrO5kEuvAW4IVp+tZ1si7F7gJ8CZrVNa/162vdRjprwAMjRYqgAiUZXILJO+U8ilLwA+DtwCrAb2wH+R/49UJr9ZPdtWksrkD0tl8nemMvkfpzL5L6cy+d1TmXxlQ1lli/8L/DJYOyv0RgLOuTbgVOBzrJ9bqsfMbDDQAswzs4+G3LwoU46JSBIoy0QklsIqgHwNyJlZKqTr9VeVTIIOfgxeWN9zRESkXJKz+Xz88B4X1rshUdQ6pXVt65TWs1untF7eblepAPLesm3qASJRl+Qskz5SyKX/Wsilv4TPv2/je1JPAL5cz3aVyeHnLDkLuA5fJHg7lcmfX+F1fhIs02Sbdgixfes45/7hnLvDOdeboU9WA78HNgHuMrOatDGClGMikgTKMhGJpbCGwPod/gnSF8xsBf6D7TrOuc07PEvaq7QHyBXAIPzTbCIi7SU2mwu59Aqg/Zf70r2OeoCUCiAr+rgtIj2V2CyTvlfIpV8HLkpl8kuBK4EzUpn8lYVcuq1ebUpl8sOAHYPVX+ILM+/HD4t7PPC9Hl8sW3yWbNM9+InfTwPODrWx7ZjZe4HPOOeu6Mnxzrm1ZnYOvhfO7sAMM9vTOZf0B7qUYyKSBMoyEYmlsAogXwvpOv3dBnOAzN15otHFJOgT5819FDiib5omIjGkbJb2uiqAqAeIRJWyTGphOnARfl6kg4D/rWNbJuKHFFlYyKVPAUhl8lsCbwDbpTL5oUHhv6cuxxdATiDbdD7ZYkcPUlXNzIYDDwHbmVmDc+7nPTnPOfeWmR0CPIov9vzRzA50zq2sRTsjQjkmIkmgLBORWAqlAOKcmxbGdWSjHiCDWP/vqCZ/uIhIcimbpQMqgEjsKMukFgq59FupTP5a/JBTX6W+BZDmYNlatu1NYAkwEt87ZE4F17sHeD4472jg6hDauBHn3DIzuwa4APiZmS1yzt3Y1TlmdgH+d9HvgE8BD+PnZ/m1mR3by2G1Ik85JiJJoCwTkbjq9RwgwRM/637u6hVOU/uF9gWQTcv26YspEemWslm6USqAbN08rXlQ8LMKIBI5yjLpI1cADjgolcnvXMd2lAogT5U2FHJpBzwTrE6o6GrZYht+eC+A08k21XLC2guBUs+P683sk50daGabAufiCzJDnHNPAYcDa4B9gNE1bGefU46JSBIoy0QkCaqZBH2JmW0V/LwU/4RS+1dpu/TMugJIMPxV6UupdyfOm7umTm0SkXhRNktX3gRW4Yda2aZ5WnMDMCTYpwKIRImyTGqukEv/G7grWD09rOumMvntUpn8wApO2TVYtrbb3rsCiHcdPtd3Afbtxfk9EvTY+DpwI77n+u1mtmcnh38Q//fnq86514LzZ+KH9N2jtC1BlGMikgTKMhGJvWqGwPpv1s9ZUbMP1f1M6Z/nIPzEUl1NgC4i0hFls3SqdUprW/O05leBsfhhsBaV7VYBRKJEWSZ95efAZ4AvpzL57xRy6aXVXCyVye+JH9ZpOjClh6d1NAQWwLPBsvICSLZYJNs0HTgVOAO4329v2iRo1x7A98gWX6r42u0459rM7Dj8PIafxM/p8V7n3Kp2h34oWD7R7vw/VNuGiFKOiUgSKMtEJPZ6XQBxzv2lo5+lKm8Bq4GB+GGwVAARkYoom6UHXmF9AeS5su3v1Kc5IhtTlkkfegA/9NSuwHHAZVVe77+D5TGpTP77hVz6ha4OTmXyo1g/9NO/2u2upgcI+CG+TgU+TbZpZ+AA4BzWzwO1F9mmvcgWF3V2gZ5yzq02s8OBfwNFYDv8PCTl/itYPl7t/eJAOSYiSaAsE5EkqGYILAnZxHlzHRvOA1IqgOipXBERCcvLwXIM64daXNE6pbWtTu0REambYK6N0hwWZ6Qy+cYqL7lTsDT85OrdKQ1/9e9CLr283b51BZBUJl/5PB7Z4r+AP+P/5nsK/z7HAPOD1wTgT2SbhnV6jQo451YAuzrndnLOtS9+wPoCyBMd7BMRERERqQkVQKKnowKIeoCIiEhYShOhb1AAqVNbRESi4Eb88B5jgXSV1yrvrXF8KpMf0c3xnQ1/Bb4HhQOGA1t1sL8nLg+WjcB/8D1CdsD3BlkCfAT4HdmmaoZGXsc5t7Cj7WY2AhgfrLaEcS8RERERkZ6IdQHEzDY3sxvNbJmZLTWza81s027OGWxmV5rZIjN7y8xuN7Oty/bvZmY3m9nLZvaOmc01s548vRWW0tiKm6MCiIiIhK+jAoh6GopIv1XIpVcA1wSr5/SqtwUQnFcqgCzGZ+xJ3ZxW6gHyVAftehcoBKu9HQbrLuAU4BhgPNni1WSLK4PeIYcA7+KLPr8k29Sr990RM9vEzDYr27Sup4tz7s2w7iMiIiIi0p1YF0DwT2vtgn+C6RDg48CvujnnMuBQ4PPAJ4BtgfKJ9yYBbwBHB9e+ELjYzE4PteWdK+8BUvpiSgUQEREJS6kA8l5UABERKfk5sBLYC9inl9fYCmjC99o4L9h2ZiqTH9jFOV31AIFq5wHJFh3Z4i/JFm8gW1zdbt9s4AigDTge+CPZpm+QbTqUbNP43vYKMbNvAm8C6/5+cs49hH/A67O9eh8iIiIiIr0UWgHEzAaY2f5mdnLpaR8z27a7HhlV3G8icDBwonPu0eBD9RnAF81s207OaQJOAM5yzt3vnGvBT3a4p5l9BMA59xvn3Fedc39xzr3onLsBuA44rBbvowMaAktEQtPX2SyxoB4gEjvKMqm1Qi49H/h1sHpeV8d2oTT/x3+Aa4HX8Vl7eEcHBz1GOu0BEng2WPa2B0jXssW7gJODtc8Al+B7jTwLrCLb9DbZpjfJNr1MtulZsk3Xkm0a2c1Vl+L/jjm0fKNzbolzbk7I7yA2lGMikgTKMhGJo1AKIGa2Pf6ppTuBK4Etg13fBH4Uxj068FFgqXOufBK9WfgnmPbo5JxJwMDgOACcc/OAl4LrdaaJ9UNT1ZomQReRUNQpmyX6SgWQbfDjyoN+z0iEKcukD/0QWA3sm8rkP9aL80tFimcKufRK/H+vAGd1MqzWdsBmwT2f6WA/VNsDpCeyxV/je718D/gd8A/83FAGDMX/XTIGP4fH8cAcsk2f6OKKfwqWHzGz3s5dkijKMRFJAmWZiMRVWD1AfgY8AYwE3inb/kdgv5Du0d5o/FBV6zjn1uALFaO7OGeVc25pu+2vd3aOme2J7xre5dBawTi3w0sv/B8zvaE5QEQkLBVlc4g5JtH2BrAG/xlgx2CbCiASZcoy6ROFXPplfM9v6F0vkHUFkGB5NX6Ojf8COiqolIa/mlfIpVd3sL/8Wjt1sj8c2eJfyBazZItfJFv8IP7/m9H4CdN3wb+HQ4Hn8MWQB8g2XUS2aaPhvZxzrwJ/xxdQ0ma2pZk9YGY5MwttnpGYUY6JSBIoy0Qklno1rmsH9gb2dM6taveZtgC8p5ILmVkOXz3uysSKWtdLZrYrvrL9Pefcfd0c/i3g/BBuW94DpFT4UAFERHqj0mwOK8ckwlqntK5tntY8H//kcenLOhVAJMqUZdKXLsb3cjgwlcnvUcilH63g3FKmPgtQyKUXpjL56cBU4Czgr+2OLxVAOhv+CtYXQHZIZfIDuyiUhCtbbMM/JNZue9ODwE/xwwp/CziYbNNsfKGn9PoLcDewO75osgDfw2S0cy5T+8ZHknJMRJJAWSYisRRWD5AGoLGD7WOA5RVe68f4AkdXrxfxH6Q36FJtZgPwPScWdHLtBcAgMxvRbvvW7c8xs/cBfwZ+5Zz7QQ/afTF+qKzSa0wPzumIJkEXkbBUms1h5ZhEX2kYrNITxSqASJQpy6TPFHLpAvDbYLXSXiDte4CALxYAfCaVyY9vd3xp/o/OJkAHeBWf0QPwvTHqK1t8i2zxRPy8JkuADwKnAWcD3wYuAO5r3qrh/uCMA1nf++UJ+i/lmIgkgbJMRGIprALIfcDXytZdMAHS94AZlVzIObfQOTevm9cq4BFghJlNKjv9v/HvqbMntVrwY+yu65pnZhPwT8E+UrZtF+ABYJpz7ts9bPdK59yy0ovKCz8lmgRdRMJSUTaHmGMSfaUCSOnLuhX1aohIDyjLpK9dhJ9XMJ3K5HfvyQmpTH4g6wsU6woghVx6Ln5ODMM/CVuu1AOk0wJIIZd21Hoi9N7IFm/Ht/9/8EWPS4HL8cMsDv77ycNGAvPxD3R9Izjr8Tq0NCqUYyKSBMoyEYmlsAogZwN7mdm/gMHATazvAtfdcFa94pybC9wDXGNmHzazvYArgFucc/MBzOw9ZjbPzD4cnFMErgV+Ymb7BsWT64BHnHN/C87ZFV/8uC84bnTw2nKjRtSG5gARkbD0eTZLbJQKIKXfM+oBIlGmLJM+Vciln8f/dwY97wUyFt9LYwW+10a5Um/yY1KZfArWFUx2DrZ3NQQWRLEAApAtvkq2+GOyxe+SLZ5DtngmcCvAgAY7CLgM+A5QmiekP/cAUY6JSBIoy0QklkIpgDjnXgF2Ay7Ef9B9EsgAH3TOvdHVuVU6CpiHH6pqBvAQfozdkoH4PxSGlm37Ov4prNuB/8MPfXVY2f7DgS2Bo4HXyl599cRSqQfISGB48LO+mBKRitUxmyX6Xmm3rt8zElnKMqmTCwEHfDaVyR/fg+NLxYnnCrl0W/mOYB6RmfgCSWkOjJ3wf6ssB/7TzbX7ZiL0cNwbLA9yzv0IuD5YXwv8oy4tigDlmIgkgbJMROKq6knQzWwg8EvgAufcjcCNVbeqh5xzi4Eju9hfwHc3L9/2Ln6c2tM6OScLZMNqYy+UCiANrJ9ESj1ARKQi9cxmiQUVQCQWlGVSL4Vcel4qk78UOAf4dSqTbyvk0td3cUpH83+UuwA4ADgulcn/gLIJ0INhrrpSuma0eoB07AH8kMM7kG3akfXznDztnOuXwy0qx0QkCZRlIhJnVfcAcc6tBiaH0BYBJs6bu4r1BQ8VQESkV5TN0g0VQCQWlGVSZxn8ELsG/CaVyR/bxbGl3hkdFkAKufRfgb8Ag/BFlVJhoLvhr8qvGf0CSLb4FvBwsHYQ/oG7/9D1RO+JphwTkSRQlolInIU1B8gdwGdDupasnwekMViqACIivaFsls683G5dBRCJMmWZ1EXQM+NM4Bf4Isj1qUz+6E4O764HCPheIAAnAfsHP/ekMFCaA2SrVCY/ogfH11tpGKyDgReB54Ef1q85kaAcE5EkUJaJSCxVPQRW4Dngu3SelPYAACAASURBVMFE5C20+yLFOffzkO7TXywCtitbVwFERHpD2SydWQC0sf5BCBVAJMqUZVI3hVzapTL5M/B5eQowLZXJv13Ipf/Y7tCeFEDuBx4BPgrsEWzrtgBSyKWXpzL5+cC2wX0ereAt1MM9wMXAvu784ZPJFvfv7oR+QDkmIkmgLBORWAqrAHICsBSYFLzKOUAhWJlF7db1xZSI9IayWTrUOqV1TfO05tdYP9Sifs9IlCnLpK4KuXRbKpM/Df+304nAD1KZ/B2luTtSmXwTsHVw+LOdXKZUTLkAmFG2uSdDYIEvrGyLH2or6gWQOcDr+H8me+HnBenvlGMikgTKMhGJpVAKIM65sWFcR9ZpXwBRDxARqZiyWbrxCiqASAwoyyQKgiLIN4BjgPcBHwT+Huwuzf+xoJBLL+vmUvcATwD/FRz/Zg+b8CywL/GYB6SNbNN9+H9WB9G+AJJt2hRoI1vsN5OiK8dEJAmUZSISV2HNAbKOBcK+bj+zuOxnB7xTr4aISDIom6UD5ROh95svoSTelGVST4VceilwZ7B6TNmungx/VbqGA87Df8b/cwW3j89E6F75PCDrZZveA8wD5pJtGtzXjYoC5ZiIJIGyTETiJLQCiJkda2at+C/r3zGzOWZ2THfnSYfKe4C8PXHe3La6tUREYk3ZLF0oL4CoB4hEmrJMIuS3wfLIVCZf6k3f4wIIQCGXvgcYD0yt4L5xK4Dchy/y7Ea2aTQA2aaBwC343ofbAR+rW+vqQDkmIkmgLBOROAqlAGJmZwFX4cez/ULwuge42sy+HsY9+pnyAoiGvxKRXlE2SzdUAJFYUJZJxNwLLAS2Ag4MtpWKEp3O/9FeIZd+oZBLV9L7rlQAGZ/K5EPvxR+6bHEh64cIK/1zupANix4H9Wmb6kg5JiJJoCwTkbgKaxL0M4BTnXPTy7bdZWZPA1ngspDu01+oACIiYVA2S1dUAJG4UJZJZBRy6dWpTP5m4Ez8MFgzWD8HSI96gPT21sAqYDDwXuA/NbxXWO7FT5J7ENmmpcA3gu03A1/CD4/1jU7OTRrlmIgkgbJMRGIprKeHtgFmd7B9drBPKlM+B4i+lBKR3lI2S1dUAJG4UJZJ1JSGwfpsKpMfQR8UQAq59Frg+WD1E7W6T8hK84B8EpgW/PxT/BdobcCuZJvG1KNhdaAcE5EkUJaJSCyFVQB5Ht/1rb0jgOdCukd/oh4gIhIGZbN0pRAs326d0rqqng0R6YayTKKmBZiL743xVWAIsBr4d43v+8dgeUUqk9+5xvcKwyPAcmAkMAL4G/BNssVFwOPBMQd2cm7SKMdEJAmUZSISS2ENgXU+8Dsz+zjwcLBtL2A/Og5H6ZoKICISBmWzdKp1SusrzdOaTwXerHdbRLqhLJNIKeTSLpXJ/xa4CDgr2PxCIZdeU+Nbfw8/h8YngDtTmfyHC7l0scb37L1scTXZpj8Dn8X3cD+CbLFUcL8H2AM/DNZv6tTCvqQcE5EkUJaJSCyF0gPEOXc7/gPsm/gPuJ8Nfv6wc+6PXZ0rHSofAksFEBHpFWWzdKd1SuvVrVNab6t3O0S6oiyTiLoRcMDwYL3HE6D3ViGXXo3/gull/LBbN8RgQvQf4YdGmUy2+FLZ9nuC5QFkm8J6KC+ylGMikgTKMhGJq9A+bDrnWoCjw7peP7cU/weVoQKIiFRB2SwiSaAsk6gp5NIvpTL5B4F9g021nAC9/L5vpDL5zwEPAYfgJ539bl/cu1eyxYfxTwe39ziwBD881ofpeEz5RFGOiUgSKMtEJI5CeWLIzD5lZgd1sP0gM/tkGPfoTybOm7sW/wcBaGJaEeklZbOIJIGyTCLst2U/90kBBKCQS7cAU4PV84KCSLxki2uBmcHaRv9/J41yTESSQFkmInEVVpfpHNDYwXYL9knlSvOAqAeIiPSWsllEkkBZJlF1O/BO8PPcvrxxIZf+LfDTYPXXqUx+VF/ePySlYbAOrmsr+oZyTESSQFkmIrEUVgFkPPCvDrbPA3YM6R79TWkeEBVARKS3lM0ikgTKMomkQi69DJgC/AB4pA5N+AYwB9gcuLgO96/WvcHyQ2Sb4ljAqYRyTESSQFkmIrEUVgGkCOzQwfYd0RBOvaUeICJSLWWziCSBskwiq5BL31rIpc8r5NKuDvdeA3wlWD0xlcl/pK/bUJVscT6+gGPA/nVuTa0px0QkCZRlIhJLYRVA7gR+ambjShvMbEfgx8BdId2jv5mB/+XycL0bIiKxpWwWkSRQlol0opBLPwxcH6z+IpXJdzQ0SZSVeoEkfRgs5ZiIJIGyTERiKawCyDn4au88M/u3mf0bPw7uIuB/QrpHvzJx3twrgc0nzpv7WL3bIiKxpWwWkSRQlol07ZvAUuCDwKl1bkulSvOAHES2yeraktpSjolIEijLRCSWzLlwemubmQEHALvhJwOc45z7v1AuHkNmNhzfg6PJObes3u0RkeiqZV5Uk83KMRGphLJMpH5SmfypwC/w/61PKOTSr9e5ST2TbdoE/8XZMOADZIv/rGdzlGMikgTKMhFJgjAzY0A4TQLnKyn3BS/MbERY1xYRkd5RNotIEijLRLr1K+AEYBJwCX5y9ujLFleSbXoAOAQ/DFZdCyC1pBwTkSRQlolIHIUyBJaZfdPMjihb/z2wyMxeNbPdwriHiIhURtksIkmgLBPpXiGXXoufEN0Bx6Yy+c/UuUmVKA2DtVNdW1FDyjERSQJlmYjEVVhzgJwCvAxgZgfgu8N9Evhf4NKQ7iEiIpVRNotIEijLRHqgkEs/BlwZrP4+lcnvX8/2VOAmYAzZ4gn1bkgNKcdEJAmUZSISS2ENgTWaIATx3Zd/75y7z8wKwKMh3UNERCqjbBaRJFCWifTc14H3AJ8D7kxl8gcVcumH6tymrmWLS4Al9W5GjSnHRCQJlGUiEkth9QBZArw3+PlgYFbwswGNId1DREQqo2wWkSRQlon0UCGXXgN8CT+s1FBgRiqT/1B9WyUox0QkGZRlIhJLYfUA+QNwk5k9B2yB7/4G8EHg+ZDuISIilVE2i0gSKMtEKlDIpVemMvnDgBnAPsC9qUz+dGA50Ba8lgJ/K+TSrm4N7V+UYyKSBMoyEYmlsAogXwcK+ErwOc65t4Lt2wC/COkeIiJSGWWziCSBskykQoVc+p1UJv9p4D7gI8CNHRx2InBtnzas/1KOiUgSKMtEJJbMOT30UwtmNhwoAk3OuWX1bo+IRFdU8yKq7RKRaIpqZkS1XSJ9IZXJjwAuA3bGD39swHBgAjAfGF/IpVfUr4XREtW8iGq7RCSaopoZUW2XiERTmJkRVg8QERERERERiZBCLr0UOK58WyqT3wR4BtgeOB24pA5NExERERHpE2FNgi4iIiIiIiIRV8ilVwLfDVYzQS8REREREZFEUgFERERERESkf7kReBoYCXyjzm0REREREakZFUBERERERET6kUIuvRb4drD6tVQmP7qe7RERERERqZWqCiBmNtLMzggmJWm/r6mzfSIiUjvKZhFJAmWZSM3dBfwNGAp8p85tSSTlmIgkgbJMROKu2h4gpwMf72gmdudcEdgbOKPKe4iISGWUzSKSBMoykRoq5NIO+FawenIqk9+hnu1JKOWYiCSBskxEYq3aAshk4Oou9v8SOLzKe3TKzDY3sxvNbJmZLTWza81s027OGWxmV5rZIjN7y8xuN7OtOzl2CzN7xcycmWlyQBGJi7pms4hISJRlIjVWyKUfBO4DBgDfr29rEkk5JiJJoCwTkVirtgAyDniui/3PBcfUyo3ALsABwCHAx4FfdXPOZcChwOeBTwDbAn/o5NhrgTmhtFREpO/UO5tFRMKgLBPpG+cGyy+pF0jolGMikgTKMhGJtWoLIGvxBYTObAu0VXmPDpnZROBg4ETn3KPOuYfwXe6+aGYdtsnMmoATgLOcc/c751qA44A9zewj7Y49FRgB/KgW7RcRqaG6ZbOISIiUZSJ9oJBLtwD34P82PKvOzUka5ZiIJIGyTERirdoCyJPAZ7vY/7ngmFr4KLDUOfdE2bZZ+NDdo5NzJgEDg+MAcM7NA14KrgeAmb0P+C5wLD0McTPbxMyGl17AZhW8FxGRMPUqm5VjIhIxyjKRvnNpsDw+lcmPqmtLkkU5JiJJoCwTkVirtgByBXC2mZ1uZo2ljWbWaGZnAF8HrqzyHp0ZDbxRvsE5twZYHOzr7JxVzrml7ba/XjrHzDYBbga+4Zx7qYL2fAsolr1eqeBcEZEw9TablWMiEiXKMpG+8wDwd2AIcFqd25IkyjERSQJlmYjEWlUFEOfc7cAlwM+BxWb2pJk9iS9C/BT4iXPutkquaWa5YNLxrl47V9PublwMzHXO3dCL85rKXmPCbpiISE9Ukc3KMRGJDGWZSN8p5NKO9b1ATk9l8kPr2Z6kUI6JSBIoy0Qk7gZUewHn3LfN7E7gKGBHwIC/ADc55x7rxSV/DFzfzTEvAguArco3mtkAYPNgX0cWAIPMbES7XiBbl53z30CzmR1eumywfNPMLnTOnd/RhZ1zK4GVZW3p5i2IiNROb7JZOSYiUaMsE+lTt+G/rEoBU4Cr6tqahFCOiUgSKMtEJM7MOVfvNvRKMAn6v4D/CiYzx8wOxE/gN8Y5N7+Dc5qAhcCXggo2ZjYBmAd81Dn3NzMbh+/6XfIh4DfAnsALzrk36IFgfMMi0OScW9bLtyki/UBU8yKq7RKRaIpqZkS1XSJRlMrkz8A/4fsCMKGQS6+tc5P6VFTzIqrtEpFoimpmRLVdIhJNYWZGVT1AzOzTnewqAs86516r5vpdcc7NNbN7gGvM7BT85OZXALeUih9m9h7gz8CxzrnHnHNFM7sW+ImZLQaWAZcDjzjn/hZc94Xy+5hZaRLAuR3MHSIiEjn1zGYRkbAoy0Tq4jdAFhiHn/D29rq2JuaUYyKSBMoyEYm7aofAuqOLfc7MbgFOcs6tqPI+nTkKX/T4M9CG/4B+Ztn+gcAEoHwM26+XHbsJcC/wlRq1T0SkHuqdzSIiYVCWifSxQi79diqT/wXwHeCcVCb/h2B+EOkd5ZiIJIGyTERirdpJ0Bs6egEjgQOA3fEfnmvCObfYOXekc24z51yTc+5459xbZfsLzjlzzj1Ytu1d59xpzrnNnXPDnHOHOec6mzME59yDwTXU+0NEYqHe2SwiEgZlmUjdXI4fs/3DwMfq3JZYU46JSBIoy0Qk7qoqgHTGOVd0zt2P721xWC3uISIilVE2i0gSKMtEaquQS78B3BisHlHPtiSVckxEkkBZJiJxUe0QWN2ZB4yp8T1iq6WlZRCwPdBY77YkSBvw2qRJk5bXuyEiERZqNre0tGwGbEONiur9lLJMpHvKsmhTjsXbXcDxwMH1bkjCKceiT1km0j1lWfStBf4zadKkVfVuiEg91LoAsgMwv8b3iKWWlpYxDQ0Nf2hoaNgSsHq3J0Gcc251S0vLdOCiSZMmtdW7QSIRFEo2t7S0NADnNjY2HmtmA1GWhUlZJtI9ZVm0Kcfi7X5gDTAulcnvWMiln693gxJKORZ9yjKR7inLos+1tbUtbGlp+dykSZNerXdjRPpazQogZvYB4EdAvlb3iKuWlpYGM/vekCFDthszZsyihoYGTSwYEuecvf3220MXLFhw6urVqwF+UO82iURJyNl87sCBA08dPXr0qmHDhq0wM2VZSJRlIl1TlkWfcizeCrn08lQm/xCwD3AQoAJIyJRj8aAsE+masiwe2tra7JVXXtluxYoV329paTlJxVzpb6oqgJjZEqCjQBoWXHsmcH4190ioUQ0NDZ/Yeuutl2266abv1LsxSTNs2LB3gC3mz59/bEtLy8/UXVn6m77I5paWluGNjY3Hjh49etVWW221qJprSceUZdLfKcviTzkWe/fiCyAHA1fWtynxpBxLBmWZ9HfKsmTYeuutl/3nP//5xNq1a7cAFta7PSJ9qdoeIF/rZPsy4Bnn3L+qvH5SjTCzAYMGDXqr3g1JquBpgaH4cSP1AVX6m77I5tFmNnDYsGErQriWdEJZJv2csiwBlGOxdg9wMbBvKpPfpJBLr6x3g2JIOZYQyjLp55RlCTBo0KBVZrYpMBIVQKSfqaoA4pyb1t0xZra5c25xNfdJoAYAMw1nWCtBV0lDk2ZJP9RH2dzgL6NuybWkLJP+TFmWDMqxWPsnsAAYDeyFnxdEKqAcSw5lmfRnyrJkKPsOUjkm/U7N/qM3swPN7PeAJtcREYkIZbOIJIGyTKT2Crm0A+4LVg+uZ1uSSDkmIkmgLBOROAi1AGJm25vZ98ysANwKtAHHhnkPqZ/Jkyen9t9//3Hl22bNmjWssbFx0j777LNj++OfeeaZQWY2afbs2UNK25YsWdKwxx577DRu3LhdXnjhhYEAX/7yl9+7yy67TBw0aNDuO++88/tq/05E+hdl84aUZSLxpCzbkLJM+sg9wfKgurYiIZRjG1KOicSTsmxDyjKR6Ku6AGJmg8zsi2Y2C5gH7A6MAT7mnPuic+7Wau8h0XXNNdeMmjJlyhuPP/74ZoVCYWBXx86fP3/A3nvvPWHFihWNDz/88Lxx48atLu07+uij30yn0xoqTSQkyubKKMtEoklZVhllmdTATPzEt+9PZfLb1rsxcaQcq4xyTCSalGWVUZaJREtVBRAzuxyYD3wV+CMwxjl3KP5D8trqmydRViwWG/70pz9t/tWvfvWNfffdt3j11Vdv0dmxzz///MC99tprwmabbbb2oYceemb06NHr/vu4/vrrX/7Wt761cOzYsav6puUiyaZsroyyTCSalGWVUZZJLRRy6TeBJ4JV9QKpkHKsMsoxkWhSllVGWSYSPdX2ADkV+CVwoHPuSufcohDaJDFx3XXXjRw7duy7u+2228qjjjpq0U033TSqra1to+OefvrpwXvvvffO48ePf/f+++9/rqmpaeODRCRMyuYKKMtEIktZVgFlmdSQhsHqPeVYBZRjIpGlLKuAskwkeqotgBwDfBh4zcx+Z2aHmFljCO3qV9qc4613VzfU49XmXK/bPX369C2POOKIRQCHH354cfny5Y0zZszYrP1xp5122tjtt99+5YwZM14YMmRI728oIj1Vl2xWlolIyPo8y+KaY6Ask5oqFUAOTGXy+luvMvpMVgHlmEhk6TNZBZRlItEzoJqTnXM3Azeb2Vjgy8CVwFB8YeV9wL+qbWB/sGLlmoZds/d9sB73fip74JObDh5YcZX5n//85yZz5swZevfddz8PMHDgQA499NAlv/71r0cdcsghy8uP3W+//ZbOnDlzxPTp00cef/zxS8Jqu4h0rF7ZrCwTkTDVI8vimGOgLJOaewwoAiOBDwF/q29z4kOfyXpOOSYSXfpM1nPKMpFoqqoAUuKc+zdwvpllgQOBE4AbzOynwB+cc2eGcR+JjquuumrLtWvX2vbbb79baZtzjkGDBrUtWrSocYsttlg3buF555332q677vrO1KlTx7a1tXHiiScq2EX6gLK5e8oykehTlnVPWSa1VMil16Qy+ZnA4fhhsFQAqZByrHvKMZHoU5Z1T1kmEk2hFEBKnHMOuBe418w2B44FjgvzHkk0dJMBbU9lD3yyXveu9JzVq1dz2223bXH++ee/kk6ni+X7Jk+evOO11167+TnnnLOwfPull176WkNDgzvllFPGOuc46aSTFOwifaSvsllZJiK11BdZFrccA2WZ9Jl78AWQg4Hv1bktsaXPZB1TjonEiz6TdUxZJhJdVRVAzOyvwJ3AXc65Z8v3OecWAz8NXtKFBjN6272uHm655ZYRy5YtazzjjDPeLK9eA6TT6SXTp08f1T7UAX74wx8uaGxs5NRTT92hra3t3yeffPJigKeeemqTZcuWNSxYsGDAu+++2zB79uwhALvvvvu7gwcP1jiIIhWqVzYry5RlImGqR5bFLcdAWSZ9Zmaw/HAqkx9SyKXfqWtrYkKfyXpGOSYSbfpM1jPKMpHoqrYHyDXAZ/Bd4F4B7gpes4OKsCRIW1sbAwYMcNddd92oPffcc1n7QAf4whe+sOTqq68e/eijjw4ZMWLERvsvuuiiBQ0NDe60004b65zjlFNOWXz88cenHn/88U1Lx+y1117vA5g3b17rhAkTVtX2XYkkkrK5C8oykdhQlnVBWSZ97GVgMbA5MBH4e32bExvKsS4ox0RiQ1nWBWWZSPRZGFllZpsA++ED8VCgEcjjA/Fe51y/e0LIzIbjJwtscs4tK9/X0tKy84ABA+4ZP378W0OHDn23Pi2s3N577z1+7NixK6dPn/5SvdvSnRUrVgx+7rnnNl2zZs3BkyZNmlfv9oh0pau8qPK6VWVzEnMMlGUitaIs61txyTLlWHKkMvkHgH2ALxdy6WkhXG8o8J6y17OFXPrxaq9bDeVY34pLjoGyTOJFWda34pJlyjGJmzCzrCGMBjnnVjrnZjjnTnbObQt8GngNuABYZGZ/MrO9wriX9L2FCxc23nzzzU2PPfbYZgcccEBovzxFpLaUzRtSlonEk7JsQ8oyqaPWYPn+ai6SyuSvTWXyS4C3gWeBB4AbgCOra150Kcc2pBwTiSdl2YaUZSLxEeok6CXOuUeBR4Fvm9k4fChuU4t7Se0deeSRqTlz5gybOnXq60cdddTSerdHRHqnv2ezskwkGZRlyjKpmznBsrm3F0hl8sOB48s2rQBeDV4v9r5p8aIcU46JJIGyTFkmEhfVToI+EjgamNa+K4qZNQHHBvsuq+Y+Ul8zZ858od5tEJGeUzZ3TFkmEi/Kso4py6SOwugBMjxYrgG2BIqFXDqx48crxzqmHBOJF2VZx5RlIvFR7RBYpwMf72gcLudcEdgbOKPKe4iISGWUzSKSBMoykWh5GnDA1qlMfqteXqNUACkWcumlSS5+BJRjIpIEyjIRibVqCyCTgau72P9L4PAq7yEiIpVRNotIEijLRCKkkEu/xfphqno7DNZmwXJ59S2KBeWYiCSBskxEYq3aAsg44Lku9j8XHCMiIn1H2SwiSaAsE4meaucBKfUA6S+TxSrHRCQJlGUiEmvVFkDWAtt2sX9boK3Ke4iISGWUzSKSBMoykeipdh6Q/lYAUY6JSBIoy0Qk1qotgDwJfLaL/Z8LjhERkb6jbBaRJFCWiUSPeoBURjkmIkmgLBORWKu2AHIFcLaZnW5mjaWNZtZoZmcAXweurPIeIiJSGWWziCSBskwkeko9QHZNZfKNXR7Zsf5WAFGOiUgSKMtEJNYGVHOyc+52M7sE+DlwoZmVJsXbAdgUuNQ5d1uVbRQRkQoom0UkCZRlIpH0AvAOMAQ/3vuzFZ5fmgS9XxRAlGMikgTKMhGJu2p7gOCc+zbwEeB6YD7wGnAd8FHnXKba60t0TJ48ObX//vtvMLHVrFmzhjU2Nk7aZ599dmx//DPPPDPIzCbNnj17SGnbkiVLGvbYY4+dxo0bt8sLL7ww8JFHHhly6KGHjh09evT7Bw8evPsOO+ywywUXXLBVX7wfkSRTNndOWSYSH8qyzinLpB4KufRa4OlgtTfzgJR6gCwPp0XRpxzrnHJMJD6UZZ1TlolEX1U9QEqcc48Bj4VxLYmXa665ZtSUKVPe+N3vfjeqUCgMTKVSqzs7dv78+QMOPPDA8WbGww8/PG/06NFr77777uGjRo1ac+211744duzYVQ8++OCmZ5999vaNjY3u3HPPXdiX70UkaZTNPacsE4kuZVnPKcukj8wB/gs/D0ilT/z2tyGwAOVYJZRjItGlLOs5ZZlItIRSADGzDwFfAnYKNj0D3OyceyKM60s0FYvFhj/96U+bz549+19vvPHGwKuvvnqLXC63oKNjn3/++YEHHHDATqNHj159zz33PN/U1NQG8LWvfW0RsKh03Pve977FjzzyyKZ33XXXSIW6SHWUzT2jLBOJNmVZzyjLpA+V5gGppgdIvyqAKMd6RjkmEm3Ksp5RlolET9VDYAXjAD4KnAiMCV5TgUfN7IfVXl+i67rrrhs5duzYd3fbbbeVRx111KKbbrppVFtb20bHPf3004P33nvvncePH//u/fff/1wp0DuzbNmyxhEjRqypWcNF+gFlc88py0SiS1nWc8oy6UNzgmVzL87tdwUQ5VjPKcdEoktZ1nPKMpHoqaoAYmZTgDOAM4EtnHMfcM59ANgc+DpwppkdW30zE861wcrlDXV5uS7ztUvTp0/f8ogjjlgEcPjhhxeXL1/eOGPGjM3aH3faaaeN3X777VfOmDHjhSFDhriurjlz5sxh+Xx+5NSpU9/sdcNE+rm6ZbOybB1lmUj16pJlMc0xUJZJnyr1ABmXyuQ3rfDcflUA0WeyyijHRKJJn8kqoywTiZ5qh8A6DTjXOXdF+Ubn3Grg52Y2ADgdmF7lfTpkZpsDlwOHAm3A7cBXnXNvdXHOYODHwBeBTYB7ga84515vd9yXgbPwXfuWAbc6506rwduAVW83cPGYD9bk2t351itPsslmFaf7P//5z03mzJkz9O67734eYODAgRx66KFLfv3rX4865JBDNpjUcL/99ls6c+bMEdOnTx95/PHHL+nsmo8//vjgL3zhCzueddZZrx122GH94o8ikRqpTzYrywBlmUiI+j7LYphjoCyTvlXIpRemMvkFwGhgF/wTwT1V+gKov0yCrs9kPaQcE4k0fSbrIWWZSDRVWwDZBbizi/13ABdUeY+u3AhsAxwADASuA34FHNnFOZcBaeDzQBG4AvgDsFfpADM7Czgb+Ab+A/0wIBV662Psqquu2nLt2rW2/fbb71ba5pxj0KBBbYsWLWrcYost1pa2n3feea/tuuuu70ydOnVsW1sbJ5544kbB3tLSMvjggw+ecPTRR795ySWXvNZX70MkoeqdzbGhLBOJNGVZDynLpA5a8QWQ91NZAaRf9QBBOdZjyjGRSFOW9ZCyTCSa9zcBEAAAIABJREFUqi2ArAUGdbF/YHBM6MxsInAw8KHShEtmdgYww8z+xzk3v4NzmoATgCOdc/cH244D5prZR5xzfzOzkcAPgEOdc38uO31O++uFZtCwNr71ypM1u353967Q6tWrue2227Y4//zzX0mn08XyfZMnT97x2muv3fycc87ZYFKmSy+99LWGhgZ3yimnjHXOcdJJJ60L9ieeeGLwQQcdNOHzn//8ossvv/zV3r8ZEQnUJ5uVZcoykXD1fZbFLMdAWSZ1Mwf/EFql84D0twKIPpP1gHJMJPL0mawHlGUi0VVtAeTvwFHAeZ3sPyY4phY+CiwtFT8Cs/BDYe0B/LGDcybhg3lWaYNzbp6ZvRRc72/4D/INwHvMbC6+m/Zs4Gzn3MudNcbMNsEPqVWy0fh+nbIGetu9rh5uueWWEcuWLWs844wz3iyvXgOk0+kl06dPH9U+1AF++MMfLmhsbOTUU0/doa2t7d8nn3zy4scff3zwQQcdNOHjH//4snPPPXfBSy+9NABgwIABbLvttprcSaR3epXNVeUYKMuUZSJh6/ssi1mOgbJM6qY0D8j7KzyvvxVA9JmsB5RjIpGnz2Q9oCwTia5qCyA/Au4IQu3HpXk0zGw0fgiprwGfq/IenRkNvFG+wTm3xswWB/s6O2eVc25pu+2vl52zA74Aci7wVfwwWT8AZprZ+51zqzq59reA8yt+FzHS1tbGgAED3HXXXTdqzz33XNY+0AG+8IUvLLn66qtHP/roo0NGjBix0f6LLrpoQUNDgzvttNPGOud49tlnBy9ZsmTAnXfeufmdd965eem4bbfddtWrr77a2v58EemR3mZz4nMMlGUiMaIs64KyTOqs1Du+OZXJWyGX7nLyVoBUJr8J658g7i8FEOVYF5RjIrGhLOuCskwk+sy5bj+rdn0BP+zUj/DFlFIXryZgDXCOc+5nFV4vB3yzm8MmAocBU5xzE9qd/wZwvnPuqg6ufSRwnXNuk3bbHwMecM5908zOBS4EDnLO3Rfs3xJYAHzKOXdvJ+3uqLL9CtDknNvgA35LS8vOAwYMuGf8+PFvDR069N1u3mtk7L333uPHjh27cvr06S/Vuy3dWbFixeDnnntu0zVr1hw8adKkefVuj0hXzGw4Pj83yosqrllxNveHHANlmUitKMv6VlyyTDmWTKlMfjDwFtAIjCnk0t0OzZHK5EcBpSdfBxRy6ZoMlVwN5VjfikuOgbJM4kVZ1rfikmXKMYmbMLOs2h4gOOcuN7M/4icVHx9sfha4vasho7rwY+D6bo55EV+Q2Kp8o5kNADYP9nVkATDIzEa06wWyddk5pUmF/lXa6ZxbaGZvAtt11iDn3EpgZVlbunkL8bFw4cLGWbNmbfrYY49tNnXq1I2664lI9PQmm5OcY6AsE4kjZdnGlGUSBYVc+t1UJv8s/sG0ZqAnY5OXhr96O4rFj1pRjm1MOSYSP8qyjSnLROKj6gIIgHPuFeCyjvaZ2RDn3DsVXGsh658M6pSZPQKMMLNJzrmWYPN/44everST01qA1cB+wO3BdSbgCxuPBMc8HCwn4CvTmNnmwCjgPz19H0ly5JFHpubMmTNs6tSprx911FHthw8TkYgKM5uTQFkmEk/Ksg0pyyRCWvEFkA8A9/Tg+P42/8c6yrENKcdE4klZtiFlmUh8hFIA6UjQ1e104Bt0PidHrznn5prZPcA1ZnYKfnLzK4BbnHPzgza8B/gzcKxz7jHnXNHMrgV+EswVsgy4HHjEOfe34LrPmtmdwM/MbGpwzMXAPOCBsN9HHMycOfOFerdBRMJR62yOMmWZSHIoy0Qi4XHgC8BHe3h8vy2AdEQ5JiJJoCwTkThoqOZkM9vEzC42syfMbLaZfTbYfhzwb/xESB1Wh0NyFL4w8WdgBvAQMLVs/0B8T46hZdu+DvwJ3wPk//BDXx3W7rrH4nuR5IG/4HuNHOycWx3+WxARCVcEsllEpGrKMpHIeyhY7pXK5Hsyrkm/K4Aox0QkCZRlIhJ31fYA+T5wMjAL2BO41cyuAz4CnAXc6pyr2fiuzrnFwJFd7C8A1m7bu8Bpwauz85YBJwQvEZG4qWs2i4iERFkmEm1/B94FtsA/dNbdhKr9rgCCckxEkkFZJiKxVm0B5PP44aXuMrNdgTnBNXdzzrmqWyciIr2hbBaRJFCWiURYIZdelcrkHwM+DuxF9wWQzYJlfyqAKMdEJAmUZSISa1UNgQWMwU8sjnPuKWAlcJkCUESkrpTNIpIEyjKR6Hs4WH6sB8eWeoAsr1Fbokg5JiJJoCwTkVirtgDSCKwqW18DvFXlNUVEpDrKZhFJAmWZSPStmwekB8f2xyGwlGMikgTKMhGJtWqHwDLgejNbGawPBq42s7fLD3LOtZ9kXEREakfZLCJJoCwTib5HguX4VCa/dSGXfr2LY/tjAUQ5JiJJoCwTkVirtgAyrd36DVVeT0REqqdsFpEkUJaJRFwhl16SyuSfBnbBT4z7xy4O748FEOWYiCSBskxEYq2qIbCcc8f15BVWY6W+Jk+enNp///3HlW+bNWvWsMbGxkn77LPPju2Pf+aZZwaZ2aTZs2cPKW1bsmRJwx577LHTuHHjdnnhhRcGLliwoHHvvfcev9VWW71/0KBBu48ePfr9xx577HaLFy+udng2kX5L2dw1ZZlIPCjLuqYskwjp6TBY/W4SdOVY15RjIvGgLOuaskwk+vQ/jlTlmmuuGTVlypQ3Hn/88c0KhcLAro6dP3/+gL3/v717j4+qvvM//v6EhEu4KqCUa1C5qYAQgUJbLGWprAFXt/Sy9fFYtnbFWtRdfrZCtRftxVK7dl0rK+2v/lB2W21/urUorKzoT9oCokaLgIKojVQlCMg9BJLM9/fHOaOTMYEEZuac78nr+XjMI5mZMzOfM4RXar+cOZ/4xLCampp2a9as2XL22WfXtWvXTjNmzNj30EMPvbZp06ZNP/vZz/78hz/8oevs2bMHFWofAICWAUgCWoaItPRE6G3xJOhoJToGIAloGRAvp/oRWGjD9u/fX/TYY4+dvnbt2pfffffdksWLF/dcuHBhdVPbvvbaayXTpk0b2qdPn7rHH3/8te7du6ckqXfv3g3z58/fld5u6NChxzZu3Ljr7rvv7lOo/QDQttEyAElAyxCh9BEgY8sWLC+tWlhR08x2bfEjsNAKdAxAEtAyIH5YAImBlEuppq4mkqNxSktKU0V2ci+9ZMmS0wYPHlw7evToo1dcccWe+fPnD7jtttuqi4oaP9/mzZs7fuYzn+k/cuTImt/97ndvdOrUyTX3nFVVVSXLli07bcKECfzLMMAztOwDtAzwk68dk2gZIlUlaYekj0gaJ2l1M9uxAFIgvraMjgFI87VjEi0D4ogFkBioqaspmvjAxDFRvPa6v1v3Ypf2XVIn89ilS5f2/vznP79HkmbNmrX/q1/9atmKFSu6zpgxo1GQ586dO3js2LGHVqxY8XpxcdM/cjNnzhy8atWqHrW1tUVTpkzZ/8ADD1SdzEwAokPLaBngO187JtEyRKdqYYUrW7D8j5I+q+A8ICyARMzXltExAGm+dkyiZUAccQ4QnJQNGzZ0eOmll0qvvPLK9ySppKREM2fO3PuLX/yiV/a2U6dO3ff88893Wbp06WnNPd+iRYv+snbt2lf+8z//87Xt27d3mDNnzoB8zg8AEi0DkAy0DDHQkvOAsACCZtExAElAy4B44giQGCgtKU2t+7t1L0b12ifzuHvuuad3Q0ODDRo0aHT6Nuec2rdvn9qzZ0+7nj17NqRv/9a3vrXj/PPPPzJnzpzBqVRK//iP/7g3+/kGDhxYP3DgwPoxY8bU9urVq2H69OnDfvCDH+wYNGhQ3cntGYBCo2W0DPCdjx2TaBliIX0ekEllC5YXVS2saPTzXLZgeZGkLuFVFkDyzMeW0TEAmXzsmETLgLhiASQGiqxIp3J4XaHV1dXpoYce6vmd73znrYqKiv2Z933mM58559577z39xhtv3JV5+49//OMdRUVF7itf+cpg55yuuuqqD4U9LZUK3ora2lrLyw4AyAta1hgtA/zjW8ckWobY2CDpsKTuks6TtDHr/s6S0j9DfH55nvnWMjoGIJtvHZNoGRBnLICg1R588MEeBw4caHfdddftzly9lqSKioq9S5cu7ZUddUn60Y9+VN2uXTtdc801Z6VSqT9fffXV7/3617/uXl1dXTxp0qTD3bp1S7344oudbr755v5jx449NGzYsGOF2ysAbQ0tA5AEtAxxULWwor5swfJnJE1VcB6Q7AWQ9Mdf1UuqLeRsiD86BiAJaBkQXyyAoMVSqZSKi4vdkiVLek2aNOlAdtAl6XOf+9zexYsX91m/fn2nHj16fOj+2267rbqoqMjNnTt3sHNO/fr1q7vvvvt6f/Ob3xxQV1dX1KdPn2OXXHLJ3u9+97vVhdkrAG0NLQOQBLQMMbRGwQLIxyUtzrrv/fN/VC2scAWdCrFFxwAkAS0D4o8FELTY7t27SwYPHnx06dKl25vbZsqUKTXOucr09czv077//e/v/P73v78zfX3mzJlbcj8tADSNlgFIAlqGGEqfB6SpE6FzAnR8CB0DkAS0DIi/oqgHQPzt2rWr3QMPPND92Wef7Tpt2jT+owWAl2gZgCSgZYix58Kvg8oWLO+SdV/X8Cs/s6BjABKBlgH+4AgQnNAXv/jFspdeeqnznDlzdl5xxRX7op4HAE4GLQOQBLQMcVW1sGJf2YLlhyR1kdRP0taMuzkCBO+jYwCSgJYB/mABBCf0xBNPvB71DABwqmgZgCSgZYi5tyUNU/MLIAcLPhFih44BSAJaBviDj8ACAAAAAOTCW+HXflm3cwQIAAAAIsECCAAAAAAgF94Ov7IAAgAAgFhgAQQAAAAAkAssgAAAACBWWAABAAAAAORCcwsgXcOvLIAAAACgoFgAAQAAAADkQnoBpH/W7ZwEHQAAAJFgAQQAAAAAkAt8BBYAAABihQUQAAAAAEAupBdA+pQtWF6ccTsLIAAAAIgECyBoETMrP95lwoQJQ4uLi8euXLmyS+bjDhw4UNS/f/+Rc+bM6S9J48ePH3bllVcOyNzme9/73hnt27cf+/Of//y0o0eP2jXXXNNv6NCh53bq1GnMGWecMeryyy8vq6qqKink/gJIJloGwHd0DDG3U1KDgv/OPDPjdhZA0AgtA+A7Ogb4o/jEmwDSm2++uSH9/f3333/67bff3nfz5s2b0rd17949dcMNN/S96qqryjZt2vRyt27dUpI0d+7c/h07dkzdeeedbzf1vPPmzeu7ePHiM3/1q1+9NmvWrAN79uxpt2HDhtIbb7xxx7hx42p2795dPG/evAEzZsw4Z9OmTa/kf08BJBktA+A7OoY4q1pY0VC2YPkOBecA6acPjgjhJOhohJYB8B0dA/zBAghaZODAgfXp77t3796QfZsk3XXXXW+PHDmy+7XXXtt/6dKl2x999NGuv/71r3s9+eSTW0pLS13mtqlUSl/60pcG/Pa3v+25bNmyV6dNm3ZYknr27Nmwdu3abRmbHv3pT3+6/ZOf/OSIbdu2tR8yZMixPO4mgISjZQB8R8fggbf1wQJIGkeAoBFaBsB3dAzwBwsgMeBSKaVqaiL5OLKi0tKUFeXmpUtLS92SJUv+PHXq1OGf/vSnD8yfP3/Addddt+MTn/hETeZ29fX1dtlllw1et25d1yeeeGLrhAkTjhzveffu3dvOzNSzZ8/6420HIFq0jJYBvqNjdAw50dSJ0NMLIAcLPEubRMtoGeA7OkbHgFxiASQGUjU1Ra9eOG5MFK899PnnXmzXpUsqV883efLkmmuvvbZ69uzZZ48YMaJm4cKFO7K3efDBB3tJ0vr1618eM2ZM7fGer6amxm6++eb+M2bMeO/000/P2ZwAco+WNY+WAX6gY82jY2iFRgsgZQuWd5DUIbyNI0AKgJY1j5YBfqBjzaNjQOtxEnTk3O233/5OKpXSDTfcUF1S8uFzMpWXlx8qLS1N3XTTTX3r6uqafZ6jR4/ajBkzznbO6b777nsznzMDQDZaBsB3dAwRSS+A9A+/ds24jyNA0Gq0DIDv6BgQLY4AiYGi0tLU0OefezGq1871c6ZjXlxc7Jq6f8SIEUfuuOOOv1RUVAydMWPG2Y899tjr2b8Awqif9fbbb7dfvXr1Vla1gfijZbQM8B0do2PIieyPwEp//NXhqoUVDRHM0+bQMloG+I6O0TEgl1gAiQErKlIuD6/zwaRJk46sWLHi1UsuuWRoRUXF2Y8++ugbHTp0cNIHUa+qquq4evXqrX369OE/lAAP0DJaBviOjtEx5ET2Akj6CBA+/qpAaBktA3xHx+gYkEtefwSWmZ1uZr80swNmts/M7jWzLid4TEczW2Rme8zskJk9bGZnZm0zzsyeDJ9zr5mtNLPR+d2btmfixIlHVq5c+erzzz/fZcaMGWcdPXrUjh49apdccslZGzdu7Lx06dI36uvrtX379uLt27cX19bWWtQzA0A2WgbAd3QMOfb+AkjZguWmD44AYQEEeUXLAPiOjgH54fUCiKRfSjpP0jRJMyRNlvTzEzzmXyXNlPRZSRdJ6ivpv9J3hgsoj0vaLmmCpI8r+KzalWb24Q/qwykZP378kZUrV2594YUXulRUVJy1devW9k899VSPnTt3lkyaNOncQYMGjU5fVq1addzFLQCICi0D4Ds6hhxKL4B0VrD4kV4A4fwfyDtaBsB3dAzIPXOuyY+fiz0zGyHpZUnjnHPPh7dNl7RCUn/n3DtNPKa7pF2Svuiceyi8bbikVyRNdM49Y2YXSnpO0kDn3F/CbUZKeknSEOfcay2cr5uk/ZK6O+ca/WunysrK4cXFxY8PGTLkUGlpae3J7D+Or6ampuO2bdu61NfXTy8vL98S9TzA8RyvF1GiY9GjZfAJLUNT6FjbVLZg+V5JPRT8Y7XRkn4l6amqhRVTIx3sBOgYmkPL4BNahqbQMfgmly3z+QiQiZL2pRc/QqskpRQcudGUckkl4XaSJOfcFgVHe0wMb9oqaY+kL5tZezPrJOnLChZJqnK5AwAAAACQQJnnAeEjsAAAABAZn0+C3kfSu5k3OOfqzey98L7mHnPMObcv6/ad6cc45w6a2SclPSLpW+H92yRd7Jyrb24YM+sgqUPGTV2b2xYA4oiOAUgCWgbEwtsKjv7oJ06C3mp0DEAS0DIAcRG7I0DMbKGZuRNchufx9TtJulfSGkkflfQxSZskLQ/va843FByWk768la8ZASBP6BiAJKBlQPTSR4D0F0eAnAw6BiAJaBmAWIjdAoikOySNOMHlDUnVks7IfKCZFUs6PbyvKdWS2ptZj6zbz8x4zBcllUn6knPuOefcM+FtgyX9zXHm/qGk7hmX/ifYTwCIGzoGIAloGRC9pj4Ci5OgtxwdA5AEtAxALMTuI7Ccc7sUnKj8uMxsnaQeZlbunKsMb/6UgkWd9c08rFJSnaSpkh4On2eYpIGS1oXblCo4j0jm2eHT15tdMHLOHZV0NGO+E+0CAMQKHQOQBLQMiIXMBZDd4fccAdJCdAxAEtAyAHERxyNAWsQ594qkxyX9bzMbb2Yfk3S3pAedc+9Ikpn1M7MtZjY+fMx+BR9v9RMzm2Jm5ZKWSFoXHukhSU9IOk3SIjMbYWbnhdvUS/p/hdxHAAAAAPBQ+mNOOAk6AAAAIuXtAkjoCklbJD0paYWkP0qak3F/iaRhCo7qSJsn6TEFR4D8XsFHX/1t+k7n3BZJMyWNUnBUyB8k9ZU03Tm3I187AgAAAAAJ0dRHYLEAAgAAgIKL3UdgtYZz7j0F5+do7v4qSZZ1W62kueGlucc9oeBIEAAAAABA66QXQM5QcI5GiQUQAAAARMD3I0AAAAAAAPGyW8G5F03BEfkSCyAAAACIAAsgAAAAAICcqVpYkZL0Tni1S/j1YETjAAAAoA1jAQQAAAAAkGtvZ13nCBAAAAAUHAsgAAAAAIBcYwEEAAAAkWMBBC02fvz4YbNnzx5w5ZVXDujWrdsFPXv2HH3HHXf0OnDgQNGsWbPKOnfuPGbgwIHn/+Y3v+mWfsxzzz3XcfLkyUNKS0vH9OzZc/Rll102eMeOHcXp+x966KFu5eXlw7p27XpBjx49LpgyZco5mzdv7pC+f+vWre3NrPz+++/vMWHChKGdOnUaM2zYsHNXrVrVudD7DyAZaBkA39ExeIIFEBwXLQOQBLQMiD8WQGLkwIEDRc1dampqrKXbHjp0qEXbnsyMDz/8cK9evXrVr1mz5pUvf/nL786fP3/QzJkzz5o4ceKhZ5555uXJkycfuPrqqwcfPHiwaPfu3e0uvvjiYaNGjapZs2bNK8uWLXt1165dxZdffvlZ6ec7dOhQ0fXXX79z3bp1r6xYsWKrmenyyy8/u6GhodHr3nrrrf3mzZu3c/369S+fddZZtbNnzz6rrq7uZHYBQJ7RMloG+I6O0THkxFsZ39dLqo1qkLaKltEywHc+dEyiZUDcmXMu6hkSycy6SdovqbtzrtG/dqqsrBxeXFz8+JAhQw6VlpbWZjymvLnnu+iii/Y//fTTr6Wvd+rUaUxtbW2TcR43btyhZ599dmv6+mmnnTZ63759xdnbOecqW7NP48ePH9bQ0KDKysqtklRfX69u3bqNufjii/f+9re/rZKk7du3Fw8aNGj0qlWrtqxcubLb2rVru/zxj3/cln6O119/veScc84ZtWHDhk2jRo06mv0aO3bsKO7bt+/oZ599dvO4ceNqt27d2n748OEjf/KTn7w5b9683ZJUWVnZ8cILLzzvhRde2DxmzJgm/0Oqpqam47Zt27rU19dPLy8v39Ka/QQK7Xi9iNLJdCx8HC2jZWiDktQyOkbHcOrKFiz/gqQHwqvvVS2s6BnlPC2RpI6Fj6NltAxtUJJaFveOSf60jI7BN7ls2Yf+sgPHc+655x5Jf19cXKwePXrUn3/++e/f1r9//3pJqq6uLt64cWOn9evXdy0tLR2T/TxbtmzpMGrUqKMbN27scNNNN/V98cUXu+zbt684lUpJkt54440O48aNez/YY8eOrUl/P3DgwLr0a+RlJwEkHi0D4Ds6Bg9kfgTWwcimQKzRMgBJQMuAeOMvRYzs37//xebuKy4ubnSozs6dOzc0t21RUVGjbd98882Npz5doKSkpNFzm1mj24qKgsX2VCplhw8fbvepT31q/x133PFW1tO8H+ZLL730nH79+h1btGhR1YABA+pSqZTGjRt33rFjxxodnpj5GmbBXQ0NDY22ARAPtIyWAb6jY3QMOZG5ABKbf4HcltAyWgb4zoeOSbQMiDsWQGKkW7duqai3zaXRo0fXPPbYYz2GDRt2tKSk5EP3V1dXt6uqqup4zz33vDl9+vRDkrRy5couBR8UQE7RMloG+I6O0THkxDsZ37MAEgFaRssA3yWtYxItA6LASdCRNzfccMO7+/fvL7700kvPWr16denmzZs7PPzww91mzZpVVl9fr969ezf06NGjfvHixb03bdrUYdmyZV2/9rWvDYh6bgDIRMsA+I6OIQpVCytqJe0Jr7IAglNGywAkAS0DCo8FEORNWVlZ3erVq7c0NDTYzJkzh1544YXnfv3rXx/QvXv3hqKiIrVr105Llix5Y+PGjaXl5eXnff3rXx+wcOHCv0Q9NwBkomUAfEfHEKH0x2CxAIJTRssAJAEtAwrPnHMn3gqtdrwz1VdWVg4vLi5+fMiQIYdKS0trm34GnIqampqO27Zt61JfXz+9vLx8S9TzAMdzvF5EiY5Fj5bBJ7QMTaFjbVvZguUrJP21pF9ULay4Kup5ToSOoTm0DD6hZWgKHYNvctkyjgABAAAAAORD+gSvsfk/4AAAANC2sAACAAAAAMiHByW9LOmRqAcBAABA21Qc9QAAAAAAgOSpWljxlKTzop4DAAAAbRdHgAAAAAAAAAAAgMRhAQQAAAAAAAAAACQOCyDRSEmScy7qORLLOWeSnML3GkDOpSS58O8a8oSWAXlHy/KMjgF5R8cKgJYBeUfL8izj/4OkY2hzWACJxj7nXP2xY8faRz1IUh0+fLjUOVcnaUfUswAJVe2cqzt8+HBp1IMkGS0D8o6W5RkdA/KOjhUALQPyjpbl2bFjx9o75+ol7Y16FqDQOAl6NHanUqnVO3furCgpKakrKiriUJAccc7Z4cOHS6urq9s3NDTcW15efjDqmYAkKi8vP1BZWbm0urr6Gkk9O3fuXGNmtCxHaBlQGLQsf+gYUBh0LL9oGVAYtCy/UqmU7dy5s1sqlXpM0p6o5wEKjQWQCJSXl6cqKyu/feTIkZGvv/56b0kc4pc7zjlX19DQcK+k26IeBki42+rq6vTOO+/8vZmVipblEi0DCoeW5QcdAwqHjuUPLQMKh5blj0ulUtudc98pLy/nI7DQ5hjnocgPM+smab+k7s65A01tU1lZ2V7SQLEQlUspSTv4lznwSUt6EYWWzlVZWdlV0kfExyrmEi2Dd2gZstAxeIeOoQm0DN6hZWhCvaTt5eXlx6IeBGipXLaMBZA8iesvHADxE9dexHUuAPEU12bEdS4A8RPXXsR1LgDxFNdmxHUuAPGUy2awmgoAAAAAAAAAABKHBRAAAAAAAAAAAJA4LIAAAAAAAAAAAIDE4eTb+dfVzKKeAUC8dY16gBOgYwBagpYB8B0dA5AEtAxAEuSsZSyA5E/6D+mtSKcA4JOukuJ0Mjg6BuBk0DIAvqNjAJKAlgFIglNumTnncjQLMlmwnN1X0sEWPqSrgl8C/VvxmKgxc2H4OLPk59xRztxV0jsuRlFuIx2T/JybmQuDmU/u9WlZ4TFz4fg4NzO3/rV975jEn3uhMHNh+DizRMsa4X+TxZqPczNzYUQ9c05axhEgeRL+wbzd0u0zDv876JyL0wp9s5gHKgW/AAAQaUlEQVS5MHycWfJz7ohnjt171BY6Jvk5NzMXBjOflNi9T22hZcxcOD7OzcytFrv3qLUdkyJ/D08KMxcGMxcOLWuM/00WXz7OzcyFEYOZc/KanAQdAAAAAAAAAAAkDgsgAAAAAAAAAAAgcVgAiY+jkm4Nv/qCmQvDx5klP+f2ceY48fX983FuZi4MZm6bfHwPmblwfJybmdsmH99DZi4MZi4cX+eOCx/fPx9nlvycm5kLw8eZP4SToAMAAAAAAAAAgMThCBAAAAAAAAAAAJA4LIAAAAAAAAAAAIDEYQEEAAAAAAAAAAAkDgsgMWBmc82sysxqzWy9mY2PeqZMZjbZzB41s3fMzJnZZVn3m5l918x2mNkRM1tlZkOimjec6Rtm9pyZHTSzd83sETMblrVNRzNbZGZ7zOyQmT1sZmdGOPM1ZvaSmR0IL+vM7K/jOm9TzGxB+DNyZ8ZtsZrbzG4JZ8y8bInrvD6Jc8voWMFmpmOFm5OW5Qktyy1aFg0fWkbH8ifOHZP8a5mPHQtn8rplPnQsnImW5UmcW+Zbx8KZvGuZ7x2TaFlcsAASMTP7vKSfSLpV0lhJGyStNLMzIh2ssc4K5prbzP03Srpe0lckTZB0WME+dCzMeE26SNIiSR+VNE1SiaT/MbPOGdv8q6SZkj4bbt9X0n8VeM5Mb0laIKlc0oWSnpL0OzM7L7w/bvM2YmbjJF0t6aWsu+I492ZJH8m4fDzjvjjOG3setIyOFQYdKyxalmO0LC9oWYF51jI6lmMedEzyr2U+dkzyuGWedUyiZTnnQct865jkZ8u87ZhEy2LFOcclwouk9ZLuzrheJOltSQuinq2ZeZ2kyzKum6Qdkr6WcVt3SbWSvhD1vBkz9Q5nn5wx4zFJszK2GR5u89Go582Y6T1JX477vJK6SHpV0l9JelrSnXF9nyXdIulPzdwXu3l9ufjUMjpW8LnpWH7mpWX5eV9pWf7npmX5ndObltGxvL2v3nQsnM+7lvnasXCu2LfMp46FM9Cy/Lyv3rTMx46FM3nZMh86Fs5Cy2J04QiQCJlZewWrmKvStznnUuH1iVHN1UqDJfVR433Yr+CXVZz2oXv49b3wa7mC1e7MubdI2q4YzG1m7czsCwr+VcE6xXxeBf+KYLlzblXW7XGde0h4qOobZvZLMxsY3h7XeWMtAS2jY3lAxwqCluUQLSsYWpZfvrWMjuVQAjom+dEyrzomedcy3zom0bKcSkDLfOiY5FnLPOuYRMtipTjqAdq4XpLaSdqZdftOBStpPugTfm1qH/ooBsysSNKdktY45zaFN/eRdMw5ty9r80jnNrORCkLeUdIhSZc75142swsUw3klKfwFNFbSuCbujuP7vF7SP0jaquCQvu9I+oOZna94zusD31tGx3KIjhUMLcs9WpZntCy/PGwZHcs93zsmxbxlPnVM8q9lHnZMomX54HvLYt0xya+W+dYxiZbFEQsgaAsWSTpfjT+7Lq62SrpAwUr8LEn3m9lF0Y7UPDMbIOnfJE1zztVGPU9LOOf+O+PqS2a2XtKbkj4n6Ug0UwEnRMfyxMeOSbQM3qJleeJjy+gYPOVTxySPWuZjxyRaBm/51DJvOibRsrjiI7CitVtSg6Qzs24/U1J14cc5Kek5Y7kPZna3pBmSpjjn3sq4q1pSezPrkfWQSOd2zh1zzr3mnKt0zn1DwUm1/kkxnVfBYXBnSHrBzOrNrF7ByZCuD7/fqXjO/b5wBftVSecovu9z3PneMjqWQ3QsGrQsJ2hZHtGyvPO+ZXQsJ3zvmBTjlvnWMcm7lnnfMYmW5YjvLYttxyT/WuZZxyRaFkssgETIOXdMUqWkqenbwsPQpio4vMsHf1bww565D90kTVCE+2CBuyVdLulTzrk/Z21SKalOjeceJmmg4vXeF0nqoPjO+6SkkQpW49OX5yX9MuP7OM79PjPrIulsBScpi+v7HGsJaBkdyy86VgC07NTRsvygZQXjfcvo2KlLQMekGLYsQR2T4t0y7zsm0bJcSEDLYtexcIaktCzOHZNoWTydzJnTueTuIunzkmolzZY0QtLPJO2VdGbUs2XM2EUf/KV1kuaF3w8M758fznypgr/kj0h6Q1LHCGf+d0n7FKyy9sm4dMrY5h4Fh3NNUbBCu1bS2ghn/qGkyZLKwvfxh5JSCg6bi928x9mPpyXdGeP3+V/Cn4sySZMkPSFpl6TecZzXl0vcW0bHCjYzHSvcjLQsP+8rLcv9zLQsuv2IdcvoWN7e11h3LJzRq5b52LFwJu9bFveOhTPRsvy8r7FumW8dC2fyrmVJ6Fg4Jy2L+s8g6gG4OEm6NvwhOqrgpDMTop4pa75PhkHPvtwX3m+SvqtghbtW0ipJQyOeual5naR/yNimo4LPPXxP0mFJ/yWpT4Qz3yupKvw5eDd8H6fFdd7j7Ed22GM1t6QHJb0Tvs9vhdfPjuu8Pl3i3DI6VrCZ6VjhZqRl+XtvaVluZ6Zl0e1HrFtGx/L63sa2Y+F8XrXMx46FM3nfsrh3LJyJluXvvY1ty3zrWDiTdy1LQsfCOWlZxBcLdwIAAAAAAAAAACAxOAcIAAAAAAAAAABIHBZAAAAAAAAAAABA4rAAAgAAAAAAAAAAEocFEAAAAAAAAAAAkDgsgAAAAAAAAAAAgMRhAQQAAAAAAAAAACQOCyAAAAAAAAAAACBxWAABAAAAAAAAAACJwwIIEsXMyszMmdkFUc+SZmbDzewZM6s1sz/l8XVuae3zm9nTZnbnCbZxZnbZqU0HoDVoGS0DkoCW0TLAd3SMjgFJQMtoWVvHAghyyszuCyOwIOv2y8zMRTVXxG6VdFjSMElT8/g6/5Ln5wfaDFrWJFoGeIaWNYmWAR6hY02iY4BnaFmTaBkKhgUQ5EOtpPlmdlrUg+SKmbU/hYefLemPzrk3nXN7cjVTNufcoXw+fy6d4vsJFAota4yWZaFl8AQta4yWZaFl8AAda4yOZaFj8AQta4yWZaFl+cMCCPJhlaRqSd9oboOmDkEzs382s6qM6/eZ2SNmdpOZ7TSzfWb2bTMrNrMfm9l7ZvaWmX2piZcYbmZrw0PpNpnZRVmvdb6Z/beZHQqf+z/MrFfG/U+b2d1mdqeZ7Za0spn9KApnesvMjprZn8xsesb9TlK5pG+Hq/23NPM8T5vZXWZ2e7hf1dnbmlkPM/uFme0yswNm9pSZjW7uPQ3fp7vC922Pmf3IzO43s0eyXr7oeK8b+kj4fh0xszfMbFbWbCPDeY6Er/VzM+uScX/6z/JmM3tH0tbw9q+a2bbwz2mnmT3U1PsDRISWfXA/LRMtg7do2Qf30zLRMniJjn1wPx0THYO3aNkH99My0bJCYgEE+dAg6SZJ15lZ/1N8rk9J6itpsqT/peAQucck7ZU0QdJiST9r4nV+LOkOSWMkrZP0qJn1lII4SnpK0ouSLpQ0XdKZkn6T9RyzJR2T9DFJX2lmvn+SdIOkr0kapSD+y8xsSHj/RyRtDmf5iIJD75ozW8HhfxMk3ajgF8G0jPv/r6QzJP21gl8UL0h60sxOb+b55ku6QtKXwn3oJqmpzyc80etK0vckPSxptKRfSnrQzEZIkpl1Dvd7r6Rxkj4r6a8k3Z31HFMVHNo4TdIMM7tQ0l2Svh3ePl3S75vZFyAKtIyW0TIkAS2jZbQMvqNjdIyOIQloGS2jZVFxznHhkrOLpPskPRJ+v07SveH3lwU/bu9vd4ukP2U99p8lVWU9V5Wkoozbtkj6fcb1dpIOSfpCeL1MkpM0P2ObYkl/kXRjeP2bklZmvXb/8HFDw+tPS3qhBfv7tqSbsm57VtKijOt/knTLCZ7naUl/aOJ5Fobff1zSfkkdsrZ5TdKcpt5TBf+y4GtZ79Wb6T+flrxueN1Juidrm2ck/Xv4/VWS3pPUOeP+SxT8cj8z48+yWlL7jG3+NtynrlH/3HLhkn2hZbQsvI2WcfH6QstoWXgbLePi7YWO0bHwNjrGxesLLaNl4W20LKILR4Agn+ZLmp1e/TxJm51zqYzrOyVtTF9xzjVI2qNgtTfTuoxt6iU9Lyk9x2hJU8JD+g6Z2SEFvyyk4DMI0yqPN5iZdVOw4r4m6641Ga/VGi9lXd+hD/ZrtKQukvZkzT04a+b0bN0VrNQ/m74tfK+a2qfjvW7auiaup/dxhKQNzrnDGfevUXCE2bCM2zY6545lXH9CwS+ZN8LDKq8ws9Im5gOiRstah5bRMsQTLWsdWkbLED90rHXoGB1DPNGy1qFltOyUFUc9AJLLOfd7M1sp6YcKVjUzpSRZ1m0lTTxNXfbTNnNbaxbzukh6VMEvnWw7Mr4/3MT9+XS8/eqiYLZPNvG4fXl83Vxq9H465w6a2VgF+/RpSd+VdIuZjXPOneo+ATlDy1qNltEyxBAtazVaRssQM3Ss1egYHUMM0bJWo2W07JRxBAjybYGkmZImZt2+S1IfM8sM+wU5fN2Ppr8xs2IFnwP4SnjTC5LOU3AI4WtZlxaH3Dl3QNI7Cj4zMNPHJL18StN/2AuS+kiqb2Lm3U3Mtl/BvwAYl77NzNpJGnuSr//RJq6n389XJI0OP98w7WMKfnFvPd6TOufqnXOrnHM3KvhcyDIFn2UJxA0tyw1aBkSLluUGLQOiQ8dyg44B0aJluUHL0CIsgCCvnHMbFZwI6Pqsu56W1FvSjWZ2tpnNVXDColyZa2aXm9lwSYsknSbp/4T3LZJ0uqQHzGxc+PoXm9mSMHyt8WNJ883s82Y2zMwWKvjl9G+52pHQKgWH0j1iZp82szIzm2RmPwhPkNSUn0r6hpn9jZkNC2c6TcGqdWt91syuNLOhZnarpPH64MRNv5RUK+l+MzvfzKaEr/0fzrmdzT2hmc0ws+vN7AIzGyTp7xU06bi/CIAo0LKcoWVAhGhZztAyICJ0LGfoGBAhWpYztAwtwgIICuHbyvpZc869IumrkuZK2qAgEv+Sw9dcEF42KDgp0qXp1V/nXHolup2k/1HwOYl3Kjg8LtXkszXvLkk/kXRH+DzTw9faloN9eJ9zzik4WdLvJS2R9KqkByUNUrB63ZQfSXpA0lIFvxAOSVqpIMCt9R1JX1DwGYh/L+nvnHMvh7PVSLpYwS/K5yQ9JOlJSdee4Dn3KTi501MKVsa/Ej7v5pOYDygEWnaKaBkQC7TsFNEyIHJ07BTRMSAWaNkpomVoKQt+VgAknZkVKYjnb5xz34p6HgA4GbQMQBLQMgC+o2MAkoCWtQ2cBB1IqPBQuU9LWi2pg4JV5sGSfhXlXADQGrQMQBLQMgC+o2MAkoCWtU18BBaQXClJ/6DgULs1kkZK+qvwkEoA8AUtA5AEtAyA7+gYgCSgZW0QH4EFAAAAAAAAAAAShyNAAAAAAAAAAABA4rAAAgAAAAAAAAAAEocFEAAAAAAAAAAAkDgsgAAAAAAAAAAAgMRhAQQAAAAAAAAAACQOCyAAAAAAAAAAACBxWAABAAAAAAAAAACJwwIIAAAAAAAAAABIHBZAAAAAAAAAAABA4rAAAgAAAAAAAAAAEocFEAAAAAAAAAAAkDgsgAAAAAAAAAAAgMRhAQQAAAAAAAAAACQOCyAAAAAAAAAAACBx/j9uakWgQmZLCwAAAABJRU5ErkJggg=="]}}],"execution_count":62},{"cell_type":"markdown","source":["### Binary prediction"],"metadata":{}},{"cell_type":"code","source":["args = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_binary.csv'),\n                          '--checkpoint_path',os.path.join(CHEMPROP_DIR,'JAK','checkpoints','binary-4x','model','fold_0/model_0/model.pt'),\n                          '--preds_path',os.path.join(CHEMPROP_DIR,'JAK','binary-4x','model','fold_0/model_0/test_preds.csv')])\nmodify_predict_args(args)\nmake_predictions(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading training args\nLoading data\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 3290.06it/s]\nValidating SMILES\nTest size = 183\nPredicting with an ensemble of 1 models\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 28.53it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 30.98it/s]\r100%|██████████| 1/1 [00:02&lt;00:00,  2.31s/it]\nSaving predictions to /dbfs/FileStore/chemprop/JAK/binary-4x/model/fold_0/model_0/test_preds.csv\nOut[26]: \n[[0.0005128700868226588,\n  0.00015196736785583198,\n  1.6617885876257787e-06,\n  5.619858711725101e-07],\n [0.2190067172050476,\n  0.0390586331486702,\n  0.001957789994776249,\n  0.00018602256022859365],\n [0.7969126105308533,\n  0.6684944033622742,\n  0.015236832201480865,\n  0.004957668017596006],\n [0.9132694005966187,\n  0.9217473864555359,\n  0.21086424589157104,\n  0.0064592077396810055],\n [0.0068490272387862206,\n  0.0032943019177764654,\n  3.339870090712793e-05,\n  8.165388862835243e-06],\n [0.010779643431305885,\n  0.01954513229429722,\n  0.0024906371254473925,\n  0.0001404148933943361],\n [0.0209833774715662,\n  0.032631948590278625,\n  0.008278336375951767,\n  0.0005981403519399464],\n [0.7709829807281494,\n  0.7956750392913818,\n  0.029872728511691093,\n  0.004681069403886795],\n [0.9975852966308594,\n  0.989043116569519,\n  0.640504777431488,\n  0.2239779829978943],\n [0.6351542472839355,\n  0.5888552665710449,\n  0.045101016759872437,\n  0.0048145754262804985],\n [0.00043367242324166,\n  0.0001614002976566553,\n  6.930392828508047e-07,\n  1.3103937135383603e-07],\n [0.667460024356842,\n  0.7710250020027161,\n  0.02485506236553192,\n  0.001978896325454116],\n [0.1542903035879135,\n  0.28968629240989685,\n  0.01326699461787939,\n  0.0015456480905413628],\n [0.24233011901378632,\n  0.041341666132211685,\n  0.0024679629132151604,\n  0.0010113909374922514],\n [0.0025381080340594053,\n  0.002432985929772258,\n  1.880215495475568e-05,\n  5.861415957042482e-06],\n [0.04648199304938316,\n  0.06804268807172775,\n  0.0009621008648537099,\n  0.00010764262697193772],\n [0.1553841233253479,\n  0.09932295978069305,\n  0.0019962205551564693,\n  0.0003539227764122188],\n [0.7816954255104065,\n  0.801270604133606,\n  0.06082118675112724,\n  0.006340498570352793],\n [0.20430868864059448,\n  0.121622733771801,\n  0.0020100742112845182,\n  0.0011937017552554607],\n [0.007352146785706282,\n  0.019477542489767075,\n  0.00015090777014847845,\n  5.356611291063018e-05],\n [0.8355287909507751,\n  0.872798502445221,\n  0.3381950557231903,\n  0.01699880138039589],\n [0.019772401079535484,\n  0.031587857753038406,\n  0.0005274334107525647,\n  3.98871889046859e-05],\n [0.20347179472446442,\n  0.13551613688468933,\n  0.0035786700900644064,\n  0.0005206021014600992],\n [0.9850760102272034,\n  0.911613404750824,\n  0.15694186091423035,\n  0.032941609621047974],\n [0.16129255294799805,\n  0.11265905946493149,\n  0.001148352399468422,\n  3.853790258290246e-05],\n [0.992225170135498,\n  0.9729312062263489,\n  0.4504210352897644,\n  0.07195060700178146],\n [0.00023929160670377314,\n  0.00041510461596772075,\n  3.2197581276705023e-06,\n  3.0796235250818427e-07],\n [0.06870619207620621,\n  0.07917722314596176,\n  0.00046444189501926303,\n  0.0001055387983797118],\n [0.009578248485922813,\n  0.005382736679166555,\n  4.994314076611772e-05,\n  7.808296686562244e-06],\n [0.0006946129142306745,\n  0.0003659510985016823,\n  3.980704605055507e-06,\n  1.8052396626444533e-06],\n [0.8530596494674683,\n  0.559244692325592,\n  0.02397686056792736,\n  0.006184426136314869],\n [0.8516318798065186,\n  0.8481366038322449,\n  0.02086547762155533,\n  0.001028398983180523],\n [0.14614692330360413,\n  0.18251767754554749,\n  0.0010217742528766394,\n  0.00012801392585970461],\n [0.7970317006111145,\n  0.8327784538269043,\n  0.09064248949289322,\n  0.0014701106119900942],\n [0.0921485647559166,\n  0.030589520931243896,\n  0.005747768562287092,\n  0.00013855643919669092],\n [0.9011344313621521,\n  0.6716132164001465,\n  0.15682294964790344,\n  0.01986788958311081],\n [0.8686853051185608,\n  0.9514350891113281,\n  0.42577409744262695,\n  0.0519477017223835],\n [0.05043697729706764,\n  0.039586931467056274,\n  0.0011390168219804764,\n  0.00021739160001743585],\n [0.5971553325653076,\n  0.7702434062957764,\n  0.7335339188575745,\n  0.27015382051467896],\n [0.0038505608681589365,\n  0.0009923161705955863,\n  6.286671850830317e-05,\n  2.710709622988361e-06],\n [0.5318904519081116,\n  0.4133106768131256,\n  0.06843617558479309,\n  0.010801831260323524],\n [0.2552424669265747,\n  0.2063358724117279,\n  0.0244753398001194,\n  0.0012041630689054728],\n [0.9976811408996582,\n  0.9975411891937256,\n  0.2642969489097595,\n  0.012251455336809158],\n [0.11756472289562225,\n  0.035822752863168716,\n  0.00026581736165098846,\n  0.00014365876268129796],\n [0.13318857550621033,\n  0.08520115911960602,\n  0.008679307997226715,\n  0.00019513102597557008],\n [0.029679613187909126,\n  0.011958934366703033,\n  0.00011855577031383291,\n  3.855874456348829e-05],\n [0.0009744048584252596,\n  0.0013000824255868793,\n  2.117331314366311e-05,\n  3.580318207241362e-06],\n [3.5025157558266073e-05,\n  3.055644265259616e-05,\n  8.041590149332478e-08,\n  5.831723104421371e-09],\n [0.3261151611804962,\n  0.46074366569519043,\n  0.002599346451461315,\n  0.0005442620604299009],\n [0.9872402548789978,\n  0.8953063488006592,\n  0.3267711102962494,\n  0.2888937294483185],\n [0.009022488258779049,\n  0.009583610109984875,\n  0.00010851554543478414,\n  1.45417316161911e-05],\n [0.8931145668029785,\n  0.9460121989250183,\n  0.1304783821105957,\n  0.022222895175218582],\n [0.2806032598018646,\n  0.015351408161222935,\n  0.0016108066774904728,\n  0.00026281148893758655],\n [0.6098816990852356,\n  0.6494945883750916,\n  0.003367129247635603,\n  0.0004612219054251909],\n [0.09767995774745941,\n  0.21136373281478882,\n  0.003254069946706295,\n  0.00035933955223299563],\n [0.9602718949317932,\n  0.9376988410949707,\n  0.3694846034049988,\n  0.12080834805965424],\n [0.9906166195869446,\n  0.9915512800216675,\n  0.14383800327777863,\n  0.00568817974999547],\n [0.2752811014652252,\n  0.26282787322998047,\n  0.0733412429690361,\n  0.008248395286500454],\n [0.9962849617004395,\n  0.9941535592079163,\n  0.4052788019180298,\n  0.045691054314374924],\n [0.15917269885540009,\n  0.07920511066913605,\n  0.0010542173404246569,\n  0.00025021348847076297],\n [0.0008808231796137989,\n  0.0014607161283493042,\n  2.302354187122546e-05,\n  2.0880595457128948e-06],\n [0.009495203383266926,\n  0.0015549412928521633,\n  0.00010349623335059732,\n  5.973045335849747e-05],\n [0.0011198673164471984,\n  0.0006675001932308078,\n  3.1392752134706825e-05,\n  7.623736451023433e-07],\n [0.7274923324584961,\n  0.7812238335609436,\n  0.1413058340549469,\n  0.015743574127554893],\n [0.0014336577150970697,\n  0.0009007881744764745,\n  5.548168337554671e-05,\n  9.289591389460838e-07],\n [0.003492747200652957,\n  0.004171644803136587,\n  6.480228330474347e-05,\n  3.772920308620087e-06],\n [0.9416600465774536,\n  0.4143313765525818,\n  0.11921735107898712,\n  0.014754648320376873],\n [0.3017578423023224,\n  0.59160977602005,\n  0.0036040497943758965,\n  0.0009215082391165197],\n [0.9173768758773804,\n  0.5722650289535522,\n  0.04924463853240013,\n  0.00505867600440979],\n [0.9798417091369629,\n  0.6506170034408569,\n  0.18359176814556122,\n  0.2042074352502823],\n [0.9979981780052185,\n  0.9749294519424438,\n  0.4932149648666382,\n  0.5530668497085571],\n [0.005737640894949436,\n  0.007501565385609865,\n  4.85671353089856e-06,\n  3.055657487038843e-07],\n [0.0005105245509184897,\n  0.0002841511450242251,\n  1.0284392374160234e-05,\n  3.838974294012587e-07],\n [0.06114727631211281,\n  0.13931240141391754,\n  0.0024168279487639666,\n  4.7356148570543155e-05],\n [0.06474995613098145,\n  0.16624240577220917,\n  0.001139732077717781,\n  0.00026432174490764737],\n [0.9516798853874207,\n  0.9544437527656555,\n  0.09589216858148575,\n  0.00415576808154583],\n [0.29802176356315613,\n  0.21417517960071564,\n  0.16905663907527924,\n  0.02090834267437458],\n [0.9962462782859802,\n  0.9796209931373596,\n  0.7287138104438782,\n  0.8202255964279175],\n [0.001124185393564403,\n  0.0002929871261585504,\n  2.269447804792435e-06,\n  4.019099151264527e-07],\n [0.04013446718454361,\n  0.02736816741526127,\n  5.6723809393588454e-05,\n  7.0034370764915366e-06],\n [1.1654733498289715e-06,\n  1.1658279390758253e-06,\n  5.487607590559662e-10,\n  2.115377664702045e-11],\n [0.8983191251754761,\n  0.8998264670372009,\n  0.06768407672643661,\n  0.011674453504383564],\n [0.10703153908252716,\n  0.05949171632528305,\n  0.0010536335175856948,\n  0.00022265168081503361],\n [0.7432904243469238,\n  0.8933400511741638,\n  0.012947294861078262,\n  0.0016902077477425337],\n [0.8609629273414612,\n  0.9351510405540466,\n  0.12643685936927795,\n  0.004103593062609434],\n [0.0036180049646645784,\n  0.00370068964548409,\n  3.926230419892818e-05,\n  5.883841367904097e-06],\n [0.9654273390769958,\n  0.7216220498085022,\n  0.09982811659574509,\n  0.022233108058571815],\n [0.8180095553398132,\n  0.8241989612579346,\n  0.04891841858625412,\n  0.0022740154527127743],\n [0.035119857639074326,\n  0.18847490847110748,\n  0.0007644821307621896,\n  7.666568126296625e-05],\n [0.00043143078801222146,\n  0.000586732174269855,\n  3.88887065128074e-06,\n  4.2694929902609147e-07],\n [0.012209748849272728,\n  0.020947473123669624,\n  3.080684473388828e-05,\n  2.615462790345191e-06],\n [0.9550468325614929,\n  0.8236004114151001,\n  0.041318636387586594,\n  0.014714019373059273],\n [0.001048645586706698,\n  0.0002989592030644417,\n  3.5534014841687167e-06,\n  1.5029042970127193e-06],\n [0.9589725732803345,\n  0.8879530429840088,\n  0.16913917660713196,\n  0.02457354962825775],\n [0.2071690857410431,\n  0.338531494140625,\n  0.001997521612793207,\n  0.00024594334536232054],\n [0.014122957363724709,\n  0.12104687094688416,\n  0.00020515453070402145,\n  2.4733140890020877e-05],\n [0.04347566142678261,\n  0.007149223238229752,\n  0.0006339139654301107,\n  0.0001389022363582626],\n [1.7795175153878517e-05,\n  2.1487117919605225e-05,\n  1.9929942141061474e-07,\n  2.3964098261330946e-08],\n [0.966242253780365,\n  0.9699215292930603,\n  0.5723778605461121,\n  0.02516322210431099],\n [0.003147175768390298,\n  0.002167105209082365,\n  4.958669524057768e-05,\n  4.173054549028166e-06],\n [0.005519702564924955,\n  0.010720734484493732,\n  0.0001716376282274723,\n  3.96156137867365e-05],\n [0.0022439269814640284,\n  0.005263038910925388,\n  5.226612483966164e-05,\n  9.090282219403889e-06],\n [0.0034961497876793146,\n  0.0011358140036463737,\n  4.184364570392063e-06,\n  7.838519309188996e-07],\n [0.9934744238853455,\n  0.917751133441925,\n  0.28250470757484436,\n  0.09646067023277283],\n [0.03436947241425514,\n  0.03853459283709526,\n  0.0008689876995049417,\n  6.496129208244383e-05],\n [0.9668558835983276,\n  0.9156597256660461,\n  0.4149194359779358,\n  0.026352539658546448],\n [0.6212486624717712,\n  0.7618751525878906,\n  0.04808693751692772,\n  0.005489477422088385],\n [0.701974630355835,\n  0.3141535520553589,\n  0.03502073138952255,\n  0.00440807081758976],\n [0.00023600010899826884,\n  0.0007107601268216968,\n  2.000464064622065e-06,\n  1.5343610471063585e-07],\n [0.21333454549312592,\n  0.21486914157867432,\n  0.04528362303972244,\n  0.0035154547076672316],\n [0.1468396782875061,\n  0.3534112572669983,\n  0.0022657359950244427,\n  0.0003971567202825099],\n [0.9641147255897522,\n  0.8718898892402649,\n  0.17952795326709747,\n  0.057734839618206024],\n [0.15379640460014343,\n  0.1765819489955902,\n  0.001562406774610281,\n  0.00028933462454006076],\n [0.0010405266657471657,\n  0.0008266301010735333,\n  1.1183274182258174e-06,\n  7.867647155990198e-08],\n [0.012630442157387733,\n  0.007218725513666868,\n  0.0008304647635668516,\n  1.7085238141589798e-05],\n [0.9981837868690491,\n  0.9978475570678711,\n  0.8491082787513733,\n  0.08603665977716446],\n [0.0008766375831328332,\n  0.0015296121127903461,\n  1.931354017870035e-05,\n  2.9756606636510696e-06],\n [0.005388535093516111,\n  0.01166345551609993,\n  1.5542207620455883e-05,\n  1.4339173048938392e-06],\n [0.07683809846639633,\n  0.020240576937794685,\n  0.00011736078886315227,\n  3.560222830856219e-05],\n [0.8085939288139343,\n  0.6544232368469238,\n  0.28058895468711853,\n  0.023374782875180244],\n [0.7759913802146912,\n  0.7991362810134888,\n  0.016970904543995857,\n  0.0015147777739912271],\n [0.009967755526304245,\n  0.040873296558856964,\n  0.0001962803362403065,\n  3.0277018595370464e-05],\n [6.609738920815289e-05,\n  3.868525891448371e-05,\n  1.966702569689005e-07,\n  1.734534293973411e-08],\n [6.968660272832494e-06,\n  5.649614195135655e-06,\n  3.366535139548432e-08,\n  4.720430712978896e-09],\n [0.0925929844379425,\n  0.1148565486073494,\n  0.001056417590007186,\n  0.00014168345660436898],\n [0.017095377668738365,\n  0.02821635641157627,\n  0.00042123181628994644,\n  4.7287685447372496e-05],\n [0.204363614320755,\n  0.08693938702344894,\n  0.002479707356542349,\n  0.0012938928557559848],\n [0.4561876356601715,\n  0.7114863395690918,\n  0.010722004808485508,\n  0.0020848303101956844],\n [0.16930200159549713,\n  0.10469014942646027,\n  0.0013463270151987672,\n  0.0006316849030554295],\n [0.9118702411651611,\n  0.6096407175064087,\n  0.11381200700998306,\n  0.016745958477258682],\n [0.8647555708885193,\n  0.7948800921440125,\n  0.3474773168563843,\n  0.13237763941287994],\n [0.19859066605567932,\n  0.1058819368481636,\n  0.007716338615864515,\n  0.0006346160662360489],\n [6.556422886205837e-05,\n  1.5908073692116886e-05,\n  4.308960654952898e-08,\n  5.132254177198092e-09],\n [0.8450676798820496,\n  0.9761962890625,\n  0.5702946186065674,\n  0.04636228829622269],\n [0.9675268530845642,\n  0.9521663188934326,\n  0.05081653222441673,\n  0.0014815586619079113],\n [0.847735583782196,\n  0.8472582101821899,\n  0.2944056987762451,\n  0.04194602370262146],\n [0.03896393999457359,\n  0.03739127516746521,\n  0.0007459614425897598,\n  0.0001379581808578223],\n [0.22048243880271912,\n  0.47836899757385254,\n  0.004216220695525408,\n  0.0007501531508751214],\n [0.00925233494490385,\n  0.0023991249036043882,\n  9.434815183340106e-06,\n  1.7766809605745948e-06],\n [0.04943989962339401,\n  0.03344741463661194,\n  0.0014269024832174182,\n  2.1307427232386544e-05],\n [0.0909196063876152,\n  0.02555338479578495,\n  0.0005692447884939611,\n  0.00028411432867869735],\n [0.000571899232454598,\n  0.0004021818458568305,\n  3.533045628500986e-06,\n  3.5821793176182837e-07],\n [0.0003895804693456739,\n  0.00024685083189979196,\n  2.2444129399445956e-07,\n  1.814693284529767e-08],\n [0.876437246799469,\n  0.49019837379455566,\n  0.02707543969154358,\n  0.010723421350121498],\n [0.9389294385910034,\n  0.8942236304283142,\n  0.4579833447933197,\n  0.08526529371738434],\n [0.0077377790585160255,\n  0.016119368374347687,\n  0.00020245522318873554,\n  2.702156052691862e-05],\n [0.8775098323822021,\n  0.7303430438041687,\n  0.017829816788434982,\n  0.0028466794174164534],\n [1.3999448356116773e-06,\n  9.010892085825617e-07,\n  2.9710232052337915e-09,\n  2.756833095141076e-10],\n [0.003979912493377924,\n  0.009259365499019623,\n  0.00010870144615182653,\n  1.4207260392140597e-05],\n [0.2546599507331848,\n  0.2517198920249939,\n  0.00896687526255846,\n  0.0012589787365868688],\n [0.09108123183250427,\n  0.09465306252241135,\n  0.01562067586928606,\n  0.0003503114858176559],\n [0.9077129364013672,\n  0.9566197395324707,\n  0.06554350256919861,\n  0.006408969406038523],\n [0.0008339026826433837,\n  0.00045636791037395597,\n  1.2412657497407054e-06,\n  1.6968205329703778e-07],\n [0.28741419315338135,\n  0.5734515190124512,\n  0.007863686420023441,\n  0.0015993035631254315],\n [0.9964105486869812,\n  0.969061017036438,\n  0.42321470379829407,\n  0.3897029757499695],\n [0.0017493831692263484,\n  0.004434716887772083,\n  2.8168884455226362e-05,\n  3.006602355526411e-06],\n [0.992251455783844,\n  0.988768458366394,\n  0.09123384207487106,\n  0.004413783084601164],\n [0.9701271653175354,\n  0.9531668424606323,\n  0.7495959997177124,\n  0.05514651536941528],\n [0.18693818151950836,\n  0.2320929765701294,\n  0.002459866926074028,\n  8.895453356672078e-05],\n [0.1308300942182541,\n  0.0449492447078228,\n  0.00040677955257706344,\n  0.00016101155779324472],\n [0.8881925940513611,\n  0.6993701457977295,\n  0.31086403131484985,\n  0.07264798134565353],\n [0.0005732313147746027,\n  5.2652023441623896e-05,\n  5.149227945366874e-06,\n  1.3817558510709205e-06],\n [0.0008320287452079356,\n  0.0003511639661155641,\n  2.4452099296468077e-06,\n  1.7413636044238956e-07],\n [0.021377500146627426,\n  0.024476129561662674,\n  0.00015787914162501693,\n  2.6074087145389058e-05],\n [0.03255200386047363,\n  0.07786592841148376,\n  0.002505009062588215,\n  0.0002974396338686347],\n [0.0046516843140125275,\n  0.0040947552770376205,\n  4.2477033275645226e-05,\n  6.83498319631326e-06],\n [0.0010301542934030294,\n  0.00046805539750494063,\n  2.166774493161938e-06,\n  2.8194205015097396e-07],\n [0.359958678483963,\n  0.7420966029167175,\n  0.16895553469657898,\n  0.018408728763461113],\n [0.29669567942619324,\n  0.17352639138698578,\n  0.0029881272930651903,\n  0.0007027026149444282],\n [0.00020345801021903753,\n  1.5645427993149497e-05,\n  8.405470452998998e-07,\n  1.6650471934553934e-07],\n [0.006486634723842144,\n  0.003913953434675932,\n  3.868791463901289e-05,\n  8.834720574668609e-06],\n [0.00031243942794390023,\n  0.000691789377015084,\n  3.7178997445153072e-06,\n  3.878988650285464e-07],\n [0.03581572696566582,\n  0.07052983343601227,\n  0.0011266532819718122,\n  0.00013830749958287925],\n [0.03424389287829399,\n  0.06960931420326233,\n  0.0008271497208625078,\n  0.00017475766071584076],\n [0.00369649869389832,\n  0.0014409071300178766,\n  7.858273420424666e-06,\n  1.8210336065749289e-06],\n [0.6110234260559082,\n  0.8322404026985168,\n  0.02375626377761364,\n  0.0022657273802906275],\n [0.9798810482025146,\n  0.8780339956283569,\n  0.510888934135437,\n  0.09415782988071442],\n [0.4303606152534485,\n  0.8479886054992676,\n  0.04176010191440582,\n  0.00495542399585247],\n [0.9739589095115662,\n  0.9835395812988281,\n  0.9348623752593994,\n  0.5120860934257507],\n [0.9976755976676941,\n  0.9938094019889832,\n  0.9593913555145264,\n  0.42787572741508484],\n [0.5496093034744263,\n  0.3389388918876648,\n  0.009394379332661629,\n  0.001319771632552147],\n [0.10170825570821762,\n  0.24033191800117493,\n  0.00039841642137616873,\n  3.274801929364912e-05],\n [0.12833315134048462,\n  0.22317497432231903,\n  0.0008113825460895896,\n  6.924480840098113e-05]]\n</div>"]}}],"execution_count":64},{"cell_type":"code","source":["args = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                          '--checkpoint_path',\n                          os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext','fold_0/model_0/model.pt'),\n                          '--preds_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext','fold_0/model_0/test_preds_bin76_ext.csv')])\nmodify_predict_args(args)\nmake_predictions(args)\nargs = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                          '--checkpoint_path',\n                          os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_int','fold_0/model_0/model.pt'),\n                          '--preds_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_int','fold_0/model_0/test_preds_bin76_int.csv')])\nmodify_predict_args(args)\nmake_predictions(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading training args\nLoading data\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 3091.79it/s]\nValidating SMILES\nTest size = 183\nPredicting with an ensemble of 1 models\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 41.63it/s]\r100%|██████████| 1/1 [00:01&lt;00:00,  1.44s/it]\nSaving predictions to /dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_ext/fold_0/model_0/test_preds_bin76_ext.csv\nLoading training args\nLoading data\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 3126.37it/s]\nValidating SMILES\nTest size = 183\nPredicting with an ensemble of 1 models\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 44.14it/s]\r100%|██████████| 1/1 [00:01&lt;00:00,  1.64s/it]\nSaving predictions to /dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_int/fold_0/model_0/test_preds_bin76_int.csv\nOut[17]: \n[[0.06014738231897354,\n  0.030724361538887024,\n  0.0009406624594703317,\n  8.45462091092486e-06],\n [0.772223711013794,\n  0.585080623626709,\n  0.11495141685009003,\n  0.007724123541265726],\n [0.9712619781494141,\n  0.9433736801147461,\n  0.4703846275806427,\n  0.08065924048423767],\n [0.9421650767326355,\n  0.9384544491767883,\n  0.7171423435211182,\n  0.2871761918067932],\n [0.09993401169776917,\n  0.04339313879609108,\n  0.0037698058877140284,\n  7.707404438406229e-05],\n [0.38971561193466187,\n  0.6626189947128296,\n  0.17335286736488342,\n  0.025110894814133644],\n [0.2593590319156647,\n  0.46042537689208984,\n  0.07604837417602539,\n  0.00885128602385521],\n [0.887377917766571,\n  0.886754035949707,\n  0.0908963605761528,\n  0.015149186365306377],\n [0.988902747631073,\n  0.9753164649009705,\n  0.7379891276359558,\n  0.40454190969467163],\n [0.9516304135322571,\n  0.9390068054199219,\n  0.2928336560726166,\n  0.08970867097377777],\n [0.046683963388204575,\n  0.02379547618329525,\n  0.0007168896845541894,\n  5.867109848622931e-06],\n [0.9425426721572876,\n  0.9389627575874329,\n  0.217233344912529,\n  0.06679137051105499],\n [0.42954370379447937,\n  0.4379112124443054,\n  0.050689391791820526,\n  0.002239322755485773],\n [0.9014804363250732,\n  0.8011505007743835,\n  0.11992793530225754,\n  0.01878935471177101],\n [0.3793914020061493,\n  0.3471786379814148,\n  0.00773630989715457,\n  0.0004238924302626401],\n [0.2612203061580658,\n  0.27962028980255127,\n  0.02337951585650444,\n  0.0007447936804965138],\n [0.41220623254776,\n  0.4018276035785675,\n  0.018349600955843925,\n  0.001066876808181405],\n [0.9291411638259888,\n  0.9084290266036987,\n  0.2941938638687134,\n  0.031885914504528046],\n [0.40743905305862427,\n  0.2920627295970917,\n  0.016520945355296135,\n  0.0010447034146636724],\n [0.19001919031143188,\n  0.21801689267158508,\n  0.0053452784195542336,\n  0.0002000641979975626],\n [0.9832509160041809,\n  0.9885502457618713,\n  0.9493700265884399,\n  0.8415719270706177],\n [0.06651124358177185,\n  0.04974551126360893,\n  0.002192542189732194,\n  1.1601034202612936e-05],\n [0.6125721335411072,\n  0.6621711254119873,\n  0.012666256166994572,\n  0.0010945700341835618],\n [0.9912341237068176,\n  0.9828788042068481,\n  0.7970808744430542,\n  0.5707358717918396],\n [0.15438628196716309,\n  0.2523336410522461,\n  0.0026189310010522604,\n  0.00010658720566425472],\n [0.9828224778175354,\n  0.9693312048912048,\n  0.6617459654808044,\n  0.3377903699874878],\n [0.04566917568445206,\n  0.08726557344198227,\n  0.000789688783697784,\n  1.1147272743983194e-05],\n [0.6861124038696289,\n  0.6665822267532349,\n  0.026115622371435165,\n  0.003282550722360611],\n [0.08244436234235764,\n  0.04506472498178482,\n  0.004557876847684383,\n  4.1953007894335315e-05],\n [0.03195923939347267,\n  0.018832333385944366,\n  0.00030174656421877444,\n  1.4992210708442144e-06],\n [0.9647533893585205,\n  0.9321601390838623,\n  0.33985576033592224,\n  0.090879887342453],\n [0.9203800559043884,\n  0.8975135087966919,\n  0.27129635214805603,\n  0.07864350080490112],\n [0.8493413329124451,\n  0.8275324702262878,\n  0.09530819207429886,\n  0.021571509540081024],\n [0.7532939910888672,\n  0.7429472208023071,\n  0.17302103340625763,\n  0.044421128928661346],\n [0.36274436116218567,\n  0.2544756829738617,\n  0.08003945648670197,\n  0.008650951087474823],\n [0.9491993188858032,\n  0.8891228437423706,\n  0.4255569279193878,\n  0.054418954998254776],\n [0.9640703201293945,\n  0.9761006236076355,\n  0.8271608352661133,\n  0.622519314289093],\n [0.42914459109306335,\n  0.35656389594078064,\n  0.032157912850379944,\n  0.0025636896025389433],\n [0.9359571933746338,\n  0.9532763361930847,\n  0.8543822169303894,\n  0.7154676914215088],\n [0.533280074596405,\n  0.6274579167366028,\n  0.4016267955303192,\n  0.05241883546113968],\n [0.8398125767707825,\n  0.8179045915603638,\n  0.06977588683366776,\n  0.011460809968411922],\n [0.8712512850761414,\n  0.885392963886261,\n  0.15249651670455933,\n  0.04229110851883888],\n [0.982475221157074,\n  0.97175532579422,\n  0.6925169229507446,\n  0.34360793232917786],\n [0.37982678413391113,\n  0.2636636197566986,\n  0.018097182735800743,\n  0.0010541941737756133],\n [0.6291773915290833,\n  0.5953206419944763,\n  0.11241055279970169,\n  0.017713652923703194],\n [0.2771179676055908,\n  0.1946529746055603,\n  0.005687444005161524,\n  0.00021999447199050337],\n [0.04423996061086655,\n  0.04555153846740723,\n  0.0006617242470383644,\n  6.4815726545930374e-06],\n [0.009445663541555405,\n  0.00766723183915019,\n  6.935219425940886e-05,\n  1.629974804018275e-07],\n [0.9384801983833313,\n  0.9327628016471863,\n  0.16441047191619873,\n  0.0461372472345829],\n [0.9927777647972107,\n  0.9856681823730469,\n  0.8591563701629639,\n  0.6890744566917419],\n [0.193593829870224,\n  0.14030522108078003,\n  0.011557675898075104,\n  0.00017467736324761063],\n [0.9672036170959473,\n  0.9759263396263123,\n  0.7271115779876709,\n  0.4940541684627533],\n [0.8980849981307983,\n  0.7826200127601624,\n  0.39617642760276794,\n  0.10295504331588745],\n [0.8092436194419861,\n  0.8045105934143066,\n  0.06542662531137466,\n  0.01551795657724142],\n [0.6767072081565857,\n  0.7190974950790405,\n  0.030307471752166748,\n  0.00364530086517334],\n [0.9855475425720215,\n  0.9823670387268066,\n  0.8817753791809082,\n  0.7028970122337341],\n [0.9903619289398193,\n  0.9833362102508545,\n  0.8781236410140991,\n  0.569454550743103],\n [0.8387079834938049,\n  0.8859148621559143,\n  0.6546576619148254,\n  0.3522200584411621],\n [0.9861519932746887,\n  0.9766895771026611,\n  0.8483988642692566,\n  0.5658085942268372],\n [0.516409695148468,\n  0.4064866602420807,\n  0.027487847954034805,\n  0.0027104972396045923],\n [0.24099749326705933,\n  0.27562394738197327,\n  0.019454937428236008,\n  0.00042905917507596314],\n [0.20705591142177582,\n  0.1006726399064064,\n  0.005402427166700363,\n  0.00021546425705309957],\n [0.14524464309215546,\n  0.08782924711704254,\n  0.013618849217891693,\n  0.0005258980672806501],\n [0.9167450666427612,\n  0.9561077952384949,\n  0.6135839223861694,\n  0.3273980915546417],\n [0.22208271920681,\n  0.15294025838375092,\n  0.021274607628583908,\n  0.0009601735509932041],\n [0.23783907294273376,\n  0.32692334055900574,\n  0.00894844438880682,\n  0.0004372554540168494],\n [0.9369467496871948,\n  0.8453474640846252,\n  0.3694789707660675,\n  0.0460687130689621],\n [0.8563119173049927,\n  0.8717126846313477,\n  0.05867823585867882,\n  0.011408003978431225],\n [0.947235107421875, 0.872198760509491, 0.373898446559906, 0.0470363087952137],\n [0.9808926582336426,\n  0.9602463841438293,\n  0.7107653617858887,\n  0.4900933802127838],\n [0.994124710559845,\n  0.9871189594268799,\n  0.871228039264679,\n  0.7008625268936157],\n [0.1420278549194336,\n  0.2655298113822937,\n  0.0011577996192499995,\n  3.7299720133887604e-05],\n [0.04884258285164833,\n  0.019458366557955742,\n  0.0028028935194015503,\n  1.4933480997569859e-05],\n [0.7703655958175659,\n  0.7605205178260803,\n  0.1102675348520279,\n  0.021202493458986282],\n [0.5616035461425781,\n  0.6258034110069275,\n  0.008894335478544235,\n  0.0005991568323224783],\n [0.9817095398902893,\n  0.9788690209388733,\n  0.8582623600959778,\n  0.608792245388031],\n [0.6106293201446533,\n  0.6609917283058167,\n  0.4349607527256012,\n  0.13795354962348938],\n [0.9805456399917603,\n  0.961348295211792,\n  0.7726927995681763,\n  0.5883945226669312],\n [0.13806936144828796,\n  0.0667119026184082,\n  0.0058293454349040985,\n  0.0001530093140900135],\n [0.7996097803115845,\n  0.7518581748008728,\n  0.05795105919241905,\n  0.010514768771827221],\n [0.0056855021975934505,\n  0.005345066078007221,\n  5.022282857680693e-05,\n  8.153361363838485e-08],\n [0.9791373014450073,\n  0.97266685962677,\n  0.4211399257183075,\n  0.19418711960315704],\n [0.8671732544898987,\n  0.7887467741966248,\n  0.1354653239250183,\n  0.02338278293609619],\n [0.9320903420448303,\n  0.9375734925270081,\n  0.15594244003295898,\n  0.0411415733397007],\n [0.9350661039352417,\n  0.927278459072113,\n  0.35396870970726013,\n  0.14043410122394562],\n [0.1983611285686493,\n  0.1691714972257614,\n  0.005147336050868034,\n  0.00012991110270377249],\n [0.9810962080955505,\n  0.9670625329017639,\n  0.7083765864372253,\n  0.40940842032432556],\n [0.8539331555366516,\n  0.8456082344055176,\n  0.3200133442878723,\n  0.08018969744443893],\n [0.33719944953918457,\n  0.3738754391670227,\n  0.023528452962636948,\n  0.0007342429016716778],\n [0.02164902538061142,\n  0.010761095210909843,\n  0.0005197310238145292,\n  1.105501951315091e-06],\n [0.4656215012073517,\n  0.4197850227355957,\n  0.01792849972844124,\n  0.0007843901985324919],\n [0.9870886206626892,\n  0.9729500412940979,\n  0.6949220299720764,\n  0.20905208587646484],\n [0.06354895979166031,\n  0.033087875694036484,\n  0.0008968081674538553,\n  8.36545859783655e-06],\n [0.9848212599754333, 0.9681861996650696, 0.5569362044334412, 0.2364501953125],\n [0.7184889316558838,\n  0.7580704092979431,\n  0.023920170962810516,\n  0.003088611178100109],\n [0.2398890107870102,\n  0.34878429770469666,\n  0.00812277290970087,\n  0.0004214289947412908],\n [0.43157315254211426,\n  0.3264963924884796,\n  0.027413321658968925,\n  0.0015636441530659795],\n [0.2377908080816269,\n  0.27009791135787964,\n  0.006470508873462677,\n  0.0002947550092358142],\n [0.9407446980476379,\n  0.9259023070335388,\n  0.5733290910720825,\n  0.2955540716648102],\n [0.05965249612927437,\n  0.03354137763381004,\n  0.002380040939897299,\n  1.2630569472094066e-05],\n [0.2696070075035095,\n  0.2687402665615082,\n  0.009766687639057636,\n  0.00039509765338152647],\n [0.2220911830663681,\n  0.22555334866046906,\n  0.01741926744580269,\n  0.00038834218867123127],\n [0.17464329302310944,\n  0.12750859558582306,\n  0.005963035859167576,\n  0.0001905286480905488],\n [0.9929090738296509,\n  0.9844642281532288,\n  0.8125011324882507,\n  0.6140776872634888],\n [0.04570192098617554,\n  0.024657873436808586,\n  0.0016338479472324252,\n  6.3703605519549455e-06],\n [0.9719318747520447,\n  0.9485771059989929,\n  0.6862729787826538,\n  0.28007861971855164],\n [0.4877050518989563,\n  0.4547645151615143,\n  0.049237269908189774,\n  0.002210228471085429],\n [0.9305452108383179,\n  0.8519733548164368,\n  0.43551337718963623,\n  0.08088146150112152],\n [0.02173323929309845,\n  0.038709890097379684,\n  0.00025224516866728663,\n  1.8567287725090864e-06],\n [0.6707634329795837,\n  0.8005860447883606,\n  0.4672435224056244,\n  0.182134747505188],\n [0.708682656288147,\n  0.7669649720191956,\n  0.015897078439593315,\n  0.0014434505719691515],\n [0.9831205606460571,\n  0.9658745527267456,\n  0.6001712679862976,\n  0.2648725211620331],\n [0.4447173476219177,\n  0.4962705373764038,\n  0.014967451803386211,\n  0.0010541410883888602],\n [0.5520545244216919,\n  0.517910361289978,\n  0.01687241718173027,\n  0.0017669651424512267],\n [0.3576697111129761,\n  0.21138857305049896,\n  0.04736202210187912,\n  0.003667727578431368],\n [0.9667242765426636,\n  0.9751667976379395,\n  0.8850192427635193,\n  0.6647399067878723],\n [0.05153489485383034,\n  0.053668420761823654,\n  0.0008423628751188517,\n  7.828084562788717e-06],\n [0.504117488861084,\n  0.5219738483428955,\n  0.010135931894183159,\n  0.0009140797192230821],\n [0.2450515329837799,\n  0.14954102039337158,\n  0.010272271931171417,\n  0.000447448663180694],\n [0.8200386166572571,\n  0.8588153123855591,\n  0.6242272257804871,\n  0.30448463559150696],\n [0.9088062644004822,\n  0.9112475514411926,\n  0.09478975087404251,\n  0.0229782797396183],\n [0.2761276066303253,\n  0.37781238555908203,\n  0.006545156240463257,\n  0.00039131479570642114],\n [0.023936398327350616,\n  0.019709479063749313,\n  0.0003681424423120916,\n  2.248992814202211e-06],\n [0.015408115461468697,\n  0.007414242252707481,\n  0.00011322704085614532,\n  2.8479905722633703e-07],\n [0.5945643782615662,\n  0.6614709496498108,\n  0.012962037697434425,\n  0.001148958457633853],\n [0.284557044506073,\n  0.30900242924690247,\n  0.02286043018102646,\n  0.00073898711707443],\n [0.5827468037605286,\n  0.40937453508377075,\n  0.04961100593209267,\n  0.00537382997572422],\n [0.8851991295814514,\n  0.8882241249084473,\n  0.20256754755973816,\n  0.02470785565674305],\n [0.3475074768066406,\n  0.2582719027996063,\n  0.009865565225481987,\n  0.0005275990697555244],\n [0.9683618545532227,\n  0.9238690137863159,\n  0.5902991890907288,\n  0.15166324377059937],\n [0.9606907963752747,\n  0.9078330397605896,\n  0.6099295616149902,\n  0.38299956917762756],\n [0.7593560814857483,\n  0.673478901386261,\n  0.11795369535684586,\n  0.02133002132177353],\n [0.06328301876783371,\n  0.03214364871382713,\n  0.001164688146673143,\n  1.2216908544360194e-05],\n [0.929718554019928,\n  0.9664172530174255,\n  0.8630502820014954,\n  0.6469410061836243],\n [0.8292211890220642,\n  0.8624311089515686,\n  0.2463478147983551,\n  0.05240689963102341],\n [0.7355515360832214,\n  0.833895206451416,\n  0.3359910249710083,\n  0.09814957529306412],\n [0.7165964245796204,\n  0.6756389737129211,\n  0.04141345992684364,\n  0.0059021818451583385],\n [0.5627705454826355,\n  0.6765938401222229,\n  0.010612037032842636,\n  0.0007240259437821805],\n [0.2343786507844925,\n  0.13790331780910492,\n  0.010391647927463055,\n  0.0004193419881630689],\n [0.47733592987060547,\n  0.4039413630962372,\n  0.05105799809098244,\n  0.006557088810950518],\n [0.7639333009719849,\n  0.664669394493103,\n  0.07372298836708069,\n  0.008619161322712898],\n [0.6700636148452759,\n  0.6657237410545349,\n  0.018004395067691803,\n  0.0021488675847649574],\n [0.011169353500008583,\n  0.016831358894705772,\n  0.00010941902291961014,\n  4.360873049336078e-07],\n [0.9521305561065674,\n  0.8838115334510803,\n  0.40907588601112366,\n  0.051215387880802155],\n [0.9768983125686646,\n  0.9764373302459717,\n  0.9086096882820129,\n  0.7672826647758484],\n [0.24307872354984283,\n  0.2733690142631531,\n  0.019119353964924812,\n  0.0004590631870087236],\n [0.9799759387969971,\n  0.9695438742637634,\n  0.5032579898834229,\n  0.15843552350997925],\n [0.016010701656341553,\n  0.006277075503021479,\n  0.00014213337271939963,\n  3.613267551827448e-07],\n [0.04832596704363823,\n  0.07242041826248169,\n  0.0025252599734812975,\n  2.159935684176162e-05],\n [0.5135972499847412,\n  0.5704120993614197,\n  0.012721162289381027,\n  0.001038371934555471],\n [0.8154600262641907,\n  0.8405908942222595,\n  0.2547328472137451,\n  0.08765485882759094],\n [0.9584057331085205,\n  0.9530908465385437,\n  0.44166338443756104,\n  0.09600932151079178],\n [0.09243837743997574,\n  0.04699054732918739,\n  0.005075122695416212,\n  5.0998798542423174e-05],\n [0.8275806903839111,\n  0.8591615557670593,\n  0.04093924164772034,\n  0.004660344682633877],\n [0.9916461110115051,\n  0.9832627773284912,\n  0.8119584918022156,\n  0.5965914130210876],\n [0.2899976670742035,\n  0.35979118943214417,\n  0.03350285440683365,\n  0.0009301140671595931],\n [0.9779608845710754,\n  0.9648943543434143,\n  0.7207516431808472,\n  0.3098468780517578],\n [0.7766321897506714,\n  0.8078106641769409,\n  0.7509661316871643,\n  0.4031423330307007],\n [0.44766440987586975,\n  0.4093446135520935,\n  0.04002465307712555,\n  0.003954400308430195],\n [0.34229734539985657,\n  0.2513517141342163,\n  0.008658311329782009,\n  0.00043080217437818646],\n [0.9445819854736328,\n  0.8752769827842712,\n  0.5044580698013306,\n  0.11975273489952087],\n [0.03385058417916298,\n  0.016310403123497963,\n  0.00031897667213343084,\n  1.5645982784917578e-06],\n [0.10665901750326157,\n  0.06520598381757736,\n  0.0017886057030409575,\n  2.732245411607437e-05],\n [0.36267077922821045,\n  0.38933321833610535,\n  0.00911725964397192,\n  0.000636884942650795],\n [0.14487364888191223,\n  0.14120617508888245,\n  0.009583904407918453,\n  0.00013834561104886234],\n [0.16315478086471558,\n  0.13535811007022858,\n  0.004478913266211748,\n  9.62155609158799e-05],\n [0.10233251750469208,\n  0.04856260493397713,\n  0.0065248324535787106,\n  6.991762347752228e-05],\n [0.43792954087257385,\n  0.6229655742645264,\n  0.16082438826560974,\n  0.028170475736260414],\n [0.4328795373439789,\n  0.32836583256721497,\n  0.0356924831867218,\n  0.0030462632421404123],\n [0.2938305735588074,\n  0.0958985909819603,\n  0.004314634948968887,\n  0.0001019935225485824],\n [0.0981854572892189,\n  0.055191997438669205,\n  0.00270432629622519,\n  5.289691762300208e-05],\n [0.1416272073984146,\n  0.2572459876537323,\n  0.00335891917347908,\n  8.475121285300702e-05],\n [0.06958188116550446,\n  0.09510031342506409,\n  0.004149188287556171,\n  4.442568388185464e-05],\n [0.6159834265708923,\n  0.6424684524536133,\n  0.019931474700570107,\n  0.00213185278698802],\n [0.12436991930007935,\n  0.08746708184480667,\n  0.003781531937420368,\n  8.944710862124339e-05],\n [0.8049258589744568,\n  0.8164607286453247,\n  0.07979843020439148,\n  0.006001040805131197],\n [0.9723951816558838,\n  0.9297746419906616,\n  0.5974791049957275,\n  0.13450726866722107],\n [0.4608042538166046,\n  0.4983782470226288,\n  0.02512495405972004,\n  0.0022418340668082237],\n [0.9807183742523193,\n  0.9860091805458069,\n  0.899640679359436,\n  0.7351741194725037],\n [0.9876970648765564,\n  0.9759253263473511,\n  0.9055836796760559,\n  0.7076981067657471],\n [0.9144569635391235,\n  0.9078881740570068,\n  0.34640297293663025,\n  0.1355348527431488],\n [0.8145484924316406,\n  0.8526726365089417,\n  0.04554484412074089,\n  0.008191373199224472],\n [0.8160831928253174,\n  0.8168517351150513,\n  0.08440420031547546,\n  0.006774841342121363]]\n</div>"]}}],"execution_count":65},{"cell_type":"code","source":["args = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'),\n                          '--checkpoint_path',\n                          os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext','fold_0/model_0/model.pt'),\n                          '--preds_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext','fold_0/model_0/val_preds_bin76_ext.csv')])\nmodify_predict_args(args)\nmake_predictions(args)\nargs = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'),\n                          '--checkpoint_path',\n                          os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_int','fold_0/model_0/model.pt'),\n                          '--preds_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_int','fold_0/model_0/val_preds_bin76_int.csv')])\nmodify_predict_args(args)\nmake_predictions(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading training args\nLoading data\n\n\n\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 182/182 [00:00&lt;00:00, 3166.01it/s]Validating SMILES\nTest size = 182\nPredicting with an ensemble of 1 models\n\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\n\n\r100%|██████████| 4/4 [00:00&lt;00:00, 45.09it/s]\n\n\r100%|██████████| 1/1 [00:01&lt;00:00,  1.41s/it]Saving predictions to /dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_ext/fold_0/model_0/val_preds_bin76_ext.csv\nLoading training args\nLoading data\n\n\n\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 182/182 [00:00&lt;00:00, 3204.80it/s]Validating SMILES\nTest size = 182\nPredicting with an ensemble of 1 models\n\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\n\n\r100%|██████████| 4/4 [00:00&lt;00:00, 45.16it/s]\n\n\r100%|██████████| 1/1 [00:01&lt;00:00,  1.38s/it]Saving predictions to /dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_int/fold_0/model_0/val_preds_bin76_int.csv\nOut[34]: \n[[0.4778834283351898,\n  0.2351142168045044,\n  0.007805065717548132,\n  0.0004002431232947856],\n [0.03284074366092682,\n  0.04740620777010918,\n  0.000313765078317374,\n  5.6528479035478085e-06],\n [0.04585736617445946,\n  0.034592997282743454,\n  0.001045508892275393,\n  1.9468636310193688e-05],\n [0.05815904214978218,\n  0.006437425501644611,\n  0.0035113997291773558,\n  1.0830783139681444e-05],\n [0.9649506211280823,\n  0.8321712017059326,\n  0.3615501821041107,\n  0.012894111685454845],\n [0.0661563128232956,\n  0.0464491993188858,\n  0.0019046184606850147,\n  7.04408303136006e-05],\n [0.0718972310423851,\n  0.1163259819149971,\n  0.009049229323863983,\n  0.00012653139128815383],\n [0.7602165937423706,\n  0.757743239402771,\n  0.1899527907371521,\n  0.05215475708246231],\n [4.556955346401992e-08,\n  1.9405180240461561e-10,\n  3.996944268613856e-12,\n  2.00706263724557e-19],\n [0.24257752299308777,\n  0.21546240150928497,\n  0.04785468801856041,\n  0.0037942249327898026],\n [0.8603323698043823,\n  0.8819162845611572,\n  0.04697493836283684,\n  0.008587494492530823],\n [0.07700617611408234,\n  0.015868637710809708,\n  0.0027585141360759735,\n  2.907989255618304e-05],\n [0.07770708203315735,\n  0.006004742346704006,\n  0.0016848527593538165,\n  4.093422376172384e-06],\n [0.014134641736745834,\n  0.005018272902816534,\n  0.0003526764048729092,\n  3.8029320421628654e-06],\n [0.2890487015247345,\n  0.11011975258588791,\n  0.0025885358918458223,\n  0.00015067226195242256],\n [0.005838701035827398,\n  0.0011972887441515923,\n  0.0002043053973466158,\n  1.8857957684303983e-07],\n [0.7659364938735962,\n  0.633080780506134,\n  0.020032614469528198,\n  0.002250009449198842],\n [0.9889636039733887,\n  0.9761629104614258,\n  0.7116623520851135,\n  0.25621911883354187],\n [0.9543026089668274,\n  0.9345523118972778,\n  0.2982119023799896,\n  0.1049564778804779],\n [0.9311882853507996,\n  0.6498689651489258,\n  0.5723152160644531,\n  0.13355381786823273],\n [0.06606392562389374,\n  0.06308254599571228,\n  0.006907637696713209,\n  4.737100607599132e-05],\n [0.011669503524899483,\n  0.005099525209516287,\n  0.00016527852858416736,\n  5.91790410453541e-07],\n [0.06489400565624237,\n  0.011484814807772636,\n  0.0017211337108165026,\n  2.0548859538394026e-05],\n [0.036598291248083115,\n  0.15471747517585754,\n  0.023964611813426018,\n  0.0009572483249939978],\n [0.0031989600975066423,\n  0.0011291373521089554,\n  2.8375734473229386e-05,\n  5.179627393658848e-08],\n [0.02527710795402527,\n  0.0036845537833869457,\n  0.0010582534596323967,\n  1.404906811330875e-06],\n [0.010008693672716618,\n  0.0014415088808164,\n  0.0001038965056068264,\n  2.2487988360353484e-07],\n [0.9945968389511108,\n  0.9923895001411438,\n  0.9693421125411987,\n  0.8278583288192749],\n [0.043000899255275726,\n  0.01207007560878992,\n  0.003021534066647291,\n  4.3227511923760176e-05],\n [0.9806832671165466,\n  0.9748358726501465,\n  0.7896752953529358,\n  0.4935900866985321],\n [0.007726440206170082,\n  0.0008548032492399216,\n  7.329767686314881e-05,\n  1.606022550504349e-07],\n [0.9149460792541504,\n  0.8453561067581177,\n  0.28772035241127014,\n  0.04061398282647133],\n [0.3717472553253174,\n  0.3847150206565857,\n  0.031072763726115227,\n  0.0032988958992064],\n [0.04884545877575874,\n  0.0160053838044405,\n  0.000624792359303683,\n  4.76597097076592e-06],\n [0.11611983180046082,\n  0.016635576263070107,\n  0.010182001627981663,\n  0.0003671593149192631],\n [0.9336883425712585,\n  0.8868216276168823,\n  0.4696424603462219,\n  0.1302812695503235],\n [0.005091908387839794,\n  0.007090000901371241,\n  0.00034885443164967,\n  4.7620136456316686e-07],\n [0.960491955280304,\n  0.9336050152778625,\n  0.22839468717575073,\n  0.055439725518226624],\n [0.967741072177887,\n  0.9462078809738159,\n  0.3403942883014679,\n  0.0724114179611206],\n [0.01805960386991501,\n  0.003805166343227029,\n  0.0010997939389199018,\n  1.8591721300253994e-06],\n [0.6462982296943665,\n  0.5596284866333008,\n  0.007166825234889984,\n  0.00044725550105795264],\n [0.04938040301203728,\n  0.06753314286470413,\n  0.0012024787720292807,\n  1.8776263459585607e-05],\n [0.17850421369075775,\n  0.06511016935110092,\n  0.0006369890179485083,\n  1.2485623301472515e-05],\n [0.2575283944606781,\n  0.12800031900405884,\n  0.013607273809611797,\n  0.0006163482903502882],\n [0.9923305511474609,\n  0.9889215230941772,\n  0.9578230977058411,\n  0.776634931564331],\n [0.9976492524147034,\n  0.9921256899833679,\n  0.9619182348251343,\n  0.8252525329589844],\n [0.5950953364372253,\n  0.544521689414978,\n  0.0046399058774113655,\n  0.00033721470390446484],\n [0.08689342439174652,\n  0.09219992905855179,\n  0.01491914689540863,\n  0.00014661265595350415],\n [0.08189978450536728,\n  0.017574626952409744,\n  0.007736690808087587,\n  0.000111398272565566],\n [0.7765357494354248,\n  0.4099600315093994,\n  0.35863104462623596,\n  0.0791243240237236],\n [0.583702802658081,\n  0.36586859822273254,\n  0.3770040273666382,\n  0.03887138143181801],\n [0.04181325435638428,\n  0.07342680543661118,\n  0.002985094441100955,\n  9.056872659130022e-05],\n [0.4454186260700226,\n  0.04471896216273308,\n  0.003132760990411043,\n  8.334751328220591e-05],\n [0.47366324067115784,\n  0.3793814182281494,\n  0.022554388269782066,\n  0.0015312298201024532],\n [0.7703275084495544,\n  0.346789687871933,\n  0.04586627334356308,\n  0.007143404334783554],\n [2.5935110897989944e-05,\n  4.377563891466707e-05,\n  2.487128938355454e-07,\n  2.883029925904168e-11],\n [0.008024745620787144,\n  0.0014206140767782927,\n  6.608100375160575e-05,\n  1.3849084723460692e-07],\n [0.45586538314819336,\n  0.37521857023239136,\n  0.04340565949678421,\n  0.0055835917592048645],\n [0.018308842554688454,\n  0.001997370505705476,\n  0.001141069456934929,\n  3.041407808268559e-06],\n [0.6505366563796997,\n  0.13359434902668,\n  0.05753406137228012,\n  0.0003874432877637446],\n [0.0046796854585409164,\n  0.0013560749357566237,\n  3.3616790460655466e-05,\n  4.345947246520154e-08],\n [0.15244755148887634,\n  0.03728094324469566,\n  0.024957722052931786,\n  0.0005552748334594071],\n [0.030506204813718796,\n  0.07560808211565018,\n  0.009231376461684704,\n  3.0009940019226633e-05],\n [0.0009921645978465676,\n  0.0005757583421654999,\n  9.698667781776749e-06,\n  1.477219058187984e-08],\n [0.13255666196346283,\n  0.01595175452530384,\n  0.0027365488931536674,\n  5.782774314866401e-05],\n [0.05051419511437416,\n  0.0019799938891083,\n  0.00026552987401373684,\n  1.8784181747832918e-06],\n [0.369108110666275,\n  0.21514692902565002,\n  0.006396715994924307,\n  0.00020897432113997638],\n [0.0019153995672240853,\n  0.0001268227497348562,\n  1.114187398343347e-05,\n  5.4988422704127515e-09],\n [0.9000495672225952,\n  0.5017745494842529,\n  0.30092549324035645,\n  0.006257588043808937],\n [0.9476338028907776,\n  0.6818602681159973,\n  0.376860648393631,\n  0.11853890866041183],\n [0.21245791018009186,\n  0.19383291900157928,\n  0.036758169531822205,\n  0.00034719498944468796],\n [0.05268809571862221,\n  0.03508186340332031,\n  0.0015554797137156129,\n  3.538607052178122e-05],\n [0.0072037698701024055,\n  0.0008355280733667314,\n  7.22934928489849e-05,\n  1.5862707414271426e-07],\n [0.02065725438296795,\n  0.010046049952507019,\n  0.00030463450821116567,\n  2.7260412025498226e-06],\n [0.9494895339012146,\n  0.912167489528656,\n  0.11928432434797287,\n  0.023883871734142303],\n [0.9540324807167053,\n  0.770184338092804,\n  0.554282009601593,\n  0.04295128211379051],\n [0.9872188568115234,\n  0.9446807503700256,\n  0.7824999690055847,\n  0.48857083916664124],\n [0.9794458746910095,\n  0.9644356369972229,\n  0.8740766644477844,\n  0.26108697056770325],\n [0.6253180503845215,\n  0.6072253584861755,\n  0.25073108077049255,\n  0.048480186611413956],\n [0.9708260893821716,\n  0.8581032752990723,\n  0.6694982647895813,\n  0.06612537056207657],\n [0.003975194878876209,\n  0.0018820640398189425,\n  4.118839206057601e-05,\n  9.271839473967702e-08],\n [0.8818738460540771,\n  0.7433785200119019,\n  0.030643321573734283,\n  0.0034775694366544485],\n [0.8797997236251831,\n  0.7872669696807861,\n  0.12271197140216827,\n  0.02120133861899376],\n [0.6982041597366333,\n  0.47400224208831787,\n  0.027421915903687477,\n  0.0009652200387790799],\n [0.9436160326004028,\n  0.9225677847862244,\n  0.16610170900821686,\n  0.03552008047699928],\n [0.9828304052352905,\n  0.980284571647644,\n  0.7241677045822144,\n  0.45464757084846497],\n [0.9147977232933044,\n  0.8247233629226685,\n  0.48780810832977295,\n  0.01387436967343092],\n [0.9157004356384277,\n  0.8624552488327026,\n  0.14241264760494232,\n  0.02386406809091568],\n [0.059456586837768555,\n  0.01656726375222206,\n  0.0012466522166505456,\n  1.2982158295926638e-05],\n [0.06148762255907059,\n  0.09186912328004837,\n  0.0005779421771876514,\n  1.3443544958136044e-05],\n [0.9901818633079529,\n  0.9890640377998352,\n  0.9710581302642822,\n  0.861209511756897],\n [0.1725746989250183,\n  0.12472362816333771,\n  0.005312290973961353,\n  0.00020447045972105116],\n [0.7583521604537964,\n  0.2968335747718811,\n  0.05472686141729355,\n  0.004735121503472328],\n [0.0023911939933896065,\n  0.0005509371403604746,\n  1.3517271327145863e-05,\n  1.3614932292682624e-08],\n [0.8181446194648743,\n  0.4495208263397217,\n  0.2376336306333542,\n  0.0057677011936903],\n [0.0011931094340980053,\n  8.592711674282327e-05,\n  5.906812293687835e-05,\n  1.0316683507483049e-08],\n [0.9314081072807312,\n  0.597687304019928,\n  0.09105377644300461,\n  0.009576886892318726],\n [0.24690978229045868,\n  0.09561086446046829,\n  0.004155563190579414,\n  0.00015223278023768216],\n [0.032859280705451965,\n  0.019254878163337708,\n  0.0006978156161494553,\n  8.172157322405837e-06],\n [0.03282327204942703,\n  0.00645500747486949,\n  0.0011863743420690298,\n  1.0272648978570942e-05],\n [0.6613019108772278,\n  0.2100854068994522,\n  0.25844430923461914,\n  0.0152636943385005],\n [0.047919172793626785,\n  0.036247946321964264,\n  0.0016681903507560492,\n  2.6311832698411308e-05],\n [0.6904559135437012,\n  0.3519279360771179,\n  0.059005431830883026,\n  0.013144025579094887],\n [0.1919342279434204,\n  0.192011758685112,\n  0.039466727524995804,\n  0.0005845056148245931],\n [0.951755702495575,\n  0.911735475063324,\n  0.46809014678001404,\n  0.11251407116651535],\n [0.22466324269771576,\n  0.20369867980480194,\n  0.03542420268058777,\n  0.00037375729880295694],\n [0.3487151563167572,\n  0.13436715304851532,\n  0.00945247896015644,\n  0.00044741458259522915],\n [0.49880266189575195,\n  0.41073915362358093,\n  0.03806708753108978,\n  0.0005830131121911108],\n [0.9570184350013733,\n  0.8086357116699219,\n  0.6591584086418152,\n  0.32866743206977844],\n [0.017326105386018753,\n  0.0027052650693804026,\n  0.0007523710373789072,\n  1.0181294101130334e-06],\n [0.29081177711486816,\n  0.14377139508724213,\n  0.018160216510295868,\n  0.001094990293495357],\n [0.9966493248939514,\n  0.9929810762405396,\n  0.9762352705001831,\n  0.8981549143791199],\n [0.920699954032898,\n  0.8239119648933411,\n  0.0977405458688736,\n  0.013218491338193417],\n [0.0040349205955863,\n  0.00018255363102070987,\n  3.141892739222385e-05,\n  2.2125659526750496e-08],\n [0.2917667627334595,\n  0.1658719778060913,\n  0.025862378999590874,\n  0.0015356724616140127],\n [0.16152706742286682,\n  0.04577593877911568,\n  0.005946766585111618,\n  0.00016686155868228525],\n [0.5855379700660706,\n  0.30315837264060974,\n  0.14296334981918335,\n  0.0016204316634684801],\n [0.6388435959815979,\n  0.5888819694519043,\n  0.020499123260378838,\n  0.002052624011412263],\n [0.8594265580177307,\n  0.7707833647727966,\n  0.18183600902557373,\n  0.029234616085886955],\n [0.5606167912483215,\n  0.6831836104393005,\n  0.09773708879947662,\n  0.011051717214286327],\n [0.8472522497177124,\n  0.9045101404190063,\n  0.29297882318496704,\n  0.09185120463371277],\n [0.011295048519968987,\n  0.001450507203117013,\n  0.0001336321292910725,\n  3.770153966797807e-07],\n [0.008543216623365879,\n  0.009354542940855026,\n  0.00016014707216527313,\n  9.25224412640091e-07],\n [0.5752487182617188,\n  0.5534543395042419,\n  0.03615787625312805,\n  0.0034695810172706842],\n [0.020180057734251022,\n  0.0023254891857504845,\n  0.0002845828130375594,\n  1.236670755133673e-06],\n [0.06983094662427902,\n  0.06356107443571091,\n  0.0067024086602032185,\n  5.2842813602183014e-05],\n [0.9026521444320679,\n  0.681805431842804,\n  0.0465477854013443,\n  0.00377915077842772],\n [0.010076930746436119,\n  0.02750927209854126,\n  0.00471057603135705,\n  0.00011593747331062332],\n [0.9673648476600647,\n  0.9367768168449402,\n  0.36918872594833374,\n  0.08836071193218231],\n [0.9809918999671936,\n  0.9776455163955688,\n  0.190418541431427,\n  0.03481575474143028],\n [0.9300551414489746,\n  0.9147502183914185,\n  0.6182088255882263,\n  0.275318443775177],\n [0.3590737581253052,\n  0.1279042661190033,\n  0.008569115772843361,\n  0.00037081161281093955],\n [0.9618036150932312,\n  0.9486926794052124,\n  0.17110490798950195,\n  0.06908007711172104],\n [0.30053430795669556,\n  0.029777826741337776,\n  0.0031234067864716053,\n  7.42510674172081e-05],\n [0.00012532675464171916,\n  6.55625990475528e-05,\n  1.9830986275337636e-06,\n  1.1392756876782428e-10],\n [0.03896045684814453,\n  0.006988632492721081,\n  0.0027097640559077263,\n  7.177884526754497e-06],\n [0.9698928594589233,\n  0.962352991104126,\n  0.493701696395874,\n  0.2753044068813324],\n [0.5442749261856079,\n  0.6119704246520996,\n  0.2736565172672272,\n  0.008877100422978401],\n [0.10295963287353516,\n  0.03035935014486313,\n  0.007698487490415573,\n  2.7898124244529754e-05],\n [0.9673861861228943,\n  0.9586048126220703,\n  0.13809344172477722,\n  0.022736407816410065],\n [0.002568181836977601,\n  0.00020002794917672873,\n  2.6333647838328034e-05,\n  1.9545312923696656e-08],\n [0.5251829028129578,\n  0.13947930932044983,\n  0.10822100192308426,\n  0.001126146875321865],\n [0.8268935084342957,\n  0.8461838364601135,\n  0.03382325917482376,\n  0.0053894552402198315],\n [0.5474647879600525,\n  0.11754564195871353,\n  0.018576160073280334,\n  0.000859132269397378],\n [0.41295191645622253,\n  0.27302825450897217,\n  0.010407621040940285,\n  0.0006021994049660861],\n [0.050528835505247116,\n  0.008896294981241226,\n  0.0014110860647633672,\n  1.510920628788881e-05],\n [0.7076394557952881,\n  0.7819728851318359,\n  0.014916092157363892,\n  0.0016180082457140088],\n [0.059457771480083466,\n  0.07143475860357285,\n  0.011442531831562519,\n  6.259441579459235e-05],\n [0.9970065951347351,\n  0.9878271222114563,\n  0.9481008052825928,\n  0.694004237651825],\n [0.4070362150669098,\n  0.5346711874008179,\n  0.03580428659915924,\n  0.0034282286651432514],\n [0.01369523350149393,\n  0.003616894828155637,\n  0.006150654051452875,\n  4.1032042645383626e-05],\n [0.010103779844939709,\n  0.01851952075958252,\n  0.0016694528749212623,\n  2.491675695637241e-05],\n [0.03314542397856712,\n  0.007061685435473919,\n  0.000389027496566996,\n  1.9038510572499945e-06],\n [0.009752023965120316,\n  0.0013421343173831701,\n  0.00019813093240372837,\n  6.315395921774325e-07],\n [0.9977618455886841,\n  0.9950077533721924,\n  0.9748425483703613,\n  0.8919429779052734],\n [0.0352553054690361,\n  0.006460302975028753,\n  0.0022818162105977535,\n  1.759242150001228e-05],\n [0.0120481476187706,\n  0.01178771536797285,\n  0.0005999382701702416,\n  5.4122683650348336e-06],\n [0.9851581454277039,\n  0.9547467231750488,\n  0.5184275507926941,\n  0.2476874738931656],\n [0.004073305055499077,\n  0.0011722082272171974,\n  3.084005948039703e-05,\n  5.0166683251973154e-08],\n [0.8237797617912292,\n  0.7237078547477722,\n  0.026629619300365448,\n  0.005397981032729149],\n [0.46371975541114807,\n  0.34814316034317017,\n  0.006246567238122225,\n  0.0004093355964869261],\n [0.5823381543159485,\n  0.39400309324264526,\n  0.013002351857721806,\n  0.001111540594138205],\n [0.8639314770698547,\n  0.842319130897522,\n  0.30142611265182495,\n  0.03812583163380623],\n [0.7877275347709656,\n  0.8488936424255371,\n  0.7038261294364929,\n  0.2868063747882843],\n [0.8368578553199768,\n  0.7390958666801453,\n  0.11876777559518814,\n  0.014765200205147266],\n [0.03920008987188339,\n  0.014560731127858162,\n  0.003575169015675783,\n  1.1703266864060424e-05],\n [0.8786783814430237,\n  0.4947623610496521,\n  0.1547011286020279,\n  0.02856462635099888],\n [0.0452863983809948,\n  0.008315983228385448,\n  0.002541555790230632,\n  6.095116077631246e-06],\n [0.03848039358854294,\n  0.015762701630592346,\n  0.000990913831628859,\n  1.0324417416995857e-05],\n [0.0010736222611740232,\n  0.00031444482738152146,\n  3.3021951821865514e-05,\n  8.748511248768409e-09],\n [0.010190819390118122,\n  0.010880275629460812,\n  0.0012024191673845053,\n  4.654511485568946e-06],\n [0.5225210785865784,\n  0.3464486300945282,\n  0.005384713411331177,\n  0.0002815161133185029],\n [0.029548389837145805,\n  0.0314767099916935,\n  0.001831975532695651,\n  2.558273081376683e-05],\n [0.995316743850708,\n  0.9833515286445618,\n  0.605675220489502,\n  0.2521992325782776],\n [0.02193361334502697,\n  0.0038995170034468174,\n  0.001156671904027462,\n  1.4554261724697426e-06],\n [0.0069630867801606655,\n  0.0010410284157842398,\n  9.497301653027534e-05,\n  2.4668608489264443e-07],\n [0.3981391489505768,\n  0.3784361481666565,\n  0.021929815411567688,\n  0.002033533528447151],\n [0.19206833839416504,\n  0.07396002858877182,\n  0.006671487353742123,\n  0.00016830896493047476],\n [0.9621556401252747,\n  0.9498550891876221,\n  0.20890454947948456,\n  0.04979348182678223],\n [0.6046491861343384,\n  0.3489077687263489,\n  0.022277673706412315,\n  0.0017755766166374087],\n [0.022501083090901375,\n  0.010617210529744625,\n  0.000322981970384717,\n  3.0031405913177878e-06],\n [0.6301516890525818,\n  0.36089882254600525,\n  0.027218202129006386,\n  0.00226369546726346]]\n</div>"]}}],"execution_count":66},{"cell_type":"code","source":["args = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                          '--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtest-183.csv'),\n                          '--checkpoint_path',\n                          os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext_Feat_SLogP','fold_0/model_0/model.pt'),\n                          '--preds_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext_Feat_SLogP','fold_0/model_0/test_preds_bin76_ext_Feat_SLogP.csv')])\nmodify_predict_args(args)\nmake_predictions(args)\nargs = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'),\n                          '--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtest-183.csv'),\n                          '--checkpoint_path',\n                          os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_int_Feat_SLogP','fold_0/model_0/model.pt'),\n                          '--preds_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_int_Feat_SLogP','fold_0/model_0/test_preds_bin76_int_Feat_SLogP.csv')])\nmodify_predict_args(args)\nmake_predictions(args)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading training args\nLoading data\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 2833.63it/s]\nValidating SMILES\nTest size = 183\nPredicting with an ensemble of 1 models\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 28.71it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 31.45it/s]\r100%|██████████| 1/1 [00:02&lt;00:00,  2.85s/it]\nSaving predictions to /dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_ext_Feat_SLogP/fold_0/model_0/test_preds_bin76_ext_Feat_SLogP.csv\nLoading training args\nLoading data\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 2890.50it/s]\nValidating SMILES\nTest size = 183\nPredicting with an ensemble of 1 models\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 28.13it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 30.68it/s]\r100%|██████████| 1/1 [00:02&lt;00:00,  2.50s/it]\nSaving predictions to /dbfs/FileStore/chemprop/JAK/hyperopt_4x-bin76_int_Feat_SLogP/fold_0/model_0/test_preds_bin76_int_Feat_SLogP.csv\nOut[19]: \n[[0.021509801968932152,\n  0.009683476760983467,\n  0.0005746048991568387,\n  3.049027691304218e-05],\n [0.494133859872818,\n  0.2800716757774353,\n  0.024666771292686462,\n  0.001775816548615694],\n [0.8795645236968994,\n  0.7977457642555237,\n  0.23091095685958862,\n  0.018174413591623306],\n [0.9304183125495911,\n  0.8870624303817749,\n  0.5983377695083618,\n  0.12444499135017395],\n [0.11076237261295319,\n  0.037804752588272095,\n  0.005893657449632883,\n  0.0005669749225489795],\n [0.2605248987674713,\n  0.42928799986839294,\n  0.0856688916683197,\n  0.009336891584098339],\n [0.2584491968154907,\n  0.36287668347358704,\n  0.050967417657375336,\n  0.006813549902290106],\n [0.8944602012634277,\n  0.8386507034301758,\n  0.0691104605793953,\n  0.012074950151145458],\n [0.9823343753814697,\n  0.9421291947364807,\n  0.5253525376319885,\n  0.15982869267463684],\n [0.9107779264450073,\n  0.8538690805435181,\n  0.1533123254776001,\n  0.046223245561122894],\n [0.014247865416109562,\n  0.004729895386844873,\n  0.00030305833206512034,\n  1.4242899851524271e-05],\n [0.8190213441848755,\n  0.7976062893867493,\n  0.06677927076816559,\n  0.017885765060782433],\n [0.41819071769714355,\n  0.39270609617233276,\n  0.07426220923662186,\n  0.004441095981746912],\n [0.7161812782287598,\n  0.46724799275398254,\n  0.03640718385577202,\n  0.004011727869510651],\n [0.06220848485827446,\n  0.08098039776086807,\n  0.0018145465292036533,\n  0.00019775042892433703],\n [0.45990970730781555,\n  0.3914739191532135,\n  0.04663348197937012,\n  0.00351711711846292],\n [0.5080682635307312,\n  0.4324767589569092,\n  0.021859586238861084,\n  0.0031127852853387594],\n [0.8240869045257568,\n  0.7521774768829346,\n  0.17670311033725739,\n  0.012494317255914211],\n [0.28984975814819336,\n  0.2190825343132019,\n  0.01471368782222271,\n  0.0022151805460453033],\n [0.18584533035755157,\n  0.2185235321521759,\n  0.00857454165816307,\n  0.0009881322039291263],\n [0.9616071581840515,\n  0.9637580513954163,\n  0.9203590750694275,\n  0.6423345804214478],\n [0.13403698801994324,\n  0.1141287311911583,\n  0.014197123236954212,\n  0.0006980433827266097],\n [0.5112892389297485,\n  0.4583916664123535,\n  0.005827958229929209,\n  0.001508526154793799],\n [0.9813661575317383,\n  0.9461495280265808,\n  0.5076994895935059,\n  0.232519268989563],\n [0.15962861478328705,\n  0.25586384534835815,\n  0.0006179022020660341,\n  0.0001815243304008618],\n [0.9803168773651123,\n  0.9399414658546448,\n  0.4239911437034607,\n  0.13216809928417206],\n [0.01877620257437229,\n  0.02458665706217289,\n  0.0001605216384632513,\n  1.9635832359199412e-05],\n [0.6015543341636658,\n  0.5622212290763855,\n  0.017645670101046562,\n  0.003945017233490944],\n [0.09128052741289139,\n  0.037146009504795074,\n  0.007140273228287697,\n  0.0002260478213429451],\n [0.031239770352840424,\n  0.01784043200314045,\n  0.0006683735409751534,\n  4.1364022763445973e-05],\n [0.962264358997345,\n  0.8821510672569275,\n  0.25811219215393066,\n  0.05060400068759918],\n [0.8989814519882202,\n  0.8584733009338379,\n  0.16384956240653992,\n  0.04895942285656929],\n [0.5960773825645447,\n  0.4998340606689453,\n  0.023163042962551117,\n  0.004007159732282162],\n [0.7069718837738037,\n  0.6893077492713928,\n  0.16031424701213837,\n  0.04987311363220215],\n [0.3828223943710327,\n  0.18975140154361725,\n  0.07113262265920639,\n  0.009049173444509506],\n [0.9303090572357178,\n  0.7879835367202759,\n  0.32642337679862976,\n  0.024633117020130157],\n [0.9462980628013611,\n  0.9438890218734741,\n  0.5905066132545471,\n  0.27004319429397583],\n [0.28283873200416565,\n  0.230600506067276,\n  0.0242274422198534,\n  0.003923597279936075],\n [0.9067874550819397,\n  0.9119784832000732,\n  0.7529924511909485,\n  0.37059757113456726],\n [0.09414821118116379,\n  0.03635658696293831,\n  0.003420001594349742,\n  0.00025350128998979926],\n [0.8112961053848267,\n  0.733883798122406,\n  0.06642138957977295,\n  0.017511378973722458],\n [0.8962387442588806,\n  0.8646093010902405,\n  0.12269210070371628,\n  0.031807947903871536],\n [0.9632609486579895,\n  0.9361400604248047,\n  0.4258923828601837,\n  0.16396810114383698],\n [0.2618747055530548,\n  0.18072599172592163,\n  0.014216088689863682,\n  0.0015459624119102955],\n [0.3530588746070862,\n  0.331129789352417,\n  0.054821666330099106,\n  0.009520246647298336],\n [0.11881657689809799,\n  0.06780868768692017,\n  0.0023074985947459936,\n  0.00031571940053254366],\n [0.01619202084839344,\n  0.01947772316634655,\n  0.00032162535353563726,\n  2.5716875825310126e-05],\n [0.015133785083889961,\n  0.0077383206225931644,\n  0.00010886284871958196,\n  7.79980473453179e-06],\n [0.7963895797729492,\n  0.7882551550865173,\n  0.05224291980266571,\n  0.013068828731775284],\n [0.9878376722335815,\n  0.9612646698951721,\n  0.5374963879585266,\n  0.24608652293682098],\n [0.08262503147125244,\n  0.03668510168790817,\n  0.005005413666367531,\n  0.0001429554249625653],\n [0.9008650779724121,\n  0.9012600779533386,\n  0.3436615467071533,\n  0.1259370595216751],\n [0.764472484588623,\n  0.5552322268486023,\n  0.30320391058921814,\n  0.07121496647596359],\n [0.4940335154533386,\n  0.5166023969650269,\n  0.011553111486136913,\n  0.003541773185133934],\n [0.5309181213378906,\n  0.5093621015548706,\n  0.020740101113915443,\n  0.004321033135056496],\n [0.9454312324523926,\n  0.9060657024383545,\n  0.6576962471008301,\n  0.2804943919181824],\n [0.9762694835662842,\n  0.9539122581481934,\n  0.7181897759437561,\n  0.2208450436592102],\n [0.7636643052101135,\n  0.7658143639564514,\n  0.45626509189605713,\n  0.144166961312294],\n [0.9642564654350281,\n  0.9290179014205933,\n  0.6326480507850647,\n  0.28784385323524475],\n [0.42500925064086914,\n  0.2679349184036255,\n  0.01862337812781334,\n  0.0032993380445986986],\n [0.23480568826198578,\n  0.22807838022708893,\n  0.02270377241075039,\n  0.000918095582164824],\n [0.2701328694820404,\n  0.11620935052633286,\n  0.013253486715257168,\n  0.0018034634413197637],\n [0.1889924556016922,\n  0.08403799682855606,\n  0.02826651744544506,\n  0.002340850653126836],\n [0.8681096434593201,\n  0.8901911973953247,\n  0.2911025583744049,\n  0.11646630614995956],\n [0.20013245940208435,\n  0.09123097360134125,\n  0.0179706122726202,\n  0.0013019881444051862],\n [0.13672879338264465,\n  0.17267689108848572,\n  0.005358587484806776,\n  0.0007289460045285523],\n [0.9004533290863037,\n  0.6615637540817261,\n  0.1542448103427887,\n  0.013535364530980587],\n [0.6765676736831665,\n  0.6732919812202454,\n  0.023298656567931175,\n  0.005077680107206106],\n [0.9384965896606445,\n  0.766032338142395,\n  0.17761369049549103,\n  0.013645767234265804],\n [0.9690243005752563,\n  0.8811414241790771,\n  0.3476446866989136,\n  0.14555412530899048],\n [0.9930558800697327,\n  0.9777544140815735,\n  0.7352204322814941,\n  0.4483124911785126],\n [0.155321404337883,\n  0.2811569273471832,\n  0.00022332562366500497,\n  6.956540892133489e-05],\n [0.02018430083990097,\n  0.004511381033807993,\n  0.0012187474640086293,\n  1.8725031623034738e-05],\n [0.723939061164856,\n  0.5964379906654358,\n  0.07430844753980637,\n  0.013802807778120041],\n [0.22485266625881195,\n  0.34647610783576965,\n  0.00578764732927084,\n  0.0009262572857551277],\n [0.8899980187416077,\n  0.8565360903739929,\n  0.42642685770988464,\n  0.14051553606987],\n [0.45280665159225464,\n  0.3585169315338135,\n  0.1880224496126175,\n  0.02391074411571026],\n [0.974780797958374,\n  0.9303399920463562,\n  0.568964421749115,\n  0.32457056641578674],\n [0.03356228768825531,\n  0.010534015484154224,\n  0.0012252615997567773,\n  8.435564086539671e-05],\n [0.5798113346099854,\n  0.41240236163139343,\n  0.018974892795085907,\n  0.003145044669508934],\n [0.0002756213361863047,\n  0.000513006467372179,\n  3.718044752076821e-07,\n  2.1642819092448917e-08],\n [0.9552111625671387,\n  0.9368098974227905,\n  0.19253350794315338,\n  0.06483874469995499],\n [0.6318624019622803,\n  0.5165503621101379,\n  0.05895119532942772,\n  0.011117089539766312],\n [0.7093932032585144,\n  0.7185875177383423,\n  0.03282416984438896,\n  0.007895621471107006],\n [0.8446552753448486,\n  0.7848851680755615,\n  0.2413252890110016,\n  0.0666985958814621],\n [0.1360204815864563,\n  0.08925958722829819,\n  0.005010769236832857,\n  0.0006658761412836611],\n [0.9404072165489197,\n  0.839962899684906,\n  0.3142473101615906,\n  0.0797397643327713],\n [0.9251972436904907,\n  0.8736182451248169,\n  0.4691317081451416,\n  0.13152343034744263],\n [0.3904886245727539,\n  0.38470906019210815,\n  0.03583088517189026,\n  0.0018882565200328827],\n [0.03177966922521591,\n  0.021774623543024063,\n  0.0022001215256750584,\n  5.156259430805221e-05],\n [0.48309674859046936,\n  0.363832950592041,\n  0.019454123452305794,\n  0.0011731890263035893],\n [0.9581083059310913,\n  0.8974475264549255,\n  0.38609829545021057,\n  0.036923520267009735],\n [0.03711948171257973,\n  0.018395259976387024,\n  0.0009512078831903636,\n  5.901592885493301e-05],\n [0.9394934177398682,\n  0.8733166456222534,\n  0.2269279807806015,\n  0.07802233844995499],\n [0.5415374636650085,\n  0.5069429874420166,\n  0.010474877431988716,\n  0.002235102467238903],\n [0.24548478424549103,\n  0.3320932388305664,\n  0.006667651701718569,\n  0.001111973775550723],\n [0.2027120590209961,\n  0.13089121878147125,\n  0.009677653200924397,\n  0.0009280991507694125],\n [0.022329464554786682,\n  0.01898135244846344,\n  0.0004116991185583174,\n  3.584588193916716e-05],\n [0.946974515914917,\n  0.8948832750320435,\n  0.43798622488975525,\n  0.10972301661968231],\n [0.07987283170223236,\n  0.03185438737273216,\n  0.007258474826812744,\n  0.00019510182028170675],\n [0.07729629427194595,\n  0.10731937736272812,\n  0.004363438580185175,\n  0.0005102542345412076],\n [0.2668441832065582,\n  0.2887885570526123,\n  0.027114661410450935,\n  0.0011268759844824672],\n [0.12090682983398438,\n  0.07289740443229675,\n  0.004043114371597767,\n  0.00043418866698630154],\n [0.9897177219390869,\n  0.96475750207901,\n  0.6421051621437073,\n  0.3808288276195526],\n [0.13553954660892487,\n  0.07631951570510864,\n  0.0186974685639143,\n  0.0006717927171848714],\n [0.9271116256713867,\n  0.8289602994918823,\n  0.4662364423274994,\n  0.10677674412727356],\n [0.4921540319919586,\n  0.45482251048088074,\n  0.06532712280750275,\n  0.004787631798535585],\n [0.8739383220672607,\n  0.6879599094390869,\n  0.2581969201564789,\n  0.026173947378993034],\n [0.007086758967489004,\n  0.010547214187681675,\n  5.015419810661115e-05,\n  5.923369371885201e-06],\n [0.7275580763816833,\n  0.7413499355316162,\n  0.3199334442615509,\n  0.1002778708934784],\n [0.4921998381614685,\n  0.5565662980079651,\n  0.008955179713666439,\n  0.001967919757589698],\n [0.9909296631813049,\n  0.968309223651886,\n  0.6697595715522766,\n  0.27009472250938416],\n [0.36777666211128235,\n  0.28670376539230347,\n  0.004683987237513065,\n  0.0006175140151754022],\n [0.18228064477443695,\n  0.13044825196266174,\n  0.0024173613637685776,\n  0.00031356202089227736],\n [0.2465151846408844,\n  0.1285915970802307,\n  0.03126836195588112,\n  0.004105133004486561],\n [0.9633099436759949,\n  0.9337522387504578,\n  0.6943354606628418,\n  0.2997857630252838],\n [0.027585651725530624,\n  0.03704604506492615,\n  0.0013528951676562428,\n  0.00013281860447023064],\n [0.456413209438324,\n  0.393351286649704,\n  0.008233445696532726,\n  0.001469352631829679],\n [0.14410772919654846,\n  0.06803245842456818,\n  0.008395060896873474,\n  0.0008414696785621345],\n [0.7668297290802002,\n  0.7464287877082825,\n  0.3721860647201538,\n  0.12970970571041107],\n [0.7853657007217407,\n  0.7833094596862793,\n  0.03726808354258537,\n  0.011461138725280762],\n [0.3930148184299469,\n  0.4566689133644104,\n  0.012130211107432842,\n  0.0020602853037416935],\n [0.012631691060960293,\n  0.007210966199636459,\n  0.0001474859018344432,\n  1.2191897440061439e-05],\n [0.00971160363405943,\n  0.0048637231811881065,\n  0.00026209611678496003,\n  1.2023193448840175e-05],\n [0.4145177900791168,\n  0.38223230838775635,\n  0.0039452738128602505,\n  0.0009849135531112552],\n [0.17671526968479156,\n  0.15932568907737732,\n  0.011274936608970165,\n  0.0006237030029296875],\n [0.3793000876903534,\n  0.25763624906539917,\n  0.02953401394188404,\n  0.0048331813886761665],\n [0.7131693363189697,\n  0.7041604518890381,\n  0.08819665759801865,\n  0.006868681404739618],\n [0.1181165874004364,\n  0.06967566162347794,\n  0.0026371439453214407,\n  0.00033865123987197876],\n [0.9629713296890259,\n  0.8534554839134216,\n  0.39040982723236084,\n  0.04602125659584999],\n [0.9591385126113892,\n  0.9025955200195312,\n  0.6978491544723511,\n  0.4195415675640106],\n [0.5908192992210388,\n  0.45666494965553284,\n  0.04688434302806854,\n  0.010314271785318851],\n [0.02016865648329258,\n  0.008868785575032234,\n  0.0004645800217986107,\n  2.632435644045472e-05],\n [0.8889686465263367,\n  0.9335728883743286,\n  0.7707404494285583,\n  0.29801514744758606],\n [0.8713125586509705,\n  0.8504677414894104,\n  0.09306196868419647,\n  0.029811639338731766],\n [0.7896715402603149,\n  0.7327608466148376,\n  0.11089705675840378,\n  0.025673216208815575],\n [0.5786988139152527,\n  0.5518894195556641,\n  0.038899075239896774,\n  0.006054827012121677],\n [0.6438333988189697,\n  0.612104058265686,\n  0.008641889318823814,\n  0.0013353079557418823],\n [0.049741409718990326,\n  0.02066381834447384,\n  0.001784642692655325,\n  0.0001298213319387287],\n [0.15719980001449585,\n  0.11512637883424759,\n  0.010684252716600895,\n  0.0018094675615429878],\n [0.3281536400318146,\n  0.29060131311416626,\n  0.020347189158201218,\n  0.0028781674336642027],\n [0.7569476962089539,\n  0.6615650653839111,\n  0.03544168919324875,\n  0.006104857660830021],\n [0.0053473603911697865,\n  0.011510578915476799,\n  2.25604344450403e-05,\n  2.5793624445213936e-06],\n [0.8654123544692993,\n  0.7363932728767395,\n  0.20303651690483093,\n  0.0136481411755085],\n [0.9628103971481323,\n  0.930312991142273,\n  0.8678572177886963,\n  0.5159541964530945],\n [0.1807010918855667,\n  0.17654697597026825,\n  0.013258376158773899,\n  0.0005500157712958753],\n [0.9616878628730774,\n  0.9181987643241882,\n  0.23488731682300568,\n  0.04158040136098862],\n [0.001951853628270328,\n  0.0006813268992118537,\n  3.107241718680598e-05,\n  8.755557132644753e-07],\n [0.0108255073428154,\n  0.009491687640547752,\n  0.00021025523892603815,\n  6.97747145750327e-06],\n [0.24328531324863434,\n  0.23375341296195984,\n  0.002645464614033699,\n  0.000630986294709146],\n [0.7965674996376038,\n  0.7585822939872742,\n  0.13423646986484528,\n  0.045668721199035645],\n [0.9195132255554199,\n  0.9172409772872925,\n  0.30768442153930664,\n  0.039322029799222946],\n [0.04531150311231613,\n  0.017851632088422775,\n  0.0031695987563580275,\n  9.035420225700364e-05],\n [0.7572408318519592,\n  0.7729546427726746,\n  0.03475120663642883,\n  0.005718796513974667],\n [0.9893524050712585,\n  0.9662418961524963,\n  0.5306329727172852,\n  0.2302858531475067],\n [0.08668219298124313,\n  0.12979017198085785,\n  0.008079253137111664,\n  0.0002280102635268122],\n [0.9516064524650574,\n  0.9118807315826416,\n  0.4539881646633148,\n  0.15756623446941376],\n [0.7187988758087158,\n  0.5835291743278503,\n  0.627573549747467,\n  0.09908509999513626],\n [0.4368790090084076,\n  0.37599238753318787,\n  0.0543038584291935,\n  0.00784275121986866],\n [0.1144692599773407,\n  0.07698597759008408,\n  0.0020996849052608013,\n  0.0002879026287700981],\n [0.9518941640853882,\n  0.861480176448822,\n  0.6004550457000732,\n  0.1290835738182068],\n [0.02931143157184124,\n  0.011808550916612148,\n  0.0007955305045470595,\n  3.9190592360682786e-05],\n [0.05268942937254906,\n  0.02315133437514305,\n  0.0009274777257815003,\n  7.428392564179376e-05],\n [0.12268511950969696,\n  0.13661126792430878,\n  0.003269657026976347,\n  0.0004093937750440091],\n [0.18062929809093475,\n  0.17082317173480988,\n  0.028950829058885574,\n  0.0014487415319308639],\n [0.08526616543531418,\n  0.045683324337005615,\n  0.0027698641642928123,\n  0.00018779617676045746],\n [0.02388366498053074,\n  0.007594328373670578,\n  0.001130390097387135,\n  2.6152030841330998e-05],\n [0.7914497256278992,\n  0.8262792229652405,\n  0.4154404401779175,\n  0.10620637983083725],\n [0.4107942581176758,\n  0.24843056499958038,\n  0.031482990831136703,\n  0.003443934954702854],\n [0.18223941326141357,\n  0.05246954411268234,\n  0.003664117306470871,\n  0.00030396695365197957],\n [0.10254267603158951,\n  0.045126888900995255,\n  0.003096358384937048,\n  0.00032515983912162483],\n [0.033698342740535736,\n  0.0651424452662468,\n  0.0007940400391817093,\n  6.552796548930928e-05],\n [0.049397002905607224,\n  0.03755446523427963,\n  0.0013574614422395825,\n  6.156934978207573e-05],\n [0.42733463644981384,\n  0.4251680076122284,\n  0.012647760100662708,\n  0.0019770213402807713],\n [0.061262596398591995,\n  0.0331408753991127,\n  0.0017166194738820195,\n  0.00015613592404406518],\n [0.7531713843345642,\n  0.738045334815979,\n  0.07884446531534195,\n  0.006324925925582647],\n [0.9620952010154724,\n  0.8212560415267944,\n  0.30082815885543823,\n  0.0265578031539917],\n [0.6066380739212036,\n  0.6191612482070923,\n  0.05550134927034378,\n  0.009305303916335106],\n [0.9775621891021729,\n  0.9770616888999939,\n  0.853175699710846,\n  0.5584637522697449],\n [0.9799304008483887,\n  0.9322509169578552,\n  0.7466223835945129,\n  0.3506762981414795],\n [0.7938444018363953,\n  0.7917479872703552,\n  0.20533254742622375,\n  0.05559677630662918],\n [0.4945299029350281,\n  0.5643205642700195,\n  0.006573161110281944,\n  0.001827548141591251],\n [0.6457532048225403,\n  0.580462634563446,\n  0.039091724902391434,\n  0.0033759865909814835]]\n</div>"]}}],"execution_count":67},{"cell_type":"markdown","source":["# Unused material"],"metadata":{}},{"cell_type":"markdown","source":["### Regression hyperparameter tuning with SLogP Feature"],"metadata":{}},{"cell_type":"code","source":["parser = ArgumentParser()\nadd_train_args(parser)\nparser.add_argument('--num_iters', type=int, default=20,\n                    help='Number of hyperparameter choices to try')\nparser.add_argument('--config_save_path', type=str, required=True,\n                    help='Path to .json file where best hyperparameter settings will be written')\nparser.add_argument('--log_dir', type=str,\n                    help='(Optional) Path to a directory where all results of the hyperparameter optimization will be written')\nif not os.path.exists(os.path.join(CHEMPROP_DIR,'JAK','checkpoints','Feat_SLogP')):\n  os.mkdir(os.path.join(CHEMPROP_DIR,'JAK','checkpoints','Feat_SLogP'))\n\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-1460.csv'),\n                          '--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtrain-1460.csv'),\n                          '--dataset_type','regression',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','checkpoints','Feat_SLogP'),\n                          '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182.csv'),\n                          '--separate_val_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPval-182.csv'),\n                          '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183.csv'),\n                          '--separate_test_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtest-183.csv'),\n                          '--config_save_path',os.path.join(CHEMPROP_DIR,'JAK','Feat_SLogP','regression-4x.json'),\n                          '--log_dir',os.path.join(CHEMPROP_DIR,'JAK','Feat_SLogP')])\nmodify_train_args(args)\n\ngrid_search(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]{&#39;depth&#39;: 4, &#39;dropout&#39;: 0.25, &#39;ffn_num_layers&#39;: 3, &#39;hidden_size&#39;: 1400}\n{&#39;depth&#39;: 4, &#39;dropout&#39;: 0.25, &#39;ffn_num_layers&#39;: 3, &#39;hidden_size&#39;: 1400}\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.25,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/Feat_SLogP/depth_4_dropout_0.25_ffn_num_layers_3_hidden_size_1400/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n\n*** WARNING: skipped 7094499 bytes of output ***\n\n\r 95%|█████████▌| 19/20 [33:15&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r  7%|6         | 2/29 [00:00&lt;00:01, 16.18it/s]\n\r 95%|█████████▌| 19/20 [33:15&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:15&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 14%|#3        | 4/29 [00:00&lt;00:01, 16.21it/s]\n\r 95%|█████████▌| 19/20 [33:15&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:15&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 21%|##        | 6/29 [00:00&lt;00:01, 16.26it/s]\n\r 95%|█████████▌| 19/20 [33:15&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:15&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 28%|##7       | 8/29 [00:00&lt;00:01, 16.18it/s]\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]Loss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\nLoss = 5.2295e-03, PNorm = 70.1325, GNorm = 0.9310, lr_0 = 1.0584e-04\n\r                                                                               \r\r 34%|###4      | 10/29 [00:00&lt;00:01, 16.03it/s]\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 41%|####1     | 12/29 [00:00&lt;00:01, 16.14it/s]\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 48%|####8     | 14/29 [00:00&lt;00:00, 16.16it/s]\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 55%|#####5    | 16/29 [00:00&lt;00:00, 16.29it/s]\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 62%|######2   | 18/29 [00:01&lt;00:00, 16.29it/s]\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]Loss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\nLoss = 5.4046e-03, PNorm = 70.1398, GNorm = 1.2998, lr_0 = 1.0288e-04\n\r                                                                               \r\r 69%|######8   | 20/29 [00:01&lt;00:00, 16.12it/s]\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 76%|#######5  | 22/29 [00:01&lt;00:00, 16.29it/s]\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 83%|########2 | 24/29 [00:01&lt;00:00, 16.41it/s]\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:16&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 90%|########9 | 26/29 [00:01&lt;00:00, 16.53it/s]\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 97%|#########6| 28/29 [00:01&lt;00:00, 16.53it/s]\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]Loss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\nLoss = 5.9432e-03, PNorm = 70.1447, GNorm = 1.1773, lr_0 = 1.0000e-04\n\r                                                                               \r\r100%|##########| 29/29 [00:01&lt;00:00, 16.28it/s]\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r 25%|##5       | 1/4 [00:00&lt;00:00,  4.06it/s]\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r100%|##########| 4/4 [00:00&lt;00:00, 13.19it/s]\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]Validation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\nValidation rmse = 0.498029\n\r                                                                               \r\r100%|##########| 30/30 [02:09&lt;00:00,  3.48s/it]\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:17&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]Model 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nModel 0 best validation rmse = 0.480356 on epoch 27\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\r                                                                               \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 95%|█████████▌| 19/20 [33:19&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:19&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\r100%|##########| 4/4 [00:00&lt;00:00, 49.32it/s]\n\r 95%|█████████▌| 19/20 [33:19&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]\r                                                                               \r\n\r 95%|█████████▌| 19/20 [33:19&lt;01:44, 104.68s/it, best loss: 0.5095699475359967]Model 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nModel 0 test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\nEnsemble test rmse = 0.569680\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nSeed 0 ==&gt; test rmse = 0.569680\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nOverall test rmse = 0.569680 +/- 0.000000\nnum params: 6,141,704\nnum params: 6,141,704\n0.569680028419621 +/- 0.0 rmse\n0.569680028419621 +/- 0.0 rmse\n\r100%|██████████| 20/20 [33:19&lt;00:00, 114.06s/it, best loss: 0.5095699475359967]\nbest\nbest\n{&#39;depth&#39;: 6, &#39;dropout&#39;: 0.0, &#39;ffn_num_layers&#39;: 3, &#39;hidden_size&#39;: 400}\n{&#39;depth&#39;: 6, &#39;dropout&#39;: 0.0, &#39;ffn_num_layers&#39;: 3, &#39;hidden_size&#39;: 400}\nnum params: 644,504\nnum params: 644,504\n0.5095699475359967 +/- 0.0 rmse\n0.5095699475359967 +/- 0.0 rmse\n</div>"]}}],"execution_count":70},{"cell_type":"code","source":["%sh cp /dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv /dbfs/FileStore/chemprop/JAK/SLogPall-1825.csv \nsed 1d /dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv >> /dbfs/FileStore/chemprop/JAK/SLogPall-1825.csv \nsed 1d /dbfs/FileStore/chemprop/JAK/SLogPval-182.csv >> /dbfs/FileStore/chemprop/JAK/SLogPall-1825.csv \nwc /dbfs/FileStore/chemprop/JAK/SLogPall-1825.csv "],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"> 1826  1826 32188 /dbfs/FileStore/chemprop/JAK/SLogPall-1825.csv\n</div>"]}}],"execution_count":71},{"cell_type":"markdown","source":["#### SLogP 10-fold CV - all data"],"metadata":{}},{"cell_type":"code","source":["\nparser = ArgumentParser()\nadd_train_args(parser)\nparser.add_argument('--num_iters', type=int, default=20,\n                    help='Number of hyperparameter choices to try')\nparser.add_argument('--config_save_path', type=str, required=True,\n                    help='Path to .json file where best hyperparameter settings will be written')\nparser.add_argument('--log_dir', type=str,\n                    help='(Optional) Path to a directory where all results of the hyperparameter optimization will be written')\nif not os.path.exists(os.path.join(VIRTUAL_SCREENING,'Chemprop-Feat_SLogP')):\n  os.mkdir(os.path.join(VIRTUAL_SCREENING,'Chemprop-Feat_SLogP'))\n\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','all-1825.csv'),\n                          '--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPall-1825.csv'),\n                          '--dataset_type','regression',\n                          '--save_dir',os.path.join(VIRTUAL_SCREENING,'Chemprop-Feat_SLogP'),\n                          #'--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182.csv'),\n                          #'--separate_val_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPval-182.csv'),\n                          #'--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183.csv'),\n                          #'--separate_test_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtest-183.csv'),\n                          '--config_save_path',os.path.join(VIRTUAL_SCREENING,'Chemprop-Feat_SLogP','regression-4x.json'),\n                          '--log_dir',os.path.join(VIRTUAL_SCREENING,'Chemprop-Feat_SLogP'),\n                          '--save_smiles_splits',\n                          '--split_type','scaffold_balanced',\n                          '--num_folds','10',\n                          '--seed','13'\n                          ])\nmodify_train_args(args)\n\ngrid_search(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]{&#39;depth&#39;: 2, &#39;dropout&#39;: 0.15000000000000002, &#39;ffn_num_layers&#39;: 2, &#39;hidden_size&#39;: 1800}\n{&#39;depth&#39;: 2, &#39;dropout&#39;: 0.15000000000000002, &#39;ffn_num_layers&#39;: 2, &#39;hidden_size&#39;: 1800}\n{&#39;depth&#39;: 2, &#39;dropout&#39;: 0.15000000000000002, &#39;ffn_num_layers&#39;: 2, &#39;hidden_size&#39;: 1800}\nFold 0\nFold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/ZINC/virtual_screening/Chemprop-Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/all-1825.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 2,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPall-1825.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 2,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1800,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/ZINC/virtual_screening/Chemprop-Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 10,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/ZINC/virtual_screening/Chemprop-Feat_SLogP/depth_2_dropout_0.15000000000000002_ffn_num_layers_2_hidden_size_1800/fold_0&#39;,\n &#39;save_smiles_splits&#39;: True,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;scaffold_balanced&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPall-1825.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/ZINC/virtual_screening/Chemprop-Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/all-1825.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 2,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPall-1825.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 2,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1800,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/ZINC/virtual_screening/Chemprop-Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 10,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/ZINC/virtual_screening/Chemprop-Feat_SLogP/depth_2_dropout_0.15000000000000002_ffn_num_layers_2_hidden_size_1800/fold_0&#39;,\n &#39;save_smiles_splits&#39;: True,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;scaffold_balanced&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPall-1825.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/ZINC/virtual_screening/Chemprop-Feat_SLogP/regression-4x.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/all-1825.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 2,\n &#39;dropout&#39;: 0.15000000000000002,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPall-1825.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 2,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1800,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/ZINC/virtual_screening/Chemprop-Feat_SLogP&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 10,\n &#39;num_iters&#39;: 20,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/ZINC/virtual_screening/Chemprop-Feat_SLogP/depth_2_dropout_0.15000000000000002_ffn_num_layers_2_hidden_size_1800/fold_0&#39;,\n &#39;save_smiles_splits&#39;: True,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;scaffold_balanced&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPall-1825.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\nLoading data\n\n\n\r                                                    \r\r  0%|          | 0/1825 [00:00&lt;?, ?it/s]\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 16%|#6        | 295/1825 [00:00&lt;00:00, 2948.88it/s]\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 32%|###2      | 587/1825 [00:00&lt;00:00, 2938.31it/s]\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 48%|####7     | 873/1825 [00:00&lt;00:00, 2912.76it/s]\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 63%|######3   | 1156/1825 [00:00&lt;00:00, 2884.86it/s]\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 75%|#######5  | 1374/1825 [00:00&lt;00:00, 2331.35it/s]\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 91%|######### | 1652/1825 [00:00&lt;00:00, 2447.24it/s]\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:00&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r100%|##########| 1825/1825 [00:00&lt;00:00, 2612.02it/s]\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]Number of tasks = 4\nNumber of tasks = 4\nNumber of tasks = 4\nSplitting data with seed 13\nSplitting data with seed 13\nSplitting data with seed 13\n\n\n\r                                                    \r\r  0%|          | 0/1825 [00:00&lt;?, ?it/s]\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 16%|#6        | 299/1825 [00:00&lt;00:00, 2985.43it/s]\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 32%|###2      | 585/1825 [00:00&lt;00:00, 2945.24it/s]\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 48%|####8     | 877/1825 [00:00&lt;00:00, 2937.34it/s]\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 64%|######3   | 1167/1825 [00:00&lt;00:00, 2926.00it/s]\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 80%|########  | 1462/1825 [00:00&lt;00:00, 2930.32it/s]\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 96%|#########5| 1747/1825 [00:00&lt;00:00, 2905.64it/s]\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r100%|##########| 1825/1825 [00:00&lt;00:00, 2900.74it/s]\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:01&lt;?, ?it/s, best loss: ?]Total scaffolds = 539 | train scaffolds = 376 | val scaffolds = 87 | test scaffolds = 76\nTotal scaffolds = 539 | train scaffolds = 376 | val scaffolds = 87 | test scaffolds = 76\nTotal scaffolds = 539 | train scaffolds = 376 | val scaffolds = 87 | test scaffolds = 76\nLabel averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([8.42196021, 8.24984682, 7.37055482, 7.06242526]), array([137, 137, 137, 137])), (array([6.68895519, 6.56932242, 6.81466945, 5.81875876]), array([2, 2, 2, 2])), (array([7.44977165, 7.23657201, 6.75945075, 5.66958623]), array([1, 1, 1, 1])), (array([8.49214413, 8.52724355, 7.69464863, 6.86646109]), array([1, 1, 1, 1])), (array([8.35710815, 7.87773023, 7.52371352, 6.79191497]), array([44, 44, 44, 44])), (array([9.16289453, 8.92572443, 8.54297201, 7.96684103]), array([3, 3, 3, 3])), (array([7.34582346, 7.60906489, 6.92811799, 6.16941133]), array([1, 1, 1, 1])), (array([6.09312647, 6.4723701 , 5.537602  , 4.38510278]), array([1, 1, 1, 1])), (array([7.27818938, 7.04672366, 6.53610701, 5.8728952 ]), array([1, 1, 1, 1])), (array([7.12726117, 6.99567863, 6.43297363, 5.53610701]), array([1, 1, 1, 1]))]\nLabel averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([8.42196021, 8.24984682, 7.37055482, 7.06242526]), array([137, 137, 137, 137])), (array([6.68895519, 6.56932242, 6.81466945, 5.81875876]), array([2, 2, 2, 2])), (array([7.44977165, 7.23657201, 6.75945075, 5.66958623]), array([1, 1, 1, 1])), (array([8.49214413, 8.52724355, 7.69464863, 6.86646109]), array([1, 1, 1, 1])), (array([8.35710815, 7.87773023, 7.52371352, 6.79191497]), array([44, 44, 44, 44])), (array([9.16289453, 8.92572443, 8.54297201, 7.96684103]), array([3, 3, 3, 3])), (array([7.34582346, 7.60906489, 6.92811799, 6.16941133]), array([1, 1, 1, 1])), (array([6.09312647, 6.4723701 , 5.537602  , 4.38510278]), array([1, 1, 1, 1])), (array([7.27818938, 7.04672366, 6.53610701, 5.8728952 ]), array([1, 1, 1, 1])), (array([7.12726117, 6.99567863, 6.43297363, 5.53610701]), array([1, 1, 1, 1]))]\nLabel averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([8.42196021, 8.24984682, 7.37055482, 7.06242526]), array([137, 137, 137, 137])), (array([6.68895519, 6.56932242, 6.81466945, 5.81875876]), array([2, 2, 2, 2])), (array([7.44977165, 7.23657201, 6.75945075, 5.66958623]), array([1, 1, 1, 1])), (array([8.49214413, 8.52724355, 7.69464863, 6.86646109]), array([1, 1, 1, 1])), (array([8.35710815, 7.87773023, 7.52371352, 6.79191497]), array([44, 44, 44, 44])), (array([9.16289453, 8.92572443, 8.54297201, 7.96684103]), array([3, 3, 3, 3])), (array([7.34582346, 7.60906489, 6.92811799, 6.16941133]), array([1, 1, 1, 1])), (array([6.09312647, 6.4723701 , 5.537602  , 4.38510278]), array([1, 1, 1, 1])), (array([7.27818938, 7.04672366, 6.53610701, 5.8728952 ]), array([1, 1, 1, 1])), (array([7.12726117, 6.99567863, 6.43297363, 5.53610701]), array([1, 1, 1, 1]))]\nTotal size = 1,825 | train size = 1,460 | val size = 182 | test size = 183\nTotal size = 1,825 | train size = 1,460 | val size = 182 | test size = 183\nTotal size = 1,825 | train size = 1,460 | val size = 182 | test size = 183\nFitting scaler\nFitting scaler\nFitting scaler\nBuilding model 0\nBuilding model 0\nBuilding model 0\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1800, bias=False)\n      (W_h): Linear(in_features=1800, out_features=1800, bias=False)\n      (W_o): Linear(in_features=1933, out_features=1800, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=1801, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1800, bias=False)\n      (W_h): Linear(in_features=1800, out_features=1800, bias=False)\n      (W_o): Linear(in_features=1933, out_features=1800, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=1801, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.15000000000000002)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1800, bias=False)\n      (W_h): Linear(in_features=1800, out_features=1800, bias=False)\n      (W_o): Linear(in_features=1933, out_features=1800, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.15000000000000002)\n    (1): Linear(in_features=1801, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.15000000000000002)\n    (4): Linear(in_features=300, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 7,527,604\nNumber of parameters = 7,527,604\nNumber of parameters = 7,527,604\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\n\n\r                                                    \r\r  0%|          | 0/30 [00:00&lt;?, ?it/s]\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]Epoch 0\nEpoch 0\nEpoch 0\n\n\n\r                                                    \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r  3%|3         | 1/29 [00:00&lt;00:03,  7.36it/s]\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r  7%|6         | 2/29 [00:00&lt;00:03,  7.63it/s]\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 10%|#         | 3/29 [00:00&lt;00:03,  7.86it/s]\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 14%|#3        | 4/29 [00:00&lt;00:03,  7.89it/s]\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 17%|#7        | 5/29 [00:00&lt;00:03,  7.89it/s]\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 21%|##        | 6/29 [00:00&lt;00:02,  7.97it/s]\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 24%|##4       | 7/29 [00:00&lt;00:02,  8.10it/s]\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:10&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 28%|##7       | 8/29 [00:00&lt;00:02,  8.12it/s]\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 31%|###1      | 9/29 [00:01&lt;00:02,  8.08it/s]\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]Loss = 1.9795e-02, PNorm = 66.7996, GNorm = 1.1913, lr_0 = 2.5517e-04\nLoss = 1.9795e-02, PNorm = 66.7996, GNorm = 1.1913, lr_0 = 2.5517e-04\nLoss = 1.9795e-02, PNorm = 66.7996, GNorm = 1.1913, lr_0 = 2.5517e-04\n\n\n\r                                                    \r\r 34%|###4      | 10/29 [00:01&lt;00:02,  6.50it/s]\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 38%|###7      | 11/29 [00:01&lt;00:02,  6.90it/s]\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 41%|####1     | 12/29 [00:01&lt;00:02,  7.16it/s]\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 45%|####4     | 13/29 [00:01&lt;00:02,  7.38it/s]\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 48%|####8     | 14/29 [00:01&lt;00:01,  7.59it/s]\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:11&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 52%|#####1    | 15/29 [00:01&lt;00:01,  7.71it/s]\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 55%|#####5    | 16/29 [00:02&lt;00:01,  7.85it/s]\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 59%|#####8    | 17/29 [00:02&lt;00:01,  7.95it/s]\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 62%|######2   | 18/29 [00:02&lt;00:01,  8.03it/s]\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 66%|######5   | 19/29 [00:02&lt;00:01,  8.14it/s]\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]Loss = 1.8438e-02, PNorm = 66.8465, GNorm = 1.4469, lr_0 = 4.1034e-04\nLoss = 1.8438e-02, PNorm = 66.8465, GNorm = 1.4469, lr_0 = 4.1034e-04\nLoss = 1.8438e-02, PNorm = 66.8465, GNorm = 1.4469, lr_0 = 4.1034e-04\n\n\n\r                                                    \r\r 69%|######8   | 20/29 [00:02&lt;00:01,  6.00it/s]\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 72%|#######2  | 21/29 [00:02&lt;00:01,  6.53it/s]\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:12&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 76%|#######5  | 22/29 [00:02&lt;00:00,  7.03it/s]\n\n\n\r  0%|          | 0/20 [00:13&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:13&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 79%|#######9  | 23/29 [00:03&lt;00:00,  7.32it/s]\n\n\n\r  0%|          | 0/20 [00:13&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:13&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 83%|########2 | 24/29 [00:03&lt;00:00,  7.59it/s]\n\n\n\r  0%|          | 0/20 [00:13&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:13&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 86%|########6 | 25/29 [00:03&lt;00:00,  7.81it/s]\n\n\n\r  0%|          | 0/20 [00:13&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:13&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 90%|########9 | 26/29 [00:03&lt;00:00,  7.64it/s]\n\n\n\r  0%|          | 0/20 [00:13&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\n\n\n\r  0%|          | 0/20 [00:13&lt;?, ?it/s, best loss: ?]\n\n\r                                                    \r\r 93%|#########3| 27/29 [00:03&lt;00:00,  7.76it/s]\n\n\n\r  0%|          | 0/20 [00:13&lt;?, ?it/s, best loss: ?]\n\n*** WARNING: skipped 50199095 bytes of output ***\n\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r100%|##########| 4/4 [00:00&lt;00:00, 61.08it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Validation rmse = 0.600806\nValidation rmse = 0.600806\nValidation rmse = 0.600806\n\n\n\r                                                                                  \r\r 90%|######### | 27/30 [00:41&lt;00:03,  1.12s/it]\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Epoch 27\nEpoch 27\nEpoch 27\n\n\n\r                                                                                  \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 10%|#         | 3/29 [00:00&lt;00:01, 25.76it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 21%|##        | 6/29 [00:00&lt;00:00, 26.81it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:35&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Loss = 4.1123e-03, PNorm = 52.8906, GNorm = 0.9062, lr_0 = 1.2546e-04\nLoss = 4.1123e-03, PNorm = 52.8906, GNorm = 0.9062, lr_0 = 1.2546e-04\nLoss = 4.1123e-03, PNorm = 52.8906, GNorm = 0.9062, lr_0 = 1.2546e-04\n\n\n\r                                                                                  \r\r 31%|###1      | 9/29 [00:00&lt;00:00, 27.45it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 45%|####4     | 13/29 [00:00&lt;00:00, 28.29it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Loss = 5.0811e-03, PNorm = 52.9007, GNorm = 2.3951, lr_0 = 1.2196e-04\nLoss = 5.0811e-03, PNorm = 52.9007, GNorm = 2.3951, lr_0 = 1.2196e-04\nLoss = 5.0811e-03, PNorm = 52.9007, GNorm = 2.3951, lr_0 = 1.2196e-04\n\n\n\r                                                                                  \r\r 59%|#####8    | 17/29 [00:00&lt;00:00, 27.80it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 72%|#######2  | 21/29 [00:00&lt;00:00, 28.51it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 86%|########6 | 25/29 [00:00&lt;00:00, 29.05it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Loss = 4.1619e-03, PNorm = 52.9080, GNorm = 0.6999, lr_0 = 1.1855e-04\nLoss = 4.1619e-03, PNorm = 52.9080, GNorm = 0.6999, lr_0 = 1.1855e-04\nLoss = 4.1619e-03, PNorm = 52.9080, GNorm = 0.6999, lr_0 = 1.1855e-04\n\n\n\r                                                                                  \r\r 97%|#########6| 28/29 [00:00&lt;00:00, 29.16it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r100%|##########| 29/29 [00:01&lt;00:00, 28.97it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r100%|##########| 4/4 [00:00&lt;00:00, 48.76it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Validation rmse = 0.628705\nValidation rmse = 0.628705\nValidation rmse = 0.628705\n\n\n\r                                                                                  \r\r 93%|#########3| 28/30 [00:42&lt;00:02,  1.11s/it]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Epoch 28\nEpoch 28\nEpoch 28\n\n\n\r                                                                                  \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 14%|#3        | 4/29 [00:00&lt;00:00, 30.33it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:36&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Loss = 4.1939e-03, PNorm = 52.9173, GNorm = 0.8558, lr_0 = 1.1523e-04\nLoss = 4.1939e-03, PNorm = 52.9173, GNorm = 0.8558, lr_0 = 1.1523e-04\nLoss = 4.1939e-03, PNorm = 52.9173, GNorm = 0.8558, lr_0 = 1.1523e-04\n\n\n\r                                                                                  \r\r 28%|##7       | 8/29 [00:00&lt;00:00, 30.14it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 38%|###7      | 11/29 [00:00&lt;00:00, 29.85it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 48%|####8     | 14/29 [00:00&lt;00:00, 29.50it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 59%|#####8    | 17/29 [00:00&lt;00:00, 28.21it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Loss = 4.3973e-03, PNorm = 52.9250, GNorm = 0.8398, lr_0 = 1.1201e-04\nLoss = 4.3973e-03, PNorm = 52.9250, GNorm = 0.8398, lr_0 = 1.1201e-04\nLoss = 4.3973e-03, PNorm = 52.9250, GNorm = 0.8398, lr_0 = 1.1201e-04\n\n\n\r                                                                                  \r\r 69%|######8   | 20/29 [00:00&lt;00:00, 28.40it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 79%|#######9  | 23/29 [00:00&lt;00:00, 28.79it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 90%|########9 | 26/29 [00:00&lt;00:00, 29.08it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Loss = 4.4794e-03, PNorm = 52.9323, GNorm = 1.7840, lr_0 = 1.0888e-04\nLoss = 4.4794e-03, PNorm = 52.9323, GNorm = 1.7840, lr_0 = 1.0888e-04\nLoss = 4.4794e-03, PNorm = 52.9323, GNorm = 1.7840, lr_0 = 1.0888e-04\n\n\n\r                                                                                  \r\r100%|##########| 29/29 [00:01&lt;00:00, 27.90it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r100%|##########| 4/4 [00:00&lt;00:00, 60.47it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Validation rmse = 0.623584\nValidation rmse = 0.623584\nValidation rmse = 0.623584\n\n\n\r                                                                                  \r\r 97%|#########6| 29/30 [00:43&lt;00:01,  1.10s/it]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Epoch 29\nEpoch 29\nEpoch 29\n\n\n\r                                                                                  \r\r  0%|          | 0/29 [00:00&lt;?, ?it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:37&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 14%|#3        | 4/29 [00:00&lt;00:00, 30.29it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 28%|##7       | 8/29 [00:00&lt;00:00, 30.37it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Loss = 4.6911e-03, PNorm = 52.9418, GNorm = 0.8788, lr_0 = 1.0584e-04\nLoss = 4.6911e-03, PNorm = 52.9418, GNorm = 0.8788, lr_0 = 1.0584e-04\nLoss = 4.6911e-03, PNorm = 52.9418, GNorm = 0.8788, lr_0 = 1.0584e-04\n\n\n\r                                                                                  \r\r 38%|###7      | 11/29 [00:00&lt;00:00, 29.86it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 48%|####8     | 14/29 [00:00&lt;00:00, 29.03it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 62%|######2   | 18/29 [00:00&lt;00:00, 29.47it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Loss = 4.4178e-03, PNorm = 52.9486, GNorm = 0.7553, lr_0 = 1.0288e-04\nLoss = 4.4178e-03, PNorm = 52.9486, GNorm = 0.7553, lr_0 = 1.0288e-04\nLoss = 4.4178e-03, PNorm = 52.9486, GNorm = 0.7553, lr_0 = 1.0288e-04\n\n\n\r                                                                                  \r\r 72%|#######2  | 21/29 [00:00&lt;00:00, 29.52it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 86%|########6 | 25/29 [00:00&lt;00:00, 29.88it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r 97%|#########6| 28/29 [00:00&lt;00:00, 29.35it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Loss = 3.9507e-03, PNorm = 52.9559, GNorm = 1.1415, lr_0 = 1.0000e-04\nLoss = 3.9507e-03, PNorm = 52.9559, GNorm = 1.1415, lr_0 = 1.0000e-04\nLoss = 3.9507e-03, PNorm = 52.9559, GNorm = 1.1415, lr_0 = 1.0000e-04\n\n\n\r                                                                                  \r\r100%|##########| 29/29 [00:00&lt;00:00, 29.51it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r100%|##########| 4/4 [00:00&lt;00:00, 58.59it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Validation rmse = 0.596746\nValidation rmse = 0.596746\nValidation rmse = 0.596746\n\n\n\r                                                                                  \r\r100%|##########| 30/30 [00:44&lt;00:00,  1.09s/it]\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:38&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Model 0 best validation rmse = 0.585214 on epoch 17\nModel 0 best validation rmse = 0.585214 on epoch 17\nModel 0 best validation rmse = 0.585214 on epoch 17\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\n\n\r                                                                                  \r\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:39&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:39&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\r100%|##########| 4/4 [00:00&lt;00:00, 13.66it/s]\n\n\n\r 95%|█████████▌| 19/20 [5:51:39&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]\n\n\r                                                                                  \r\n\n\n\r 95%|█████████▌| 19/20 [5:51:39&lt;19:35, 1175.43s/it, best loss: 0.5603974030183922]Model 0 test rmse = 0.636039\nModel 0 test rmse = 0.636039\nModel 0 test rmse = 0.636039\nEnsemble test rmse = 0.636039\nEnsemble test rmse = 0.636039\nEnsemble test rmse = 0.636039\n10-fold cross validation\n10-fold cross validation\n10-fold cross validation\nSeed 13 ==&gt; test rmse = 0.582012\nSeed 13 ==&gt; test rmse = 0.582012\nSeed 13 ==&gt; test rmse = 0.582012\nSeed 14 ==&gt; test rmse = 0.603026\nSeed 14 ==&gt; test rmse = 0.603026\nSeed 14 ==&gt; test rmse = 0.603026\nSeed 15 ==&gt; test rmse = 0.599021\nSeed 15 ==&gt; test rmse = 0.599021\nSeed 15 ==&gt; test rmse = 0.599021\nSeed 16 ==&gt; test rmse = 0.569116\nSeed 16 ==&gt; test rmse = 0.569116\nSeed 16 ==&gt; test rmse = 0.569116\nSeed 17 ==&gt; test rmse = 0.713836\nSeed 17 ==&gt; test rmse = 0.713836\nSeed 17 ==&gt; test rmse = 0.713836\nSeed 18 ==&gt; test rmse = 0.539931\nSeed 18 ==&gt; test rmse = 0.539931\nSeed 18 ==&gt; test rmse = 0.539931\nSeed 19 ==&gt; test rmse = 0.556047\nSeed 19 ==&gt; test rmse = 0.556047\nSeed 19 ==&gt; test rmse = 0.556047\nSeed 20 ==&gt; test rmse = 0.532042\nSeed 20 ==&gt; test rmse = 0.532042\nSeed 20 ==&gt; test rmse = 0.532042\nSeed 21 ==&gt; test rmse = 0.608180\nSeed 21 ==&gt; test rmse = 0.608180\nSeed 21 ==&gt; test rmse = 0.608180\nSeed 22 ==&gt; test rmse = 0.636039\nSeed 22 ==&gt; test rmse = 0.636039\nSeed 22 ==&gt; test rmse = 0.636039\nOverall test rmse = 0.593925 +/- 0.050400\nOverall test rmse = 0.593925 +/- 0.050400\nOverall test rmse = 0.593925 +/- 0.050400\nnum params: 1,478,804\nnum params: 1,478,804\nnum params: 1,478,804\n0.5939250685818486 +/- 0.05039952832728787 rmse\n0.5939250685818486 +/- 0.05039952832728787 rmse\n0.5939250685818486 +/- 0.05039952832728787 rmse\n\n\n\r100%|██████████| 20/20 [5:51:39&lt;00:00, 985.57s/it, best loss: 0.5603974030183922] best\nbest\nbest\n{&#39;depth&#39;: 6, &#39;dropout&#39;: 0.0, &#39;ffn_num_layers&#39;: 2, &#39;hidden_size&#39;: 1900}\n{&#39;depth&#39;: 6, &#39;dropout&#39;: 0.0, &#39;ffn_num_layers&#39;: 2, &#39;hidden_size&#39;: 1900}\n{&#39;depth&#39;: 6, &#39;dropout&#39;: 0.0, &#39;ffn_num_layers&#39;: 2, &#39;hidden_size&#39;: 1900}\nnum params: 8,325,704\nnum params: 8,325,704\nnum params: 8,325,704\n0.5603974030183922 +/- 0.03756259963519687 rmse\n0.5603974030183922 +/- 0.03756259963519687 rmse\n0.5603974030183922 +/- 0.03756259963519687 rmse\n</div>"]}}],"execution_count":73},{"cell_type":"code","source":["%sh ls /dbfs/FileStore/ZINC/virtual_screening/Chemprop-Feat_SLogP"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">depth_2_dropout_0.05_ffn_num_layers_1_hidden_size_2100\ndepth_2_dropout_0.15000000000000002_ffn_num_layers_2_hidden_size_1000\ndepth_2_dropout_0.15000000000000002_ffn_num_layers_2_hidden_size_1800\ndepth_3_dropout_0.15000000000000002_ffn_num_layers_3_hidden_size_1600\ndepth_3_dropout_0.1_ffn_num_layers_3_hidden_size_300\ndepth_3_dropout_0.1_ffn_num_layers_3_hidden_size_700\ndepth_3_dropout_0.25_ffn_num_layers_2_hidden_size_1500\ndepth_3_dropout_0.25_ffn_num_layers_2_hidden_size_700\ndepth_3_dropout_0.30000000000000004_ffn_num_layers_1_hidden_size_1700\ndepth_3_dropout_0.4_ffn_num_layers_1_hidden_size_1200\ndepth_4_dropout_0.0_ffn_num_layers_2_hidden_size_1200\ndepth_4_dropout_0.15000000000000002_ffn_num_layers_2_hidden_size_600\ndepth_4_dropout_0.15000000000000002_ffn_num_layers_3_hidden_size_1900\ndepth_4_dropout_0.25_ffn_num_layers_2_hidden_size_1000\ndepth_4_dropout_0.25_ffn_num_layers_3_hidden_size_2200\ndepth_4_dropout_0.2_ffn_num_layers_2_hidden_size_2000\ndepth_4_dropout_0.30000000000000004_ffn_num_layers_2_hidden_size_1100\ndepth_4_dropout_0.35000000000000003_ffn_num_layers_2_hidden_size_1500\ndepth_4_dropout_0.4_ffn_num_layers_2_hidden_size_1500\ndepth_4_dropout_0.4_ffn_num_layers_3_hidden_size_1200\ndepth_6_dropout_0.0_ffn_num_layers_2_hidden_size_1900\ndepth_6_dropout_0.4_ffn_num_layers_1_hidden_size_2200\nfold_0\nfold_1\nfold_2\nfold_3\nfold_4\nfold_5\nfold_6\nfold_7\nfold_8\nfold_9\nquiet.log\nregression-4x.json\nverbose.log\n</div>"]}}],"execution_count":74},{"cell_type":"markdown","source":["### HLM Regression hyperparameter tuning"],"metadata":{}},{"cell_type":"code","source":["data = os.path.join(CHEMPROP_DIR,'hlm_eh.csv')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":76},{"cell_type":"code","source":["data = os.path.join(CHEMPROP_DIR,'hlm_eh.csv')\nparser = ArgumentParser()\nadd_train_args(parser)\nparser.add_argument('--num_iters', type=int, default=30,\n                    help='Number of hyperparameter choices to try')\nparser.add_argument('--config_save_path', type=str, required=True,\n                    help='Path to .json file where best hyperparameter settings will be written')\nparser.add_argument('--log_dir', type=str,\n                    help='(Optional) Path to a directory where all results of the hyperparameter optimization will be written')\nargs = parser.parse_args(['--data_path',data,\n                          '--dataset_type','regression',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'HLM','checkpoints'),\n                          '--config_save_path',os.path.join(CHEMPROP_DIR,'HLM','configs','regression-HLM_Eh.json'),\n                          '--log_dir',os.path.join(CHEMPROP_DIR,'HLM','configs'),\n                          '--save_smiles_splits',\n                          '--split_type','scaffold_balanced',\n                          '--num_folds','5'])\nmodify_train_args(args)\n\ngrid_search(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]{&#39;depth&#39;: 4, &#39;dropout&#39;: 0.05, &#39;ffn_num_layers&#39;: 2, &#39;hidden_size&#39;: 1100}\n{&#39;depth&#39;: 4, &#39;dropout&#39;: 0.05, &#39;ffn_num_layers&#39;: 2, &#39;hidden_size&#39;: 1100}\nFold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/HLM/configs/regression-HLM_Eh.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/hlm_eh.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.05,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 2,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1100,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/HLM/configs&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 5,\n &#39;num_iters&#39;: 30,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/HLM/checkpoints/depth_4_dropout_0.05_ffn_num_layers_2_hidden_size_1100/fold_0&#39;,\n &#39;save_smiles_splits&#39;: True,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;scaffold_balanced&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;config_save_path&#39;: &#39;/dbfs/FileStore/chemprop/HLM/configs/regression-HLM_Eh.json&#39;,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/hlm_eh.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 4,\n &#39;dropout&#39;: 0.05,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 300,\n &#39;ffn_num_layers&#39;: 2,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1100,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_dir&#39;: &#39;/dbfs/FileStore/chemprop/HLM/configs&#39;,\n &#39;log_frequency&#39;: 10,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 5,\n &#39;num_iters&#39;: 30,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/HLM/checkpoints/depth_4_dropout_0.05_ffn_num_layers_2_hidden_size_1100/fold_0&#39;,\n &#39;save_smiles_splits&#39;: True,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;scaffold_balanced&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\n\n\r                                                    \r\r  0%|          | 0/804 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 29%|##9       | 237/804 [00:00&lt;00:00, 2366.45it/s]\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 56%|#####6    | 451/804 [00:00&lt;00:00, 2290.88it/s]\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 78%|#######7  | 625/804 [00:00&lt;00:00, 2090.88it/s]\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r100%|#########9| 802/804 [00:00&lt;00:00, 1982.70it/s]\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r100%|##########| 804/804 [00:00&lt;00:00, 2000.75it/s]\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]Warning: 2 SMILES are invalid.\nWarning: 2 SMILES are invalid.\nNumber of tasks = 1\nNumber of tasks = 1\nSplitting data with seed 0\nSplitting data with seed 0\n\n\r                                                    \r\r  0%|          | 0/802 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 26%|##5       | 207/802 [00:00&lt;00:00, 2063.63it/s]\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 49%|####9     | 395/802 [00:00&lt;00:00, 2002.10it/s]\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 69%|######8   | 552/802 [00:00&lt;00:00, 1844.55it/s]\n\n\r  0%|          | 0/30 [00:01&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:01&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 87%|########7 | 698/802 [00:00&lt;00:00, 1707.59it/s]\n\n\r  0%|          | 0/30 [00:01&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:01&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r100%|##########| 802/802 [00:00&lt;00:00, 1681.85it/s]\n\n\r  0%|          | 0/30 [00:01&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:01&lt;?, ?it/s, best loss: ?]Total scaffolds = 310 | train scaffolds = 253 | val scaffolds = 28 | test scaffolds = 29\nTotal scaffolds = 310 | train scaffolds = 253 | val scaffolds = 28 | test scaffolds = 29\nLabel averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([75.]), array([1])), (array([67.5]), array([2])), (array([84.]), array([1])), (array([35.]), array([2])), (array([81.]), array([1])), (array([53.]), array([2])), (array([80.6]), array([5])), (array([96.]), array([2])), (array([28.]), array([1])), (array([69.5]), array([4]))]\nLabel averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([75.]), array([1])), (array([67.5]), array([2])), (array([84.]), array([1])), (array([35.]), array([2])), (array([81.]), array([1])), (array([53.]), array([2])), (array([80.6]), array([5])), (array([96.]), array([2])), (array([28.]), array([1])), (array([69.5]), array([4]))]\nTotal size = 802 | train size = 641 | val size = 80 | test size = 81\nTotal size = 802 | train size = 641 | val size = 80 | test size = 81\nFitting scaler\nFitting scaler\nBuilding model 0\nBuilding model 0\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.05)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1100, bias=False)\n      (W_h): Linear(in_features=1100, out_features=1100, bias=False)\n      (W_o): Linear(in_features=1233, out_features=1100, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.05)\n    (1): Linear(in_features=1100, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.05)\n    (4): Linear(in_features=300, out_features=1, bias=True)\n  )\n)\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.05)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1100, bias=False)\n      (W_h): Linear(in_features=1100, out_features=1100, bias=False)\n      (W_o): Linear(in_features=1233, out_features=1100, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.05)\n    (1): Linear(in_features=1100, out_features=300, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.05)\n    (4): Linear(in_features=300, out_features=1, bias=True)\n  )\n)\nNumber of parameters = 3,059,701\nNumber of parameters = 3,059,701\nMoving model to cuda\nMoving model to cuda\n\n\r                                                    \r\r  0%|          | 0/30 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:07&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:07&lt;?, ?it/s, best loss: ?]Epoch 0\nEpoch 0\n\n\r                                                    \r\r  0%|          | 0/12 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:07&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:07&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r  8%|8         | 1/12 [00:00&lt;00:01,  5.52it/s]\n\n\r  0%|          | 0/30 [00:07&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:07&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 17%|#6        | 2/12 [00:00&lt;00:01,  5.60it/s]\n\n\r  0%|          | 0/30 [00:07&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:07&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 25%|##5       | 3/12 [00:00&lt;00:01,  5.61it/s]\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 33%|###3      | 4/12 [00:00&lt;00:01,  5.81it/s]\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 42%|####1     | 5/12 [00:00&lt;00:01,  6.03it/s]\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 50%|#####     | 6/12 [00:01&lt;00:01,  5.21it/s]\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 58%|#####8    | 7/12 [00:01&lt;00:00,  5.67it/s]\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 67%|######6   | 8/12 [00:01&lt;00:00,  5.94it/s]\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:08&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 75%|#######5  | 9/12 [00:01&lt;00:00,  6.15it/s]\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]Loss = 2.7051e-02, PNorm = 54.7288, GNorm = 5.8131, lr_0 = 4.7500e-04\nLoss = 2.7051e-02, PNorm = 54.7288, GNorm = 5.8131, lr_0 = 4.7500e-04\n\n\r                                                    \r\r 83%|########3 | 10/12 [00:01&lt;00:00,  6.43it/s]\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 92%|#########1| 11/12 [00:01&lt;00:00,  6.75it/s]\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r100%|##########| 12/12 [00:01&lt;00:00,  7.15it/s]\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 50%|#####     | 1/2 [00:00&lt;00:00,  8.05it/s]\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r100%|##########| 2/2 [00:00&lt;00:00, 10.40it/s]\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:09&lt;?, ?it/s, best loss: ?]Validation rmse = 23.762855\nValidation rmse = 23.762855\n\n\r                                                    \r\r  3%|3         | 1/30 [00:04&lt;02:02,  4.21s/it]\n\n\r  0%|          | 0/30 [00:11&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:11&lt;?, ?it/s, best loss: ?]Epoch 1\nEpoch 1\n\n\r                                                    \r\r  0%|          | 0/12 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:11&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:11&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 17%|#6        | 2/12 [00:00&lt;00:01,  7.26it/s]\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 33%|###3      | 4/12 [00:00&lt;00:00,  8.59it/s]\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 50%|#####     | 6/12 [00:00&lt;00:00,  9.75it/s]\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]Loss = 2.2068e-02, PNorm = 54.7749, GNorm = 0.7568, lr_0 = 8.5000e-04\nLoss = 2.2068e-02, PNorm = 54.7749, GNorm = 0.7568, lr_0 = 8.5000e-04\n\n\r                                                    \r\r 67%|######6   | 8/12 [00:00&lt;00:00, 10.91it/s]\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 83%|########3 | 10/12 [00:00&lt;00:00, 11.91it/s]\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r100%|##########| 12/12 [00:00&lt;00:00, 12.93it/s]\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r100%|##########| 2/2 [00:00&lt;00:00, 49.35it/s]\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:12&lt;?, ?it/s, best loss: ?]Validation rmse = 22.996218\nValidation rmse = 22.996218\n\n\r                                                    \r\r  7%|6         | 2/30 [00:07&lt;01:47,  3.85s/it]\n\n\r  0%|          | 0/30 [00:14&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:14&lt;?, ?it/s, best loss: ?]Epoch 2\nEpoch 2\n\n\r                                                    \r\r  0%|          | 0/12 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:14&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:14&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 17%|#6        | 2/12 [00:00&lt;00:00, 15.57it/s]\n\n\r  0%|          | 0/30 [00:14&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:14&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 33%|###3      | 4/12 [00:00&lt;00:00, 15.60it/s]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]Loss = 1.9075e-02, PNorm = 54.8440, GNorm = 1.0488, lr_0 = 9.5972e-04\nLoss = 1.9075e-02, PNorm = 54.8440, GNorm = 1.0488, lr_0 = 9.5972e-04\n\n\r                                                    \r\r 50%|#####     | 6/12 [00:00&lt;00:00, 15.71it/s]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 67%|######6   | 8/12 [00:00&lt;00:00, 15.90it/s]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 83%|########3 | 10/12 [00:00&lt;00:00, 15.91it/s]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r100%|##########| 12/12 [00:00&lt;00:00, 15.85it/s]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r100%|##########| 2/2 [00:00&lt;00:00, 49.10it/s]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]Validation rmse = 23.022010\nValidation rmse = 23.022010\n\n\r                                                    \r\r 10%|#         | 3/30 [00:08&lt;01:19,  2.94s/it]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]Epoch 3\nEpoch 3\n\n\r                                                    \r\r  0%|          | 0/12 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 17%|#6        | 2/12 [00:00&lt;00:00, 16.44it/s]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]Loss = 2.0693e-02, PNorm = 54.8962, GNorm = 1.2438, lr_0 = 8.9615e-04\nLoss = 2.0693e-02, PNorm = 54.8962, GNorm = 1.2438, lr_0 = 8.9615e-04\n\n\r                                                    \r\r 33%|###3      | 4/12 [00:00&lt;00:00, 16.39it/s]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 50%|#####     | 6/12 [00:00&lt;00:00, 16.44it/s]\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:15&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 67%|######6   | 8/12 [00:00&lt;00:00, 16.33it/s]\n\n\r  0%|          | 0/30 [00:16&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:16&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 83%|########3 | 10/12 [00:00&lt;00:00, 16.43it/s]\n\n\r  0%|          | 0/30 [00:16&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:16&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r100%|##########| 12/12 [00:00&lt;00:00, 16.54it/s]\n\n\r  0%|          | 0/30 [00:16&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:16&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:16&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:16&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r100%|##########| 2/2 [00:00&lt;00:00, 47.17it/s]\n\n\r  0%|          | 0/30 [00:16&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:16&lt;?, ?it/s, best loss: ?]Validation rmse = 22.662678\nValidation rmse = 22.662678\n\n\r                                                    \r\r 13%|#3        | 4/30 [00:10&lt;01:15,  2.91s/it]\n\n\r  0%|          | 0/30 [00:18&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:18&lt;?, ?it/s, best loss: ?]Epoch 4\nEpoch 4\n\n\r                                                    \r\r  0%|          | 0/12 [00:00&lt;?, ?it/s]\n\n\r  0%|          | 0/30 [00:18&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:18&lt;?, ?it/s, best loss: ?]Loss = 1.9295e-02, PNorm = 54.9400, GNorm = 0.6997, lr_0 = 8.3679e-04\nLoss = 1.9295e-02, PNorm = 54.9400, GNorm = 0.6997, lr_0 = 8.3679e-04\n\n\r                                                    \r\r 17%|#6        | 2/12 [00:00&lt;00:00, 15.42it/s]\n\n\r  0%|          | 0/30 [00:18&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:18&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 33%|###3      | 4/12 [00:00&lt;00:00, 15.68it/s]\n\n\r  0%|          | 0/30 [00:18&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n\r  0%|          | 0/30 [00:18&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\r 50%|#####     | 6/12 [00:00&lt;00:00, 15.87it/s]\n\n\r  0%|          | 0/30 [00:18&lt;?, ?it/s, best loss: ?]\n\r                                                    \r\n\n*** WARNING: skipped 24131602 bytes of output ***\n\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:06&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r100%|##########| 12/12 [00:01&lt;00:00,  6.76it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r100%|##########| 2/2 [00:00&lt;00:00, 27.62it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Validation rmse = 17.845549\nValidation rmse = 17.845549\n\n\r                                                                                \r\r 90%|######### | 27/30 [01:53&lt;00:07,  2.55s/it]\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Epoch 27\nEpoch 27\n\n\r                                                                                \r\r  0%|          | 0/12 [00:00&lt;?, ?it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r  8%|8         | 1/12 [00:00&lt;00:01,  6.80it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 17%|#6        | 2/12 [00:00&lt;00:01,  6.79it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 25%|##5       | 3/12 [00:00&lt;00:01,  6.78it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 33%|###3      | 4/12 [00:00&lt;00:01,  6.80it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 42%|####1     | 5/12 [00:00&lt;00:01,  6.75it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Loss = 6.8696e-03, PNorm = 68.1283, GNorm = 3.1618, lr_0 = 1.2282e-04\nLoss = 6.8696e-03, PNorm = 68.1283, GNorm = 3.1618, lr_0 = 1.2282e-04\n\n\r                                                                                \r\r 50%|#####     | 6/12 [00:00&lt;00:00,  6.71it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:07&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 58%|#####8    | 7/12 [00:01&lt;00:00,  6.61it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 67%|######6   | 8/12 [00:01&lt;00:00,  6.64it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 75%|#######5  | 9/12 [00:01&lt;00:00,  6.66it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 83%|########3 | 10/12 [00:01&lt;00:00,  6.62it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 92%|#########1| 11/12 [00:01&lt;00:00,  6.64it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r100%|##########| 12/12 [00:01&lt;00:00,  6.65it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r100%|##########| 2/2 [00:00&lt;00:00, 27.77it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Validation rmse = 19.239290\nValidation rmse = 19.239290\n\n\r                                                                                \r\r 93%|#########3| 28/30 [01:54&lt;00:04,  2.35s/it]\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Epoch 28\nEpoch 28\n\n\r                                                                                \r\r  0%|          | 0/12 [00:00&lt;?, ?it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:08&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r  8%|8         | 1/12 [00:00&lt;00:01,  6.30it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 17%|#6        | 2/12 [00:00&lt;00:01,  6.42it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 25%|##5       | 3/12 [00:00&lt;00:01,  6.53it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Loss = 5.6805e-03, PNorm = 68.1543, GNorm = 0.6461, lr_0 = 1.1469e-04\nLoss = 5.6805e-03, PNorm = 68.1543, GNorm = 0.6461, lr_0 = 1.1469e-04\n\n\r                                                                                \r\r 33%|###3      | 4/12 [00:00&lt;00:01,  6.47it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 42%|####1     | 5/12 [00:00&lt;00:01,  6.57it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 50%|#####     | 6/12 [00:00&lt;00:00,  6.61it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:09&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 58%|#####8    | 7/12 [00:01&lt;00:00,  6.63it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 67%|######6   | 8/12 [00:01&lt;00:00,  6.72it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 75%|#######5  | 9/12 [00:01&lt;00:00,  6.77it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 83%|########3 | 10/12 [00:01&lt;00:00,  6.69it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 92%|#########1| 11/12 [00:01&lt;00:00,  6.50it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r100%|##########| 12/12 [00:01&lt;00:00,  6.54it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 50%|#####     | 1/2 [00:00&lt;00:00,  5.53it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r100%|##########| 2/2 [00:00&lt;00:00,  9.10it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Validation rmse = 17.992277\nValidation rmse = 17.992277\n\n\r                                                                                \r\r 97%|#########6| 29/30 [01:56&lt;00:02,  2.25s/it]\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Epoch 29\nEpoch 29\n\n\r                                                                                \r\r  0%|          | 0/12 [00:00&lt;?, ?it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:10&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r  8%|8         | 1/12 [00:00&lt;00:01,  6.81it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Loss = 6.2625e-03, PNorm = 68.1775, GNorm = 1.3952, lr_0 = 1.0709e-04\nLoss = 6.2625e-03, PNorm = 68.1775, GNorm = 1.3952, lr_0 = 1.0709e-04\n\n\r                                                                                \r\r 17%|#6        | 2/12 [00:00&lt;00:01,  6.77it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 25%|##5       | 3/12 [00:00&lt;00:01,  6.75it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 33%|###3      | 4/12 [00:00&lt;00:01,  6.62it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 42%|####1     | 5/12 [00:00&lt;00:01,  6.68it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 50%|#####     | 6/12 [00:00&lt;00:00,  6.71it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:11&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 58%|#####8    | 7/12 [00:01&lt;00:00,  6.63it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 67%|######6   | 8/12 [00:01&lt;00:00,  6.66it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 75%|#######5  | 9/12 [00:01&lt;00:00,  6.68it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 83%|########3 | 10/12 [00:01&lt;00:00,  6.68it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r 92%|#########1| 11/12 [00:01&lt;00:00,  6.71it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Loss = 6.1621e-03, PNorm = 68.1980, GNorm = 1.0222, lr_0 = 1.0000e-04\nLoss = 6.1621e-03, PNorm = 68.1980, GNorm = 1.0222, lr_0 = 1.0000e-04\n\n\r                                                                                \r\r100%|##########| 12/12 [00:01&lt;00:00,  6.67it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r100%|##########| 2/2 [00:00&lt;00:00, 27.27it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Validation rmse = 18.630218\nValidation rmse = 18.630218\n\n\r                                                                                \r\r100%|##########| 30/30 [01:58&lt;00:00,  2.14s/it]\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:12&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Model 0 best validation rmse = 17.483374 on epoch 23\nModel 0 best validation rmse = 17.483374 on epoch 23\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\nMoving model to cuda\n\n\r                                                                                \r\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:15&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:15&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\r100%|##########| 2/2 [00:00&lt;00:00, 27.40it/s]\n\n\r 97%|█████████▋| 29/30 [3:56:15&lt;11:24, 684.96s/it, best loss: 16.58770441523654]\n\r                                                                                \r\n\n\r 97%|█████████▋| 29/30 [3:56:15&lt;11:24, 684.96s/it, best loss: 16.58770441523654]Model 0 test rmse = 17.406960\nModel 0 test rmse = 17.406960\nEnsemble test rmse = 17.406960\nEnsemble test rmse = 17.406960\n5-fold cross validation\n5-fold cross validation\nSeed 0 ==&gt; test rmse = 17.534950\nSeed 0 ==&gt; test rmse = 17.534950\nSeed 1 ==&gt; test rmse = 15.063834\nSeed 1 ==&gt; test rmse = 15.063834\nSeed 2 ==&gt; test rmse = 15.948268\nSeed 2 ==&gt; test rmse = 15.948268\nSeed 3 ==&gt; test rmse = 19.332737\nSeed 3 ==&gt; test rmse = 19.332737\nSeed 4 ==&gt; test rmse = 17.406960\nSeed 4 ==&gt; test rmse = 17.406960\nOverall test rmse = 17.057350 +/- 1.465052\nOverall test rmse = 17.057350 +/- 1.465052\nnum params: 8,564,001\nnum params: 8,564,001\n17.057349777164006 +/- 1.4650519764229386 rmse\n17.057349777164006 +/- 1.4650519764229386 rmse\n\n\r100%|██████████| 30/30 [3:56:15&lt;00:00, 702.09s/it, best loss: 16.58770441523654]best\nbest\n{&#39;depth&#39;: 6, &#39;dropout&#39;: 0.0, &#39;ffn_num_layers&#39;: 1, &#39;hidden_size&#39;: 1900}\n{&#39;depth&#39;: 6, &#39;dropout&#39;: 0.0, &#39;ffn_num_layers&#39;: 1, &#39;hidden_size&#39;: 1900}\nnum params: 7,755,801\nnum params: 7,755,801\n16.58770441523654 +/- 0.8112090180308419 rmse\n16.58770441523654 +/- 0.8112090180308419 rmse\n</div>"]}}],"execution_count":77},{"cell_type":"code","source":["%sh cat /dbfs/FileStore/chemprop/HLM/configs/regression-HLM_Eh.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{\n    &#34;depth&#34;: 6,\n    &#34;dropout&#34;: 0.0,\n    &#34;ffn_num_layers&#34;: 1,\n    &#34;hidden_size&#34;: 1900\n}</div>"]}}],"execution_count":78},{"cell_type":"markdown","source":["# HLM Classification hyperparameter tuning"],"metadata":{}},{"cell_type":"code","source":["hlm_dataset = pd.read_csv('/dbfs/FileStore/tables/HLM_dataset_Series_B.csv',sep=';')\nhlm_dataset = hlm_dataset.filter(['SMILES','HLM Eh% 1065']).rename(columns={'SMILES':'smiles','HLM Eh% 1065':'HLM_Eh%'})\nhlm_dataset.to_csv(os.path.join(CHEMPROP_DIR,'hlm_eh.csv'),index=False)\nhlm_dataset['HLM_binary'] = hlm_dataset['HLM_Eh%'].apply(lambda x: 1 if x > 33 else 0)\nhlm_dataset['HLM_ternary'] = hlm_dataset['HLM_Eh%'].apply(lambda x: 0 if x < 33 else x)\nhlm_dataset['HLM_ternary'] = hlm_dataset['HLM_ternary'].apply(lambda x: 1 if (x >= 33 and x < 66) else x)\nhlm_dataset['HLM_ternary'] = hlm_dataset['HLM_ternary'].apply(lambda x: 2 if x >= 66 else x)\nhlm_dataset.filter(['smiles','HLM_binary']).to_csv(os.path.join(CHEMPROP_DIR,'hlm_eh_binary.csv'),index=None,float_format='%.0f')\nhlm_dataset.filter(['smiles','HLM_ternary']).to_csv(os.path.join(CHEMPROP_DIR,'hlm_eh_ternary.csv'),index=None,float_format='%.0f')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":80},{"cell_type":"code","source":["data = os.path.join(CHEMPROP_DIR,'hlm_eh_binary.csv')\nparser = ArgumentParser()\nadd_train_args(parser)\nparser.add_argument('--num_iters', type=int, default=30,\n                    help='Number of hyperparameter choices to try')\nparser.add_argument('--config_save_path', type=str, required=True,\n                    help='Path to .json file where best hyperparameter settings will be written')\nparser.add_argument('--log_dir', type=str,\n                    help='(Optional) Path to a directory where all results of the hyperparameter optimization will be written')\nargs = parser.parse_args(['--data_path',data,\n                          '--dataset_type','classification',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'HLM','checkpoints'),\n                          '--config_save_path',os.path.join(CHEMPROP_DIR,'HLM','configs','classification-HLM_Eh.json'),\n                          '--log_dir',os.path.join(CHEMPROP_DIR,'HLM','configs'),\n                          '--save_smiles_splits',\n                          '--split_type','scaffold_balanced'#,\n                          #'--num_folds','5'\n                         ])\nmodify_train_args(args)\n\ngrid_search(args)"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"code","source":["os.listdir(os.path.join(CHEMPROP_DIR,'HLM','checkpoints'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[8]: \n[&#39;depth_2_dropout_0.0_ffn_num_layers_1_hidden_size_1700&#39;,\n &#39;depth_2_dropout_0.15000000000000002_ffn_num_layers_1_hidden_size_1900&#39;,\n &#39;depth_3_dropout_0.05_ffn_num_layers_3_hidden_size_500&#39;,\n &#39;depth_3_dropout_0.1_ffn_num_layers_1_hidden_size_600&#39;,\n &#39;depth_3_dropout_0.25_ffn_num_layers_3_hidden_size_1300&#39;,\n &#39;depth_3_dropout_0.2_ffn_num_layers_2_hidden_size_1900&#39;,\n &#39;depth_3_dropout_0.30000000000000004_ffn_num_layers_2_hidden_size_900&#39;,\n &#39;depth_3_dropout_0.35000000000000003_ffn_num_layers_2_hidden_size_2200&#39;,\n &#39;depth_3_dropout_0.35000000000000003_ffn_num_layers_2_hidden_size_2400&#39;,\n &#39;depth_4_dropout_0.05_ffn_num_layers_2_hidden_size_1100&#39;,\n &#39;depth_4_dropout_0.15000000000000002_ffn_num_layers_2_hidden_size_2200&#39;,\n &#39;depth_4_dropout_0.1_ffn_num_layers_1_hidden_size_2400&#39;,\n &#39;depth_4_dropout_0.4_ffn_num_layers_2_hidden_size_1500&#39;,\n &#39;depth_5_dropout_0.05_ffn_num_layers_2_hidden_size_1200&#39;,\n &#39;depth_5_dropout_0.05_ffn_num_layers_2_hidden_size_800&#39;,\n &#39;depth_5_dropout_0.1_ffn_num_layers_2_hidden_size_1200&#39;,\n &#39;depth_5_dropout_0.25_ffn_num_layers_3_hidden_size_1000&#39;,\n &#39;depth_5_dropout_0.25_ffn_num_layers_3_hidden_size_2100&#39;,\n &#39;depth_5_dropout_0.2_ffn_num_layers_1_hidden_size_1400&#39;,\n &#39;depth_5_dropout_0.2_ffn_num_layers_3_hidden_size_400&#39;,\n &#39;depth_5_dropout_0.30000000000000004_ffn_num_layers_2_hidden_size_1900&#39;,\n &#39;depth_6_dropout_0.05_ffn_num_layers_2_hidden_size_400&#39;,\n &#39;depth_6_dropout_0.0_ffn_num_layers_1_hidden_size_1600&#39;,\n &#39;depth_6_dropout_0.0_ffn_num_layers_1_hidden_size_1700&#39;,\n &#39;depth_6_dropout_0.0_ffn_num_layers_1_hidden_size_1900&#39;,\n &#39;depth_6_dropout_0.0_ffn_num_layers_1_hidden_size_2000&#39;,\n &#39;depth_6_dropout_0.15000000000000002_ffn_num_layers_1_hidden_size_1200&#39;,\n &#39;depth_6_dropout_0.1_ffn_num_layers_1_hidden_size_1900&#39;,\n &#39;depth_6_dropout_0.2_ffn_num_layers_1_hidden_size_700&#39;,\n &#39;fold_0&#39;,\n &#39;fold_1&#39;,\n &#39;fold_2&#39;,\n &#39;fold_3&#39;,\n &#39;fold_4&#39;,\n &#39;quiet.log&#39;,\n &#39;verbose.log&#39;]\n</div>"]}}],"execution_count":82},{"cell_type":"markdown","source":["### HLM Regression"],"metadata":{}},{"cell_type":"code","source":["#{ \"depth\": 6, \"dropout\": 0.0, \"ffn_num_layers\": 1, \"hidden_size\": 1900 }\nparser = ArgumentParser()\nadd_train_args(parser)\n\nargs = parser.parse_args(['--data_path',data,\n                          '--dataset_type','regression',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'HLM','checkpoints'),\n                          '--log_frequency','1',\n                          '--save_smiles_splits',\n                          '--split_type','scaffold_balanced',\n                          '--depth','6',\n                          '--dropout','0.0',\n                          '--hidden_size','1900',\n                          '--ffn_num_layers','2',\n                          '--num_folds','5',\n                          '--seed','13',\n                          '--epochs','25',\n                          '--ensemble_size','10'\n                         ])\nmodify_train_args(args)\nlogger = create_logger(name='train', save_dir=args.save_dir, quiet=args.quiet)\n\ncross_validate(args, logger)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/hlm_eh.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.0,\n &#39;ensemble_size&#39;: 10,\n &#39;epochs&#39;: 25,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 1900,\n &#39;ffn_num_layers&#39;: 2,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1900,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 5,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/HLM/checkpoints/fold_0&#39;,\n &#39;save_smiles_splits&#39;: True,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;scaffold_balanced&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/hlm_eh.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.0,\n &#39;ensemble_size&#39;: 10,\n &#39;epochs&#39;: 25,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 1900,\n &#39;ffn_num_layers&#39;: 2,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 1900,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 5,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/HLM/checkpoints/fold_0&#39;,\n &#39;save_smiles_splits&#39;: True,\n &#39;seed&#39;: 13,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;scaffold_balanced&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\n\n\r  0%|          | 0/804 [00:00&lt;?, ?it/s]\n\r 33%|███▎      | 263/804 [00:00&lt;00:00, 2619.65it/s]\n\r 60%|██████    | 484/804 [00:00&lt;00:00, 2479.50it/s]\n\r 83%|████████▎ | 664/804 [00:00&lt;00:00, 2227.03it/s]\n\r100%|██████████| 804/804 [00:00&lt;00:00, 2103.38it/s]Warning: 2 SMILES are invalid.\nWarning: 2 SMILES are invalid.\nNumber of tasks = 1\nNumber of tasks = 1\nSplitting data with seed 13\nSplitting data with seed 13\n\n\r  0%|          | 0/802 [00:00&lt;?, ?it/s]\n\r 27%|██▋       | 213/802 [00:00&lt;00:00, 2126.85it/s]\n\r 51%|█████     | 408/802 [00:00&lt;00:00, 2067.69it/s]\n\r 70%|███████   | 562/802 [00:00&lt;00:00, 1871.10it/s]\n\r 89%|████████▉ | 715/802 [00:00&lt;00:00, 1749.86it/s]\n\r100%|██████████| 802/802 [00:00&lt;00:00, 1729.99it/s]Total scaffolds = 310 | train scaffolds = 252 | val scaffolds = 21 | test scaffolds = 37\nTotal scaffolds = 310 | train scaffolds = 252 | val scaffolds = 21 | test scaffolds = 37\nLabel averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([92.]), array([2])), (array([51.71428571]), array([7])), (array([86.]), array([1])), (array([87.]), array([1])), (array([94.]), array([2])), (array([37.]), array([1])), (array([88.]), array([2])), (array([90.]), array([1])), (array([28.]), array([1])), (array([52.]), array([1]))]\nLabel averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([92.]), array([2])), (array([51.71428571]), array([7])), (array([86.]), array([1])), (array([87.]), array([1])), (array([94.]), array([2])), (array([37.]), array([1])), (array([88.]), array([2])), (array([90.]), array([1])), (array([28.]), array([1])), (array([52.]), array([1]))]\nTotal size = 802 | train size = 641 | val size = 80 | test size = 81\nTotal size = 802 | train size = 641 | val size = 80 | test size = 81\nFitting scaler\nFitting scaler\nBuilding model 0\nBuilding model 0\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.0)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1900, bias=False)\n      (W_h): Linear(in_features=1900, out_features=1900, bias=False)\n      (W_o): Linear(in_features=2033, out_features=1900, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.0)\n    (1): Linear(in_features=1900, out_features=1900, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.0)\n    (4): Linear(in_features=1900, out_features=1, bias=True)\n  )\n)\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.0)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=1900, bias=False)\n      (W_h): Linear(in_features=1900, out_features=1900, bias=False)\n      (W_o): Linear(in_features=2033, out_features=1900, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.0)\n    (1): Linear(in_features=1900, out_features=1900, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.0)\n    (4): Linear(in_features=1900, out_features=1, bias=True)\n  )\n)\nNumber of parameters = 11,367,701\nNumber of parameters = 11,367,701\nMoving model to cuda\nMoving model to cuda\n\n\r  0%|          | 0/25 [00:00&lt;?, ?it/s]Epoch 0\nEpoch 0\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 2.0039e-02, PNorm = 77.7350, GNorm = 1.6008, lr_0 = 1.3750e-04\nLoss = 2.0039e-02, PNorm = 77.7350, GNorm = 1.6008, lr_0 = 1.3750e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.86it/s]Loss = 9.7289e-02, PNorm = 77.7347, GNorm = 84.6571, lr_0 = 1.7500e-04\nLoss = 9.7289e-02, PNorm = 77.7347, GNorm = 84.6571, lr_0 = 1.7500e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.85it/s]Loss = 2.2356e-02, PNorm = 77.7356, GNorm = 5.4820, lr_0 = 2.1250e-04\nLoss = 2.2356e-02, PNorm = 77.7356, GNorm = 5.4820, lr_0 = 2.1250e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.89it/s]Loss = 5.8898e-02, PNorm = 77.7362, GNorm = 41.2633, lr_0 = 2.5000e-04\nLoss = 5.8898e-02, PNorm = 77.7362, GNorm = 41.2633, lr_0 = 2.5000e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.90it/s]Loss = 3.5313e-02, PNorm = 77.7378, GNorm = 22.0674, lr_0 = 2.8750e-04\nLoss = 3.5313e-02, PNorm = 77.7378, GNorm = 22.0674, lr_0 = 2.8750e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.92it/s]Loss = 2.0776e-02, PNorm = 77.7410, GNorm = 0.7142, lr_0 = 3.2500e-04\nLoss = 2.0776e-02, PNorm = 77.7410, GNorm = 0.7142, lr_0 = 3.2500e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.92it/s]Loss = 2.9432e-02, PNorm = 77.7449, GNorm = 11.2916, lr_0 = 3.6250e-04\nLoss = 2.9432e-02, PNorm = 77.7449, GNorm = 11.2916, lr_0 = 3.6250e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.93it/s]Loss = 2.3180e-02, PNorm = 77.7503, GNorm = 6.5950, lr_0 = 4.0000e-04\nLoss = 2.3180e-02, PNorm = 77.7503, GNorm = 6.5950, lr_0 = 4.0000e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.86it/s]Loss = 2.0555e-02, PNorm = 77.7565, GNorm = 7.3192, lr_0 = 4.3750e-04\nLoss = 2.0555e-02, PNorm = 77.7565, GNorm = 7.3192, lr_0 = 4.3750e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.88it/s]Loss = 2.0556e-02, PNorm = 77.7639, GNorm = 2.8573, lr_0 = 4.7500e-04\nLoss = 2.0556e-02, PNorm = 77.7639, GNorm = 2.8573, lr_0 = 4.7500e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.89it/s]Loss = 2.1300e-02, PNorm = 77.7727, GNorm = 1.0704, lr_0 = 5.1250e-04\nLoss = 2.1300e-02, PNorm = 77.7727, GNorm = 1.0704, lr_0 = 5.1250e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.88it/s]Loss = 1.8593e-02, PNorm = 77.7826, GNorm = 1.0917, lr_0 = 5.5000e-04\nLoss = 1.8593e-02, PNorm = 77.7826, GNorm = 1.0917, lr_0 = 5.5000e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.90it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.67it/s]Validation rmse = 23.190625\nValidation rmse = 23.190625\n\n\r  4%|▍         | 1/25 [00:07&lt;03:05,  7.75s/it]Epoch 1\nEpoch 1\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 2.0890e-02, PNorm = 77.7930, GNorm = 4.4318, lr_0 = 5.8750e-04\nLoss = 2.0890e-02, PNorm = 77.7930, GNorm = 4.4318, lr_0 = 5.8750e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.89it/s]Loss = 2.6684e-02, PNorm = 77.8046, GNorm = 6.4210, lr_0 = 6.2500e-04\nLoss = 2.6684e-02, PNorm = 77.8046, GNorm = 6.4210, lr_0 = 6.2500e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.93it/s]Loss = 1.8690e-02, PNorm = 77.8171, GNorm = 2.0686, lr_0 = 6.6250e-04\nLoss = 1.8690e-02, PNorm = 77.8171, GNorm = 2.0686, lr_0 = 6.6250e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.93it/s]Loss = 2.9789e-02, PNorm = 77.8298, GNorm = 9.3938, lr_0 = 7.0000e-04\nLoss = 2.9789e-02, PNorm = 77.8298, GNorm = 9.3938, lr_0 = 7.0000e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.92it/s]Loss = 2.1020e-02, PNorm = 77.8436, GNorm = 2.2749, lr_0 = 7.3750e-04\nLoss = 2.1020e-02, PNorm = 77.8436, GNorm = 2.2749, lr_0 = 7.3750e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.90it/s]Loss = 2.1795e-02, PNorm = 77.8584, GNorm = 1.6729, lr_0 = 7.7500e-04\nLoss = 2.1795e-02, PNorm = 77.8584, GNorm = 1.6729, lr_0 = 7.7500e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.96it/s]Loss = 1.8390e-02, PNorm = 77.8741, GNorm = 0.5924, lr_0 = 8.1250e-04\nLoss = 1.8390e-02, PNorm = 77.8741, GNorm = 0.5924, lr_0 = 8.1250e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.94it/s]Loss = 2.0008e-02, PNorm = 77.8899, GNorm = 2.1147, lr_0 = 8.5000e-04\nLoss = 2.0008e-02, PNorm = 77.8899, GNorm = 2.1147, lr_0 = 8.5000e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.81it/s]Loss = 1.7713e-02, PNorm = 77.9062, GNorm = 1.8637, lr_0 = 8.8750e-04\nLoss = 1.7713e-02, PNorm = 77.9062, GNorm = 1.8637, lr_0 = 8.8750e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.76it/s]Loss = 1.9498e-02, PNorm = 77.9229, GNorm = 2.7294, lr_0 = 9.2500e-04\nLoss = 1.9498e-02, PNorm = 77.9229, GNorm = 2.7294, lr_0 = 9.2500e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.80it/s]Loss = 1.8135e-02, PNorm = 77.9400, GNorm = 0.8270, lr_0 = 9.6250e-04\nLoss = 1.8135e-02, PNorm = 77.9400, GNorm = 0.8270, lr_0 = 9.6250e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.81it/s]Loss = 2.1318e-02, PNorm = 77.9578, GNorm = 0.9463, lr_0 = 1.0000e-03\nLoss = 2.1318e-02, PNorm = 77.9578, GNorm = 0.9463, lr_0 = 1.0000e-03\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.86it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.36it/s]Validation rmse = 24.387174\nValidation rmse = 24.387174\n\n\r  8%|▊         | 2/25 [00:09&lt;02:17,  5.97s/it]Epoch 2\nEpoch 2\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 2.0846e-02, PNorm = 77.9764, GNorm = 1.0326, lr_0 = 9.9169e-04\nLoss = 2.0846e-02, PNorm = 77.9764, GNorm = 1.0326, lr_0 = 9.9169e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.91it/s]Loss = 2.0216e-02, PNorm = 77.9940, GNorm = 0.5173, lr_0 = 9.8345e-04\nLoss = 2.0216e-02, PNorm = 77.9940, GNorm = 0.5173, lr_0 = 9.8345e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.90it/s]Loss = 2.5147e-02, PNorm = 78.0095, GNorm = 3.6421, lr_0 = 9.7528e-04\nLoss = 2.5147e-02, PNorm = 78.0095, GNorm = 3.6421, lr_0 = 9.7528e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.92it/s]Loss = 2.0610e-02, PNorm = 78.0242, GNorm = 2.6405, lr_0 = 9.6718e-04\nLoss = 2.0610e-02, PNorm = 78.0242, GNorm = 2.6405, lr_0 = 9.6718e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.91it/s]Loss = 2.0849e-02, PNorm = 78.0387, GNorm = 0.7898, lr_0 = 9.5914e-04\nLoss = 2.0849e-02, PNorm = 78.0387, GNorm = 0.7898, lr_0 = 9.5914e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.92it/s]Loss = 1.8290e-02, PNorm = 78.0529, GNorm = 1.3102, lr_0 = 9.5118e-04\nLoss = 1.8290e-02, PNorm = 78.0529, GNorm = 1.3102, lr_0 = 9.5118e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.95it/s]Loss = 1.8281e-02, PNorm = 78.0666, GNorm = 0.1836, lr_0 = 9.4327e-04\nLoss = 1.8281e-02, PNorm = 78.0666, GNorm = 0.1836, lr_0 = 9.4327e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.92it/s]Loss = 1.8175e-02, PNorm = 78.0792, GNorm = 1.7399, lr_0 = 9.3544e-04\nLoss = 1.8175e-02, PNorm = 78.0792, GNorm = 1.7399, lr_0 = 9.3544e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.73it/s]Loss = 2.0715e-02, PNorm = 78.0911, GNorm = 1.3948, lr_0 = 9.2767e-04\nLoss = 2.0715e-02, PNorm = 78.0911, GNorm = 1.3948, lr_0 = 9.2767e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.75it/s]Loss = 2.0856e-02, PNorm = 78.1025, GNorm = 1.5522, lr_0 = 9.1996e-04\nLoss = 2.0856e-02, PNorm = 78.1025, GNorm = 1.5522, lr_0 = 9.1996e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.81it/s]Loss = 2.0489e-02, PNorm = 78.1134, GNorm = 0.5783, lr_0 = 9.1232e-04\nLoss = 2.0489e-02, PNorm = 78.1134, GNorm = 0.5783, lr_0 = 9.1232e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  5.40it/s]Loss = 2.1031e-02, PNorm = 78.1239, GNorm = 1.2806, lr_0 = 9.0474e-04\nLoss = 2.1031e-02, PNorm = 78.1239, GNorm = 1.2806, lr_0 = 9.0474e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  5.79it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.40it/s]Validation rmse = 23.784945\nValidation rmse = 23.784945\n\n\r 12%|█▏        | 3/25 [00:11&lt;01:44,  4.76s/it]Epoch 3\nEpoch 3\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 2.0723e-02, PNorm = 78.1339, GNorm = 0.7294, lr_0 = 8.9722e-04\nLoss = 2.0723e-02, PNorm = 78.1339, GNorm = 0.7294, lr_0 = 8.9722e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  7.03it/s]Loss = 1.7986e-02, PNorm = 78.1435, GNorm = 0.3382, lr_0 = 8.8977e-04\nLoss = 1.7986e-02, PNorm = 78.1435, GNorm = 0.3382, lr_0 = 8.8977e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  7.01it/s]Loss = 2.0747e-02, PNorm = 78.1529, GNorm = 1.2495, lr_0 = 8.8237e-04\nLoss = 2.0747e-02, PNorm = 78.1529, GNorm = 1.2495, lr_0 = 8.8237e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.99it/s]Loss = 2.1940e-02, PNorm = 78.1619, GNorm = 0.2976, lr_0 = 8.7504e-04\nLoss = 2.1940e-02, PNorm = 78.1619, GNorm = 0.2976, lr_0 = 8.7504e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.99it/s]Loss = 1.6951e-02, PNorm = 78.1704, GNorm = 0.4416, lr_0 = 8.6777e-04\nLoss = 1.6951e-02, PNorm = 78.1704, GNorm = 0.4416, lr_0 = 8.6777e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.96it/s]Loss = 2.4898e-02, PNorm = 78.1781, GNorm = 1.0275, lr_0 = 8.6056e-04\nLoss = 2.4898e-02, PNorm = 78.1781, GNorm = 1.0275, lr_0 = 8.6056e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.93it/s]Loss = 1.9551e-02, PNorm = 78.1853, GNorm = 0.6684, lr_0 = 8.5341e-04\nLoss = 1.9551e-02, PNorm = 78.1853, GNorm = 0.6684, lr_0 = 8.5341e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.83it/s]Loss = 2.1393e-02, PNorm = 78.1926, GNorm = 0.6645, lr_0 = 8.4632e-04\nLoss = 2.1393e-02, PNorm = 78.1926, GNorm = 0.6645, lr_0 = 8.4632e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.88it/s]Loss = 1.9720e-02, PNorm = 78.1993, GNorm = 1.2713, lr_0 = 8.3929e-04\nLoss = 1.9720e-02, PNorm = 78.1993, GNorm = 1.2713, lr_0 = 8.3929e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.90it/s]Loss = 1.9210e-02, PNorm = 78.2057, GNorm = 1.7767, lr_0 = 8.3232e-04\nLoss = 1.9210e-02, PNorm = 78.2057, GNorm = 1.7767, lr_0 = 8.3232e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.92it/s]Loss = 1.8656e-02, PNorm = 78.2122, GNorm = 0.9615, lr_0 = 8.2540e-04\nLoss = 1.8656e-02, PNorm = 78.2122, GNorm = 0.9615, lr_0 = 8.2540e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.89it/s]Loss = 1.6703e-02, PNorm = 78.2188, GNorm = 0.2119, lr_0 = 8.1855e-04\nLoss = 1.6703e-02, PNorm = 78.2188, GNorm = 0.2119, lr_0 = 8.1855e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.92it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.76it/s]Validation rmse = 23.482244\nValidation rmse = 23.482244\n\n\r 16%|█▌        | 4/25 [00:13&lt;01:21,  3.88s/it]Epoch 4\nEpoch 4\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 2.1732e-02, PNorm = 78.2252, GNorm = 0.6339, lr_0 = 8.1175e-04\nLoss = 2.1732e-02, PNorm = 78.2252, GNorm = 0.6339, lr_0 = 8.1175e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  7.07it/s]Loss = 1.8300e-02, PNorm = 78.2319, GNorm = 0.6467, lr_0 = 8.0500e-04\nLoss = 1.8300e-02, PNorm = 78.2319, GNorm = 0.6467, lr_0 = 8.0500e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  7.01it/s]Loss = 2.3049e-02, PNorm = 78.2378, GNorm = 1.5506, lr_0 = 7.9831e-04\nLoss = 2.3049e-02, PNorm = 78.2378, GNorm = 1.5506, lr_0 = 7.9831e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  7.01it/s]Loss = 2.2844e-02, PNorm = 78.2434, GNorm = 1.4943, lr_0 = 7.9168e-04\nLoss = 2.2844e-02, PNorm = 78.2434, GNorm = 1.4943, lr_0 = 7.9168e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  7.00it/s]Loss = 2.2007e-02, PNorm = 78.2493, GNorm = 0.8919, lr_0 = 7.8510e-04\nLoss = 2.2007e-02, PNorm = 78.2493, GNorm = 0.8919, lr_0 = 7.8510e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.98it/s]Loss = 1.9610e-02, PNorm = 78.2555, GNorm = 0.7520, lr_0 = 7.7858e-04\nLoss = 1.9610e-02, PNorm = 78.2555, GNorm = 0.7520, lr_0 = 7.7858e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.96it/s]Loss = 1.6643e-02, PNorm = 78.2619, GNorm = 0.2752, lr_0 = 7.7211e-04\nLoss = 1.6643e-02, PNorm = 78.2619, GNorm = 0.2752, lr_0 = 7.7211e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.96it/s]Loss = 2.1841e-02, PNorm = 78.2669, GNorm = 1.5776, lr_0 = 7.6570e-04\nLoss = 2.1841e-02, PNorm = 78.2669, GNorm = 1.5776, lr_0 = 7.6570e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.96it/s]Loss = 1.7768e-02, PNorm = 78.2717, GNorm = 1.0494, lr_0 = 7.5934e-04\nLoss = 1.7768e-02, PNorm = 78.2717, GNorm = 1.0494, lr_0 = 7.5934e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.98it/s]Loss = 2.0010e-02, PNorm = 78.2764, GNorm = 0.6789, lr_0 = 7.5303e-04\nLoss = 2.0010e-02, PNorm = 78.2764, GNorm = 0.6789, lr_0 = 7.5303e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.98it/s]Loss = 1.6331e-02, PNorm = 78.2812, GNorm = 0.3230, lr_0 = 7.4677e-04\nLoss = 1.6331e-02, PNorm = 78.2812, GNorm = 0.3230, lr_0 = 7.4677e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.97it/s]Loss = 1.7591e-02, PNorm = 78.2861, GNorm = 0.7954, lr_0 = 7.4057e-04\nLoss = 1.7591e-02, PNorm = 78.2861, GNorm = 0.7954, lr_0 = 7.4057e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.95it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.89it/s]Validation rmse = 23.618883\nValidation rmse = 23.618883\n\n\r 20%|██        | 5/25 [00:15&lt;01:05,  3.25s/it]Epoch 5\nEpoch 5\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.8824e-02, PNorm = 78.2908, GNorm = 0.7679, lr_0 = 7.3442e-04\nLoss = 1.8824e-02, PNorm = 78.2908, GNorm = 0.7679, lr_0 = 7.3442e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  7.00it/s]Loss = 1.5713e-02, PNorm = 78.2959, GNorm = 0.5158, lr_0 = 7.2831e-04\nLoss = 1.5713e-02, PNorm = 78.2959, GNorm = 0.5158, lr_0 = 7.2831e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  7.01it/s]Loss = 2.3874e-02, PNorm = 78.3011, GNorm = 0.9126, lr_0 = 7.2226e-04\nLoss = 2.3874e-02, PNorm = 78.3011, GNorm = 0.9126, lr_0 = 7.2226e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  7.00it/s]Loss = 2.1998e-02, PNorm = 78.3064, GNorm = 0.3536, lr_0 = 7.1626e-04\nLoss = 2.1998e-02, PNorm = 78.3064, GNorm = 0.3536, lr_0 = 7.1626e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.99it/s]Loss = 1.6250e-02, PNorm = 78.3123, GNorm = 0.2893, lr_0 = 7.1031e-04\nLoss = 1.6250e-02, PNorm = 78.3123, GNorm = 0.2893, lr_0 = 7.1031e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.99it/s]Loss = 1.7981e-02, PNorm = 78.3178, GNorm = 0.6292, lr_0 = 7.0441e-04\nLoss = 1.7981e-02, PNorm = 78.3178, GNorm = 0.6292, lr_0 = 7.0441e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.99it/s]Loss = 2.1462e-02, PNorm = 78.3234, GNorm = 0.9677, lr_0 = 6.9856e-04\nLoss = 2.1462e-02, PNorm = 78.3234, GNorm = 0.9677, lr_0 = 6.9856e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.97it/s]Loss = 1.9738e-02, PNorm = 78.3291, GNorm = 0.5416, lr_0 = 6.9276e-04\nLoss = 1.9738e-02, PNorm = 78.3291, GNorm = 0.5416, lr_0 = 6.9276e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.98it/s]Loss = 1.7027e-02, PNorm = 78.3353, GNorm = 0.3897, lr_0 = 6.8700e-04\nLoss = 1.7027e-02, PNorm = 78.3353, GNorm = 0.3897, lr_0 = 6.8700e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  7.00it/s]Loss = 1.9946e-02, PNorm = 78.3421, GNorm = 0.3338, lr_0 = 6.8129e-04\nLoss = 1.9946e-02, PNorm = 78.3421, GNorm = 0.3338, lr_0 = 6.8129e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.97it/s]Loss = 1.8109e-02, PNorm = 78.3481, GNorm = 1.5030, lr_0 = 6.7563e-04\nLoss = 1.8109e-02, PNorm = 78.3481, GNorm = 1.5030, lr_0 = 6.7563e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.96it/s]Loss = 2.1122e-02, PNorm = 78.3541, GNorm = 0.1954, lr_0 = 6.7002e-04\nLoss = 2.1122e-02, PNorm = 78.3541, GNorm = 0.1954, lr_0 = 6.7002e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.98it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.04it/s]Validation rmse = 23.900342\nValidation rmse = 23.900342\n\n\r 24%|██▍       | 6/25 [00:16&lt;00:53,  2.82s/it]Epoch 6\nEpoch 6\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.9931e-02, PNorm = 78.3604, GNorm = 0.5746, lr_0 = 6.6445e-04\nLoss = 1.9931e-02, PNorm = 78.3604, GNorm = 0.5746, lr_0 = 6.6445e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.95it/s]Loss = 1.8329e-02, PNorm = 78.3674, GNorm = 0.2967, lr_0 = 6.5893e-04\nLoss = 1.8329e-02, PNorm = 78.3674, GNorm = 0.2967, lr_0 = 6.5893e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.95it/s]Loss = 2.1439e-02, PNorm = 78.3740, GNorm = 1.8638, lr_0 = 6.5346e-04\nLoss = 2.1439e-02, PNorm = 78.3740, GNorm = 1.8638, lr_0 = 6.5346e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.93it/s]Loss = 2.1232e-02, PNorm = 78.3811, GNorm = 0.2390, lr_0 = 6.4803e-04\nLoss = 2.1232e-02, PNorm = 78.3811, GNorm = 0.2390, lr_0 = 6.4803e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.95it/s]Loss = 1.7497e-02, PNorm = 78.3884, GNorm = 1.6178, lr_0 = 6.4264e-04\nLoss = 1.7497e-02, PNorm = 78.3884, GNorm = 1.6178, lr_0 = 6.4264e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.92it/s]Loss = 2.1815e-02, PNorm = 78.3950, GNorm = 0.3260, lr_0 = 6.3731e-04\nLoss = 2.1815e-02, PNorm = 78.3950, GNorm = 0.3260, lr_0 = 6.3731e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.97it/s]Loss = 1.3986e-02, PNorm = 78.4019, GNorm = 0.5120, lr_0 = 6.3201e-04\nLoss = 1.3986e-02, PNorm = 78.4019, GNorm = 0.5120, lr_0 = 6.3201e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.94it/s]Loss = 1.8597e-02, PNorm = 78.4093, GNorm = 0.8634, lr_0 = 6.2676e-04\nLoss = 1.8597e-02, PNorm = 78.4093, GNorm = 0.8634, lr_0 = 6.2676e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.95it/s]Loss = 1.9492e-02, PNorm = 78.4160, GNorm = 2.5136, lr_0 = 6.2155e-04\nLoss = 1.9492e-02, PNorm = 78.4160, GNorm = 2.5136, lr_0 = 6.2155e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.97it/s]Loss = 1.5159e-02, PNorm = 78.4228, GNorm = 0.2550, lr_0 = 6.1639e-04\nLoss = 1.5159e-02, PNorm = 78.4228, GNorm = 0.2550, lr_0 = 6.1639e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.47it/s]Loss = 1.9101e-02, PNorm = 78.4295, GNorm = 1.2900, lr_0 = 6.1127e-04\nLoss = 1.9101e-02, PNorm = 78.4295, GNorm = 1.2900, lr_0 = 6.1127e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  5.84it/s]Loss = 1.4251e-02, PNorm = 78.4367, GNorm = 0.5133, lr_0 = 6.0619e-04\nLoss = 1.4251e-02, PNorm = 78.4367, GNorm = 0.5133, lr_0 = 6.0619e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.16it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 29.17it/s]Validation rmse = 23.111581\nValidation rmse = 23.111581\n\n\r 28%|██▊       | 7/25 [00:26&lt;01:26,  4.78s/it]Epoch 7\nEpoch 7\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.4493e-02, PNorm = 78.4444, GNorm = 0.6750, lr_0 = 6.0115e-04\nLoss = 1.4493e-02, PNorm = 78.4444, GNorm = 0.6750, lr_0 = 6.0115e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.60it/s]Loss = 1.7818e-02, PNorm = 78.4528, GNorm = 0.5527, lr_0 = 5.9616e-04\nLoss = 1.7818e-02, PNorm = 78.4528, GNorm = 0.5527, lr_0 = 5.9616e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.69it/s]Loss = 1.7071e-02, PNorm = 78.4615, GNorm = 0.4277, lr_0 = 5.9121e-04\nLoss = 1.7071e-02, PNorm = 78.4615, GNorm = 0.4277, lr_0 = 5.9121e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.79it/s]Loss = 1.6790e-02, PNorm = 78.4705, GNorm = 0.4028, lr_0 = 5.8629e-04\nLoss = 1.6790e-02, PNorm = 78.4705, GNorm = 0.4028, lr_0 = 5.8629e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.84it/s]Loss = 1.6069e-02, PNorm = 78.4802, GNorm = 0.4262, lr_0 = 5.8142e-04\n\n*** WARNING: skipped 3371569 bytes of output ***\n\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:02,  4.17it/s]Loss = 8.0575e-03, PNorm = 79.4288, GNorm = 4.2290, lr_0 = 2.4012e-04\nLoss = 8.0575e-03, PNorm = 79.4288, GNorm = 4.2290, lr_0 = 2.4012e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  4.73it/s]Loss = 7.6420e-03, PNorm = 79.4335, GNorm = 2.3043, lr_0 = 2.3813e-04\nLoss = 7.6420e-03, PNorm = 79.4335, GNorm = 2.3043, lr_0 = 2.3813e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.22it/s]Loss = 8.4017e-03, PNorm = 79.4387, GNorm = 2.1478, lr_0 = 2.3615e-04\nLoss = 8.4017e-03, PNorm = 79.4387, GNorm = 2.1478, lr_0 = 2.3615e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.60it/s]Loss = 1.3582e-02, PNorm = 79.4454, GNorm = 5.1949, lr_0 = 2.3419e-04\nLoss = 1.3582e-02, PNorm = 79.4454, GNorm = 5.1949, lr_0 = 2.3419e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.86it/s]Loss = 4.5598e-03, PNorm = 79.4521, GNorm = 0.8410, lr_0 = 2.3224e-04\nLoss = 4.5598e-03, PNorm = 79.4521, GNorm = 0.8410, lr_0 = 2.3224e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.14it/s]Loss = 7.6440e-03, PNorm = 79.4581, GNorm = 2.4471, lr_0 = 2.3031e-04\nLoss = 7.6440e-03, PNorm = 79.4581, GNorm = 2.4471, lr_0 = 2.3031e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.36it/s]Loss = 1.0422e-02, PNorm = 79.4635, GNorm = 1.9306, lr_0 = 2.2840e-04\nLoss = 1.0422e-02, PNorm = 79.4635, GNorm = 1.9306, lr_0 = 2.2840e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.52it/s]Loss = 1.0043e-02, PNorm = 79.4685, GNorm = 2.7868, lr_0 = 2.2650e-04\nLoss = 1.0043e-02, PNorm = 79.4685, GNorm = 2.7868, lr_0 = 2.2650e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.66it/s]Loss = 8.0256e-03, PNorm = 79.4738, GNorm = 3.0138, lr_0 = 2.2462e-04\nLoss = 8.0256e-03, PNorm = 79.4738, GNorm = 3.0138, lr_0 = 2.2462e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.75it/s]Loss = 1.1006e-02, PNorm = 79.4791, GNorm = 1.0725, lr_0 = 2.2275e-04\nLoss = 1.1006e-02, PNorm = 79.4791, GNorm = 1.0725, lr_0 = 2.2275e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.81it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 26.11it/s]Validation rmse = 18.456042\nValidation rmse = 18.456042\n\n\r 68%|██████▊   | 17/25 [01:40&lt;00:26,  3.33s/it]Epoch 17\nEpoch 17\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 7.1787e-03, PNorm = 79.4846, GNorm = 1.2557, lr_0 = 2.2090e-04\nLoss = 7.1787e-03, PNorm = 79.4846, GNorm = 1.2557, lr_0 = 2.2090e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.98it/s]Loss = 9.7352e-03, PNorm = 79.4914, GNorm = 3.7483, lr_0 = 2.1907e-04\nLoss = 9.7352e-03, PNorm = 79.4914, GNorm = 3.7483, lr_0 = 2.1907e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.94it/s]Loss = 1.0715e-02, PNorm = 79.4989, GNorm = 4.1143, lr_0 = 2.1725e-04\nLoss = 1.0715e-02, PNorm = 79.4989, GNorm = 4.1143, lr_0 = 2.1725e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.93it/s]Loss = 9.8395e-03, PNorm = 79.5068, GNorm = 2.5243, lr_0 = 2.1544e-04\nLoss = 9.8395e-03, PNorm = 79.5068, GNorm = 2.5243, lr_0 = 2.1544e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.92it/s]Loss = 8.7220e-03, PNorm = 79.5142, GNorm = 1.3924, lr_0 = 2.1365e-04\nLoss = 8.7220e-03, PNorm = 79.5142, GNorm = 1.3924, lr_0 = 2.1365e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.95it/s]Loss = 1.2845e-02, PNorm = 79.5197, GNorm = 5.9569, lr_0 = 2.1188e-04\nLoss = 1.2845e-02, PNorm = 79.5197, GNorm = 5.9569, lr_0 = 2.1188e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.91it/s]Loss = 8.7968e-03, PNorm = 79.5240, GNorm = 5.2022, lr_0 = 2.1012e-04\nLoss = 8.7968e-03, PNorm = 79.5240, GNorm = 5.2022, lr_0 = 2.1012e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.94it/s]Loss = 9.9320e-03, PNorm = 79.5284, GNorm = 1.7589, lr_0 = 2.0837e-04\nLoss = 9.9320e-03, PNorm = 79.5284, GNorm = 1.7589, lr_0 = 2.0837e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.87it/s]Loss = 4.5908e-03, PNorm = 79.5333, GNorm = 2.1066, lr_0 = 2.0664e-04\nLoss = 4.5908e-03, PNorm = 79.5333, GNorm = 2.1066, lr_0 = 2.0664e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.87it/s]Loss = 1.1850e-02, PNorm = 79.5387, GNorm = 4.4747, lr_0 = 2.0492e-04\nLoss = 1.1850e-02, PNorm = 79.5387, GNorm = 4.4747, lr_0 = 2.0492e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.85it/s]Loss = 7.8905e-03, PNorm = 79.5443, GNorm = 3.3733, lr_0 = 2.0322e-04\nLoss = 7.8905e-03, PNorm = 79.5443, GNorm = 3.3733, lr_0 = 2.0322e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.87it/s]Loss = 8.8252e-03, PNorm = 79.5502, GNorm = 3.1050, lr_0 = 2.0153e-04\nLoss = 8.8252e-03, PNorm = 79.5502, GNorm = 3.1050, lr_0 = 2.0153e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.91it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.83it/s]Validation rmse = 18.060960\nValidation rmse = 18.060960\n\n\r 72%|███████▏  | 18/25 [01:41&lt;00:20,  2.88s/it]Epoch 18\nEpoch 18\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.0235e-02, PNorm = 79.5558, GNorm = 0.7816, lr_0 = 1.9986e-04\nLoss = 1.0235e-02, PNorm = 79.5558, GNorm = 0.7816, lr_0 = 1.9986e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.94it/s]Loss = 8.3456e-03, PNorm = 79.5610, GNorm = 2.4547, lr_0 = 1.9820e-04\nLoss = 8.3456e-03, PNorm = 79.5610, GNorm = 2.4547, lr_0 = 1.9820e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.84it/s]Loss = 4.6811e-03, PNorm = 79.5656, GNorm = 2.0902, lr_0 = 1.9655e-04\nLoss = 4.6811e-03, PNorm = 79.5656, GNorm = 2.0902, lr_0 = 1.9655e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.87it/s]Loss = 9.3514e-03, PNorm = 79.5698, GNorm = 3.1408, lr_0 = 1.9492e-04\nLoss = 9.3514e-03, PNorm = 79.5698, GNorm = 3.1408, lr_0 = 1.9492e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.89it/s]Loss = 7.8866e-03, PNorm = 79.5737, GNorm = 2.3082, lr_0 = 1.9330e-04\nLoss = 7.8866e-03, PNorm = 79.5737, GNorm = 2.3082, lr_0 = 1.9330e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.87it/s]Loss = 5.7970e-03, PNorm = 79.5778, GNorm = 0.9326, lr_0 = 1.9169e-04\nLoss = 5.7970e-03, PNorm = 79.5778, GNorm = 0.9326, lr_0 = 1.9169e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.86it/s]Loss = 7.3020e-03, PNorm = 79.5822, GNorm = 2.3865, lr_0 = 1.9010e-04\nLoss = 7.3020e-03, PNorm = 79.5822, GNorm = 2.3865, lr_0 = 1.9010e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.89it/s]Loss = 6.8553e-03, PNorm = 79.5863, GNorm = 1.0181, lr_0 = 1.8852e-04\nLoss = 6.8553e-03, PNorm = 79.5863, GNorm = 1.0181, lr_0 = 1.8852e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.91it/s]Loss = 8.9414e-03, PNorm = 79.5911, GNorm = 2.3567, lr_0 = 1.8696e-04\nLoss = 8.9414e-03, PNorm = 79.5911, GNorm = 2.3567, lr_0 = 1.8696e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.88it/s]Loss = 9.2783e-03, PNorm = 79.5959, GNorm = 2.1544, lr_0 = 1.8540e-04\nLoss = 9.2783e-03, PNorm = 79.5959, GNorm = 2.1544, lr_0 = 1.8540e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.96it/s]Loss = 8.9968e-03, PNorm = 79.6016, GNorm = 4.0899, lr_0 = 1.8386e-04\nLoss = 8.9968e-03, PNorm = 79.6016, GNorm = 4.0899, lr_0 = 1.8386e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.95it/s]Loss = 7.4136e-03, PNorm = 79.6070, GNorm = 1.8688, lr_0 = 1.8233e-04\nLoss = 7.4136e-03, PNorm = 79.6070, GNorm = 1.8688, lr_0 = 1.8233e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.82it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 29.06it/s]Validation rmse = 17.867137\nValidation rmse = 17.867137\n\n\r 76%|███████▌  | 19/25 [01:43&lt;00:15,  2.56s/it]Epoch 19\nEpoch 19\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.1322e-02, PNorm = 79.6112, GNorm = 9.0290, lr_0 = 1.8082e-04\nLoss = 1.1322e-02, PNorm = 79.6112, GNorm = 9.0290, lr_0 = 1.8082e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.99it/s]Loss = 9.7742e-03, PNorm = 79.6152, GNorm = 2.5987, lr_0 = 1.7932e-04\nLoss = 9.7742e-03, PNorm = 79.6152, GNorm = 2.5987, lr_0 = 1.7932e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  7.00it/s]Loss = 8.1692e-03, PNorm = 79.6196, GNorm = 1.1604, lr_0 = 1.7783e-04\nLoss = 8.1692e-03, PNorm = 79.6196, GNorm = 1.1604, lr_0 = 1.7783e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.86it/s]Loss = 6.2994e-03, PNorm = 79.6243, GNorm = 1.2522, lr_0 = 1.7635e-04\nLoss = 6.2994e-03, PNorm = 79.6243, GNorm = 1.2522, lr_0 = 1.7635e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.89it/s]Loss = 9.4907e-03, PNorm = 79.6292, GNorm = 1.3552, lr_0 = 1.7489e-04\nLoss = 9.4907e-03, PNorm = 79.6292, GNorm = 1.3552, lr_0 = 1.7489e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.89it/s]Loss = 7.0736e-03, PNorm = 79.6344, GNorm = 2.0658, lr_0 = 1.7343e-04\nLoss = 7.0736e-03, PNorm = 79.6344, GNorm = 2.0658, lr_0 = 1.7343e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.92it/s]Loss = 6.2956e-03, PNorm = 79.6395, GNorm = 1.4287, lr_0 = 1.7199e-04\nLoss = 6.2956e-03, PNorm = 79.6395, GNorm = 1.4287, lr_0 = 1.7199e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.93it/s]Loss = 6.2496e-03, PNorm = 79.6441, GNorm = 0.8807, lr_0 = 1.7056e-04\nLoss = 6.2496e-03, PNorm = 79.6441, GNorm = 0.8807, lr_0 = 1.7056e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.95it/s]Loss = 9.1365e-03, PNorm = 79.6483, GNorm = 2.1775, lr_0 = 1.6915e-04\nLoss = 9.1365e-03, PNorm = 79.6483, GNorm = 2.1775, lr_0 = 1.6915e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.87it/s]Loss = 7.2883e-03, PNorm = 79.6523, GNorm = 1.8479, lr_0 = 1.6774e-04\nLoss = 7.2883e-03, PNorm = 79.6523, GNorm = 1.8479, lr_0 = 1.6774e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.91it/s]Loss = 5.9468e-03, PNorm = 79.6564, GNorm = 1.2793, lr_0 = 1.6635e-04\nLoss = 5.9468e-03, PNorm = 79.6564, GNorm = 1.2793, lr_0 = 1.6635e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.90it/s]Loss = 7.9555e-03, PNorm = 79.6603, GNorm = 1.0107, lr_0 = 1.6496e-04\nLoss = 7.9555e-03, PNorm = 79.6603, GNorm = 1.0107, lr_0 = 1.6496e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.94it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 26.46it/s]Validation rmse = 18.291222\nValidation rmse = 18.291222\n\n\r 80%|████████  | 20/25 [01:45&lt;00:11,  2.34s/it]Epoch 20\nEpoch 20\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 8.4266e-03, PNorm = 79.6645, GNorm = 3.3504, lr_0 = 1.6359e-04\nLoss = 8.4266e-03, PNorm = 79.6645, GNorm = 3.3504, lr_0 = 1.6359e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.95it/s]Loss = 8.0466e-03, PNorm = 79.6690, GNorm = 2.1578, lr_0 = 1.6224e-04\nLoss = 8.0466e-03, PNorm = 79.6690, GNorm = 2.1578, lr_0 = 1.6224e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.95it/s]Loss = 4.2978e-03, PNorm = 79.6735, GNorm = 0.8595, lr_0 = 1.6089e-04\nLoss = 4.2978e-03, PNorm = 79.6735, GNorm = 0.8595, lr_0 = 1.6089e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.95it/s]Loss = 7.9145e-03, PNorm = 79.6780, GNorm = 1.2491, lr_0 = 1.5955e-04\nLoss = 7.9145e-03, PNorm = 79.6780, GNorm = 1.2491, lr_0 = 1.5955e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.96it/s]Loss = 6.7321e-03, PNorm = 79.6820, GNorm = 2.8947, lr_0 = 1.5823e-04\nLoss = 6.7321e-03, PNorm = 79.6820, GNorm = 2.8947, lr_0 = 1.5823e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.95it/s]Loss = 6.5565e-03, PNorm = 79.6856, GNorm = 1.3579, lr_0 = 1.5691e-04\nLoss = 6.5565e-03, PNorm = 79.6856, GNorm = 1.3579, lr_0 = 1.5691e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.92it/s]Loss = 6.5087e-03, PNorm = 79.6889, GNorm = 1.6445, lr_0 = 1.5561e-04\nLoss = 6.5087e-03, PNorm = 79.6889, GNorm = 1.6445, lr_0 = 1.5561e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.89it/s]Loss = 1.0365e-02, PNorm = 79.6925, GNorm = 2.9392, lr_0 = 1.5431e-04\nLoss = 1.0365e-02, PNorm = 79.6925, GNorm = 2.9392, lr_0 = 1.5431e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.90it/s]Loss = 6.1192e-03, PNorm = 79.6956, GNorm = 3.2686, lr_0 = 1.5303e-04\nLoss = 6.1192e-03, PNorm = 79.6956, GNorm = 3.2686, lr_0 = 1.5303e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.91it/s]Loss = 4.7288e-03, PNorm = 79.6989, GNorm = 1.2465, lr_0 = 1.5176e-04\nLoss = 4.7288e-03, PNorm = 79.6989, GNorm = 1.2465, lr_0 = 1.5176e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.91it/s]Loss = 7.2417e-03, PNorm = 79.7027, GNorm = 4.3477, lr_0 = 1.5050e-04\nLoss = 7.2417e-03, PNorm = 79.7027, GNorm = 4.3477, lr_0 = 1.5050e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  5.34it/s]Loss = 9.8604e-03, PNorm = 79.7068, GNorm = 3.7840, lr_0 = 1.4925e-04\nLoss = 9.8604e-03, PNorm = 79.7068, GNorm = 3.7840, lr_0 = 1.4925e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  5.74it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 29.05it/s]Validation rmse = 18.118496\nValidation rmse = 18.118496\n\n\r 84%|████████▍ | 21/25 [01:47&lt;00:08,  2.22s/it]Epoch 21\nEpoch 21\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 6.3366e-03, PNorm = 79.7108, GNorm = 2.7017, lr_0 = 1.4801e-04\nLoss = 6.3366e-03, PNorm = 79.7108, GNorm = 2.7017, lr_0 = 1.4801e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.87it/s]Loss = 6.8569e-03, PNorm = 79.7142, GNorm = 2.9732, lr_0 = 1.4678e-04\nLoss = 6.8569e-03, PNorm = 79.7142, GNorm = 2.9732, lr_0 = 1.4678e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.89it/s]Loss = 5.4032e-03, PNorm = 79.7176, GNorm = 1.0288, lr_0 = 1.4556e-04\nLoss = 5.4032e-03, PNorm = 79.7176, GNorm = 1.0288, lr_0 = 1.4556e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.93it/s]Loss = 3.8129e-03, PNorm = 79.7210, GNorm = 0.9656, lr_0 = 1.4435e-04\nLoss = 3.8129e-03, PNorm = 79.7210, GNorm = 0.9656, lr_0 = 1.4435e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.93it/s]Loss = 6.6436e-03, PNorm = 79.7243, GNorm = 1.8252, lr_0 = 1.4315e-04\nLoss = 6.6436e-03, PNorm = 79.7243, GNorm = 1.8252, lr_0 = 1.4315e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.93it/s]Loss = 8.0964e-03, PNorm = 79.7274, GNorm = 1.9393, lr_0 = 1.4196e-04\nLoss = 8.0964e-03, PNorm = 79.7274, GNorm = 1.9393, lr_0 = 1.4196e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.91it/s]Loss = 6.1945e-03, PNorm = 79.7308, GNorm = 2.0463, lr_0 = 1.4078e-04\nLoss = 6.1945e-03, PNorm = 79.7308, GNorm = 2.0463, lr_0 = 1.4078e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.92it/s]Loss = 6.8523e-03, PNorm = 79.7343, GNorm = 2.6436, lr_0 = 1.3961e-04\nLoss = 6.8523e-03, PNorm = 79.7343, GNorm = 2.6436, lr_0 = 1.3961e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.84it/s]Loss = 4.6801e-03, PNorm = 79.7375, GNorm = 1.2018, lr_0 = 1.3845e-04\nLoss = 4.6801e-03, PNorm = 79.7375, GNorm = 1.2018, lr_0 = 1.3845e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.89it/s]Loss = 5.7261e-03, PNorm = 79.7411, GNorm = 4.0187, lr_0 = 1.3730e-04\nLoss = 5.7261e-03, PNorm = 79.7411, GNorm = 4.0187, lr_0 = 1.3730e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.92it/s]Loss = 1.0788e-02, PNorm = 79.7453, GNorm = 8.1854, lr_0 = 1.3616e-04\nLoss = 1.0788e-02, PNorm = 79.7453, GNorm = 8.1854, lr_0 = 1.3616e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.94it/s]Loss = 9.0171e-03, PNorm = 79.7495, GNorm = 2.0432, lr_0 = 1.3503e-04\nLoss = 9.0171e-03, PNorm = 79.7495, GNorm = 2.0432, lr_0 = 1.3503e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.93it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.95it/s]Validation rmse = 18.957650\nValidation rmse = 18.957650\n\n\r 88%|████████▊ | 22/25 [01:49&lt;00:06,  2.10s/it]Epoch 22\nEpoch 22\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.0722e-02, PNorm = 79.7523, GNorm = 11.2057, lr_0 = 1.3391e-04\nLoss = 1.0722e-02, PNorm = 79.7523, GNorm = 11.2057, lr_0 = 1.3391e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.99it/s]Loss = 8.1340e-03, PNorm = 79.7551, GNorm = 2.8590, lr_0 = 1.3280e-04\nLoss = 8.1340e-03, PNorm = 79.7551, GNorm = 2.8590, lr_0 = 1.3280e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.98it/s]Loss = 5.5980e-03, PNorm = 79.7579, GNorm = 1.4365, lr_0 = 1.3169e-04\nLoss = 5.5980e-03, PNorm = 79.7579, GNorm = 1.4365, lr_0 = 1.3169e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.97it/s]Loss = 5.5094e-03, PNorm = 79.7606, GNorm = 1.4294, lr_0 = 1.3060e-04\nLoss = 5.5094e-03, PNorm = 79.7606, GNorm = 1.4294, lr_0 = 1.3060e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.96it/s]Loss = 7.4102e-03, PNorm = 79.7637, GNorm = 3.5369, lr_0 = 1.2951e-04\nLoss = 7.4102e-03, PNorm = 79.7637, GNorm = 3.5369, lr_0 = 1.2951e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.96it/s]Loss = 5.8438e-03, PNorm = 79.7669, GNorm = 1.9433, lr_0 = 1.2844e-04\nLoss = 5.8438e-03, PNorm = 79.7669, GNorm = 1.9433, lr_0 = 1.2844e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.84it/s]Loss = 6.9843e-03, PNorm = 79.7701, GNorm = 3.4062, lr_0 = 1.2737e-04\nLoss = 6.9843e-03, PNorm = 79.7701, GNorm = 3.4062, lr_0 = 1.2737e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.86it/s]Loss = 7.1208e-03, PNorm = 79.7735, GNorm = 2.4594, lr_0 = 1.2631e-04\nLoss = 7.1208e-03, PNorm = 79.7735, GNorm = 2.4594, lr_0 = 1.2631e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.88it/s]Loss = 8.5912e-03, PNorm = 79.7769, GNorm = 1.1457, lr_0 = 1.2526e-04\nLoss = 8.5912e-03, PNorm = 79.7769, GNorm = 1.1457, lr_0 = 1.2526e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.93it/s]Loss = 6.9487e-03, PNorm = 79.7804, GNorm = 1.6194, lr_0 = 1.2422e-04\nLoss = 6.9487e-03, PNorm = 79.7804, GNorm = 1.6194, lr_0 = 1.2422e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.95it/s]Loss = 6.4832e-03, PNorm = 79.7834, GNorm = 3.4734, lr_0 = 1.2319e-04\nLoss = 6.4832e-03, PNorm = 79.7834, GNorm = 3.4734, lr_0 = 1.2319e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.91it/s]Loss = 1.0776e-02, PNorm = 79.7860, GNorm = 3.7920, lr_0 = 1.2217e-04\nLoss = 1.0776e-02, PNorm = 79.7860, GNorm = 3.7920, lr_0 = 1.2217e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.82it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.82it/s]Validation rmse = 17.642910\nValidation rmse = 17.642910\n\n\r 92%|█████████▏| 23/25 [01:58&lt;00:08,  4.32s/it]Epoch 23\nEpoch 23\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 6.5237e-03, PNorm = 79.7884, GNorm = 2.4071, lr_0 = 1.2115e-04\nLoss = 6.5237e-03, PNorm = 79.7884, GNorm = 2.4071, lr_0 = 1.2115e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.89it/s]Loss = 8.1060e-03, PNorm = 79.7908, GNorm = 1.3470, lr_0 = 1.2015e-04\nLoss = 8.1060e-03, PNorm = 79.7908, GNorm = 1.3470, lr_0 = 1.2015e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.90it/s]Loss = 5.1975e-03, PNorm = 79.7932, GNorm = 0.9601, lr_0 = 1.1915e-04\nLoss = 5.1975e-03, PNorm = 79.7932, GNorm = 0.9601, lr_0 = 1.1915e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.92it/s]Loss = 6.0012e-03, PNorm = 79.7956, GNorm = 1.6915, lr_0 = 1.1816e-04\nLoss = 6.0012e-03, PNorm = 79.7956, GNorm = 1.6915, lr_0 = 1.1816e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.93it/s]Loss = 6.4156e-03, PNorm = 79.7982, GNorm = 2.6219, lr_0 = 1.1718e-04\nLoss = 6.4156e-03, PNorm = 79.7982, GNorm = 2.6219, lr_0 = 1.1718e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.94it/s]Loss = 6.6919e-03, PNorm = 79.8011, GNorm = 1.9070, lr_0 = 1.1620e-04\nLoss = 6.6919e-03, PNorm = 79.8011, GNorm = 1.9070, lr_0 = 1.1620e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.80it/s]Loss = 7.4121e-03, PNorm = 79.8045, GNorm = 5.7552, lr_0 = 1.1524e-04\nLoss = 7.4121e-03, PNorm = 79.8045, GNorm = 5.7552, lr_0 = 1.1524e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.82it/s]Loss = 7.4487e-03, PNorm = 79.8078, GNorm = 1.9462, lr_0 = 1.1428e-04\nLoss = 7.4487e-03, PNorm = 79.8078, GNorm = 1.9462, lr_0 = 1.1428e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.86it/s]Loss = 6.0450e-03, PNorm = 79.8109, GNorm = 1.9082, lr_0 = 1.1333e-04\nLoss = 6.0450e-03, PNorm = 79.8109, GNorm = 1.9082, lr_0 = 1.1333e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.88it/s]Loss = 6.9021e-03, PNorm = 79.8135, GNorm = 3.9280, lr_0 = 1.1239e-04\nLoss = 6.9021e-03, PNorm = 79.8135, GNorm = 3.9280, lr_0 = 1.1239e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.88it/s]Loss = 8.2067e-03, PNorm = 79.8158, GNorm = 2.0218, lr_0 = 1.1146e-04\nLoss = 8.2067e-03, PNorm = 79.8158, GNorm = 2.0218, lr_0 = 1.1146e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.90it/s]Loss = 5.9546e-03, PNorm = 79.8182, GNorm = 1.7936, lr_0 = 1.1053e-04\nLoss = 5.9546e-03, PNorm = 79.8182, GNorm = 1.7936, lr_0 = 1.1053e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.86it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.65it/s]Validation rmse = 17.522476\nValidation rmse = 17.522476\n\n\r 96%|█████████▌| 24/25 [02:08&lt;00:06,  6.08s/it]Epoch 24\nEpoch 24\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 7.6967e-03, PNorm = 79.8206, GNorm = 1.6964, lr_0 = 1.0961e-04\nLoss = 7.6967e-03, PNorm = 79.8206, GNorm = 1.6964, lr_0 = 1.0961e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:01,  6.99it/s]Loss = 7.1737e-03, PNorm = 79.8231, GNorm = 0.9598, lr_0 = 1.0870e-04\nLoss = 7.1737e-03, PNorm = 79.8231, GNorm = 0.9598, lr_0 = 1.0870e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  6.88it/s]Loss = 4.3936e-03, PNorm = 79.8256, GNorm = 1.1274, lr_0 = 1.0780e-04\nLoss = 4.3936e-03, PNorm = 79.8256, GNorm = 1.1274, lr_0 = 1.0780e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  6.87it/s]Loss = 5.5514e-03, PNorm = 79.8284, GNorm = 3.5604, lr_0 = 1.0690e-04\nLoss = 5.5514e-03, PNorm = 79.8284, GNorm = 3.5604, lr_0 = 1.0690e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  6.92it/s]Loss = 6.8205e-03, PNorm = 79.8315, GNorm = 4.2178, lr_0 = 1.0601e-04\nLoss = 6.8205e-03, PNorm = 79.8315, GNorm = 4.2178, lr_0 = 1.0601e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  6.90it/s]Loss = 4.6568e-03, PNorm = 79.8347, GNorm = 1.5185, lr_0 = 1.0513e-04\nLoss = 4.6568e-03, PNorm = 79.8347, GNorm = 1.5185, lr_0 = 1.0513e-04\n\n\n\r 50%|█████     | 6/12 [00:00&lt;00:00,  6.88it/s]Loss = 6.4500e-03, PNorm = 79.8379, GNorm = 1.3605, lr_0 = 1.0426e-04\nLoss = 6.4500e-03, PNorm = 79.8379, GNorm = 1.3605, lr_0 = 1.0426e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  6.90it/s]Loss = 7.4494e-03, PNorm = 79.8406, GNorm = 5.3185, lr_0 = 1.0339e-04\nLoss = 7.4494e-03, PNorm = 79.8406, GNorm = 5.3185, lr_0 = 1.0339e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  6.83it/s]Loss = 4.4672e-03, PNorm = 79.8430, GNorm = 2.4428, lr_0 = 1.0253e-04\nLoss = 4.4672e-03, PNorm = 79.8430, GNorm = 2.4428, lr_0 = 1.0253e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  6.83it/s]Loss = 5.7723e-03, PNorm = 79.8455, GNorm = 1.3878, lr_0 = 1.0168e-04\nLoss = 5.7723e-03, PNorm = 79.8455, GNorm = 1.3878, lr_0 = 1.0168e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  6.85it/s]Loss = 4.8915e-03, PNorm = 79.8480, GNorm = 1.2548, lr_0 = 1.0084e-04\nLoss = 4.8915e-03, PNorm = 79.8480, GNorm = 1.2548, lr_0 = 1.0084e-04\n\n\n\r 92%|█████████▏| 11/12 [00:01&lt;00:00,  6.89it/s]Loss = 4.6528e-03, PNorm = 79.8502, GNorm = 0.7932, lr_0 = 1.0000e-04\nLoss = 4.6528e-03, PNorm = 79.8502, GNorm = 0.7932, lr_0 = 1.0000e-04\n\n\n\r100%|██████████| 12/12 [00:01&lt;00:00,  6.90it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.88it/s]Validation rmse = 18.401633\nValidation rmse = 18.401633\n\n\r100%|██████████| 25/25 [02:10&lt;00:00,  4.80s/it]Model 9 best validation rmse = 17.522476 on epoch 23\nModel 9 best validation rmse = 17.522476 on epoch 23\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\nMoving model to cuda\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\r100%|██████████| 2/2 [00:00&lt;00:00, 28.71it/s]Model 9 test rmse = 17.575135\nModel 9 test rmse = 17.575135\nEnsemble test rmse = 17.418819\nEnsemble test rmse = 17.418819\n5-fold cross validation\n5-fold cross validation\nSeed 13 ==&gt; test rmse = 18.189116\nSeed 13 ==&gt; test rmse = 18.189116\nSeed 14 ==&gt; test rmse = 17.371650\nSeed 14 ==&gt; test rmse = 17.371650\nSeed 15 ==&gt; test rmse = 17.379233\nSeed 15 ==&gt; test rmse = 17.379233\nSeed 16 ==&gt; test rmse = 16.433916\nSeed 16 ==&gt; test rmse = 16.433916\nSeed 17 ==&gt; test rmse = 17.418819\nSeed 17 ==&gt; test rmse = 17.418819\nOverall test rmse = 17.358547 +/- 0.556600\nOverall test rmse = 17.358547 +/- 0.556600\nOut[13]: (17.35854671130955, 0.5566004299330413)\n</div>"]}}],"execution_count":84},{"cell_type":"markdown","source":["#### Testing different features"],"metadata":{}},{"cell_type":"code","source":["# Test RMSE\n#Ensemble-10: 0.44\n#Ensemble-3: 0.52\n#Feature morgan_count: 0.60\n#Feature morgan: 0.62\n#Feature BalabanJ: 0.54\n#Feature AATS0dv: 0.53\n#Feature AATSC5d: 0.53\n#Feature ABC: 0.53\n#Feature ECIndex: 0.53\n#Feature SLogP: 0.51 best: {'depth': 6, 'dropout': 0.0, 'ffn_num_layers': 3, 'hidden_size': 400}\n#%tb\nparser = ArgumentParser()\nadd_train_args(parser)\nif not os.path.exists(os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x')):\n  os.mkdir(os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x'))\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','all_2188-sparse-regression.csv'),#os.path.join(CHEMPROP_DIR,'JAK','all_1825-regression.csv'),#os.path.join(CHEMPROP_DIR,'JAK','train-1460.csv'),\n                          '--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtrain-sparse.csv'),\n                          '--dataset_type','regression',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-2188-sparse-regression-scaffold-FeatSlogP'),\n                          #'--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182.csv'),\n                          #'--separate_val_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPval-182.csv'),\n                          #'--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183.csv'),\n                          #'--separate_test_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtest-183.csv'),\n                          '--log_frequency','1',\n                          '--depth','6',#'4',\n                          '--dropout','0.0',#'0.1',\n                          '--hidden_size','400',#'2000',\n                          '--ffn_num_layers','3'#'2'\n                        #,'--atom_messages'\n                        ,'--ensemble_size','5'\n                        ,'--split_type','scaffold_balanced'\n                        #,'--features_generator','rdkit_2d'\n                        #,'--seed','13',\n                          ])\nmodify_train_args(args)\nlogger = create_logger(name='train', save_dir=args.save_dir, quiet=args.quiet)\n\ncross_validate(args, logger)\n#sparse ensemble (5):0.538572\n#sparse ensemble (5) SLogP:0.513054"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fold 0\nFold 0\nFold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/all_2188-sparse-regression.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.0,\n &#39;ensemble_size&#39;: 5,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-sparse.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 400,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-2188-sparse-regression-scaffold-FeatSlogP/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;scaffold_balanced&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-sparse.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/all_2188-sparse-regression.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.0,\n &#39;ensemble_size&#39;: 5,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-sparse.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 400,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-2188-sparse-regression-scaffold-FeatSlogP/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;scaffold_balanced&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-sparse.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/all_2188-sparse-regression.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.0,\n &#39;ensemble_size&#39;: 5,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-sparse.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 400,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-2188-sparse-regression-scaffold-FeatSlogP/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;scaffold_balanced&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-sparse.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/all_2188-sparse-regression.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.0,\n &#39;ensemble_size&#39;: 5,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-sparse.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 400,\n &#39;ffn_num_layers&#39;: 3,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 400,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-2188-sparse-regression-scaffold-FeatSlogP/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;scaffold_balanced&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-sparse.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\nLoading data\nLoading data\n\r  0%|          | 0/2188 [00:00&lt;?, ?it/s]\r 15%|█▌        | 332/2188 [00:00&lt;00:00, 3310.21it/s]\r 30%|███       | 667/2188 [00:00&lt;00:00, 3321.45it/s]\r 45%|████▌     | 994/2188 [00:00&lt;00:00, 3303.94it/s]\r 58%|█████▊    | 1279/2188 [00:00&lt;00:00, 3152.60it/s]\r 71%|███████   | 1555/2188 [00:00&lt;00:00, 3023.00it/s]\r 85%|████████▌ | 1865/2188 [00:00&lt;00:00, 3042.36it/s]\r 99%|█████████▉| 2177/2188 [00:00&lt;00:00, 3063.02it/s]\r100%|██████████| 2188/2188 [00:00&lt;00:00, 3105.70it/s]\nNumber of tasks = 4\nNumber of tasks = 4\nNumber of tasks = 4\nNumber of tasks = 4\nSplitting data with seed 0\nSplitting data with seed 0\nSplitting data with seed 0\nSplitting data with seed 0\n\r  0%|          | 0/2188 [00:00&lt;?, ?it/s]\r 16%|█▌        | 345/2188 [00:00&lt;00:00, 3440.98it/s]\r 32%|███▏      | 691/2188 [00:00&lt;00:00, 3445.81it/s]\r 47%|████▋     | 1032/2188 [00:00&lt;00:00, 3432.26it/s]\r 63%|██████▎   | 1373/2188 [00:00&lt;00:00, 3422.39it/s]\r 78%|███████▊  | 1703/2188 [00:00&lt;00:00, 3382.89it/s]\r 92%|█████████▏| 2006/2188 [00:00&lt;00:00, 3267.16it/s]\r100%|██████████| 2188/2188 [00:00&lt;00:00, 3263.83it/s]\nTotal scaffolds = 678 | train scaffolds = 549 | val scaffolds = 48 | test scaffolds = 81\nTotal scaffolds = 678 | train scaffolds = 549 | val scaffolds = 48 | test scaffolds = 81\nTotal scaffolds = 678 | train scaffolds = 549 | val scaffolds = 48 | test scaffolds = 81\nTotal scaffolds = 678 | train scaffolds = 549 | val scaffolds = 48 | test scaffolds = 81\n/local_disk0/spark-1ba4d10a-3c3e-4aa0-b24d-d7dbaf9ecfda/userFiles-94260feb-d715-4e59-ba98-9c4aee659fb8/addedFile3405381182397805412dbfs__FileStore_jars_f4d58811_6460_4f6f_891d_7a2deafa24ab_chemprop_0_0_1_py3_7_f485d-24796.egg/chemprop/data/scaffold.py:146: RuntimeWarning: Mean of empty slice\nLabel averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([8.42196021, 8.23914176, 7.36480752, 7.05563272]), array([137, 139, 139, 139])), (array([6.4134127 , 6.4698003 , 5.93930216, 5.2823295 ]), array([1, 1, 1, 1])), (array([6.87614836, 6.64016452, 6.30103   , 4.99567863]), array([1, 1, 1, 1])), (array([6.21863356, 6.22792627, 6.43027299, 4.57348874]), array([2, 2, 2, 1])), (array([7.22127676, 7.02396946, 6.64237635, 5.65069488]), array([4, 4, 4, 4])), (array([7.81247928, 7.64781748, 7.66354027, 6.68613278]), array([1, 1, 1, 1])), (array([5.97122433, 5.99384875, 5.45013953, 4.73685074]), array([8, 9, 9, 9])), (array([6.56383735, 6.8827287 , 6.60730305, 5.82681373]), array([1, 1, 1, 1])), (array([6.12901119, 6.28483264, 6.03763066,        nan]), array([1, 1, 1, 0])), (array([5.56829893, 5.62295474, 5.424383  , 4.67230384]), array([13, 14, 14, 14]))]\nLabel averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([8.42196021, 8.23914176, 7.36480752, 7.05563272]), array([137, 139, 139, 139])), (array([6.4134127 , 6.4698003 , 5.93930216, 5.2823295 ]), array([1, 1, 1, 1])), (array([6.87614836, 6.64016452, 6.30103   , 4.99567863]), array([1, 1, 1, 1])), (array([6.21863356, 6.22792627, 6.43027299, 4.57348874]), array([2, 2, 2, 1])), (array([7.22127676, 7.02396946, 6.64237635, 5.65069488]), array([4, 4, 4, 4])), (array([7.81247928, 7.64781748, 7.66354027, 6.68613278]), array([1, 1, 1, 1])), (array([5.97122433, 5.99384875, 5.45013953, 4.73685074]), array([8, 9, 9, 9])), (array([6.56383735, 6.8827287 , 6.60730305, 5.82681373]), array([1, 1, 1, 1])), (array([6.12901119, 6.28483264, 6.03763066,        nan]), array([1, 1, 1, 0])), (array([5.56829893, 5.62295474, 5.424383  , 4.67230384]), array([13, 14, 14, 14]))]\nLabel averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([8.42196021, 8.23914176, 7.36480752, 7.05563272]), array([137, 139, 139, 139])), (array([6.4134127 , 6.4698003 , 5.93930216, 5.2823295 ]), array([1, 1, 1, 1])), (array([6.87614836, 6.64016452, 6.30103   , 4.99567863]), array([1, 1, 1, 1])), (array([6.21863356, 6.22792627, 6.43027299, 4.57348874]), array([2, 2, 2, 1])), (array([7.22127676, 7.02396946, 6.64237635, 5.65069488]), array([4, 4, 4, 4])), (array([7.81247928, 7.64781748, 7.66354027, 6.68613278]), array([1, 1, 1, 1])), (array([5.97122433, 5.99384875, 5.45013953, 4.73685074]), array([8, 9, 9, 9])), (array([6.56383735, 6.8827287 , 6.60730305, 5.82681373]), array([1, 1, 1, 1])), (array([6.12901119, 6.28483264, 6.03763066,        nan]), array([1, 1, 1, 0])), (array([5.56829893, 5.62295474, 5.424383  , 4.67230384]), array([13, 14, 14, 14]))]\nLabel averages per scaffold, in decreasing order of scaffold frequency,capped at 10 scaffolds and 20 labels: [(array([8.42196021, 8.23914176, 7.36480752, 7.05563272]), array([137, 139, 139, 139])), (array([6.4134127 , 6.4698003 , 5.93930216, 5.2823295 ]), array([1, 1, 1, 1])), (array([6.87614836, 6.64016452, 6.30103   , 4.99567863]), array([1, 1, 1, 1])), (array([6.21863356, 6.22792627, 6.43027299, 4.57348874]), array([2, 2, 2, 1])), (array([7.22127676, 7.02396946, 6.64237635, 5.65069488]), array([4, 4, 4, 4])), (array([7.81247928, 7.64781748, 7.66354027, 6.68613278]), array([1, 1, 1, 1])), (array([5.97122433, 5.99384875, 5.45013953, 4.73685074]), array([8, 9, 9, 9])), (array([6.56383735, 6.8827287 , 6.60730305, 5.82681373]), array([1, 1, 1, 1])), (array([6.12901119, 6.28483264, 6.03763066,        nan]), array([1, 1, 1, 0])), (array([5.56829893, 5.62295474, 5.424383  , 4.67230384]), array([13, 14, 14, 14]))]\nTotal size = 2,188 | train size = 1,750 | val size = 218 | test size = 220\nTotal size = 2,188 | train size = 1,750 | val size = 218 | test size = 220\nTotal size = 2,188 | train size = 1,750 | val size = 218 | test size = 220\nTotal size = 2,188 | train size = 1,750 | val size = 218 | test size = 220\nFitting scaler\nFitting scaler\nFitting scaler\nFitting scaler\nBuilding model 0\nBuilding model 0\nBuilding model 0\nBuilding model 0\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.0, inplace=False)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=400, bias=False)\n      (W_h): Linear(in_features=400, out_features=400, bias=False)\n      (W_o): Linear(in_features=533, out_features=400, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.0, inplace=False)\n    (1): Linear(in_features=401, out_features=400, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.0, inplace=False)\n    (4): Linear(in_features=400, out_features=400, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.0, inplace=False)\n    (7): Linear(in_features=400, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.0, inplace=False)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=400, bias=False)\n      (W_h): Linear(in_features=400, out_features=400, bias=False)\n      (W_o): Linear(in_features=533, out_features=400, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.0, inplace=False)\n    (1): Linear(in_features=401, out_features=400, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.0, inplace=False)\n    (4): Linear(in_features=400, out_features=400, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.0, inplace=False)\n    (7): Linear(in_features=400, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.0, inplace=False)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=400, bias=False)\n      (W_h): Linear(in_features=400, out_features=400, bias=False)\n      (W_o): Linear(in_features=533, out_features=400, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.0, inplace=False)\n    (1): Linear(in_features=401, out_features=400, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.0, inplace=False)\n    (4): Linear(in_features=400, out_features=400, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.0, inplace=False)\n    (7): Linear(in_features=400, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.0, inplace=False)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=400, bias=False)\n      (W_h): Linear(in_features=400, out_features=400, bias=False)\n      (W_o): Linear(in_features=533, out_features=400, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.0, inplace=False)\n    (1): Linear(in_features=401, out_features=400, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.0, inplace=False)\n    (4): Linear(in_features=400, out_features=400, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.0, inplace=False)\n    (7): Linear(in_features=400, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 755,204\nNumber of parameters = 755,204\nNumber of parameters = 755,204\nNumber of parameters = 755,204\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s]Epoch 0\nEpoch 0\nEpoch 0\nEpoch 0\n\n\r  0%|          | 0/35 [00:00&lt;?, ?it/s]Loss = 1.5668e-02, PNorm = 43.3901, GNorm = 3.1244, lr_0 = 1.1286e-04\nLoss = 1.5668e-02, PNorm = 43.3901, GNorm = 3.1244, lr_0 = 1.1286e-04\nLoss = 1.5668e-02, PNorm = 43.3901, GNorm = 3.1244, lr_0 = 1.1286e-04\nLoss = 1.5668e-02, PNorm = 43.3901, GNorm = 3.1244, lr_0 = 1.1286e-04\nLoss = 2.1988e-02, PNorm = 43.3901, GNorm = 1.3664, lr_0 = 1.2571e-04\nLoss = 2.1988e-02, PNorm = 43.3901, GNorm = 1.3664, lr_0 = 1.2571e-04\nLoss = 2.1988e-02, PNorm = 43.3901, GNorm = 1.3664, lr_0 = 1.2571e-04\nLoss = 2.1988e-02, PNorm = 43.3901, GNorm = 1.3664, lr_0 = 1.2571e-04\nLoss = 2.0529e-02, PNorm = 43.3902, GNorm = 1.2615, lr_0 = 1.3857e-04\nLoss = 2.0529e-02, PNorm = 43.3902, GNorm = 1.2615, lr_0 = 1.3857e-04\nLoss = 2.0529e-02, PNorm = 43.3902, GNorm = 1.2615, lr_0 = 1.3857e-04\nLoss = 2.0529e-02, PNorm = 43.3902, GNorm = 1.2615, lr_0 = 1.3857e-04\nLoss = 2.0757e-02, PNorm = 43.3902, GNorm = 1.6743, lr_0 = 1.5143e-04\nLoss = 2.0757e-02, PNorm = 43.3902, GNorm = 1.6743, lr_0 = 1.5143e-04\nLoss = 2.0757e-02, PNorm = 43.3902, GNorm = 1.6743, lr_0 = 1.5143e-04\nLoss = 2.0757e-02, PNorm = 43.3902, GNorm = 1.6743, lr_0 = 1.5143e-04\n\n\r 11%|█▏        | 4/35 [00:00&lt;00:00, 34.18it/s]Loss = 2.2252e-02, PNorm = 43.3904, GNorm = 2.3715, lr_0 = 1.6429e-04\nLoss = 2.2252e-02, PNorm = 43.3904, GNorm = 2.3715, lr_0 = 1.6429e-04\nLoss = 2.2252e-02, PNorm = 43.3904, GNorm = 2.3715, lr_0 = 1.6429e-04\nLoss = 2.2252e-02, PNorm = 43.3904, GNorm = 2.3715, lr_0 = 1.6429e-04\nLoss = 1.5378e-02, PNorm = 43.3905, GNorm = 1.8252, lr_0 = 1.7714e-04\nLoss = 1.5378e-02, PNorm = 43.3905, GNorm = 1.8252, lr_0 = 1.7714e-04\nLoss = 1.5378e-02, PNorm = 43.3905, GNorm = 1.8252, lr_0 = 1.7714e-04\nLoss = 1.5378e-02, PNorm = 43.3905, GNorm = 1.8252, lr_0 = 1.7714e-04\nLoss = 2.1469e-02, PNorm = 43.3906, GNorm = 1.8982, lr_0 = 1.9000e-04\nLoss = 2.1469e-02, PNorm = 43.3906, GNorm = 1.8982, lr_0 = 1.9000e-04\nLoss = 2.1469e-02, PNorm = 43.3906, GNorm = 1.8982, lr_0 = 1.9000e-04\nLoss = 2.1469e-02, PNorm = 43.3906, GNorm = 1.8982, lr_0 = 1.9000e-04\nLoss = 1.7256e-02, PNorm = 43.3908, GNorm = 1.0812, lr_0 = 2.0286e-04\nLoss = 1.7256e-02, PNorm = 43.3908, GNorm = 1.0812, lr_0 = 2.0286e-04\nLoss = 1.7256e-02, PNorm = 43.3908, GNorm = 1.0812, lr_0 = 2.0286e-04\nLoss = 1.7256e-02, PNorm = 43.3908, GNorm = 1.0812, lr_0 = 2.0286e-04\n\n\r 23%|██▎       | 8/35 [00:00&lt;00:00, 34.42it/s]Loss = 1.3019e-02, PNorm = 43.3911, GNorm = 1.1339, lr_0 = 2.1571e-04\nLoss = 1.3019e-02, PNorm = 43.3911, GNorm = 1.1339, lr_0 = 2.1571e-04\nLoss = 1.3019e-02, PNorm = 43.3911, GNorm = 1.1339, lr_0 = 2.1571e-04\nLoss = 1.3019e-02, PNorm = 43.3911, GNorm = 1.1339, lr_0 = 2.1571e-04\nLoss = 2.3323e-02, PNorm = 43.3913, GNorm = 4.0424, lr_0 = 2.2857e-04\nLoss = 2.3323e-02, PNorm = 43.3913, GNorm = 4.0424, lr_0 = 2.2857e-04\nLoss = 2.3323e-02, PNorm = 43.3913, GNorm = 4.0424, lr_0 = 2.2857e-04\nLoss = 2.3323e-02, PNorm = 43.3913, GNorm = 4.0424, lr_0 = 2.2857e-04\nLoss = 1.7326e-02, PNorm = 43.3916, GNorm = 1.1161, lr_0 = 2.4143e-04\nLoss = 1.7326e-02, PNorm = 43.3916, GNorm = 1.1161, lr_0 = 2.4143e-04\nLoss = 1.7326e-02, PNorm = 43.3916, GNorm = 1.1161, lr_0 = 2.4143e-04\nLoss = 1.7326e-02, PNorm = 43.3916, GNorm = 1.1161, lr_0 = 2.4143e-04\nLoss = 1.8001e-02, PNorm = 43.3921, GNorm = 1.7604, lr_0 = 2.5429e-04\nLoss = 1.8001e-02, PNorm = 43.3921, GNorm = 1.7604, lr_0 = 2.5429e-04\nLoss = 1.8001e-02, PNorm = 43.3921, GNorm = 1.7604, lr_0 = 2.5429e-04\nLoss = 1.8001e-02, PNorm = 43.3921, GNorm = 1.7604, lr_0 = 2.5429e-04\n\n\r 34%|███▍      | 12/35 [00:00&lt;00:00, 34.72it/s]Loss = 2.0765e-02, PNorm = 43.3926, GNorm = 2.6148, lr_0 = 2.6714e-04\nLoss = 2.0765e-02, PNorm = 43.3926, GNorm = 2.6148, lr_0 = 2.6714e-04\nLoss = 2.0765e-02, PNorm = 43.3926, GNorm = 2.6148, lr_0 = 2.6714e-04\nLoss = 2.0765e-02, PNorm = 43.3926, GNorm = 2.6148, lr_0 = 2.6714e-04\nLoss = 1.7212e-02, PNorm = 43.3931, GNorm = 2.0306, lr_0 = 2.8000e-04\nLoss = 1.7212e-02, PNorm = 43.3931, GNorm = 2.0306, lr_0 = 2.8000e-04\nLoss = 1.7212e-02, PNorm = 43.3931, GNorm = 2.0306, lr_0 = 2.8000e-04\nLoss = 1.7212e-02, PNorm = 43.3931, GNorm = 2.0306, lr_0 = 2.8000e-04\nLoss = 1.8223e-02, PNorm = 43.3936, GNorm = 1.4803, lr_0 = 2.9286e-04\nLoss = 1.8223e-02, PNorm = 43.3936, GNorm = 1.4803, lr_0 = 2.9286e-04\nLoss = 1.8223e-02, PNorm = 43.3936, GNorm = 1.4803, lr_0 = 2.9286e-04\nLoss = 1.8223e-02, PNorm = 43.3936, GNorm = 1.4803, lr_0 = 2.9286e-04\nLoss = 1.9098e-02, PNorm = 43.3942, GNorm = 1.0109, lr_0 = 3.0571e-04\nLoss = 1.9098e-02, PNorm = 43.3942, GNorm = 1.0109, lr_0 = 3.0571e-04\nLoss = 1.9098e-02, PNorm = 43.3942, GNorm = 1.0109, lr_0 = 3.0571e-04\nLoss = 1.9098e-02, PNorm = 43.3942, GNorm = 1.0109, lr_0 = 3.0571e-04\n\n\r 46%|████▌     | 16/35 [00:00&lt;00:00, 34.70it/s]Loss = 2.1811e-02, PNorm = 43.3950, GNorm = 1.1531, lr_0 = 3.1857e-04\nLoss = 2.1811e-02, PNorm = 43.3950, GNorm = 1.1531, lr_0 = 3.1857e-04\nLoss = 2.1811e-02, PNorm = 43.3950, GNorm = 1.1531, lr_0 = 3.1857e-04\nLoss = 2.1811e-02, PNorm = 43.3950, GNorm = 1.1531, lr_0 = 3.1857e-04\nLoss = 2.3370e-02, PNorm = 43.3958, GNorm = 1.0019, lr_0 = 3.3143e-04\nLoss = 2.3370e-02, PNorm = 43.3958, GNorm = 1.0019, lr_0 = 3.3143e-04\nLoss = 2.3370e-02, PNorm = 43.3958, GNorm = 1.0019, lr_0 = 3.3143e-04\nLoss = 2.3370e-02, PNorm = 43.3958, GNorm = 1.0019, lr_0 = 3.3143e-04\nLoss = 1.7806e-02, PNorm = 43.3969, GNorm = 0.9758, lr_0 = 3.4429e-04\nLoss = 1.7806e-02, PNorm = 43.3969, GNorm = 0.9758, lr_0 = 3.4429e-04\nLoss = 1.7806e-02, PNorm = 43.3969, GNorm = 0.9758, lr_0 = 3.4429e-04\nLoss = 1.7806e-02, PNorm = 43.3969, GNorm = 0.9758, lr_0 = 3.4429e-04\nLoss = 1.4236e-02, PNorm = 43.3979, GNorm = 1.0148, lr_0 = 3.5714e-04\nLoss = 1.4236e-02, PNorm = 43.3979, GNorm = 1.0148, lr_0 = 3.5714e-04\nLoss = 1.4236e-02, PNorm = 43.3979, GNorm = 1.0148, lr_0 = 3.5714e-04\nLoss = 1.4236e-02, PNorm = 43.3979, GNorm = 1.0148, lr_0 = 3.5714e-04\n\n\r 57%|█████▋    | 20/35 [00:00&lt;00:00, 34.75it/s]Loss = 2.2038e-02, PNorm = 43.3989, GNorm = 2.1624, lr_0 = 3.7000e-04\nLoss = 2.2038e-02, PNorm = 43.3989, GNorm = 2.1624, lr_0 = 3.7000e-04\nLoss = 2.2038e-02, PNorm = 43.3989, GNorm = 2.1624, lr_0 = 3.7000e-04\nLoss = 2.2038e-02, PNorm = 43.3989, GNorm = 2.1624, lr_0 = 3.7000e-04\nLoss = 1.8497e-02, PNorm = 43.4001, GNorm = 1.1216, lr_0 = 3.8286e-04\nLoss = 1.8497e-02, PNorm = 43.4001, GNorm = 1.1216, lr_0 = 3.8286e-04\nLoss = 1.8497e-02, PNorm = 43.4001, GNorm = 1.1216, lr_0 = 3.8286e-04\nLoss = 1.8497e-02, PNorm = 43.4001, GNorm = 1.1216, lr_0 = 3.8286e-04\nLoss = 2.3563e-02, PNorm = 43.4014, GNorm = 1.5869, lr_0 = 3.9571e-04\nLoss = 2.3563e-02, PNorm = 43.4014, GNorm = 1.5869, lr_0 = 3.9571e-04\nLoss = 2.3563e-02, PNorm = 43.4014, GNorm = 1.5869, lr_0 = 3.9571e-04\nLoss = 2.3563e-02, PNorm = 43.4014, GNorm = 1.5869, lr_0 = 3.9571e-04\nLoss = 2.2832e-02, PNorm = 43.4029, GNorm = 2.9176, lr_0 = 4.0857e-04\nLoss = 2.2832e-02, PNorm = 43.4029, GNorm = 2.9176, lr_0 = 4.0857e-04\nLoss = 2.2832e-02, PNorm = 43.4029, GNorm = 2.9176, lr_0 = 4.0857e-04\nLoss = 2.2832e-02, PNorm = 43.4029, GNorm = 2.9176, lr_0 = 4.0857e-04\n\n\r 69%|██████▊   | 24/35 [00:00&lt;00:00, 34.72it/s]Loss = 1.5793e-02, PNorm = 43.4044, GNorm = 0.9507, lr_0 = 4.2143e-04\nLoss = 1.5793e-02, PNorm = 43.4044, GNorm = 0.9507, lr_0 = 4.2143e-04\nLoss = 1.5793e-02, PNorm = 43.4044, GNorm = 0.9507, lr_0 = 4.2143e-04\nLoss = 1.5793e-02, PNorm = 43.4044, GNorm = 0.9507, lr_0 = 4.2143e-04\nLoss = 1.3173e-02, PNorm = 43.4060, GNorm = 1.7527, lr_0 = 4.3429e-04\nLoss = 1.3173e-02, PNorm = 43.4060, GNorm = 1.7527, lr_0 = 4.3429e-04\nLoss = 1.3173e-02, PNorm = 43.4060, GNorm = 1.7527, lr_0 = 4.3429e-04\nLoss = 1.3173e-02, PNorm = 43.4060, GNorm = 1.7527, lr_0 = 4.3429e-04\nLoss = 1.7416e-02, PNorm = 43.4077, GNorm = 1.5419, lr_0 = 4.4714e-04\nLoss = 1.7416e-02, PNorm = 43.4077, GNorm = 1.5419, lr_0 = 4.4714e-04\nLoss = 1.7416e-02, PNorm = 43.4077, GNorm = 1.5419, lr_0 = 4.4714e-04\nLoss = 1.7416e-02, PNorm = 43.4077, GNorm = 1.5419, lr_0 = 4.4714e-04\nLoss = 1.6855e-02, PNorm = 43.4095, GNorm = 1.0282, lr_0 = 4.6000e-04\nLoss = 1.6855e-02, PNorm = 43.4095, GNorm = 1.0282, lr_0 = 4.6000e-04\nLoss = 1.6855e-02, PNorm = 43.4095, GNorm = 1.0282, lr_0 = 4.6000e-04\nLoss = 1.6855e-02, PNorm = 43.4095, GNorm = 1.0282, lr_0 = 4.6000e-04\n\n\r 80%|████████  | 28/35 [00:00&lt;00:00, 34.96it/s]Loss = 1.7586e-02, PNorm = 43.4114, GNorm = 2.6038, lr_0 = 4.7286e-04\nLoss = 1.7586e-02, PNorm = 43.4114, GNorm = 2.6038, lr_0 = 4.7286e-04\nLoss = 1.7586e-02, PNorm = 43.4114, GNorm = 2.6038, lr_0 = 4.7286e-04\nLoss = 1.7586e-02, PNorm = 43.4114, GNorm = 2.6038, lr_0 = 4.7286e-04\nLoss = 2.2832e-02, PNorm = 43.4134, GNorm = 1.9551, lr_0 = 4.8571e-04\nLoss = 2.2832e-02, PNorm = 43.4134, GNorm = 1.9551, lr_0 = 4.8571e-04\nLoss = 2.2832e-02, PNorm = 43.4134, GNorm = 1.9551, lr_0 = 4.8571e-04\nLoss = 2.2832e-02, PNorm = 43.4134, GNorm = 1.9551, lr_0 = 4.8571e-04\nLoss = 1.8222e-02, PNorm = 43.4155, GNorm = 1.8787, lr_0 = 4.9857e-04\nLoss = 1.8222e-02, PNorm = 43.4155, GNorm = 1.8787, lr_0 = 4.9857e-04\nLoss = 1.8222e-02, PNorm = 43.4155, GNorm = 1.8787, lr_0 = 4.9857e-04\nLoss = 1.8222e-02, PNorm = 43.4155, GNorm = 1.8787, lr_0 = 4.9857e-04\nLoss = 1.6177e-02, PNorm = 43.4175, GNorm = 1.1883, lr_0 = 5.1143e-04\nLoss = 1.6177e-02, PNorm = 43.4175, GNorm = 1.1883, lr_0 = 5.1143e-04\nLoss = 1.6177e-02, PNorm = 43.4175, GNorm = 1.1883, lr_0 = 5.1143e-04\nLoss = 1.6177e-02, PNorm = 43.4175, GNorm = 1.1883, lr_0 = 5.1143e-04\n\n\r 91%|█████████▏| 32/35 [00:01&lt;00:00, 21.06it/s]Loss = 1.5351e-02, PNorm = 43.4200, GNorm = 4.8431, lr_0 = 5.2429e-04\nLoss = 1.5351e-02, PNorm = 43.4200, GNorm = 4.8431, lr_0 = 5.2429e-04\nLoss = 1.5351e-02, PNorm = 43.4200, GNorm = 4.8431, lr_0 = 5.2429e-04\nLoss = 1.5351e-02, PNorm = 43.4200, GNorm = 4.8431, lr_0 = 5.2429e-04\nLoss = 2.1272e-02, PNorm = 43.4219, GNorm = 5.3432, lr_0 = 5.3714e-04\nLoss = 2.1272e-02, PNorm = 43.4219, GNorm = 5.3432, lr_0 = 5.3714e-04\nLoss = 2.1272e-02, PNorm = 43.4219, GNorm = 5.3432, lr_0 = 5.3714e-04\nLoss = 2.1272e-02, PNorm = 43.4219, GNorm = 5.3432, lr_0 = 5.3714e-04\nLoss = 1.8552e-02, PNorm = 43.4237, GNorm = 5.4719, lr_0 = 5.5000e-04\nLoss = 1.8552e-02, PNorm = 43.4237, GNorm = 5.4719, lr_0 = 5.5000e-04\nLoss = 1.8552e-02, PNorm = 43.4237, GNorm = 5.4719, lr_0 = 5.5000e-04\n\n*** WARNING: skipped 1581011 bytes of output ***\n\nLoss = 2.8070e-03, PNorm = 45.7435, GNorm = 2.9448, lr_0 = 1.1788e-04\nLoss = 2.8070e-03, PNorm = 45.7435, GNorm = 2.9448, lr_0 = 1.1788e-04\nLoss = 2.8070e-03, PNorm = 45.7435, GNorm = 2.9448, lr_0 = 1.1788e-04\n\n\r100%|██████████| 35/35 [00:01&lt;00:00, 34.13it/s]\n\r  0%|          | 0/5 [00:00&lt;?, ?it/s]\n\r100%|██████████| 5/5 [00:00&lt;00:00, 69.99it/s]Validation rmse = 0.563976\nValidation rmse = 0.563976\nValidation rmse = 0.563976\nValidation rmse = 0.563976\n\r 93%|█████████▎| 28/30 [00:38&lt;00:02,  1.27s/it]Epoch 28\nEpoch 28\nEpoch 28\nEpoch 28\n\n\r  0%|          | 0/35 [00:00&lt;?, ?it/s]Loss = 2.4588e-03, PNorm = 45.7439, GNorm = 2.3030, lr_0 = 1.1760e-04\nLoss = 2.4588e-03, PNorm = 45.7439, GNorm = 2.3030, lr_0 = 1.1760e-04\nLoss = 2.4588e-03, PNorm = 45.7439, GNorm = 2.3030, lr_0 = 1.1760e-04\nLoss = 2.4588e-03, PNorm = 45.7439, GNorm = 2.3030, lr_0 = 1.1760e-04\nLoss = 3.4274e-03, PNorm = 45.7445, GNorm = 2.0914, lr_0 = 1.1732e-04\nLoss = 3.4274e-03, PNorm = 45.7445, GNorm = 2.0914, lr_0 = 1.1732e-04\nLoss = 3.4274e-03, PNorm = 45.7445, GNorm = 2.0914, lr_0 = 1.1732e-04\nLoss = 3.4274e-03, PNorm = 45.7445, GNorm = 2.0914, lr_0 = 1.1732e-04\nLoss = 2.4539e-03, PNorm = 45.7450, GNorm = 0.6908, lr_0 = 1.1705e-04\nLoss = 2.4539e-03, PNorm = 45.7450, GNorm = 0.6908, lr_0 = 1.1705e-04\nLoss = 2.4539e-03, PNorm = 45.7450, GNorm = 0.6908, lr_0 = 1.1705e-04\nLoss = 2.4539e-03, PNorm = 45.7450, GNorm = 0.6908, lr_0 = 1.1705e-04\nLoss = 3.3526e-03, PNorm = 45.7457, GNorm = 1.8042, lr_0 = 1.1677e-04\nLoss = 3.3526e-03, PNorm = 45.7457, GNorm = 1.8042, lr_0 = 1.1677e-04\nLoss = 3.3526e-03, PNorm = 45.7457, GNorm = 1.8042, lr_0 = 1.1677e-04\nLoss = 3.3526e-03, PNorm = 45.7457, GNorm = 1.8042, lr_0 = 1.1677e-04\n\n\r 11%|█▏        | 4/35 [00:00&lt;00:00, 34.66it/s]Loss = 1.6273e-03, PNorm = 45.7463, GNorm = 0.8348, lr_0 = 1.1650e-04\nLoss = 1.6273e-03, PNorm = 45.7463, GNorm = 0.8348, lr_0 = 1.1650e-04\nLoss = 1.6273e-03, PNorm = 45.7463, GNorm = 0.8348, lr_0 = 1.1650e-04\nLoss = 1.6273e-03, PNorm = 45.7463, GNorm = 0.8348, lr_0 = 1.1650e-04\nLoss = 3.5054e-03, PNorm = 45.7469, GNorm = 2.5606, lr_0 = 1.1623e-04\nLoss = 3.5054e-03, PNorm = 45.7469, GNorm = 2.5606, lr_0 = 1.1623e-04\nLoss = 3.5054e-03, PNorm = 45.7469, GNorm = 2.5606, lr_0 = 1.1623e-04\nLoss = 3.5054e-03, PNorm = 45.7469, GNorm = 2.5606, lr_0 = 1.1623e-04\nLoss = 3.1416e-03, PNorm = 45.7475, GNorm = 2.5902, lr_0 = 1.1595e-04\nLoss = 3.1416e-03, PNorm = 45.7475, GNorm = 2.5902, lr_0 = 1.1595e-04\nLoss = 3.1416e-03, PNorm = 45.7475, GNorm = 2.5902, lr_0 = 1.1595e-04\nLoss = 3.1416e-03, PNorm = 45.7475, GNorm = 2.5902, lr_0 = 1.1595e-04\nLoss = 1.8217e-03, PNorm = 45.7481, GNorm = 1.2830, lr_0 = 1.1568e-04\nLoss = 1.8217e-03, PNorm = 45.7481, GNorm = 1.2830, lr_0 = 1.1568e-04\nLoss = 1.8217e-03, PNorm = 45.7481, GNorm = 1.2830, lr_0 = 1.1568e-04\nLoss = 1.8217e-03, PNorm = 45.7481, GNorm = 1.2830, lr_0 = 1.1568e-04\n\n\r 23%|██▎       | 8/35 [00:00&lt;00:00, 34.74it/s]Loss = 3.7998e-03, PNorm = 45.7488, GNorm = 1.4188, lr_0 = 1.1541e-04\nLoss = 3.7998e-03, PNorm = 45.7488, GNorm = 1.4188, lr_0 = 1.1541e-04\nLoss = 3.7998e-03, PNorm = 45.7488, GNorm = 1.4188, lr_0 = 1.1541e-04\nLoss = 3.7998e-03, PNorm = 45.7488, GNorm = 1.4188, lr_0 = 1.1541e-04\nLoss = 1.7501e-03, PNorm = 45.7494, GNorm = 1.1887, lr_0 = 1.1514e-04\nLoss = 1.7501e-03, PNorm = 45.7494, GNorm = 1.1887, lr_0 = 1.1514e-04\nLoss = 1.7501e-03, PNorm = 45.7494, GNorm = 1.1887, lr_0 = 1.1514e-04\nLoss = 1.7501e-03, PNorm = 45.7494, GNorm = 1.1887, lr_0 = 1.1514e-04\nLoss = 2.2189e-03, PNorm = 45.7501, GNorm = 1.3415, lr_0 = 1.1487e-04\nLoss = 2.2189e-03, PNorm = 45.7501, GNorm = 1.3415, lr_0 = 1.1487e-04\nLoss = 2.2189e-03, PNorm = 45.7501, GNorm = 1.3415, lr_0 = 1.1487e-04\nLoss = 2.2189e-03, PNorm = 45.7501, GNorm = 1.3415, lr_0 = 1.1487e-04\nLoss = 2.1267e-03, PNorm = 45.7507, GNorm = 1.9047, lr_0 = 1.1460e-04\nLoss = 2.1267e-03, PNorm = 45.7507, GNorm = 1.9047, lr_0 = 1.1460e-04\nLoss = 2.1267e-03, PNorm = 45.7507, GNorm = 1.9047, lr_0 = 1.1460e-04\nLoss = 2.1267e-03, PNorm = 45.7507, GNorm = 1.9047, lr_0 = 1.1460e-04\n\n\r 34%|███▍      | 12/35 [00:00&lt;00:00, 34.68it/s]Loss = 3.3776e-03, PNorm = 45.7513, GNorm = 2.3713, lr_0 = 1.1433e-04\nLoss = 3.3776e-03, PNorm = 45.7513, GNorm = 2.3713, lr_0 = 1.1433e-04\nLoss = 3.3776e-03, PNorm = 45.7513, GNorm = 2.3713, lr_0 = 1.1433e-04\nLoss = 3.3776e-03, PNorm = 45.7513, GNorm = 2.3713, lr_0 = 1.1433e-04\nLoss = 2.3632e-03, PNorm = 45.7518, GNorm = 1.7481, lr_0 = 1.1406e-04\nLoss = 2.3632e-03, PNorm = 45.7518, GNorm = 1.7481, lr_0 = 1.1406e-04\nLoss = 2.3632e-03, PNorm = 45.7518, GNorm = 1.7481, lr_0 = 1.1406e-04\nLoss = 2.3632e-03, PNorm = 45.7518, GNorm = 1.7481, lr_0 = 1.1406e-04\nLoss = 1.8967e-03, PNorm = 45.7523, GNorm = 1.3562, lr_0 = 1.1379e-04\nLoss = 1.8967e-03, PNorm = 45.7523, GNorm = 1.3562, lr_0 = 1.1379e-04\nLoss = 1.8967e-03, PNorm = 45.7523, GNorm = 1.3562, lr_0 = 1.1379e-04\nLoss = 1.8967e-03, PNorm = 45.7523, GNorm = 1.3562, lr_0 = 1.1379e-04\nLoss = 1.9227e-03, PNorm = 45.7529, GNorm = 1.3786, lr_0 = 1.1353e-04\nLoss = 1.9227e-03, PNorm = 45.7529, GNorm = 1.3786, lr_0 = 1.1353e-04\nLoss = 1.9227e-03, PNorm = 45.7529, GNorm = 1.3786, lr_0 = 1.1353e-04\nLoss = 1.9227e-03, PNorm = 45.7529, GNorm = 1.3786, lr_0 = 1.1353e-04\n\n\r 46%|████▌     | 16/35 [00:00&lt;00:00, 34.77it/s]Loss = 2.5013e-03, PNorm = 45.7534, GNorm = 1.7049, lr_0 = 1.1326e-04\nLoss = 2.5013e-03, PNorm = 45.7534, GNorm = 1.7049, lr_0 = 1.1326e-04\nLoss = 2.5013e-03, PNorm = 45.7534, GNorm = 1.7049, lr_0 = 1.1326e-04\nLoss = 2.5013e-03, PNorm = 45.7534, GNorm = 1.7049, lr_0 = 1.1326e-04\nLoss = 3.8876e-03, PNorm = 45.7540, GNorm = 2.4451, lr_0 = 1.1300e-04\nLoss = 3.8876e-03, PNorm = 45.7540, GNorm = 2.4451, lr_0 = 1.1300e-04\nLoss = 3.8876e-03, PNorm = 45.7540, GNorm = 2.4451, lr_0 = 1.1300e-04\nLoss = 3.8876e-03, PNorm = 45.7540, GNorm = 2.4451, lr_0 = 1.1300e-04\nLoss = 3.2096e-03, PNorm = 45.7545, GNorm = 1.4180, lr_0 = 1.1273e-04\nLoss = 3.2096e-03, PNorm = 45.7545, GNorm = 1.4180, lr_0 = 1.1273e-04\nLoss = 3.2096e-03, PNorm = 45.7545, GNorm = 1.4180, lr_0 = 1.1273e-04\nLoss = 3.2096e-03, PNorm = 45.7545, GNorm = 1.4180, lr_0 = 1.1273e-04\nLoss = 2.3801e-03, PNorm = 45.7551, GNorm = 1.1744, lr_0 = 1.1247e-04\nLoss = 2.3801e-03, PNorm = 45.7551, GNorm = 1.1744, lr_0 = 1.1247e-04\nLoss = 2.3801e-03, PNorm = 45.7551, GNorm = 1.1744, lr_0 = 1.1247e-04\nLoss = 2.3801e-03, PNorm = 45.7551, GNorm = 1.1744, lr_0 = 1.1247e-04\n\n\r 57%|█████▋    | 20/35 [00:00&lt;00:00, 34.73it/s]Loss = 2.2190e-03, PNorm = 45.7556, GNorm = 1.1823, lr_0 = 1.1220e-04\nLoss = 2.2190e-03, PNorm = 45.7556, GNorm = 1.1823, lr_0 = 1.1220e-04\nLoss = 2.2190e-03, PNorm = 45.7556, GNorm = 1.1823, lr_0 = 1.1220e-04\nLoss = 2.2190e-03, PNorm = 45.7556, GNorm = 1.1823, lr_0 = 1.1220e-04\nLoss = 2.9125e-03, PNorm = 45.7563, GNorm = 2.2472, lr_0 = 1.1194e-04\nLoss = 2.9125e-03, PNorm = 45.7563, GNorm = 2.2472, lr_0 = 1.1194e-04\nLoss = 2.9125e-03, PNorm = 45.7563, GNorm = 2.2472, lr_0 = 1.1194e-04\nLoss = 2.9125e-03, PNorm = 45.7563, GNorm = 2.2472, lr_0 = 1.1194e-04\nLoss = 4.1600e-03, PNorm = 45.7569, GNorm = 1.4678, lr_0 = 1.1168e-04\nLoss = 4.1600e-03, PNorm = 45.7569, GNorm = 1.4678, lr_0 = 1.1168e-04\nLoss = 4.1600e-03, PNorm = 45.7569, GNorm = 1.4678, lr_0 = 1.1168e-04\nLoss = 4.1600e-03, PNorm = 45.7569, GNorm = 1.4678, lr_0 = 1.1168e-04\nLoss = 2.8207e-03, PNorm = 45.7576, GNorm = 1.4432, lr_0 = 1.1141e-04\nLoss = 2.8207e-03, PNorm = 45.7576, GNorm = 1.4432, lr_0 = 1.1141e-04\nLoss = 2.8207e-03, PNorm = 45.7576, GNorm = 1.4432, lr_0 = 1.1141e-04\nLoss = 2.8207e-03, PNorm = 45.7576, GNorm = 1.4432, lr_0 = 1.1141e-04\n\n\r 69%|██████▊   | 24/35 [00:00&lt;00:00, 34.70it/s]Loss = 3.9554e-03, PNorm = 45.7584, GNorm = 1.4505, lr_0 = 1.1115e-04\nLoss = 3.9554e-03, PNorm = 45.7584, GNorm = 1.4505, lr_0 = 1.1115e-04\nLoss = 3.9554e-03, PNorm = 45.7584, GNorm = 1.4505, lr_0 = 1.1115e-04\nLoss = 3.9554e-03, PNorm = 45.7584, GNorm = 1.4505, lr_0 = 1.1115e-04\nLoss = 2.5156e-03, PNorm = 45.7592, GNorm = 0.8087, lr_0 = 1.1089e-04\nLoss = 2.5156e-03, PNorm = 45.7592, GNorm = 0.8087, lr_0 = 1.1089e-04\nLoss = 2.5156e-03, PNorm = 45.7592, GNorm = 0.8087, lr_0 = 1.1089e-04\nLoss = 2.5156e-03, PNorm = 45.7592, GNorm = 0.8087, lr_0 = 1.1089e-04\nLoss = 2.0502e-03, PNorm = 45.7599, GNorm = 0.8779, lr_0 = 1.1063e-04\nLoss = 2.0502e-03, PNorm = 45.7599, GNorm = 0.8779, lr_0 = 1.1063e-04\nLoss = 2.0502e-03, PNorm = 45.7599, GNorm = 0.8779, lr_0 = 1.1063e-04\nLoss = 2.0502e-03, PNorm = 45.7599, GNorm = 0.8779, lr_0 = 1.1063e-04\nLoss = 3.1033e-03, PNorm = 45.7606, GNorm = 1.1652, lr_0 = 1.1037e-04\nLoss = 3.1033e-03, PNorm = 45.7606, GNorm = 1.1652, lr_0 = 1.1037e-04\nLoss = 3.1033e-03, PNorm = 45.7606, GNorm = 1.1652, lr_0 = 1.1037e-04\nLoss = 3.1033e-03, PNorm = 45.7606, GNorm = 1.1652, lr_0 = 1.1037e-04\n\n\r 80%|████████  | 28/35 [00:00&lt;00:00, 34.88it/s]Loss = 2.6590e-03, PNorm = 45.7612, GNorm = 1.7384, lr_0 = 1.1011e-04\nLoss = 2.6590e-03, PNorm = 45.7612, GNorm = 1.7384, lr_0 = 1.1011e-04\nLoss = 2.6590e-03, PNorm = 45.7612, GNorm = 1.7384, lr_0 = 1.1011e-04\nLoss = 2.6590e-03, PNorm = 45.7612, GNorm = 1.7384, lr_0 = 1.1011e-04\nLoss = 2.4864e-03, PNorm = 45.7618, GNorm = 1.2242, lr_0 = 1.0985e-04\nLoss = 2.4864e-03, PNorm = 45.7618, GNorm = 1.2242, lr_0 = 1.0985e-04\nLoss = 2.4864e-03, PNorm = 45.7618, GNorm = 1.2242, lr_0 = 1.0985e-04\nLoss = 2.4864e-03, PNorm = 45.7618, GNorm = 1.2242, lr_0 = 1.0985e-04\nLoss = 2.0979e-03, PNorm = 45.7624, GNorm = 1.0058, lr_0 = 1.0960e-04\nLoss = 2.0979e-03, PNorm = 45.7624, GNorm = 1.0058, lr_0 = 1.0960e-04\nLoss = 2.0979e-03, PNorm = 45.7624, GNorm = 1.0058, lr_0 = 1.0960e-04\nLoss = 2.0979e-03, PNorm = 45.7624, GNorm = 1.0058, lr_0 = 1.0960e-04\nLoss = 2.6199e-03, PNorm = 45.7628, GNorm = 1.3861, lr_0 = 1.0934e-04\nLoss = 2.6199e-03, PNorm = 45.7628, GNorm = 1.3861, lr_0 = 1.0934e-04\nLoss = 2.6199e-03, PNorm = 45.7628, GNorm = 1.3861, lr_0 = 1.0934e-04\nLoss = 2.6199e-03, PNorm = 45.7628, GNorm = 1.3861, lr_0 = 1.0934e-04\n\n\r 91%|█████████▏| 32/35 [00:00&lt;00:00, 34.95it/s]Loss = 3.3435e-03, PNorm = 45.7633, GNorm = 1.6770, lr_0 = 1.0908e-04\nLoss = 3.3435e-03, PNorm = 45.7633, GNorm = 1.6770, lr_0 = 1.0908e-04\nLoss = 3.3435e-03, PNorm = 45.7633, GNorm = 1.6770, lr_0 = 1.0908e-04\nLoss = 3.3435e-03, PNorm = 45.7633, GNorm = 1.6770, lr_0 = 1.0908e-04\nLoss = 4.8873e-03, PNorm = 45.7638, GNorm = 2.0015, lr_0 = 1.0883e-04\nLoss = 4.8873e-03, PNorm = 45.7638, GNorm = 2.0015, lr_0 = 1.0883e-04\nLoss = 4.8873e-03, PNorm = 45.7638, GNorm = 2.0015, lr_0 = 1.0883e-04\nLoss = 4.8873e-03, PNorm = 45.7638, GNorm = 2.0015, lr_0 = 1.0883e-04\nLoss = 3.1443e-03, PNorm = 45.7642, GNorm = 1.5482, lr_0 = 1.0857e-04\nLoss = 3.1443e-03, PNorm = 45.7642, GNorm = 1.5482, lr_0 = 1.0857e-04\nLoss = 3.1443e-03, PNorm = 45.7642, GNorm = 1.5482, lr_0 = 1.0857e-04\nLoss = 3.1443e-03, PNorm = 45.7642, GNorm = 1.5482, lr_0 = 1.0857e-04\n\n\r100%|██████████| 35/35 [00:01&lt;00:00, 34.83it/s]\n\r  0%|          | 0/5 [00:00&lt;?, ?it/s]\n\r100%|██████████| 5/5 [00:00&lt;00:00, 70.51it/s]Validation rmse = 0.550351\nValidation rmse = 0.550351\nValidation rmse = 0.550351\nValidation rmse = 0.550351\n\r 97%|█████████▋| 29/30 [00:39&lt;00:01,  1.22s/it]Epoch 29\nEpoch 29\nEpoch 29\nEpoch 29\n\n\r  0%|          | 0/35 [00:00&lt;?, ?it/s]Loss = 2.6758e-03, PNorm = 45.7646, GNorm = 0.8315, lr_0 = 1.0832e-04\nLoss = 2.6758e-03, PNorm = 45.7646, GNorm = 0.8315, lr_0 = 1.0832e-04\nLoss = 2.6758e-03, PNorm = 45.7646, GNorm = 0.8315, lr_0 = 1.0832e-04\nLoss = 2.6758e-03, PNorm = 45.7646, GNorm = 0.8315, lr_0 = 1.0832e-04\nLoss = 2.8914e-03, PNorm = 45.7650, GNorm = 1.2734, lr_0 = 1.0806e-04\nLoss = 2.8914e-03, PNorm = 45.7650, GNorm = 1.2734, lr_0 = 1.0806e-04\nLoss = 2.8914e-03, PNorm = 45.7650, GNorm = 1.2734, lr_0 = 1.0806e-04\nLoss = 2.8914e-03, PNorm = 45.7650, GNorm = 1.2734, lr_0 = 1.0806e-04\nLoss = 1.9670e-03, PNorm = 45.7654, GNorm = 0.8591, lr_0 = 1.0781e-04\nLoss = 1.9670e-03, PNorm = 45.7654, GNorm = 0.8591, lr_0 = 1.0781e-04\nLoss = 1.9670e-03, PNorm = 45.7654, GNorm = 0.8591, lr_0 = 1.0781e-04\nLoss = 1.9670e-03, PNorm = 45.7654, GNorm = 0.8591, lr_0 = 1.0781e-04\nLoss = 2.2445e-03, PNorm = 45.7658, GNorm = 1.1245, lr_0 = 1.0756e-04\nLoss = 2.2445e-03, PNorm = 45.7658, GNorm = 1.1245, lr_0 = 1.0756e-04\nLoss = 2.2445e-03, PNorm = 45.7658, GNorm = 1.1245, lr_0 = 1.0756e-04\nLoss = 2.2445e-03, PNorm = 45.7658, GNorm = 1.1245, lr_0 = 1.0756e-04\n\n\r 11%|█▏        | 4/35 [00:00&lt;00:00, 33.71it/s]Loss = 4.0973e-03, PNorm = 45.7663, GNorm = 1.9207, lr_0 = 1.0730e-04\nLoss = 4.0973e-03, PNorm = 45.7663, GNorm = 1.9207, lr_0 = 1.0730e-04\nLoss = 4.0973e-03, PNorm = 45.7663, GNorm = 1.9207, lr_0 = 1.0730e-04\nLoss = 4.0973e-03, PNorm = 45.7663, GNorm = 1.9207, lr_0 = 1.0730e-04\nLoss = 2.2724e-03, PNorm = 45.7668, GNorm = 1.3619, lr_0 = 1.0705e-04\nLoss = 2.2724e-03, PNorm = 45.7668, GNorm = 1.3619, lr_0 = 1.0705e-04\nLoss = 2.2724e-03, PNorm = 45.7668, GNorm = 1.3619, lr_0 = 1.0705e-04\nLoss = 2.2724e-03, PNorm = 45.7668, GNorm = 1.3619, lr_0 = 1.0705e-04\nLoss = 1.6111e-03, PNorm = 45.7673, GNorm = 0.8737, lr_0 = 1.0680e-04\nLoss = 1.6111e-03, PNorm = 45.7673, GNorm = 0.8737, lr_0 = 1.0680e-04\nLoss = 1.6111e-03, PNorm = 45.7673, GNorm = 0.8737, lr_0 = 1.0680e-04\nLoss = 1.6111e-03, PNorm = 45.7673, GNorm = 0.8737, lr_0 = 1.0680e-04\nLoss = 2.5381e-03, PNorm = 45.7679, GNorm = 1.9373, lr_0 = 1.0655e-04\nLoss = 2.5381e-03, PNorm = 45.7679, GNorm = 1.9373, lr_0 = 1.0655e-04\nLoss = 2.5381e-03, PNorm = 45.7679, GNorm = 1.9373, lr_0 = 1.0655e-04\nLoss = 2.5381e-03, PNorm = 45.7679, GNorm = 1.9373, lr_0 = 1.0655e-04\n\n\r 23%|██▎       | 8/35 [00:00&lt;00:00, 33.60it/s]Loss = 3.1172e-03, PNorm = 45.7685, GNorm = 1.5062, lr_0 = 1.0630e-04\nLoss = 3.1172e-03, PNorm = 45.7685, GNorm = 1.5062, lr_0 = 1.0630e-04\nLoss = 3.1172e-03, PNorm = 45.7685, GNorm = 1.5062, lr_0 = 1.0630e-04\nLoss = 3.1172e-03, PNorm = 45.7685, GNorm = 1.5062, lr_0 = 1.0630e-04\nLoss = 2.6876e-03, PNorm = 45.7691, GNorm = 1.6037, lr_0 = 1.0605e-04\nLoss = 2.6876e-03, PNorm = 45.7691, GNorm = 1.6037, lr_0 = 1.0605e-04\nLoss = 2.6876e-03, PNorm = 45.7691, GNorm = 1.6037, lr_0 = 1.0605e-04\nLoss = 2.6876e-03, PNorm = 45.7691, GNorm = 1.6037, lr_0 = 1.0605e-04\nLoss = 3.1832e-03, PNorm = 45.7697, GNorm = 1.2241, lr_0 = 1.0580e-04\nLoss = 3.1832e-03, PNorm = 45.7697, GNorm = 1.2241, lr_0 = 1.0580e-04\nLoss = 3.1832e-03, PNorm = 45.7697, GNorm = 1.2241, lr_0 = 1.0580e-04\nLoss = 3.1832e-03, PNorm = 45.7697, GNorm = 1.2241, lr_0 = 1.0580e-04\nLoss = 2.7297e-03, PNorm = 45.7705, GNorm = 1.1348, lr_0 = 1.0555e-04\nLoss = 2.7297e-03, PNorm = 45.7705, GNorm = 1.1348, lr_0 = 1.0555e-04\nLoss = 2.7297e-03, PNorm = 45.7705, GNorm = 1.1348, lr_0 = 1.0555e-04\nLoss = 2.7297e-03, PNorm = 45.7705, GNorm = 1.1348, lr_0 = 1.0555e-04\n\n\r 34%|███▍      | 12/35 [00:00&lt;00:00, 33.74it/s]Loss = 2.6345e-03, PNorm = 45.7713, GNorm = 1.2162, lr_0 = 1.0530e-04\nLoss = 2.6345e-03, PNorm = 45.7713, GNorm = 1.2162, lr_0 = 1.0530e-04\nLoss = 2.6345e-03, PNorm = 45.7713, GNorm = 1.2162, lr_0 = 1.0530e-04\nLoss = 2.6345e-03, PNorm = 45.7713, GNorm = 1.2162, lr_0 = 1.0530e-04\nLoss = 2.5359e-03, PNorm = 45.7721, GNorm = 1.6946, lr_0 = 1.0506e-04\nLoss = 2.5359e-03, PNorm = 45.7721, GNorm = 1.6946, lr_0 = 1.0506e-04\nLoss = 2.5359e-03, PNorm = 45.7721, GNorm = 1.6946, lr_0 = 1.0506e-04\nLoss = 2.5359e-03, PNorm = 45.7721, GNorm = 1.6946, lr_0 = 1.0506e-04\nLoss = 2.7474e-03, PNorm = 45.7728, GNorm = 2.7694, lr_0 = 1.0481e-04\nLoss = 2.7474e-03, PNorm = 45.7728, GNorm = 2.7694, lr_0 = 1.0481e-04\nLoss = 2.7474e-03, PNorm = 45.7728, GNorm = 2.7694, lr_0 = 1.0481e-04\nLoss = 2.7474e-03, PNorm = 45.7728, GNorm = 2.7694, lr_0 = 1.0481e-04\nLoss = 1.9185e-03, PNorm = 45.7736, GNorm = 1.9603, lr_0 = 1.0457e-04\nLoss = 1.9185e-03, PNorm = 45.7736, GNorm = 1.9603, lr_0 = 1.0457e-04\nLoss = 1.9185e-03, PNorm = 45.7736, GNorm = 1.9603, lr_0 = 1.0457e-04\nLoss = 1.9185e-03, PNorm = 45.7736, GNorm = 1.9603, lr_0 = 1.0457e-04\n\n\r 46%|████▌     | 16/35 [00:00&lt;00:00, 33.55it/s]Loss = 3.4495e-03, PNorm = 45.7744, GNorm = 1.7178, lr_0 = 1.0432e-04\nLoss = 3.4495e-03, PNorm = 45.7744, GNorm = 1.7178, lr_0 = 1.0432e-04\nLoss = 3.4495e-03, PNorm = 45.7744, GNorm = 1.7178, lr_0 = 1.0432e-04\nLoss = 3.4495e-03, PNorm = 45.7744, GNorm = 1.7178, lr_0 = 1.0432e-04\nLoss = 2.8083e-03, PNorm = 45.7753, GNorm = 1.2217, lr_0 = 1.0408e-04\nLoss = 2.8083e-03, PNorm = 45.7753, GNorm = 1.2217, lr_0 = 1.0408e-04\nLoss = 2.8083e-03, PNorm = 45.7753, GNorm = 1.2217, lr_0 = 1.0408e-04\nLoss = 2.8083e-03, PNorm = 45.7753, GNorm = 1.2217, lr_0 = 1.0408e-04\nLoss = 3.5326e-03, PNorm = 45.7762, GNorm = 3.1734, lr_0 = 1.0383e-04\nLoss = 3.5326e-03, PNorm = 45.7762, GNorm = 3.1734, lr_0 = 1.0383e-04\nLoss = 3.5326e-03, PNorm = 45.7762, GNorm = 3.1734, lr_0 = 1.0383e-04\nLoss = 3.5326e-03, PNorm = 45.7762, GNorm = 3.1734, lr_0 = 1.0383e-04\nLoss = 3.7601e-03, PNorm = 45.7770, GNorm = 3.5596, lr_0 = 1.0359e-04\nLoss = 3.7601e-03, PNorm = 45.7770, GNorm = 3.5596, lr_0 = 1.0359e-04\nLoss = 3.7601e-03, PNorm = 45.7770, GNorm = 3.5596, lr_0 = 1.0359e-04\nLoss = 3.7601e-03, PNorm = 45.7770, GNorm = 3.5596, lr_0 = 1.0359e-04\n\n\r 57%|█████▋    | 20/35 [00:00&lt;00:00, 34.03it/s]Loss = 2.5465e-03, PNorm = 45.7777, GNorm = 1.2608, lr_0 = 1.0334e-04\nLoss = 2.5465e-03, PNorm = 45.7777, GNorm = 1.2608, lr_0 = 1.0334e-04\nLoss = 2.5465e-03, PNorm = 45.7777, GNorm = 1.2608, lr_0 = 1.0334e-04\nLoss = 2.5465e-03, PNorm = 45.7777, GNorm = 1.2608, lr_0 = 1.0334e-04\nLoss = 2.3598e-03, PNorm = 45.7784, GNorm = 1.8168, lr_0 = 1.0310e-04\nLoss = 2.3598e-03, PNorm = 45.7784, GNorm = 1.8168, lr_0 = 1.0310e-04\nLoss = 2.3598e-03, PNorm = 45.7784, GNorm = 1.8168, lr_0 = 1.0310e-04\nLoss = 2.3598e-03, PNorm = 45.7784, GNorm = 1.8168, lr_0 = 1.0310e-04\nLoss = 4.5151e-03, PNorm = 45.7788, GNorm = 4.9104, lr_0 = 1.0286e-04\nLoss = 4.5151e-03, PNorm = 45.7788, GNorm = 4.9104, lr_0 = 1.0286e-04\nLoss = 4.5151e-03, PNorm = 45.7788, GNorm = 4.9104, lr_0 = 1.0286e-04\nLoss = 4.5151e-03, PNorm = 45.7788, GNorm = 4.9104, lr_0 = 1.0286e-04\nLoss = 3.7365e-03, PNorm = 45.7792, GNorm = 2.2941, lr_0 = 1.0262e-04\nLoss = 3.7365e-03, PNorm = 45.7792, GNorm = 2.2941, lr_0 = 1.0262e-04\nLoss = 3.7365e-03, PNorm = 45.7792, GNorm = 2.2941, lr_0 = 1.0262e-04\nLoss = 3.7365e-03, PNorm = 45.7792, GNorm = 2.2941, lr_0 = 1.0262e-04\n\n\r 69%|██████▊   | 24/35 [00:00&lt;00:00, 33.99it/s]Loss = 1.7575e-03, PNorm = 45.7795, GNorm = 0.8133, lr_0 = 1.0238e-04\nLoss = 1.7575e-03, PNorm = 45.7795, GNorm = 0.8133, lr_0 = 1.0238e-04\nLoss = 1.7575e-03, PNorm = 45.7795, GNorm = 0.8133, lr_0 = 1.0238e-04\nLoss = 1.7575e-03, PNorm = 45.7795, GNorm = 0.8133, lr_0 = 1.0238e-04\nLoss = 1.6924e-03, PNorm = 45.7799, GNorm = 1.0920, lr_0 = 1.0214e-04\nLoss = 1.6924e-03, PNorm = 45.7799, GNorm = 1.0920, lr_0 = 1.0214e-04\nLoss = 1.6924e-03, PNorm = 45.7799, GNorm = 1.0920, lr_0 = 1.0214e-04\nLoss = 1.6924e-03, PNorm = 45.7799, GNorm = 1.0920, lr_0 = 1.0214e-04\nLoss = 3.2256e-03, PNorm = 45.7802, GNorm = 1.8189, lr_0 = 1.0190e-04\nLoss = 3.2256e-03, PNorm = 45.7802, GNorm = 1.8189, lr_0 = 1.0190e-04\nLoss = 3.2256e-03, PNorm = 45.7802, GNorm = 1.8189, lr_0 = 1.0190e-04\nLoss = 3.2256e-03, PNorm = 45.7802, GNorm = 1.8189, lr_0 = 1.0190e-04\nLoss = 5.2534e-03, PNorm = 45.7804, GNorm = 3.7906, lr_0 = 1.0166e-04\nLoss = 5.2534e-03, PNorm = 45.7804, GNorm = 3.7906, lr_0 = 1.0166e-04\nLoss = 5.2534e-03, PNorm = 45.7804, GNorm = 3.7906, lr_0 = 1.0166e-04\nLoss = 5.2534e-03, PNorm = 45.7804, GNorm = 3.7906, lr_0 = 1.0166e-04\n\n\r 80%|████████  | 28/35 [00:00&lt;00:00, 33.90it/s]Loss = 1.8263e-03, PNorm = 45.7806, GNorm = 1.0761, lr_0 = 1.0142e-04\nLoss = 1.8263e-03, PNorm = 45.7806, GNorm = 1.0761, lr_0 = 1.0142e-04\nLoss = 1.8263e-03, PNorm = 45.7806, GNorm = 1.0761, lr_0 = 1.0142e-04\nLoss = 1.8263e-03, PNorm = 45.7806, GNorm = 1.0761, lr_0 = 1.0142e-04\nLoss = 2.8460e-03, PNorm = 45.7809, GNorm = 1.0151, lr_0 = 1.0118e-04\nLoss = 2.8460e-03, PNorm = 45.7809, GNorm = 1.0151, lr_0 = 1.0118e-04\nLoss = 2.8460e-03, PNorm = 45.7809, GNorm = 1.0151, lr_0 = 1.0118e-04\nLoss = 2.8460e-03, PNorm = 45.7809, GNorm = 1.0151, lr_0 = 1.0118e-04\nLoss = 3.0423e-03, PNorm = 45.7813, GNorm = 1.5117, lr_0 = 1.0094e-04\nLoss = 3.0423e-03, PNorm = 45.7813, GNorm = 1.5117, lr_0 = 1.0094e-04\nLoss = 3.0423e-03, PNorm = 45.7813, GNorm = 1.5117, lr_0 = 1.0094e-04\nLoss = 3.0423e-03, PNorm = 45.7813, GNorm = 1.5117, lr_0 = 1.0094e-04\nLoss = 2.0912e-03, PNorm = 45.7817, GNorm = 1.6674, lr_0 = 1.0071e-04\nLoss = 2.0912e-03, PNorm = 45.7817, GNorm = 1.6674, lr_0 = 1.0071e-04\nLoss = 2.0912e-03, PNorm = 45.7817, GNorm = 1.6674, lr_0 = 1.0071e-04\nLoss = 2.0912e-03, PNorm = 45.7817, GNorm = 1.6674, lr_0 = 1.0071e-04\n\n\r 91%|█████████▏| 32/35 [00:00&lt;00:00, 33.91it/s]Loss = 3.9323e-03, PNorm = 45.7821, GNorm = 1.1506, lr_0 = 1.0047e-04\nLoss = 3.9323e-03, PNorm = 45.7821, GNorm = 1.1506, lr_0 = 1.0047e-04\nLoss = 3.9323e-03, PNorm = 45.7821, GNorm = 1.1506, lr_0 = 1.0047e-04\nLoss = 3.9323e-03, PNorm = 45.7821, GNorm = 1.1506, lr_0 = 1.0047e-04\nLoss = 3.6449e-03, PNorm = 45.7825, GNorm = 2.1886, lr_0 = 1.0024e-04\nLoss = 3.6449e-03, PNorm = 45.7825, GNorm = 2.1886, lr_0 = 1.0024e-04\nLoss = 3.6449e-03, PNorm = 45.7825, GNorm = 2.1886, lr_0 = 1.0024e-04\nLoss = 3.6449e-03, PNorm = 45.7825, GNorm = 2.1886, lr_0 = 1.0024e-04\nLoss = 1.7904e-03, PNorm = 45.7830, GNorm = 1.7825, lr_0 = 1.0000e-04\nLoss = 1.7904e-03, PNorm = 45.7830, GNorm = 1.7825, lr_0 = 1.0000e-04\nLoss = 1.7904e-03, PNorm = 45.7830, GNorm = 1.7825, lr_0 = 1.0000e-04\nLoss = 1.7904e-03, PNorm = 45.7830, GNorm = 1.7825, lr_0 = 1.0000e-04\n\n\r100%|██████████| 35/35 [00:01&lt;00:00, 33.91it/s]\n\r  0%|          | 0/5 [00:00&lt;?, ?it/s]\n\r100%|██████████| 5/5 [00:00&lt;00:00, 70.30it/s]Validation rmse = 0.543767\nValidation rmse = 0.543767\nValidation rmse = 0.543767\nValidation rmse = 0.543767\n\r100%|██████████| 30/30 [00:40&lt;00:00,  1.18s/it]\nModel 4 best validation rmse = 0.541992 on epoch 26\nModel 4 best validation rmse = 0.541992 on epoch 26\nModel 4 best validation rmse = 0.541992 on epoch 26\nModel 4 best validation rmse = 0.541992 on epoch 26\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\r  0%|          | 0/5 [00:00&lt;?, ?it/s]\r100%|██████████| 5/5 [00:00&lt;00:00, 66.99it/s]\nModel 4 test rmse = 0.530079\nModel 4 test rmse = 0.530079\nModel 4 test rmse = 0.530079\nModel 4 test rmse = 0.530079\nEnsemble test rmse = 0.513054\nEnsemble test rmse = 0.513054\nEnsemble test rmse = 0.513054\nEnsemble test rmse = 0.513054\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\nSeed 0 ==&gt; test rmse = 0.513054\nSeed 0 ==&gt; test rmse = 0.513054\nSeed 0 ==&gt; test rmse = 0.513054\nSeed 0 ==&gt; test rmse = 0.513054\nOverall test rmse = 0.513054 +/- 0.000000\nOverall test rmse = 0.513054 +/- 0.000000\nOverall test rmse = 0.513054 +/- 0.000000\nOverall test rmse = 0.513054 +/- 0.000000\nOut[25]: (0.5130543437648452, 0.0)\n</div>"]}}],"execution_count":86},{"cell_type":"code","source":["os.listdir(os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-2188-sparse-regression-scaffold','fold_0','model_1'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[20]: \n[&#39;events.out.tfevents.1566149717.0308-155047-pad750-10-139-64-4&#39;,\n &#39;events.out.tfevents.1566150883.0308-155047-pad750-10-139-64-4&#39;,\n &#39;model.pt&#39;]\n</div>"]}}],"execution_count":87},{"cell_type":"markdown","source":["### Classification 4x training"],"metadata":{}},{"cell_type":"code","source":["%sh cat /dbfs/FileStore/chemprop/JAK/configs/binary-4x.json"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{\n    &#34;depth&#34;: 6,\n    &#34;dropout&#34;: 0.30000000000000004,\n    &#34;ffn_num_layers&#34;: 1,\n    &#34;hidden_size&#34;: 2200\n}</div>"]}}],"execution_count":89},{"cell_type":"code","source":["parser = ArgumentParser()\nadd_train_args(parser)\nif not os.path.exists(ZINC_Models):\n  os.mkdir(os.path.join(CHEMPROP_DIR,'JAK','checkpoints','binary-4x','model'))\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-1460_binary.csv'),\n                         '--dataset_type','classification',\n                         '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','checkpoints','binary-4x','model'),\n                         '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_binary.csv'),\n                         '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_binary.csv'),\n                         '--log_frequency','1',\n                         '--depth','6',\n                         '--dropout','0.3',\n                         '--hidden_size','2200',\n                         '--ffn_num_layers','1'\n                         ])\nmodify_train_args(args)\nlogger = create_logger(name='train', save_dir=args.save_dir, quiet=args.quiet)\n\ncross_validate(args, logger)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/binary-4x/model/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/binary-4x/model/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\n\r  0%|          | 0/1460 [00:00&lt;?, ?it/s]\r 23%|██▎       | 343/1460 [00:00&lt;00:00, 3424.96it/s]\r 47%|████▋     | 686/1460 [00:00&lt;00:00, 3423.48it/s]\r 70%|██████▉   | 1021/1460 [00:00&lt;00:00, 3399.40it/s]\r 93%|█████████▎| 1356/1460 [00:00&lt;00:00, 3381.48it/s]\r100%|██████████| 1460/1460 [00:00&lt;00:00, 3378.01it/s]\nNumber of tasks = 4\nNumber of tasks = 4\nSplitting data with seed 0\nSplitting data with seed 0\n\r  0%|          | 0/183 [00:00&lt;?, ?it/s]\r100%|██████████| 183/183 [00:00&lt;00:00, 3362.44it/s]\n\r  0%|          | 0/182 [00:00&lt;?, ?it/s]\r100%|██████████| 182/182 [00:00&lt;00:00, 3343.74it/s]\nClass sizes\nClass sizes\nJAK1 EC50 nM 1027 0: 68.56%, 1: 31.44%\nJAK1 EC50 nM 1027 0: 68.56%, 1: 31.44%\nJAK2 EC50 nM 1024 0: 72.40%, 1: 27.60%\nJAK2 EC50 nM 1024 0: 72.40%, 1: 27.60%\nJAK3 EC50 nM 1026 0: 93.15%, 1: 6.85%\nJAK3 EC50 nM 1026 0: 93.15%, 1: 6.85%\nTYK2 EC50 nM 1025 0: 98.08%, 1: 1.92%\nTYK2 EC50 nM 1025 0: 98.08%, 1: 1.92%\nTotal size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nTotal size = 1,460 | train size = 1,460 | val size = 182 | test size = 183\nBuilding model 0\nBuilding model 0\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.3)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=2200, bias=False)\n      (W_h): Linear(in_features=2200, out_features=2200, bias=False)\n      (W_o): Linear(in_features=2333, out_features=2200, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.3)\n    (1): Linear(in_features=2200, out_features=4, bias=True)\n  )\n)\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.3)\n      (act_func): ReLU()\n      (W_i): Linear(in_features=147, out_features=2200, bias=False)\n      (W_h): Linear(in_features=2200, out_features=2200, bias=False)\n      (W_o): Linear(in_features=2333, out_features=2200, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.3)\n    (1): Linear(in_features=2200, out_features=4, bias=True)\n  )\n)\nNumber of parameters = 10,307,004\nNumber of parameters = 10,307,004\nMoving model to cuda\nMoving model to cuda\n\r  0%|          | 0/30 [00:00&lt;?, ?it/s]Epoch 0\nEpoch 0\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 1.2578e-02, PNorm = 68.8820, GNorm = 3.9179, lr_0 = 1.1552e-04\nLoss = 1.2578e-02, PNorm = 68.8820, GNorm = 3.9179, lr_0 = 1.1552e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:03,  7.70it/s]Loss = 7.1709e-03, PNorm = 68.8839, GNorm = 0.9950, lr_0 = 1.3103e-04\nLoss = 7.1709e-03, PNorm = 68.8839, GNorm = 0.9950, lr_0 = 1.3103e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:03,  7.82it/s]Loss = 1.4919e-02, PNorm = 68.8847, GNorm = 4.4973, lr_0 = 1.4655e-04\nLoss = 1.4919e-02, PNorm = 68.8847, GNorm = 4.4973, lr_0 = 1.4655e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:03,  7.91it/s]Loss = 9.8656e-03, PNorm = 68.8856, GNorm = 1.8271, lr_0 = 1.6207e-04\nLoss = 9.8656e-03, PNorm = 68.8856, GNorm = 1.8271, lr_0 = 1.6207e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  7.94it/s]Loss = 6.3740e-03, PNorm = 68.8868, GNorm = 2.2645, lr_0 = 1.7759e-04\nLoss = 6.3740e-03, PNorm = 68.8868, GNorm = 2.2645, lr_0 = 1.7759e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:02,  8.04it/s]Loss = 9.5180e-03, PNorm = 68.8882, GNorm = 0.9639, lr_0 = 1.9310e-04\nLoss = 9.5180e-03, PNorm = 68.8882, GNorm = 0.9639, lr_0 = 1.9310e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:02,  8.01it/s]Loss = 8.0898e-03, PNorm = 68.8899, GNorm = 0.8976, lr_0 = 2.0862e-04\nLoss = 8.0898e-03, PNorm = 68.8899, GNorm = 0.8976, lr_0 = 2.0862e-04\n\n\r 24%|██▍       | 7/29 [00:00&lt;00:02,  8.04it/s]Loss = 7.1580e-03, PNorm = 68.8920, GNorm = 1.0232, lr_0 = 2.2414e-04\nLoss = 7.1580e-03, PNorm = 68.8920, GNorm = 1.0232, lr_0 = 2.2414e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:02,  8.05it/s]Loss = 9.2581e-03, PNorm = 68.8943, GNorm = 0.7622, lr_0 = 2.3966e-04\nLoss = 9.2581e-03, PNorm = 68.8943, GNorm = 0.7622, lr_0 = 2.3966e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:02,  8.05it/s]Loss = 8.6280e-03, PNorm = 68.8969, GNorm = 0.4083, lr_0 = 2.5517e-04\nLoss = 8.6280e-03, PNorm = 68.8969, GNorm = 0.4083, lr_0 = 2.5517e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  8.05it/s]Loss = 7.0329e-03, PNorm = 68.8999, GNorm = 0.5534, lr_0 = 2.7069e-04\nLoss = 7.0329e-03, PNorm = 68.8999, GNorm = 0.5534, lr_0 = 2.7069e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  8.08it/s]Loss = 7.5958e-03, PNorm = 68.9031, GNorm = 0.4606, lr_0 = 2.8621e-04\nLoss = 7.5958e-03, PNorm = 68.9031, GNorm = 0.4606, lr_0 = 2.8621e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  8.08it/s]Loss = 7.4651e-03, PNorm = 68.9064, GNorm = 0.2686, lr_0 = 3.0172e-04\nLoss = 7.4651e-03, PNorm = 68.9064, GNorm = 0.2686, lr_0 = 3.0172e-04\n\n\r 45%|████▍     | 13/29 [00:01&lt;00:01,  8.14it/s]Loss = 7.3350e-03, PNorm = 68.9100, GNorm = 0.4253, lr_0 = 3.1724e-04\nLoss = 7.3350e-03, PNorm = 68.9100, GNorm = 0.4253, lr_0 = 3.1724e-04\n\n\r 48%|████▊     | 14/29 [00:01&lt;00:01,  8.11it/s]Loss = 8.1101e-03, PNorm = 68.9136, GNorm = 0.4848, lr_0 = 3.3276e-04\nLoss = 8.1101e-03, PNorm = 68.9136, GNorm = 0.4848, lr_0 = 3.3276e-04\n\n\r 52%|█████▏    | 15/29 [00:01&lt;00:01,  8.12it/s]Loss = 6.6114e-03, PNorm = 68.9174, GNorm = 0.2816, lr_0 = 3.4828e-04\nLoss = 6.6114e-03, PNorm = 68.9174, GNorm = 0.2816, lr_0 = 3.4828e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:01,  8.15it/s]Loss = 8.0232e-03, PNorm = 68.9212, GNorm = 0.3136, lr_0 = 3.6379e-04\nLoss = 8.0232e-03, PNorm = 68.9212, GNorm = 0.3136, lr_0 = 3.6379e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  8.16it/s]Loss = 7.0274e-03, PNorm = 68.9250, GNorm = 0.5377, lr_0 = 3.7931e-04\nLoss = 7.0274e-03, PNorm = 68.9250, GNorm = 0.5377, lr_0 = 3.7931e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  8.13it/s]Loss = 6.5856e-03, PNorm = 68.9289, GNorm = 0.7086, lr_0 = 3.9483e-04\nLoss = 6.5856e-03, PNorm = 68.9289, GNorm = 0.7086, lr_0 = 3.9483e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  8.15it/s]Loss = 7.7838e-03, PNorm = 68.9327, GNorm = 0.4348, lr_0 = 4.1034e-04\nLoss = 7.7838e-03, PNorm = 68.9327, GNorm = 0.4348, lr_0 = 4.1034e-04\n\n\r 69%|██████▉   | 20/29 [00:02&lt;00:01,  8.19it/s]Loss = 7.2869e-03, PNorm = 68.9366, GNorm = 0.3163, lr_0 = 4.2586e-04\nLoss = 7.2869e-03, PNorm = 68.9366, GNorm = 0.3163, lr_0 = 4.2586e-04\n\n\r 72%|███████▏  | 21/29 [00:02&lt;00:00,  8.15it/s]Loss = 1.1609e-02, PNorm = 68.9399, GNorm = 1.2729, lr_0 = 4.4138e-04\nLoss = 1.1609e-02, PNorm = 68.9399, GNorm = 1.2729, lr_0 = 4.4138e-04\n\n\r 76%|███████▌  | 22/29 [00:02&lt;00:00,  8.12it/s]Loss = 8.2673e-03, PNorm = 68.9436, GNorm = 0.4946, lr_0 = 4.5690e-04\nLoss = 8.2673e-03, PNorm = 68.9436, GNorm = 0.4946, lr_0 = 4.5690e-04\n\n\r 79%|███████▉  | 23/29 [00:02&lt;00:00,  8.16it/s]Loss = 8.1396e-03, PNorm = 68.9476, GNorm = 0.2485, lr_0 = 4.7241e-04\nLoss = 8.1396e-03, PNorm = 68.9476, GNorm = 0.2485, lr_0 = 4.7241e-04\n\n\r 83%|████████▎ | 24/29 [00:02&lt;00:00,  8.15it/s]Loss = 7.1011e-03, PNorm = 68.9521, GNorm = 0.4522, lr_0 = 4.8793e-04\nLoss = 7.1011e-03, PNorm = 68.9521, GNorm = 0.4522, lr_0 = 4.8793e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  8.12it/s]Loss = 8.7826e-03, PNorm = 68.9567, GNorm = 0.3355, lr_0 = 5.0345e-04\nLoss = 8.7826e-03, PNorm = 68.9567, GNorm = 0.3355, lr_0 = 5.0345e-04\n\n\r 90%|████████▉ | 26/29 [00:03&lt;00:00,  8.14it/s]Loss = 6.8645e-03, PNorm = 68.9616, GNorm = 0.5326, lr_0 = 5.1897e-04\nLoss = 6.8645e-03, PNorm = 68.9616, GNorm = 0.5326, lr_0 = 5.1897e-04\n\n\r 93%|█████████▎| 27/29 [00:03&lt;00:00,  8.13it/s]Loss = 8.6591e-03, PNorm = 68.9668, GNorm = 0.2840, lr_0 = 5.3448e-04\nLoss = 8.6591e-03, PNorm = 68.9668, GNorm = 0.2840, lr_0 = 5.3448e-04\n\n\r 97%|█████████▋| 28/29 [00:03&lt;00:00,  8.14it/s]Loss = 9.1147e-03, PNorm = 68.9721, GNorm = 0.2126, lr_0 = 5.5000e-04\nLoss = 9.1147e-03, PNorm = 68.9721, GNorm = 0.2126, lr_0 = 5.5000e-04\n\n\r100%|██████████| 29/29 [00:03&lt;00:00,  8.10it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 29.01it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 31.47it/s]Validation auc = 0.645808\nValidation auc = 0.645808\n\r  3%|▎         | 1/30 [00:10&lt;05:03, 10.46s/it]Epoch 1\nEpoch 1\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 7.3431e-03, PNorm = 68.9778, GNorm = 0.3058, lr_0 = 5.6552e-04\nLoss = 7.3431e-03, PNorm = 68.9778, GNorm = 0.3058, lr_0 = 5.6552e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:03,  7.73it/s]Loss = 7.7205e-03, PNorm = 68.9834, GNorm = 0.3573, lr_0 = 5.8103e-04\nLoss = 7.7205e-03, PNorm = 68.9834, GNorm = 0.3573, lr_0 = 5.8103e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:03,  7.87it/s]Loss = 7.4529e-03, PNorm = 68.9892, GNorm = 0.1813, lr_0 = 5.9655e-04\nLoss = 7.4529e-03, PNorm = 68.9892, GNorm = 0.1813, lr_0 = 5.9655e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:03,  7.88it/s]Loss = 6.7668e-03, PNorm = 68.9951, GNorm = 0.1977, lr_0 = 6.1207e-04\nLoss = 6.7668e-03, PNorm = 68.9951, GNorm = 0.1977, lr_0 = 6.1207e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  8.00it/s]Loss = 6.0964e-03, PNorm = 69.0011, GNorm = 0.1885, lr_0 = 6.2759e-04\nLoss = 6.0964e-03, PNorm = 69.0011, GNorm = 0.1885, lr_0 = 6.2759e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:02,  8.07it/s]Loss = 6.1470e-03, PNorm = 69.0069, GNorm = 0.2764, lr_0 = 6.4310e-04\nLoss = 6.1470e-03, PNorm = 69.0069, GNorm = 0.2764, lr_0 = 6.4310e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:02,  8.06it/s]Loss = 7.8855e-03, PNorm = 69.0123, GNorm = 0.7619, lr_0 = 6.5862e-04\nLoss = 7.8855e-03, PNorm = 69.0123, GNorm = 0.7619, lr_0 = 6.5862e-04\n\n\r 24%|██▍       | 7/29 [00:00&lt;00:02,  8.04it/s]Loss = 8.5242e-03, PNorm = 69.0172, GNorm = 0.4971, lr_0 = 6.7414e-04\nLoss = 8.5242e-03, PNorm = 69.0172, GNorm = 0.4971, lr_0 = 6.7414e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:02,  8.00it/s]Loss = 7.7019e-03, PNorm = 69.0217, GNorm = 0.2841, lr_0 = 6.8966e-04\nLoss = 7.7019e-03, PNorm = 69.0217, GNorm = 0.2841, lr_0 = 6.8966e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:02,  7.99it/s]Loss = 6.9611e-03, PNorm = 69.0269, GNorm = 0.2184, lr_0 = 7.0517e-04\nLoss = 6.9611e-03, PNorm = 69.0269, GNorm = 0.2184, lr_0 = 7.0517e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  7.65it/s]Loss = 7.2729e-03, PNorm = 69.0316, GNorm = 0.4394, lr_0 = 7.2069e-04\nLoss = 7.2729e-03, PNorm = 69.0316, GNorm = 0.4394, lr_0 = 7.2069e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  7.86it/s]Loss = 8.7070e-03, PNorm = 69.0369, GNorm = 0.2063, lr_0 = 7.3621e-04\nLoss = 8.7070e-03, PNorm = 69.0369, GNorm = 0.2063, lr_0 = 7.3621e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  7.92it/s]Loss = 9.1640e-03, PNorm = 69.0427, GNorm = 0.4596, lr_0 = 7.5172e-04\nLoss = 9.1640e-03, PNorm = 69.0427, GNorm = 0.4596, lr_0 = 7.5172e-04\n\n\r 45%|████▍     | 13/29 [00:01&lt;00:02,  7.99it/s]Loss = 6.8921e-03, PNorm = 69.0493, GNorm = 0.2693, lr_0 = 7.6724e-04\nLoss = 6.8921e-03, PNorm = 69.0493, GNorm = 0.2693, lr_0 = 7.6724e-04\n\n\r 48%|████▊     | 14/29 [00:01&lt;00:01,  8.01it/s]Loss = 6.4878e-03, PNorm = 69.0569, GNorm = 0.3002, lr_0 = 7.8276e-04\nLoss = 6.4878e-03, PNorm = 69.0569, GNorm = 0.3002, lr_0 = 7.8276e-04\n\n\r 52%|█████▏    | 15/29 [00:01&lt;00:01,  8.09it/s]Loss = 7.2557e-03, PNorm = 69.0635, GNorm = 0.5079, lr_0 = 7.9828e-04\nLoss = 7.2557e-03, PNorm = 69.0635, GNorm = 0.5079, lr_0 = 7.9828e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:01,  8.13it/s]Loss = 6.3905e-03, PNorm = 69.0711, GNorm = 0.2417, lr_0 = 8.1379e-04\nLoss = 6.3905e-03, PNorm = 69.0711, GNorm = 0.2417, lr_0 = 8.1379e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  8.13it/s]Loss = 6.8730e-03, PNorm = 69.0798, GNorm = 0.4393, lr_0 = 8.2931e-04\nLoss = 6.8730e-03, PNorm = 69.0798, GNorm = 0.4393, lr_0 = 8.2931e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  8.12it/s]Loss = 9.6642e-03, PNorm = 69.0882, GNorm = 0.6264, lr_0 = 8.4483e-04\nLoss = 9.6642e-03, PNorm = 69.0882, GNorm = 0.6264, lr_0 = 8.4483e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  8.05it/s]Loss = 7.1262e-03, PNorm = 69.0970, GNorm = 0.2593, lr_0 = 8.6034e-04\nLoss = 7.1262e-03, PNorm = 69.0970, GNorm = 0.2593, lr_0 = 8.6034e-04\n\n\r 69%|██████▉   | 20/29 [00:02&lt;00:01,  8.11it/s]Loss = 9.7178e-03, PNorm = 69.1019, GNorm = 1.7945, lr_0 = 8.7586e-04\nLoss = 9.7178e-03, PNorm = 69.1019, GNorm = 1.7945, lr_0 = 8.7586e-04\n\n\r 72%|███████▏  | 21/29 [00:02&lt;00:00,  8.13it/s]Loss = 7.3395e-03, PNorm = 69.1085, GNorm = 0.4000, lr_0 = 8.9138e-04\nLoss = 7.3395e-03, PNorm = 69.1085, GNorm = 0.4000, lr_0 = 8.9138e-04\n\n\r 76%|███████▌  | 22/29 [00:02&lt;00:00,  8.13it/s]Loss = 7.6959e-03, PNorm = 69.1159, GNorm = 0.4495, lr_0 = 9.0690e-04\nLoss = 7.6959e-03, PNorm = 69.1159, GNorm = 0.4495, lr_0 = 9.0690e-04\n\n\r 79%|███████▉  | 23/29 [00:02&lt;00:00,  8.09it/s]Loss = 7.1258e-03, PNorm = 69.1238, GNorm = 0.3040, lr_0 = 9.2241e-04\nLoss = 7.1258e-03, PNorm = 69.1238, GNorm = 0.3040, lr_0 = 9.2241e-04\n\n\r 83%|████████▎ | 24/29 [00:02&lt;00:00,  8.04it/s]Loss = 7.5924e-03, PNorm = 69.1319, GNorm = 0.3006, lr_0 = 9.3793e-04\nLoss = 7.5924e-03, PNorm = 69.1319, GNorm = 0.3006, lr_0 = 9.3793e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  8.10it/s]Loss = 7.8549e-03, PNorm = 69.1406, GNorm = 0.2841, lr_0 = 9.5345e-04\nLoss = 7.8549e-03, PNorm = 69.1406, GNorm = 0.2841, lr_0 = 9.5345e-04\n\n\r 90%|████████▉ | 26/29 [00:03&lt;00:00,  8.05it/s]Loss = 1.0933e-02, PNorm = 69.1490, GNorm = 0.4120, lr_0 = 9.6897e-04\nLoss = 1.0933e-02, PNorm = 69.1490, GNorm = 0.4120, lr_0 = 9.6897e-04\n\n\r 93%|█████████▎| 27/29 [00:03&lt;00:00,  8.05it/s]Loss = 7.0382e-03, PNorm = 69.1580, GNorm = 0.4498, lr_0 = 9.8448e-04\nLoss = 7.0382e-03, PNorm = 69.1580, GNorm = 0.4498, lr_0 = 9.8448e-04\n\n\r 97%|█████████▋| 28/29 [00:03&lt;00:00,  8.02it/s]Loss = 7.1962e-03, PNorm = 69.1675, GNorm = 0.3348, lr_0 = 1.0000e-03\nLoss = 7.1962e-03, PNorm = 69.1675, GNorm = 0.3348, lr_0 = 1.0000e-03\n\n\r100%|██████████| 29/29 [00:03&lt;00:00,  8.08it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 29.21it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 31.79it/s]Validation auc = 0.740956\nValidation auc = 0.740956\n\r  7%|▋         | 2/30 [00:20&lt;04:52, 10.46s/it]Epoch 2\nEpoch 2\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 6.5974e-03, PNorm = 69.1780, GNorm = 0.4266, lr_0 = 9.9717e-04\nLoss = 6.5974e-03, PNorm = 69.1780, GNorm = 0.4266, lr_0 = 9.9717e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:03,  8.07it/s]Loss = 7.2358e-03, PNorm = 69.1890, GNorm = 0.3022, lr_0 = 9.9434e-04\nLoss = 7.2358e-03, PNorm = 69.1890, GNorm = 0.3022, lr_0 = 9.9434e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:03,  8.04it/s]Loss = 6.2552e-03, PNorm = 69.2005, GNorm = 0.3222, lr_0 = 9.9153e-04\nLoss = 6.2552e-03, PNorm = 69.2005, GNorm = 0.3222, lr_0 = 9.9153e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:03,  8.08it/s]Loss = 7.4910e-03, PNorm = 69.2122, GNorm = 0.2931, lr_0 = 9.8872e-04\nLoss = 7.4910e-03, PNorm = 69.2122, GNorm = 0.2931, lr_0 = 9.8872e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  7.92it/s]Loss = 9.1618e-03, PNorm = 69.2238, GNorm = 0.7177, lr_0 = 9.8592e-04\nLoss = 9.1618e-03, PNorm = 69.2238, GNorm = 0.7177, lr_0 = 9.8592e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:02,  8.00it/s]Loss = 7.4078e-03, PNorm = 69.2354, GNorm = 0.1921, lr_0 = 9.8313e-04\nLoss = 7.4078e-03, PNorm = 69.2354, GNorm = 0.1921, lr_0 = 9.8313e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:02,  8.01it/s]Loss = 7.4677e-03, PNorm = 69.2470, GNorm = 0.1737, lr_0 = 9.8035e-04\nLoss = 7.4677e-03, PNorm = 69.2470, GNorm = 0.1737, lr_0 = 9.8035e-04\n\n\r 24%|██▍       | 7/29 [00:00&lt;00:02,  8.01it/s]Loss = 6.1137e-03, PNorm = 69.2575, GNorm = 0.4180, lr_0 = 9.7757e-04\nLoss = 6.1137e-03, PNorm = 69.2575, GNorm = 0.4180, lr_0 = 9.7757e-04\n\n\r 28%|██▊       | 8/29 [00:01&lt;00:02,  7.94it/s]Loss = 8.5981e-03, PNorm = 69.2692, GNorm = 0.4302, lr_0 = 9.7480e-04\nLoss = 8.5981e-03, PNorm = 69.2692, GNorm = 0.4302, lr_0 = 9.7480e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:02,  8.00it/s]Loss = 7.2220e-03, PNorm = 69.2799, GNorm = 0.2972, lr_0 = 9.7204e-04\nLoss = 7.2220e-03, PNorm = 69.2799, GNorm = 0.2972, lr_0 = 9.7204e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  8.06it/s]Loss = 8.2263e-03, PNorm = 69.2884, GNorm = 0.5230, lr_0 = 9.6929e-04\nLoss = 8.2263e-03, PNorm = 69.2884, GNorm = 0.5230, lr_0 = 9.6929e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  8.09it/s]Loss = 6.7305e-03, PNorm = 69.2956, GNorm = 0.7308, lr_0 = 9.6654e-04\nLoss = 6.7305e-03, PNorm = 69.2956, GNorm = 0.7308, lr_0 = 9.6654e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  8.11it/s]Loss = 7.5252e-03, PNorm = 69.3041, GNorm = 0.3161, lr_0 = 9.6381e-04\nLoss = 7.5252e-03, PNorm = 69.3041, GNorm = 0.3161, lr_0 = 9.6381e-04\n\n\r 45%|████▍     | 13/29 [00:01&lt;00:01,  8.12it/s]Loss = 7.6816e-03, PNorm = 69.3135, GNorm = 0.4283, lr_0 = 9.6108e-04\nLoss = 7.6816e-03, PNorm = 69.3135, GNorm = 0.4283, lr_0 = 9.6108e-04\n\n\r 48%|████▊     | 14/29 [00:01&lt;00:01,  8.12it/s]Loss = 5.4656e-03, PNorm = 69.3236, GNorm = 0.2126, lr_0 = 9.5836e-04\nLoss = 5.4656e-03, PNorm = 69.3236, GNorm = 0.2126, lr_0 = 9.5836e-04\n\n\r 52%|█████▏    | 15/29 [00:01&lt;00:01,  8.11it/s]Loss = 7.8340e-03, PNorm = 69.3341, GNorm = 0.5681, lr_0 = 9.5564e-04\nLoss = 7.8340e-03, PNorm = 69.3341, GNorm = 0.5681, lr_0 = 9.5564e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:01,  8.13it/s]Loss = 6.6379e-03, PNorm = 69.3446, GNorm = 0.2549, lr_0 = 9.5294e-04\nLoss = 6.6379e-03, PNorm = 69.3446, GNorm = 0.2549, lr_0 = 9.5294e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  8.18it/s]Loss = 8.0188e-03, PNorm = 69.3561, GNorm = 0.2920, lr_0 = 9.5024e-04\nLoss = 8.0188e-03, PNorm = 69.3561, GNorm = 0.2920, lr_0 = 9.5024e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  8.23it/s]Loss = 6.6337e-03, PNorm = 69.3679, GNorm = 0.2314, lr_0 = 9.4755e-04\nLoss = 6.6337e-03, PNorm = 69.3679, GNorm = 0.2314, lr_0 = 9.4755e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  8.23it/s]Loss = 7.3293e-03, PNorm = 69.3802, GNorm = 0.2198, lr_0 = 9.4486e-04\nLoss = 7.3293e-03, PNorm = 69.3802, GNorm = 0.2198, lr_0 = 9.4486e-04\n\n\r 69%|██████▉   | 20/29 [00:02&lt;00:01,  5.36it/s]Loss = 6.9340e-03, PNorm = 69.3910, GNorm = 0.5348, lr_0 = 9.4219e-04\nLoss = 6.9340e-03, PNorm = 69.3910, GNorm = 0.5348, lr_0 = 9.4219e-04\n\n\r 72%|███████▏  | 21/29 [00:02&lt;00:01,  5.96it/s]Loss = 6.2507e-03, PNorm = 69.4030, GNorm = 0.2455, lr_0 = 9.3952e-04\nLoss = 6.2507e-03, PNorm = 69.4030, GNorm = 0.2455, lr_0 = 9.3952e-04\n\n\r 76%|███████▌  | 22/29 [00:02&lt;00:01,  6.47it/s]Loss = 8.5517e-03, PNorm = 69.4148, GNorm = 0.1684, lr_0 = 9.3686e-04\nLoss = 8.5517e-03, PNorm = 69.4148, GNorm = 0.1684, lr_0 = 9.3686e-04\n\n\r 79%|███████▉  | 23/29 [00:03&lt;00:00,  6.90it/s]Loss = 8.0985e-03, PNorm = 69.4276, GNorm = 0.4097, lr_0 = 9.3421e-04\nLoss = 8.0985e-03, PNorm = 69.4276, GNorm = 0.4097, lr_0 = 9.3421e-04\n\n\r 83%|████████▎ | 24/29 [00:03&lt;00:00,  7.22it/s]Loss = 8.9890e-03, PNorm = 69.4392, GNorm = 0.3127, lr_0 = 9.3156e-04\nLoss = 8.9890e-03, PNorm = 69.4392, GNorm = 0.3127, lr_0 = 9.3156e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  7.51it/s]Loss = 6.3400e-03, PNorm = 69.4503, GNorm = 0.3050, lr_0 = 9.2892e-04\nLoss = 6.3400e-03, PNorm = 69.4503, GNorm = 0.3050, lr_0 = 9.2892e-04\n\n\r 90%|████████▉ | 26/29 [00:03&lt;00:00,  7.69it/s]Loss = 8.0863e-03, PNorm = 69.4633, GNorm = 0.4195, lr_0 = 9.2629e-04\nLoss = 8.0863e-03, PNorm = 69.4633, GNorm = 0.4195, lr_0 = 9.2629e-04\n\n\r 93%|█████████▎| 27/29 [00:03&lt;00:00,  7.78it/s]Loss = 7.7148e-03, PNorm = 69.4763, GNorm = 0.3506, lr_0 = 9.2367e-04\nLoss = 7.7148e-03, PNorm = 69.4763, GNorm = 0.3506, lr_0 = 9.2367e-04\n\n\r 97%|█████████▋| 28/29 [00:03&lt;00:00,  7.86it/s]Loss = 6.8323e-03, PNorm = 69.4885, GNorm = 0.3750, lr_0 = 9.2106e-04\nLoss = 6.8323e-03, PNorm = 69.4885, GNorm = 0.3750, lr_0 = 9.2106e-04\n\n\r100%|██████████| 29/29 [00:03&lt;00:00,  7.93it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 28.56it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 31.09it/s]Validation auc = 0.804663\nValidation auc = 0.804663\n\r 10%|█         | 3/30 [00:32&lt;04:47, 10.66s/it]Epoch 3\nEpoch 3\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 7.1836e-03, PNorm = 69.4996, GNorm = 0.5799, lr_0 = 9.1845e-04\nLoss = 7.1836e-03, PNorm = 69.4996, GNorm = 0.5799, lr_0 = 9.1845e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:03,  8.25it/s]Loss = 8.1725e-03, PNorm = 69.5119, GNorm = 0.8473, lr_0 = 9.1585e-04\nLoss = 8.1725e-03, PNorm = 69.5119, GNorm = 0.8473, lr_0 = 9.1585e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:03,  8.13it/s]Loss = 4.9221e-03, PNorm = 69.5253, GNorm = 0.3654, lr_0 = 9.1325e-04\nLoss = 4.9221e-03, PNorm = 69.5253, GNorm = 0.3654, lr_0 = 9.1325e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:03,  8.15it/s]Loss = 5.9731e-03, PNorm = 69.5397, GNorm = 0.2312, lr_0 = 9.1067e-04\nLoss = 5.9731e-03, PNorm = 69.5397, GNorm = 0.2312, lr_0 = 9.1067e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  8.13it/s]Loss = 6.2883e-03, PNorm = 69.5546, GNorm = 0.1822, lr_0 = 9.0809e-04\nLoss = 6.2883e-03, PNorm = 69.5546, GNorm = 0.1822, lr_0 = 9.0809e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:02,  8.08it/s]Loss = 6.4241e-03, PNorm = 69.5701, GNorm = 0.3451, lr_0 = 9.0552e-04\nLoss = 6.4241e-03, PNorm = 69.5701, GNorm = 0.3451, lr_0 = 9.0552e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:02,  8.11it/s]Loss = 7.7429e-03, PNorm = 69.5852, GNorm = 0.7810, lr_0 = 9.0295e-04\nLoss = 7.7429e-03, PNorm = 69.5852, GNorm = 0.7810, lr_0 = 9.0295e-04\n\n\r 24%|██▍       | 7/29 [00:00&lt;00:02,  8.05it/s]Loss = 8.3205e-03, PNorm = 69.5942, GNorm = 1.5333, lr_0 = 9.0040e-04\nLoss = 8.3205e-03, PNorm = 69.5942, GNorm = 1.5333, lr_0 = 9.0040e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:02,  8.06it/s]Loss = 7.4187e-03, PNorm = 69.6057, GNorm = 0.5427, lr_0 = 8.9785e-04\nLoss = 7.4187e-03, PNorm = 69.6057, GNorm = 0.5427, lr_0 = 8.9785e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:02,  8.04it/s]Loss = 7.1831e-03, PNorm = 69.6181, GNorm = 0.2016, lr_0 = 8.9530e-04\nLoss = 7.1831e-03, PNorm = 69.6181, GNorm = 0.2016, lr_0 = 8.9530e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  8.02it/s]Loss = 5.8249e-03, PNorm = 69.6325, GNorm = 0.2561, lr_0 = 8.9277e-04\nLoss = 5.8249e-03, PNorm = 69.6325, GNorm = 0.2561, lr_0 = 8.9277e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  7.99it/s]Loss = 5.7287e-03, PNorm = 69.6472, GNorm = 0.2323, lr_0 = 8.9024e-04\nLoss = 5.7287e-03, PNorm = 69.6472, GNorm = 0.2323, lr_0 = 8.9024e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  7.98it/s]Loss = 5.9367e-03, PNorm = 69.6623, GNorm = 0.2573, lr_0 = 8.8772e-04\n\n*** WARNING: skipped 132233 bytes of output ***\n\n\r100%|██████████| 4/4 [00:00&lt;00:00, 31.75it/s]Validation auc = 0.939312\nValidation auc = 0.939312\n\r 87%|████████▋ | 26/30 [03:14&lt;00:17,  4.35s/it]Epoch 26\nEpoch 26\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 3.1677e-03, PNorm = 75.6407, GNorm = 0.4677, lr_0 = 1.3856e-04\nLoss = 3.1677e-03, PNorm = 75.6407, GNorm = 0.4677, lr_0 = 1.3856e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:03,  8.34it/s]Loss = 2.8688e-03, PNorm = 75.6438, GNorm = 0.5358, lr_0 = 1.3816e-04\nLoss = 2.8688e-03, PNorm = 75.6438, GNorm = 0.5358, lr_0 = 1.3816e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:03,  8.31it/s]Loss = 3.1585e-03, PNorm = 75.6467, GNorm = 0.4824, lr_0 = 1.3777e-04\nLoss = 3.1585e-03, PNorm = 75.6467, GNorm = 0.4824, lr_0 = 1.3777e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:03,  8.22it/s]Loss = 3.4077e-03, PNorm = 75.6495, GNorm = 0.5421, lr_0 = 1.3738e-04\nLoss = 3.4077e-03, PNorm = 75.6495, GNorm = 0.5421, lr_0 = 1.3738e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  8.14it/s]Loss = 3.6387e-03, PNorm = 75.6522, GNorm = 0.6803, lr_0 = 1.3699e-04\nLoss = 3.6387e-03, PNorm = 75.6522, GNorm = 0.6803, lr_0 = 1.3699e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:02,  8.05it/s]Loss = 3.1166e-03, PNorm = 75.6549, GNorm = 0.5391, lr_0 = 1.3661e-04\nLoss = 3.1166e-03, PNorm = 75.6549, GNorm = 0.5391, lr_0 = 1.3661e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:02,  8.10it/s]Loss = 2.1204e-03, PNorm = 75.6577, GNorm = 0.6003, lr_0 = 1.3622e-04\nLoss = 2.1204e-03, PNorm = 75.6577, GNorm = 0.6003, lr_0 = 1.3622e-04\n\n\r 24%|██▍       | 7/29 [00:00&lt;00:02,  8.09it/s]Loss = 2.5198e-03, PNorm = 75.6605, GNorm = 0.4774, lr_0 = 1.3583e-04\nLoss = 2.5198e-03, PNorm = 75.6605, GNorm = 0.4774, lr_0 = 1.3583e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:02,  8.10it/s]Loss = 2.6197e-03, PNorm = 75.6635, GNorm = 0.5649, lr_0 = 1.3545e-04\nLoss = 2.6197e-03, PNorm = 75.6635, GNorm = 0.5649, lr_0 = 1.3545e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:02,  8.04it/s]Loss = 2.8178e-03, PNorm = 75.6664, GNorm = 0.5361, lr_0 = 1.3506e-04\nLoss = 2.8178e-03, PNorm = 75.6664, GNorm = 0.5361, lr_0 = 1.3506e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  8.08it/s]Loss = 3.5404e-03, PNorm = 75.6691, GNorm = 0.4800, lr_0 = 1.3468e-04\nLoss = 3.5404e-03, PNorm = 75.6691, GNorm = 0.4800, lr_0 = 1.3468e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  8.12it/s]Loss = 2.5645e-03, PNorm = 75.6718, GNorm = 0.4633, lr_0 = 1.3430e-04\nLoss = 2.5645e-03, PNorm = 75.6718, GNorm = 0.4633, lr_0 = 1.3430e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  8.17it/s]Loss = 4.5385e-03, PNorm = 75.6744, GNorm = 1.1039, lr_0 = 1.3392e-04\nLoss = 4.5385e-03, PNorm = 75.6744, GNorm = 1.1039, lr_0 = 1.3392e-04\n\n\r 45%|████▍     | 13/29 [00:01&lt;00:01,  8.07it/s]Loss = 1.3052e-03, PNorm = 75.6769, GNorm = 0.5082, lr_0 = 1.3354e-04\nLoss = 1.3052e-03, PNorm = 75.6769, GNorm = 0.5082, lr_0 = 1.3354e-04\n\n\r 48%|████▊     | 14/29 [00:01&lt;00:01,  8.07it/s]Loss = 2.7278e-03, PNorm = 75.6795, GNorm = 0.5617, lr_0 = 1.3316e-04\nLoss = 2.7278e-03, PNorm = 75.6795, GNorm = 0.5617, lr_0 = 1.3316e-04\n\n\r 52%|█████▏    | 15/29 [00:01&lt;00:01,  8.09it/s]Loss = 3.1680e-03, PNorm = 75.6821, GNorm = 0.5923, lr_0 = 1.3279e-04\nLoss = 3.1680e-03, PNorm = 75.6821, GNorm = 0.5923, lr_0 = 1.3279e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:01,  8.15it/s]Loss = 3.5384e-03, PNorm = 75.6845, GNorm = 0.4482, lr_0 = 1.3241e-04\nLoss = 3.5384e-03, PNorm = 75.6845, GNorm = 0.4482, lr_0 = 1.3241e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  8.15it/s]Loss = 2.2017e-03, PNorm = 75.6871, GNorm = 0.5207, lr_0 = 1.3204e-04\nLoss = 2.2017e-03, PNorm = 75.6871, GNorm = 0.5207, lr_0 = 1.3204e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  8.13it/s]Loss = 5.1143e-03, PNorm = 75.6897, GNorm = 0.8072, lr_0 = 1.3166e-04\nLoss = 5.1143e-03, PNorm = 75.6897, GNorm = 0.8072, lr_0 = 1.3166e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  8.17it/s]Loss = 2.9468e-03, PNorm = 75.6923, GNorm = 0.7362, lr_0 = 1.3129e-04\nLoss = 2.9468e-03, PNorm = 75.6923, GNorm = 0.7362, lr_0 = 1.3129e-04\n\n\r 69%|██████▉   | 20/29 [00:02&lt;00:01,  8.15it/s]Loss = 1.8215e-03, PNorm = 75.6949, GNorm = 0.3672, lr_0 = 1.3092e-04\nLoss = 1.8215e-03, PNorm = 75.6949, GNorm = 0.3672, lr_0 = 1.3092e-04\n\n\r 72%|███████▏  | 21/29 [00:02&lt;00:00,  8.09it/s]Loss = 2.4892e-03, PNorm = 75.6975, GNorm = 0.4235, lr_0 = 1.3055e-04\nLoss = 2.4892e-03, PNorm = 75.6975, GNorm = 0.4235, lr_0 = 1.3055e-04\n\n\r 76%|███████▌  | 22/29 [00:02&lt;00:00,  8.04it/s]Loss = 2.1588e-03, PNorm = 75.7000, GNorm = 0.4375, lr_0 = 1.3018e-04\nLoss = 2.1588e-03, PNorm = 75.7000, GNorm = 0.4375, lr_0 = 1.3018e-04\n\n\r 79%|███████▉  | 23/29 [00:02&lt;00:00,  8.11it/s]Loss = 2.3851e-03, PNorm = 75.7026, GNorm = 0.3814, lr_0 = 1.2981e-04\nLoss = 2.3851e-03, PNorm = 75.7026, GNorm = 0.3814, lr_0 = 1.2981e-04\n\n\r 83%|████████▎ | 24/29 [00:02&lt;00:00,  8.18it/s]Loss = 2.8338e-03, PNorm = 75.7052, GNorm = 0.8797, lr_0 = 1.2944e-04\nLoss = 2.8338e-03, PNorm = 75.7052, GNorm = 0.8797, lr_0 = 1.2944e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  8.15it/s]Loss = 3.6490e-03, PNorm = 75.7078, GNorm = 0.9412, lr_0 = 1.2907e-04\nLoss = 3.6490e-03, PNorm = 75.7078, GNorm = 0.9412, lr_0 = 1.2907e-04\n\n\r 90%|████████▉ | 26/29 [00:03&lt;00:00,  8.15it/s]Loss = 1.8711e-03, PNorm = 75.7104, GNorm = 0.4042, lr_0 = 1.2871e-04\nLoss = 1.8711e-03, PNorm = 75.7104, GNorm = 0.4042, lr_0 = 1.2871e-04\n\n\r 93%|█████████▎| 27/29 [00:03&lt;00:00,  8.18it/s]Loss = 3.2976e-03, PNorm = 75.7130, GNorm = 0.4926, lr_0 = 1.2834e-04\nLoss = 3.2976e-03, PNorm = 75.7130, GNorm = 0.4926, lr_0 = 1.2834e-04\n\n\r 97%|█████████▋| 28/29 [00:03&lt;00:00,  8.13it/s]Loss = 2.3620e-03, PNorm = 75.7156, GNorm = 0.6012, lr_0 = 1.2798e-04\nLoss = 2.3620e-03, PNorm = 75.7156, GNorm = 0.6012, lr_0 = 1.2798e-04\n\n\r100%|██████████| 29/29 [00:03&lt;00:00,  8.11it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 29.13it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 31.77it/s]Validation auc = 0.958590\nValidation auc = 0.958590\n\r 90%|█████████ | 27/30 [03:25&lt;00:18,  6.16s/it]Epoch 27\nEpoch 27\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 2.0539e-03, PNorm = 75.7182, GNorm = 0.4576, lr_0 = 1.2762e-04\nLoss = 2.0539e-03, PNorm = 75.7182, GNorm = 0.4576, lr_0 = 1.2762e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:03,  7.63it/s]Loss = 3.3512e-03, PNorm = 75.7209, GNorm = 0.6395, lr_0 = 1.2726e-04\nLoss = 3.3512e-03, PNorm = 75.7209, GNorm = 0.6395, lr_0 = 1.2726e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:03,  7.73it/s]Loss = 3.8352e-03, PNorm = 75.7235, GNorm = 0.6150, lr_0 = 1.2690e-04\nLoss = 3.8352e-03, PNorm = 75.7235, GNorm = 0.6150, lr_0 = 1.2690e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:03,  7.83it/s]Loss = 1.6182e-03, PNorm = 75.7262, GNorm = 0.3593, lr_0 = 1.2654e-04\nLoss = 1.6182e-03, PNorm = 75.7262, GNorm = 0.3593, lr_0 = 1.2654e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  7.89it/s]Loss = 2.4558e-03, PNorm = 75.7289, GNorm = 0.4516, lr_0 = 1.2618e-04\nLoss = 2.4558e-03, PNorm = 75.7289, GNorm = 0.4516, lr_0 = 1.2618e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:03,  7.94it/s]Loss = 2.7494e-03, PNorm = 75.7316, GNorm = 0.3928, lr_0 = 1.2582e-04\nLoss = 2.7494e-03, PNorm = 75.7316, GNorm = 0.3928, lr_0 = 1.2582e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:02,  7.99it/s]Loss = 3.3927e-03, PNorm = 75.7341, GNorm = 1.0977, lr_0 = 1.2546e-04\nLoss = 3.3927e-03, PNorm = 75.7341, GNorm = 1.0977, lr_0 = 1.2546e-04\n\n\r 24%|██▍       | 7/29 [00:00&lt;00:02,  7.92it/s]Loss = 2.1623e-03, PNorm = 75.7367, GNorm = 0.4704, lr_0 = 1.2511e-04\nLoss = 2.1623e-03, PNorm = 75.7367, GNorm = 0.4704, lr_0 = 1.2511e-04\n\n\r 28%|██▊       | 8/29 [00:01&lt;00:02,  7.98it/s]Loss = 3.2177e-03, PNorm = 75.7394, GNorm = 0.6770, lr_0 = 1.2476e-04\nLoss = 3.2177e-03, PNorm = 75.7394, GNorm = 0.6770, lr_0 = 1.2476e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:02,  7.98it/s]Loss = 2.2565e-03, PNorm = 75.7421, GNorm = 0.3901, lr_0 = 1.2440e-04\nLoss = 2.2565e-03, PNorm = 75.7421, GNorm = 0.3901, lr_0 = 1.2440e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  8.08it/s]Loss = 2.8608e-03, PNorm = 75.7447, GNorm = 0.4686, lr_0 = 1.2405e-04\nLoss = 2.8608e-03, PNorm = 75.7447, GNorm = 0.4686, lr_0 = 1.2405e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  8.10it/s]Loss = 3.1021e-03, PNorm = 75.7471, GNorm = 0.6526, lr_0 = 1.2370e-04\nLoss = 3.1021e-03, PNorm = 75.7471, GNorm = 0.6526, lr_0 = 1.2370e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  8.13it/s]Loss = 2.4405e-03, PNorm = 75.7496, GNorm = 0.4631, lr_0 = 1.2335e-04\nLoss = 2.4405e-03, PNorm = 75.7496, GNorm = 0.4631, lr_0 = 1.2335e-04\n\n\r 45%|████▍     | 13/29 [00:01&lt;00:01,  8.11it/s]Loss = 1.9264e-03, PNorm = 75.7522, GNorm = 0.3992, lr_0 = 1.2300e-04\nLoss = 1.9264e-03, PNorm = 75.7522, GNorm = 0.3992, lr_0 = 1.2300e-04\n\n\r 48%|████▊     | 14/29 [00:01&lt;00:01,  8.08it/s]Loss = 2.6008e-03, PNorm = 75.7549, GNorm = 0.4892, lr_0 = 1.2265e-04\nLoss = 2.6008e-03, PNorm = 75.7549, GNorm = 0.4892, lr_0 = 1.2265e-04\n\n\r 52%|█████▏    | 15/29 [00:01&lt;00:01,  8.09it/s]Loss = 2.9417e-03, PNorm = 75.7575, GNorm = 0.6012, lr_0 = 1.2230e-04\nLoss = 2.9417e-03, PNorm = 75.7575, GNorm = 0.6012, lr_0 = 1.2230e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:01,  8.07it/s]Loss = 1.9762e-03, PNorm = 75.7602, GNorm = 0.4661, lr_0 = 1.2196e-04\nLoss = 1.9762e-03, PNorm = 75.7602, GNorm = 0.4661, lr_0 = 1.2196e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  8.06it/s]Loss = 2.5634e-03, PNorm = 75.7630, GNorm = 0.6233, lr_0 = 1.2161e-04\nLoss = 2.5634e-03, PNorm = 75.7630, GNorm = 0.6233, lr_0 = 1.2161e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  8.10it/s]Loss = 2.2251e-03, PNorm = 75.7658, GNorm = 0.4787, lr_0 = 1.2127e-04\nLoss = 2.2251e-03, PNorm = 75.7658, GNorm = 0.4787, lr_0 = 1.2127e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  8.10it/s]Loss = 3.6902e-03, PNorm = 75.7683, GNorm = 0.6986, lr_0 = 1.2092e-04\nLoss = 3.6902e-03, PNorm = 75.7683, GNorm = 0.6986, lr_0 = 1.2092e-04\n\n\r 69%|██████▉   | 20/29 [00:02&lt;00:01,  8.16it/s]Loss = 3.9679e-03, PNorm = 75.7709, GNorm = 0.6216, lr_0 = 1.2058e-04\nLoss = 3.9679e-03, PNorm = 75.7709, GNorm = 0.6216, lr_0 = 1.2058e-04\n\n\r 72%|███████▏  | 21/29 [00:02&lt;00:00,  8.16it/s]Loss = 3.0745e-03, PNorm = 75.7733, GNorm = 0.6975, lr_0 = 1.2024e-04\nLoss = 3.0745e-03, PNorm = 75.7733, GNorm = 0.6975, lr_0 = 1.2024e-04\n\n\r 76%|███████▌  | 22/29 [00:02&lt;00:00,  8.13it/s]Loss = 4.0462e-03, PNorm = 75.7756, GNorm = 0.7484, lr_0 = 1.1990e-04\nLoss = 4.0462e-03, PNorm = 75.7756, GNorm = 0.7484, lr_0 = 1.1990e-04\n\n\r 79%|███████▉  | 23/29 [00:02&lt;00:00,  8.12it/s]Loss = 1.8251e-03, PNorm = 75.7779, GNorm = 0.4001, lr_0 = 1.1956e-04\nLoss = 1.8251e-03, PNorm = 75.7779, GNorm = 0.4001, lr_0 = 1.1956e-04\n\n\r 83%|████████▎ | 24/29 [00:02&lt;00:00,  8.16it/s]Loss = 3.3277e-03, PNorm = 75.7800, GNorm = 0.4943, lr_0 = 1.1922e-04\nLoss = 3.3277e-03, PNorm = 75.7800, GNorm = 0.4943, lr_0 = 1.1922e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  8.08it/s]Loss = 2.0705e-03, PNorm = 75.7822, GNorm = 0.3971, lr_0 = 1.1888e-04\nLoss = 2.0705e-03, PNorm = 75.7822, GNorm = 0.3971, lr_0 = 1.1888e-04\n\n\r 90%|████████▉ | 26/29 [00:03&lt;00:00,  8.04it/s]Loss = 2.5301e-03, PNorm = 75.7843, GNorm = 0.5711, lr_0 = 1.1855e-04\nLoss = 2.5301e-03, PNorm = 75.7843, GNorm = 0.5711, lr_0 = 1.1855e-04\n\n\r 93%|█████████▎| 27/29 [00:03&lt;00:00,  8.13it/s]Loss = 2.7729e-03, PNorm = 75.7863, GNorm = 0.5578, lr_0 = 1.1821e-04\nLoss = 2.7729e-03, PNorm = 75.7863, GNorm = 0.5578, lr_0 = 1.1821e-04\n\n\r 97%|█████████▋| 28/29 [00:03&lt;00:00,  8.12it/s]Loss = 3.1537e-03, PNorm = 75.7882, GNorm = 0.5612, lr_0 = 1.1788e-04\nLoss = 3.1537e-03, PNorm = 75.7882, GNorm = 0.5612, lr_0 = 1.1788e-04\n\n\r100%|██████████| 29/29 [00:03&lt;00:00,  8.13it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 27.66it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 30.41it/s]Validation auc = 0.953910\nValidation auc = 0.953910\n\r 93%|█████████▎| 28/30 [03:29&lt;00:10,  5.43s/it]Epoch 28\nEpoch 28\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 2.0791e-03, PNorm = 75.7902, GNorm = 0.4935, lr_0 = 1.1754e-04\nLoss = 2.0791e-03, PNorm = 75.7902, GNorm = 0.4935, lr_0 = 1.1754e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:03,  8.21it/s]Loss = 2.0807e-03, PNorm = 75.7924, GNorm = 0.4285, lr_0 = 1.1721e-04\nLoss = 2.0807e-03, PNorm = 75.7924, GNorm = 0.4285, lr_0 = 1.1721e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:03,  8.17it/s]Loss = 2.7899e-03, PNorm = 75.7945, GNorm = 0.6122, lr_0 = 1.1688e-04\nLoss = 2.7899e-03, PNorm = 75.7945, GNorm = 0.6122, lr_0 = 1.1688e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:03,  8.09it/s]Loss = 2.4645e-03, PNorm = 75.7965, GNorm = 0.5827, lr_0 = 1.1655e-04\nLoss = 2.4645e-03, PNorm = 75.7965, GNorm = 0.5827, lr_0 = 1.1655e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  8.13it/s]Loss = 3.8065e-03, PNorm = 75.7986, GNorm = 0.6223, lr_0 = 1.1622e-04\nLoss = 3.8065e-03, PNorm = 75.7986, GNorm = 0.6223, lr_0 = 1.1622e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:02,  8.20it/s]Loss = 2.6855e-03, PNorm = 75.8008, GNorm = 0.6242, lr_0 = 1.1589e-04\nLoss = 2.6855e-03, PNorm = 75.8008, GNorm = 0.6242, lr_0 = 1.1589e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:02,  8.14it/s]Loss = 3.1037e-03, PNorm = 75.8028, GNorm = 0.4744, lr_0 = 1.1556e-04\nLoss = 3.1037e-03, PNorm = 75.8028, GNorm = 0.4744, lr_0 = 1.1556e-04\n\n\r 24%|██▍       | 7/29 [00:00&lt;00:02,  8.13it/s]Loss = 2.3569e-03, PNorm = 75.8049, GNorm = 0.4138, lr_0 = 1.1523e-04\nLoss = 2.3569e-03, PNorm = 75.8049, GNorm = 0.4138, lr_0 = 1.1523e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:02,  8.05it/s]Loss = 2.8993e-03, PNorm = 75.8072, GNorm = 0.5074, lr_0 = 1.1491e-04\nLoss = 2.8993e-03, PNorm = 75.8072, GNorm = 0.5074, lr_0 = 1.1491e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:02,  8.05it/s]Loss = 3.5965e-03, PNorm = 75.8095, GNorm = 0.5977, lr_0 = 1.1458e-04\nLoss = 3.5965e-03, PNorm = 75.8095, GNorm = 0.5977, lr_0 = 1.1458e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  8.13it/s]Loss = 3.7197e-03, PNorm = 75.8120, GNorm = 0.6961, lr_0 = 1.1426e-04\nLoss = 3.7197e-03, PNorm = 75.8120, GNorm = 0.6961, lr_0 = 1.1426e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  8.17it/s]Loss = 3.1544e-03, PNorm = 75.8146, GNorm = 0.4792, lr_0 = 1.1393e-04\nLoss = 3.1544e-03, PNorm = 75.8146, GNorm = 0.4792, lr_0 = 1.1393e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  8.09it/s]Loss = 3.1424e-03, PNorm = 75.8172, GNorm = 0.5108, lr_0 = 1.1361e-04\nLoss = 3.1424e-03, PNorm = 75.8172, GNorm = 0.5108, lr_0 = 1.1361e-04\n\n\r 45%|████▍     | 13/29 [00:01&lt;00:01,  8.11it/s]Loss = 1.9542e-03, PNorm = 75.8198, GNorm = 0.3087, lr_0 = 1.1329e-04\nLoss = 1.9542e-03, PNorm = 75.8198, GNorm = 0.3087, lr_0 = 1.1329e-04\n\n\r 48%|████▊     | 14/29 [00:01&lt;00:01,  8.12it/s]Loss = 2.7084e-03, PNorm = 75.8222, GNorm = 0.7875, lr_0 = 1.1297e-04\nLoss = 2.7084e-03, PNorm = 75.8222, GNorm = 0.7875, lr_0 = 1.1297e-04\n\n\r 52%|█████▏    | 15/29 [00:01&lt;00:01,  8.14it/s]Loss = 2.5352e-03, PNorm = 75.8246, GNorm = 0.6767, lr_0 = 1.1265e-04\nLoss = 2.5352e-03, PNorm = 75.8246, GNorm = 0.6767, lr_0 = 1.1265e-04\n\n\r 55%|█████▌    | 16/29 [00:01&lt;00:01,  8.02it/s]Loss = 3.4072e-03, PNorm = 75.8269, GNorm = 0.6451, lr_0 = 1.1233e-04\nLoss = 3.4072e-03, PNorm = 75.8269, GNorm = 0.6451, lr_0 = 1.1233e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  8.09it/s]Loss = 2.9499e-03, PNorm = 75.8290, GNorm = 0.5510, lr_0 = 1.1201e-04\nLoss = 2.9499e-03, PNorm = 75.8290, GNorm = 0.5510, lr_0 = 1.1201e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  8.10it/s]Loss = 2.5423e-03, PNorm = 75.8313, GNorm = 0.6081, lr_0 = 1.1169e-04\nLoss = 2.5423e-03, PNorm = 75.8313, GNorm = 0.6081, lr_0 = 1.1169e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  8.09it/s]Loss = 3.1894e-03, PNorm = 75.8337, GNorm = 0.5115, lr_0 = 1.1138e-04\nLoss = 3.1894e-03, PNorm = 75.8337, GNorm = 0.5115, lr_0 = 1.1138e-04\n\n\r 69%|██████▉   | 20/29 [00:02&lt;00:01,  8.12it/s]Loss = 2.5844e-03, PNorm = 75.8360, GNorm = 0.6169, lr_0 = 1.1106e-04\nLoss = 2.5844e-03, PNorm = 75.8360, GNorm = 0.6169, lr_0 = 1.1106e-04\n\n\r 72%|███████▏  | 21/29 [00:02&lt;00:00,  8.10it/s]Loss = 1.9144e-03, PNorm = 75.8385, GNorm = 0.3897, lr_0 = 1.1075e-04\nLoss = 1.9144e-03, PNorm = 75.8385, GNorm = 0.3897, lr_0 = 1.1075e-04\n\n\r 76%|███████▌  | 22/29 [00:02&lt;00:00,  8.13it/s]Loss = 3.5149e-03, PNorm = 75.8407, GNorm = 0.6403, lr_0 = 1.1043e-04\nLoss = 3.5149e-03, PNorm = 75.8407, GNorm = 0.6403, lr_0 = 1.1043e-04\n\n\r 79%|███████▉  | 23/29 [00:02&lt;00:00,  8.14it/s]Loss = 2.5632e-03, PNorm = 75.8428, GNorm = 0.7329, lr_0 = 1.1012e-04\nLoss = 2.5632e-03, PNorm = 75.8428, GNorm = 0.7329, lr_0 = 1.1012e-04\n\n\r 83%|████████▎ | 24/29 [00:02&lt;00:00,  8.05it/s]Loss = 2.3308e-03, PNorm = 75.8448, GNorm = 0.6057, lr_0 = 1.0981e-04\nLoss = 2.3308e-03, PNorm = 75.8448, GNorm = 0.6057, lr_0 = 1.0981e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  8.08it/s]Loss = 3.1894e-03, PNorm = 75.8469, GNorm = 0.5270, lr_0 = 1.0950e-04\nLoss = 3.1894e-03, PNorm = 75.8469, GNorm = 0.5270, lr_0 = 1.0950e-04\n\n\r 90%|████████▉ | 26/29 [00:03&lt;00:00,  8.12it/s]Loss = 2.3235e-03, PNorm = 75.8488, GNorm = 0.5728, lr_0 = 1.0919e-04\nLoss = 2.3235e-03, PNorm = 75.8488, GNorm = 0.5728, lr_0 = 1.0919e-04\n\n\r 93%|█████████▎| 27/29 [00:03&lt;00:00,  8.15it/s]Loss = 2.9076e-03, PNorm = 75.8507, GNorm = 0.6793, lr_0 = 1.0888e-04\nLoss = 2.9076e-03, PNorm = 75.8507, GNorm = 0.6793, lr_0 = 1.0888e-04\n\n\r 97%|█████████▋| 28/29 [00:03&lt;00:00,  8.09it/s]Loss = 2.5028e-03, PNorm = 75.8525, GNorm = 0.4326, lr_0 = 1.0857e-04\nLoss = 2.5028e-03, PNorm = 75.8525, GNorm = 0.4326, lr_0 = 1.0857e-04\n\n\r100%|██████████| 29/29 [00:03&lt;00:00,  8.09it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 29.00it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 31.65it/s]Validation auc = 0.940039\nValidation auc = 0.940039\n\r 97%|█████████▋| 29/30 [03:32&lt;00:04,  4.92s/it]Epoch 29\nEpoch 29\n\n\r  0%|          | 0/29 [00:00&lt;?, ?it/s]Loss = 1.7480e-03, PNorm = 75.8543, GNorm = 0.4748, lr_0 = 1.0826e-04\nLoss = 1.7480e-03, PNorm = 75.8543, GNorm = 0.4748, lr_0 = 1.0826e-04\n\n\r  3%|▎         | 1/29 [00:00&lt;00:03,  8.10it/s]Loss = 2.7673e-03, PNorm = 75.8561, GNorm = 0.6708, lr_0 = 1.0796e-04\nLoss = 2.7673e-03, PNorm = 75.8561, GNorm = 0.6708, lr_0 = 1.0796e-04\n\n\r  7%|▋         | 2/29 [00:00&lt;00:03,  8.07it/s]Loss = 2.7276e-03, PNorm = 75.8580, GNorm = 0.5467, lr_0 = 1.0765e-04\nLoss = 2.7276e-03, PNorm = 75.8580, GNorm = 0.5467, lr_0 = 1.0765e-04\n\n\r 10%|█         | 3/29 [00:00&lt;00:03,  8.07it/s]Loss = 3.2154e-03, PNorm = 75.8598, GNorm = 0.7608, lr_0 = 1.0735e-04\nLoss = 3.2154e-03, PNorm = 75.8598, GNorm = 0.7608, lr_0 = 1.0735e-04\n\n\r 14%|█▍        | 4/29 [00:00&lt;00:03,  8.12it/s]Loss = 3.3848e-03, PNorm = 75.8616, GNorm = 0.6266, lr_0 = 1.0704e-04\nLoss = 3.3848e-03, PNorm = 75.8616, GNorm = 0.6266, lr_0 = 1.0704e-04\n\n\r 17%|█▋        | 5/29 [00:00&lt;00:02,  8.13it/s]Loss = 3.1421e-03, PNorm = 75.8636, GNorm = 0.7973, lr_0 = 1.0674e-04\nLoss = 3.1421e-03, PNorm = 75.8636, GNorm = 0.7973, lr_0 = 1.0674e-04\n\n\r 21%|██        | 6/29 [00:00&lt;00:02,  8.15it/s]Loss = 3.3011e-03, PNorm = 75.8656, GNorm = 0.5370, lr_0 = 1.0644e-04\nLoss = 3.3011e-03, PNorm = 75.8656, GNorm = 0.5370, lr_0 = 1.0644e-04\n\n\r 24%|██▍       | 7/29 [00:00&lt;00:02,  8.18it/s]Loss = 2.8988e-03, PNorm = 75.8676, GNorm = 0.6287, lr_0 = 1.0614e-04\nLoss = 2.8988e-03, PNorm = 75.8676, GNorm = 0.6287, lr_0 = 1.0614e-04\n\n\r 28%|██▊       | 8/29 [00:00&lt;00:02,  8.11it/s]Loss = 3.0011e-03, PNorm = 75.8698, GNorm = 0.6899, lr_0 = 1.0584e-04\nLoss = 3.0011e-03, PNorm = 75.8698, GNorm = 0.6899, lr_0 = 1.0584e-04\n\n\r 31%|███       | 9/29 [00:01&lt;00:02,  8.08it/s]Loss = 2.3514e-03, PNorm = 75.8719, GNorm = 0.4655, lr_0 = 1.0554e-04\nLoss = 2.3514e-03, PNorm = 75.8719, GNorm = 0.4655, lr_0 = 1.0554e-04\n\n\r 34%|███▍      | 10/29 [00:01&lt;00:02,  8.11it/s]Loss = 2.7471e-03, PNorm = 75.8743, GNorm = 0.8224, lr_0 = 1.0524e-04\nLoss = 2.7471e-03, PNorm = 75.8743, GNorm = 0.8224, lr_0 = 1.0524e-04\n\n\r 38%|███▊      | 11/29 [00:01&lt;00:02,  8.10it/s]Loss = 2.7168e-03, PNorm = 75.8766, GNorm = 0.5030, lr_0 = 1.0494e-04\nLoss = 2.7168e-03, PNorm = 75.8766, GNorm = 0.5030, lr_0 = 1.0494e-04\n\n\r 41%|████▏     | 12/29 [00:01&lt;00:02,  8.15it/s]Loss = 2.8378e-03, PNorm = 75.8790, GNorm = 0.6318, lr_0 = 1.0464e-04\nLoss = 2.8378e-03, PNorm = 75.8790, GNorm = 0.6318, lr_0 = 1.0464e-04\n\n\r 45%|████▍     | 13/29 [00:01&lt;00:01,  8.18it/s]Loss = 2.6734e-03, PNorm = 75.8812, GNorm = 0.6394, lr_0 = 1.0435e-04\nLoss = 2.6734e-03, PNorm = 75.8812, GNorm = 0.6394, lr_0 = 1.0435e-04\n\n\r 48%|████▊     | 14/29 [00:01&lt;00:01,  8.20it/s]Loss = 2.6218e-03, PNorm = 75.8832, GNorm = 0.4863, lr_0 = 1.0405e-04\nLoss = 2.6218e-03, PNorm = 75.8832, GNorm = 0.4863, lr_0 = 1.0405e-04\n\n\r 52%|█████▏    | 15/29 [00:01&lt;00:01,  8.19it/s]Loss = 2.9577e-03, PNorm = 75.8853, GNorm = 0.5797, lr_0 = 1.0376e-04\nLoss = 2.9577e-03, PNorm = 75.8853, GNorm = 0.5797, lr_0 = 1.0376e-04\n\n\r 55%|█████▌    | 16/29 [00:02&lt;00:02,  5.37it/s]Loss = 2.1400e-03, PNorm = 75.8874, GNorm = 0.5164, lr_0 = 1.0346e-04\nLoss = 2.1400e-03, PNorm = 75.8874, GNorm = 0.5164, lr_0 = 1.0346e-04\n\n\r 59%|█████▊    | 17/29 [00:02&lt;00:01,  6.01it/s]Loss = 2.4268e-03, PNorm = 75.8895, GNorm = 0.7414, lr_0 = 1.0317e-04\nLoss = 2.4268e-03, PNorm = 75.8895, GNorm = 0.7414, lr_0 = 1.0317e-04\n\n\r 62%|██████▏   | 18/29 [00:02&lt;00:01,  6.54it/s]Loss = 2.4751e-03, PNorm = 75.8917, GNorm = 0.5597, lr_0 = 1.0288e-04\nLoss = 2.4751e-03, PNorm = 75.8917, GNorm = 0.5597, lr_0 = 1.0288e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  6.98it/s]Loss = 1.6545e-03, PNorm = 75.8940, GNorm = 0.3425, lr_0 = 1.0258e-04\nLoss = 1.6545e-03, PNorm = 75.8940, GNorm = 0.3425, lr_0 = 1.0258e-04\n\n\r 69%|██████▉   | 20/29 [00:02&lt;00:01,  7.25it/s]Loss = 2.9089e-03, PNorm = 75.8962, GNorm = 0.5941, lr_0 = 1.0229e-04\nLoss = 2.9089e-03, PNorm = 75.8962, GNorm = 0.5941, lr_0 = 1.0229e-04\n\n\r 72%|███████▏  | 21/29 [00:02&lt;00:01,  7.49it/s]Loss = 3.3391e-03, PNorm = 75.8984, GNorm = 1.1115, lr_0 = 1.0200e-04\nLoss = 3.3391e-03, PNorm = 75.8984, GNorm = 1.1115, lr_0 = 1.0200e-04\n\n\r 76%|███████▌  | 22/29 [00:02&lt;00:00,  7.70it/s]Loss = 3.8031e-03, PNorm = 75.9005, GNorm = 0.7769, lr_0 = 1.0172e-04\nLoss = 3.8031e-03, PNorm = 75.9005, GNorm = 0.7769, lr_0 = 1.0172e-04\n\n\r 79%|███████▉  | 23/29 [00:03&lt;00:00,  7.82it/s]Loss = 2.4120e-03, PNorm = 75.9027, GNorm = 0.5188, lr_0 = 1.0143e-04\nLoss = 2.4120e-03, PNorm = 75.9027, GNorm = 0.5188, lr_0 = 1.0143e-04\n\n\r 83%|████████▎ | 24/29 [00:03&lt;00:00,  7.91it/s]Loss = 4.5212e-03, PNorm = 75.9047, GNorm = 1.1269, lr_0 = 1.0114e-04\nLoss = 4.5212e-03, PNorm = 75.9047, GNorm = 1.1269, lr_0 = 1.0114e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  7.94it/s]Loss = 2.2735e-03, PNorm = 75.9069, GNorm = 0.5505, lr_0 = 1.0085e-04\nLoss = 2.2735e-03, PNorm = 75.9069, GNorm = 0.5505, lr_0 = 1.0085e-04\n\n\r 90%|████████▉ | 26/29 [00:03&lt;00:00,  7.99it/s]Loss = 2.0937e-03, PNorm = 75.9091, GNorm = 0.4723, lr_0 = 1.0057e-04\nLoss = 2.0937e-03, PNorm = 75.9091, GNorm = 0.4723, lr_0 = 1.0057e-04\n\n\r 93%|█████████▎| 27/29 [00:03&lt;00:00,  8.03it/s]Loss = 2.0045e-03, PNorm = 75.9115, GNorm = 0.7870, lr_0 = 1.0028e-04\nLoss = 2.0045e-03, PNorm = 75.9115, GNorm = 0.7870, lr_0 = 1.0028e-04\n\n\r 97%|█████████▋| 28/29 [00:03&lt;00:00,  8.10it/s]Loss = 3.5225e-03, PNorm = 75.9137, GNorm = 1.0310, lr_0 = 1.0000e-04\nLoss = 3.5225e-03, PNorm = 75.9137, GNorm = 1.0310, lr_0 = 1.0000e-04\n\n\r100%|██████████| 29/29 [00:03&lt;00:00,  8.00it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 29.11it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 31.76it/s]Validation auc = 0.950818\nValidation auc = 0.950818\n\r100%|██████████| 30/30 [03:36&lt;00:00,  4.62s/it]\nModel 0 best validation auc = 0.958590 on epoch 26\nModel 0 best validation auc = 0.958590 on epoch 26\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\nMoving model to cuda\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 27.94it/s]\r100%|██████████| 4/4 [00:00&lt;00:00, 30.55it/s]\nModel 0 test auc = 0.941984\nModel 0 test auc = 0.941984\nEnsemble test auc = 0.941984\nEnsemble test auc = 0.941984\n1-fold cross validation\n1-fold cross validation\nSeed 0 ==&gt; test auc = 0.941984\nSeed 0 ==&gt; test auc = 0.941984\nOverall test auc = 0.941984 +/- 0.000000\nOverall test auc = 0.941984 +/- 0.000000\nOut[22]: (0.9419836935809168, 0.0)\n</div>"]}}],"execution_count":90},{"cell_type":"markdown","source":["#### Classification 4x training different mods"],"metadata":{}},{"cell_type":"code","source":["\n#Feature SLogP: \n\nparser = ArgumentParser()\nadd_train_args(parser)\nif not os.path.exists(os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x')):\n  os.mkdir(os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x'))\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'JAK','train-1460_binary.csv'),\n                          '--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtrain-1460.csv'),\n                          '--dataset_type','classification',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-mods-bin'),\n                          '--separate_val_path',os.path.join(CHEMPROP_DIR,'JAK','val-182_binary.csv'),\n                          '--separate_val_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPval-182.csv'),\n                          '--separate_test_path',os.path.join(CHEMPROP_DIR,'JAK','test-183_binary.csv'),\n                          '--separate_test_features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPtest-183.csv'),\n                          '--log_frequency','1',\n                          '--depth','6',\n                          '--dropout','0.3',\n                          '--hidden_size','2200',\n                          '--ffn_num_layers','1'\n                        #,'--atom_messages'\n                        #,'--ensemble_size','3'\n                        #,'--features_generator','rdkit_2d'\n                        #,'--seed','13',\n                          ])\nmodify_train_args(args)\nlogger = create_logger(name='train', save_dir=args.save_dir, quiet=args.quiet)\n\ncross_validate(args, logger)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\nFold 0\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;ReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: None,\n &#39;checkpoint_paths&#39;: None,\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/train-1460_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;regression&#39;,\n &#39;depth&#39;: 6,\n &#39;dropout&#39;: 0.3,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 30,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtrain-1460.csv&#39;],\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2200,\n &#39;ffn_num_layers&#39;: 1,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2200,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;rmse&#39;,\n &#39;minimize_score&#39;: True,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/hyperopt_4x-mods-bin/fold_0&#39;,\n &#39;save_smiles_splits&#39;: False,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPtest-183.csv&#39;],\n &#39;separate_test_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/test-183_binary.csv&#39;,\n &#39;separate_val_features_path&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/SLogPval-182.csv&#39;],\n &#39;separate_val_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/val-182_binary.csv&#39;,\n\n*** WARNING: skipped 1104935 bytes of output ***\n\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\nLoss = 1.6378e-02, PNorm = 73.9162, GNorm = 1.4370, lr_0 = 1.0288e-04\n\n\r 66%|██████▌   | 19/29 [00:02&lt;00:01,  8.11it/s]Loss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\nLoss = 6.6365e-03, PNorm = 73.9177, GNorm = 1.1427, lr_0 = 1.0258e-04\n\n\r 69%|██████▉   | 20/29 [00:02&lt;00:01,  8.11it/s]Loss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\nLoss = 1.0347e-02, PNorm = 73.9192, GNorm = 1.7206, lr_0 = 1.0229e-04\n\n\r 72%|███████▏  | 21/29 [00:02&lt;00:00,  8.08it/s]Loss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\nLoss = 8.6905e-03, PNorm = 73.9209, GNorm = 1.2073, lr_0 = 1.0200e-04\n\n\r 76%|███████▌  | 22/29 [00:02&lt;00:00,  8.09it/s]Loss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\nLoss = 6.5272e-03, PNorm = 73.9227, GNorm = 1.0032, lr_0 = 1.0172e-04\n\n\r 79%|███████▉  | 23/29 [00:02&lt;00:00,  8.10it/s]Loss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\nLoss = 7.3986e-03, PNorm = 73.9244, GNorm = 1.3113, lr_0 = 1.0143e-04\n\n\r 83%|████████▎ | 24/29 [00:02&lt;00:00,  8.15it/s]Loss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\nLoss = 6.8111e-03, PNorm = 73.9262, GNorm = 0.9965, lr_0 = 1.0114e-04\n\n\r 86%|████████▌ | 25/29 [00:03&lt;00:00,  8.18it/s]Loss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\nLoss = 6.6138e-03, PNorm = 73.9279, GNorm = 1.0006, lr_0 = 1.0085e-04\n\n\r 90%|████████▉ | 26/29 [00:03&lt;00:00,  8.19it/s]Loss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\nLoss = 1.1026e-02, PNorm = 73.9296, GNorm = 1.3324, lr_0 = 1.0057e-04\n\n\r 93%|█████████▎| 27/29 [00:03&lt;00:00,  8.21it/s]Loss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\nLoss = 1.2817e-02, PNorm = 73.9312, GNorm = 1.3222, lr_0 = 1.0028e-04\n\n\r 97%|█████████▋| 28/29 [00:03&lt;00:00,  8.16it/s]Loss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\nLoss = 9.1840e-03, PNorm = 73.9329, GNorm = 1.0707, lr_0 = 1.0000e-04\n\n\r100%|██████████| 29/29 [00:03&lt;00:00,  8.14it/s]\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\n\r100%|██████████| 4/4 [00:00&lt;00:00, 32.96it/s]Validation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\nValidation rmse = 0.248165\n\r100%|██████████| 30/30 [03:14&lt;00:00,  4.05s/it]\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nModel 0 best validation rmse = 0.247486 on epoch 23\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\nMoving model to cuda\n\r  0%|          | 0/4 [00:00&lt;?, ?it/s]\r 75%|███████▌  | 3/4 [00:00&lt;00:00, 29.51it/s]\r100%|██████████| 4/4 [00:00&lt;00:00, 32.08it/s]\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nModel 0 test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\nEnsemble test rmse = 0.253606\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\n1-fold cross validation\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nSeed 0 ==&gt; test rmse = 0.253606\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOverall test rmse = 0.253606 +/- 0.000000\nOut[59]: (0.2536064060346279, 0.0)\n</div>"]}}],"execution_count":92},{"cell_type":"code","source":["%sh head /dbfs/FileStore/chemprop/JAK/all-1825.csv"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">smiles,JAK1 EC50 nM 1027,JAK2 EC50 nM 1024,JAK3 EC50 nM 1026,TYK2 EC50 nM 1025\nCc1c[nH]c(C(=O)N2CCCN(c3ncnc4[nH]ccc34)CC23CC3)c1,7.856985199745905,7.94692155651658,6.886056647693163,6.82102305270683\nO=S(=O)(N(CCN1CCOCC1)C1CCC1)N1CCN(c2ncnc3[nH]ccc23)CC12CC2,7.069560405233299,7.357535479757878,6.33161408331,6.157390760389438\nCN(C[C@@H]1CCCN1S(C)(=O)=O)S(=O)(=O)N1CCN(c2ncnc3[nH]ccc23)CC12CC2,7.732828271596986,7.8326826652518236,6.966576244513051,6.376750709602099\nCC(=O)N1CC[C@H]1COC(=O)N1C2CCC1CN(c1ncnc3[nH]ccc13)C2,6.7544873321858505,6.665546248849069,6.244887733604929,5.379863945026242\nN#Cc1ccc(CC(=O)N2CCN(c3ncnc4[nH]ccc34)C3(CC3)C2)cc1,6.504455662453552,6.1487416512809245,5.782516055786093,5.235077015350112\nO=C(OC1CCC(O)C1)N1C2CCC1CN(c1ncnc3[nH]ccc13)C2,7.476253533188435,7.26440110030182,6.46344155742847,6.136677139879544\nCC(C)COC(=O)N1CCC[C@@H]1CN(C)S(=O)(=O)N1CCN(c2ncnc3[nH]ccc23)CC12CC2,7.52432881167557,7.153044674980176,6.045757490560675,5.906578314837765\nN#CCCNC(=O)N1C2CCC1CN(c1ncnc3[nH]ccc13)C2,7.1732774798310075,6.987162775294828,6.350665141287858,5.841637507904751\nC[C@@H](OC(=O)N1C2CCC1CN(c1ncnc3[nH]ccc13)C2)c1c(F)cccc1F,6.134896025358872,5.94692155651658,5.359518563029578,5.046240308266771\n</div>"]}}],"execution_count":93},{"cell_type":"code","source":["#{'depth': 6, 'dropout': 0.0, 'ffn_num_layers': 2, 'hidden_size': 1900}\nargs = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'JAK','all-1825.csv'),\n                          '--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPall-1825.csv'),\n                          '--checkpoint_path',os.path.join(VIRTUAL_SCREENING,'Chemprop-Feat_SLogP','fold_0/model_0/model.pt'),\n                          '--preds_path',os.path.join(VIRTUAL_SCREENING,'Chemprop-Feat_SLogP','fold_0/model_0/all-1825_preds.csv')])\nmodify_predict_args(args)\nmake_predictions(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading training args\nLoading data\n\r  0%|          | 0/1825 [00:00&lt;?, ?it/s]\r 16%|█▌        | 287/1825 [00:00&lt;00:00, 2865.89it/s]\r 31%|███▏      | 571/1825 [00:00&lt;00:00, 2857.78it/s]\r 47%|████▋     | 850/1825 [00:00&lt;00:00, 2833.62it/s]\r 62%|██████▏   | 1130/1825 [00:00&lt;00:00, 2822.68it/s]\r 77%|███████▋  | 1412/1825 [00:00&lt;00:00, 2819.13it/s]\r 90%|█████████ | 1646/1825 [00:00&lt;00:00, 2171.08it/s]\r100%|██████████| 1825/1825 [00:00&lt;00:00, 2504.10it/s]\nValidating SMILES\nTest size = 1,825\nPredicting with an ensemble of 1 models\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\r  0%|          | 0/37 [00:00&lt;?, ?it/s]\n\r  5%|▌         | 2/37 [00:00&lt;00:02, 14.03it/s]\n\r  8%|▊         | 3/37 [00:00&lt;00:02, 11.97it/s]\n\r 11%|█         | 4/37 [00:00&lt;00:03, 10.70it/s]\n\r 14%|█▎        | 5/37 [00:00&lt;00:03,  9.89it/s]\n\r 16%|█▌        | 6/37 [00:00&lt;00:03,  9.51it/s]\n\r 19%|█▉        | 7/37 [00:00&lt;00:03,  9.46it/s]\n\r 22%|██▏       | 8/37 [00:00&lt;00:03,  9.25it/s]\n\r 24%|██▍       | 9/37 [00:00&lt;00:03,  9.15it/s]\n\r 27%|██▋       | 10/37 [00:01&lt;00:02,  9.02it/s]\n\r 30%|██▉       | 11/37 [00:01&lt;00:03,  6.97it/s]\n\r 32%|███▏      | 12/37 [00:01&lt;00:03,  7.45it/s]\n\r 35%|███▌      | 13/37 [00:01&lt;00:03,  7.87it/s]\n\r 38%|███▊      | 14/37 [00:01&lt;00:02,  8.22it/s]\n\r 41%|████      | 15/37 [00:01&lt;00:02,  8.19it/s]\n\r 43%|████▎     | 16/37 [00:01&lt;00:02,  8.19it/s]\n\r 46%|████▌     | 17/37 [00:01&lt;00:02,  8.12it/s]\n\r 49%|████▊     | 18/37 [00:02&lt;00:02,  8.37it/s]\n\r 51%|█████▏    | 19/37 [00:02&lt;00:02,  8.70it/s]\n\r 54%|█████▍    | 20/37 [00:02&lt;00:01,  8.80it/s]\n\r 57%|█████▋    | 21/37 [00:02&lt;00:01,  8.48it/s]\n\r 59%|█████▉    | 22/37 [00:02&lt;00:02,  6.06it/s]\n\r 62%|██████▏   | 23/37 [00:02&lt;00:02,  6.65it/s]\n\r 65%|██████▍   | 24/37 [00:02&lt;00:01,  7.12it/s]\n\r 68%|██████▊   | 25/37 [00:03&lt;00:01,  7.61it/s]\n\r 70%|███████   | 26/37 [00:03&lt;00:01,  8.06it/s]\n\r 73%|███████▎  | 27/37 [00:03&lt;00:01,  8.16it/s]\n\r 76%|███████▌  | 28/37 [00:03&lt;00:01,  7.98it/s]\n\r 78%|███████▊  | 29/37 [00:03&lt;00:00,  8.22it/s]\n\r 81%|████████  | 30/37 [00:03&lt;00:00,  8.46it/s]\n\r 84%|████████▍ | 31/37 [00:03&lt;00:00,  8.58it/s]\n\r 86%|████████▋ | 32/37 [00:03&lt;00:00,  8.55it/s]\n\r 89%|████████▉ | 33/37 [00:03&lt;00:00,  8.68it/s]\n\r 92%|█████████▏| 34/37 [00:04&lt;00:00,  5.72it/s]\n\r 95%|█████████▍| 35/37 [00:04&lt;00:00,  6.42it/s]\n\r 97%|█████████▋| 36/37 [00:04&lt;00:00,  6.93it/s]\n\r100%|██████████| 37/37 [00:04&lt;00:00,  8.09it/s]\r100%|██████████| 1/1 [00:07&lt;00:00,  7.04s/it]\nSaving predictions to /dbfs/FileStore/ZINC/virtual_screening/Chemprop-Feat_SLogP/fold_0/model_0/all-1825_preds.csv\nOut[9]: \n[[7.783344350236434, 7.91078807226859, 6.665475520712388, 6.622547156599178],\n [6.8854536228630225, 6.968761293937778, 6.351577780955328, 5.586660958550681],\n [8.205988762897784, 8.166738238746262, 7.27715258063684, 6.852073798091436],\n [6.668993770579485, 6.564092729970373, 6.117653478789947, 5.268403597286792],\n [6.639674977530826, 6.785585453040344, 6.204380820006107, 5.329047504055187],\n [7.382541703357999, 7.226478797281626, 6.604832468690959, 6.013217149662152],\n [7.217781911305193, 7.083110605235154, 6.045124558280376, 5.869792552317546],\n [6.803565509780166, 6.7656665677258845, 6.331889114704026, 5.579774049858067],\n [6.036932482151425, 5.935410456431465, 5.34115674142539, 4.906029131868722],\n [9.008164047647249, 8.818801503980152, 7.867015728962981, 7.637727613170808],\n [8.932043369972272, 8.567932702638908, 7.931738662989423, 7.774020708834813],\n [7.265399482231216, 7.353107980338085, 6.869240453014007, 6.082651116936325],\n [6.202850512580785, 6.2029379488986205, 5.783941714706484, 5.133243603523924],\n [8.800752789293503, 8.892737677426902, 8.149022909924472, 7.736399466559119],\n [6.580645510482574, 6.878876808172409, 6.266668204852259, 5.587724440362646],\n [5.84426324114187, 6.496395808306446, 6.104061450387615, 5.168092791935768],\n [8.637235904022553, 8.470209512435385, 7.66825789636274, 7.264060098251839],\n [7.069436318918966, 7.06593530498591, 6.091743936317769, 5.8695879419510515],\n [5.971849874269737, 6.142972118526391, 5.740027831684891, 5.00562981231018],\n [6.60629580760564, 6.525006820926062, 5.828368032329443, 5.48232208582342],\n [8.949023921309877, 8.759897610700422, 8.42520081868688, 7.891701139277175],\n [7.49926018639247, 7.839899700090195, 6.72800030697727, 6.556232735507438],\n [6.831888727927844, 6.811698476241874, 6.504213042957558, 5.472394764385074],\n [8.824652445701659, 8.653386783773415, 7.7346382832143865, 7.457128851954475],\n [7.927141762142907, 8.047167764252567, 7.021849675831609, 6.75346686753327],\n [7.353748011353952, 7.687264343618537, 6.688908011961936, 6.278374541956879],\n [6.735521405839519, 6.602469690647988, 6.280904792832977, 5.292586764358298],\n [6.431911773615195, 6.086619581102406, 5.488501697927352, 5.08785443688901],\n [8.345208252025023, 8.227278556069061, 7.157120300758734, 7.005149025949708],\n [8.835856235402805, 8.63143084101481, 8.303750994563803, 7.818635722446022],\n [7.935181927715662, 8.050242881434631, 7.121406685793182, 6.6575491736887],\n [7.594972288612431, 7.5082520545323055, 7.076519707670942, 6.047725786794425],\n [7.92881387720884, 7.901115487968022, 7.051233328709574, 6.690165095031217],\n [7.199895579908454, 7.0643640760239, 6.254143058863848, 5.8240897038431],\n [6.945065740063497, 7.104168506383518, 6.524606635394285, 5.723961616276311],\n [7.387692778183266, 7.226959686652742, 6.683705980453775, 5.886605412414624],\n [8.412322542932886, 8.40370821904284, 7.525787446013691, 7.126425823295681],\n [7.024566308737957, 6.914474972939342, 5.896872934286885, 5.7237637162798585],\n [8.749821178911468, 8.210251031336886, 7.834432268620649, 7.209567196261585],\n [8.631515099757577, 8.469469644545015, 7.966679536702128, 7.597496819825751],\n [7.672877474364441, 7.346876348402287, 6.80241615557093, 6.099081227312232],\n [6.965374260416468, 7.16236843033017, 6.210655685394085, 5.947379752000686],\n [4.400158202888691, 4.546976508018582, 4.314171202289415, 3.8201362497207416],\n [6.136363829029582, 5.9615792668162815, 5.419081462949305, 4.867446740924185],\n [7.4253155764090595, 7.780144667146724, 6.920234529690609, 6.334405544640108],\n [8.692551917940774, 8.33378212075919, 7.882010015227535, 7.528406178848759],\n [6.3507702370869215, 6.209306710572427, 5.846714320498086, 4.921758910281341],\n [8.940502617995989, 8.445404352075162, 8.116456001716328, 7.882830924799254],\n [7.173704152285404, 7.409960044123657, 7.0936129505987955, 6.131436401848883],\n [8.597607084845057, 8.21863907998127, 7.689942555266142, 7.434068441630295],\n [7.067148523307922, 6.977442417853725, 6.477371031137617, 5.733212196896044],\n [7.30223521551585, 7.564996223209571, 6.943808176566859, 6.305827315860855],\n [7.402813591361209, 7.210015113720961, 6.567876372640847, 5.999770336996973],\n [6.413559668841075, 6.33577372430331, 5.913491374377277, 5.248846503401658],\n [7.259126965783829, 7.332832984086132, 6.782511398773665, 6.208354165230799],\n [8.05887300712211, 7.760338236469973, 7.097293317635529, 6.8273025230056055],\n [8.49982905111688, 8.65017005297224, 7.617027775298997, 7.355390694976024],\n [6.804860839233091, 6.503896543645065, 6.077296016529667, 5.1721843282286395],\n [7.23892647424525, 7.117213849544236, 6.92552545589591, 6.374571317295458],\n [7.649811603715622, 7.5286583280946315, 6.9387278154156675, 6.27705036226883],\n [7.742213290855262, 7.402194875851366, 7.006078948425388, 6.161563052010034],\n [7.310274454444098, 7.35436248361885, 6.757535466651149, 6.205264109726702],\n [7.554308254326018, 7.465629625249391, 6.680043613862971, 6.138483373836877],\n [6.457771253262644, 6.42413380052775, 5.980429178689083, 4.969579691340036],\n [5.254043778323155, 5.603030968580209, 5.1156477390122514, 4.442356318308624],\n [8.360166745482612, 7.978872258215705, 7.605813217559053, 6.7787783833271344],\n [7.366657170702107, 7.660497251453989, 6.703356881063616, 6.265250623910101],\n [6.486669170354844, 6.195888957703952, 5.709534419063044, 4.997266678128342],\n [6.591555643550462, 6.399567413713349, 6.037724001719362, 5.267248239325019],\n [5.777938690487531, 5.587282147707939, 5.391396363306216, 4.426788819011492],\n [6.899022866963201, 6.590877730150858, 6.112534779975011, 5.297334071563406],\n [6.828700592557671, 7.065656644154208, 5.99478932348324, 5.863859187207727],\n [5.645092303574263, 5.481960457052682, 5.219487358110582, 4.616323332653144],\n [8.886232815617259, 8.692896653403926, 7.858070566185727, 7.530280608880154],\n [7.333923901889645, 7.1078498828024905, 6.377560639645409, 5.896436244323759],\n [8.210396780923006, 7.885275393982269, 7.5167535832702885, 7.146849170033331],\n [6.982939614112889, 6.984183369167505, 6.522549010383708, 5.7100305525531265],\n [6.450296639756381, 6.108276891877897, 5.832779468187069, 4.905557169175853],\n [7.023199089606391, 7.455338072797379, 6.655135962336652, 6.145749841329908],\n [7.432085207742375, 7.676910539492079, 7.162042102200953, 6.3562919477657776],\n [7.721311314479454, 7.6974024813165105, 6.707186314673539, 6.341211481534231],\n [7.516608395147401, 7.528981002483167, 6.668748510517102, 6.190044375356933],\n [7.159536893669252, 7.624691535258113, 6.640333047952962, 6.310830015555734],\n [7.242801701571046, 7.293388034451953, 6.695612660816521, 5.932901094122298],\n [6.675769753911434, 6.706827154306915, 6.605399184555912, 5.5092445377850785],\n [8.723141828118157, 8.879403220214733, 7.954022509150436, 7.66716276319424],\n [6.846820139452442, 6.914868619188272, 6.299603495411585, 5.7476215142840115],\n [6.038720248430109, 6.2771349026696095, 5.620317052672016, 4.882473496235531],\n [8.103312426525221, 8.126494333021661, 7.206335004754081, 6.887747523222041],\n [6.496082802436644, 6.587066651281591, 6.21413684262759, 5.38585145748749],\n [7.3275099974256355, 6.786288624627705, 6.265978808463323, 5.572127750955836],\n [6.200938635721759, 5.870241453404271, 5.715052413773896, 4.6685102159788805],\n [6.670891658095639, 6.729389713912579, 6.451133245239664, 5.497072373889085],\n [5.8668819759245885, 5.830818400514615, 5.63867453787261, 4.87745436312402],\n [7.192324146997151,\n  7.3547208364968935,\n  6.752102818882205,\n  6.0986358894104935],\n [7.474730292152924,\n  7.4553868246886905,\n  6.7689835920434485,\n  6.295083852656266],\n [6.589703071939847, 6.572317763403633, 6.0703517146792745, 5.254744023951508],\n [7.062019067696825, 7.327617618300502, 6.655907218765698, 5.996586643870933],\n [6.943881368817276, 7.271508929735612, 6.699651624972166, 5.80904340457901],\n [7.062952706874668, 6.573495951303119, 6.225183055426291, 5.457622220258774],\n [9.332176000561997, 8.9291355685215, 8.29162733519495, 8.219217077685094],\n [5.657714516988893, 5.596271146453937, 5.323218706778222, 4.5670441645055835],\n [8.439805264621494, 8.580189377878115, 7.539787825302994, 7.338645637576233],\n [8.183092034763144, 7.797085842782086, 7.380902380065826, 6.897604218015709],\n [7.018948840281325,\n  7.219950458955824,\n  6.7040482004210835,\n  6.0128888448079065],\n [7.123281957247827, 6.842640997000851, 6.6706122078025665, 5.620091910254403],\n [7.401501492631894, 7.242046268545505, 6.535927029427537, 5.922689225034347],\n [5.062400302890435, 5.037102674205042, 5.085477482469056, 4.16618824329234],\n [9.266116051751874, 8.81064502166166, 8.386513612814428, 8.213846768541451],\n [6.94858776637356, 7.032374472747854, 6.5170799905169545, 5.741062379208014],\n [6.393829434443619, 6.181974401812718, 5.739765723771747, 4.997250796919245],\n [8.916466176901984, 8.67991452471421, 7.707589661813579, 7.532599936445253],\n [8.816098620154772, 8.469964356923823, 8.135521528798193, 7.609236164854113],\n [7.916606830424838, 7.630222113758972, 7.076146992555223, 6.4537090178762115],\n [6.332689190776804, 6.435667993231537, 6.0229385644420645, 5.151703831506142],\n [5.9625939511045765, 6.069237747237722, 5.756538780268555, 4.853463671833184],\n [6.180888798644867, 6.055096631562687, 5.790464423962556, 4.815240397525331],\n [7.6090380946010105, 7.504420241151499, 6.535100201642092, 6.133568468150031],\n [5.985935588169326, 6.188810697493989, 5.701191753970239, 5.164158949338854],\n [6.591554627230682, 6.663641272409672, 6.11772294907079, 5.271967362975956],\n [6.891969727248561, 7.21601323998352, 7.0394627653091515, 5.734474753019219],\n [8.223340749223667, 7.85658757770598, 7.19025633462784, 6.7850342935569135],\n [7.506518988876955, 7.250439997689566, 6.708993576483833, 6.09225317415685],\n [8.325550714383683, 7.8026747667542224, 7.480573295486124, 6.557291939458501],\n [7.806341215761283,\n  7.584785681158467,\n  7.0844288772376345,\n  6.3538963876010275],\n [6.694788684029068, 6.829044329023848, 6.292994228230259, 5.374570878090918],\n [7.562298810789452, 7.577617784589164, 6.596421014875777, 6.379607408071259],\n [6.176953847586046, 6.315066886287639, 5.962838738859866, 5.092194704231394],\n [7.921163679514491, 7.62656915359263, 7.394607352442619, 6.880464479018019],\n [6.088980489921372, 6.643850686613382, 6.077772717994286, 5.219005935188261],\n [8.40387435485973, 8.120577810644319, 7.637754552460381, 7.1983927525511575],\n [6.465856734740938, 6.604301314815614, 5.9153054882842975, 5.318358164178958],\n [7.487582440445116, 7.070948889244813, 6.631864586640326, 5.744071365054055],\n [6.582280828794074, 6.745680011567793, 6.146531987308002, 5.261787787543826],\n [7.259580453648561, 7.1324765681897535, 6.39894234581042, 5.991984392498207],\n [8.726031763307816, 8.41157146103893, 7.85634242601099, 7.544556473783935],\n [7.993575507390211, 7.869494608264349, 6.850944594086546, 6.569639048293433],\n [7.140504153559296, 7.2276328327418, 6.773062296534504, 5.903328828871032],\n [4.927978500749241, 5.128831548069519, 4.595888526171624, 4.0115189111111285],\n [6.915459507867302, 7.026617361971668, 6.200726692970337, 5.577344728144049],\n [8.402059506648053, 8.367297784084545, 7.636097781193643, 7.111840107055267],\n [7.100765123469055, 7.383502635471259, 6.9518178151539365, 6.046101961124179],\n [8.309247330937954, 8.273399041868178, 7.462347837897054, 7.0208888139976375],\n [8.70545343994469, 8.49916328062884, 7.472574087177723, 7.370512849381449],\n [6.4579367340364335, 6.537027253398966, 5.858016994011208, 5.204602014056202],\n [5.732836331669479, 5.955826144768683, 5.686431729087195, 4.788001104258248],\n [7.253808205667601, 7.299361575016749, 6.739153792210335, 6.028574474577632],\n [7.1761028762113925, 7.408005505221454, 6.733044563785763, 5.994953255923379],\n [5.496021148539519, 5.58228308792279, 5.276009878032742, 4.874406289372492],\n [6.502429181418719, 6.720445003808146, 6.227929443950084, 5.378180050617357],\n [6.849431304104464, 7.446771191569888, 6.590731048358986, 6.1764853422377755],\n [7.616305909450614, 7.644891612183792, 7.047826071886945, 6.324067073237387],\n [6.853046413349784, 7.005455946439325, 6.3031502769888705, 5.545237956128377],\n [7.835312756254106, 7.626399109966651, 7.053850075076477, 6.374484418003429],\n [7.920876210475197, 7.842685373119132, 6.909007825605136, 6.523459344147963],\n [8.239679703861738, 8.040738318887275, 7.408193538915238, 6.767433608057171],\n [7.287718357840837, 7.030554402138897, 6.311383551947792, 5.867939120152403],\n [7.567843769313457, 7.589475463181441, 6.637851031090076, 6.222525212057551],\n [8.551810161131817, 8.276652413955418, 7.367636380024252, 7.1135291631142845],\n [8.251100806645516, 8.176589132968951, 7.181149423421644, 6.911589467976973],\n [6.557486093576245, 6.45688560857426, 5.999743571976742, 5.262212106609405],\n [7.700049740251644, 7.932286494103912, 6.986863514992937, 6.519427390349079],\n [8.402512307002345, 8.165234845678722, 7.079391429928464, 7.074640499697136],\n [8.042926292136073, 7.713745925341601, 6.655965151235356, 6.634944313610664],\n [7.138229330971341, 7.091219470386846, 6.55821586557018, 5.827898649677628],\n [7.630035268753972, 7.13219416620572, 6.909500957497074, 6.356520449845604],\n [8.875163897529433, 8.68030690557338, 8.377251622715713, 7.752628943860776],\n [7.554234608506577, 7.85818202381922, 6.802193180980187, 6.5135597868656605],\n [7.6413318349998525, 7.97542616176648, 6.9111469942179635, 6.720920342600924],\n [7.561439893534347, 7.436106166895349, 7.166039515631511, 6.13428610727605],\n [6.380825563059382, 6.22626491395259, 5.653314803196528, 5.153033715008726],\n [7.796399964254239, 7.610465060791294, 7.125860865103211, 6.203849658086682],\n [6.85385964852531, 6.591193747488923, 5.829992478251766, 5.579521851784033],\n [7.404592666611175, 7.072846643781582, 6.598022798979088, 5.829627296919887],\n [7.97074708192322, 8.096243815409998, 6.912584352341213, 6.796221039612416],\n [8.867333093831611, 8.569743200299254, 8.28804068244737, 7.809147482887344],\n [8.445888177646168, 8.396640302018733, 7.398159148342908, 7.114150543380061],\n [8.523206737642104, 8.22404675062467, 7.219649152033043, 7.135552262069659],\n [7.075661875414109, 7.043158564409305, 6.626206653371463, 5.587107869195398],\n [5.759612412192556, 6.171462754068066, 5.323415385078576, 4.901737179190632],\n [7.7982245272873545, 7.530030835847538, 7.034865483062516, 6.558697873821548],\n [6.057469913589236, 5.839332933117948, 5.521728744193117, 4.601862932573241],\n [7.265253042507445, 6.806952989561032, 6.481676435970944, 5.579848982323522],\n [7.102594319724703, 7.313072451296618, 6.763607245872119, 6.090435950030054],\n [7.638736931464329, 7.501073574078009, 6.9428364324124585, 6.299723905780222],\n [7.76153732614957, 7.585801871660333, 6.892752047643081, 6.3881973248006005],\n [7.950006924580813, 8.05469303714912, 7.313510998594517, 6.824491325316516],\n [5.084267200169116, 5.46046995765759, 4.9161265609294515, 4.3032804321887035],\n [7.402559683293513, 7.445552817229877, 6.859050743178093, 6.120303911931148],\n [7.1458999447365885,\n  6.711220037380575,\n  5.901330813485734,\n  5.5497991064846035],\n [8.287853381058639, 8.478972061431474, 7.580323174323049, 7.170207632626824],\n [8.637649247255927, 8.32115364099863, 7.347403830116627, 7.213062963534336],\n [7.41678668806151, 7.546243174490211, 6.690360133128399, 6.129872508171064],\n [8.15204842747872, 8.375546034669036, 7.571599225950946, 6.961808255689496],\n [7.2648786183433405, 7.302697211192559, 6.6806720837932705, 5.93879154082373],\n [6.1312144356157185, 6.074913295391401, 5.72688777391139, 5.011766781230985],\n [6.0506703755517695, 5.884370025215608, 5.428253876792413, 4.863567028645601],\n [7.041329457317655, 6.502329330920117, 5.854019556239276, 5.317257719412019],\n [6.771199012841693, 6.925592357259915, 6.057226797240281, 5.6963599630270245],\n [8.95907221520329, 8.779618655076915, 8.240534304748483, 7.778676028050612],\n [6.792066090589719, 6.296587904484987, 6.119172964708419, 5.182637854517685],\n [7.793268832467089, 7.374961505613075, 7.017745099502869, 6.081173095025121],\n [6.267165978378199,\n  6.7781429806475835,\n  6.066464835427169,\n  5.2724900448823835],\n [8.413747781967258, 8.51228911358162, 7.641888496656618, 7.214678820640158],\n [5.867991438424582, 5.694161698207318, 5.5027531828288385, 4.574337106505416],\n [7.80647166339438, 8.027209871923363, 7.331283317168813, 6.645572561159646],\n [7.125901551375682, 7.291275198041752, 6.360095898670528, 6.05807147184494],\n [8.912466300944198, 8.691696403709077, 8.081367424547368, 7.6677683740904214],\n [7.5766186817774575, 7.411288816843019, 6.606071249886699, 6.181523568449043],\n [7.6854904213340065,\n  7.3886423598332005,\n  6.9003490025786505,\n  6.105923036726197],\n [6.714504988867169, 6.825446330786601, 6.533366073489399, 5.701487804133196],\n [8.81039850098652, 8.340555146952513, 7.910118167947374, 7.445447887145468],\n [7.940448257687882, 7.5305740594185195, 6.788033169440128, 6.313843614781178],\n [5.1340405647016585, 5.4084556164629, 5.323078500465099, 4.469575368626085],\n [7.107926441456878, 6.803555693142947, 6.169437463473274, 5.6689805348655895],\n [7.1517865585846945, 7.025114931700683, 6.719148627246119, 5.53075887885186],\n [7.563861473446004, 7.225896167823042, 6.70776801265407, 5.923301098942566],\n [8.820962846194728, 8.684125961899822, 8.024995918508301, 7.596640241090044],\n [7.821352258929915, 7.891796332528828, 6.53443865178479, 6.724278379528587],\n [8.254611414303918, 8.245453679278723, 7.357931425605015, 6.923248400404346],\n [6.261264986594365, 6.30009281404953, 5.909035247757343, 5.107260603367978],\n [7.112452292793204, 6.918435092715436, 6.563408318747748, 5.670180684547385],\n [8.101261792124301, 7.947934193829357, 7.148689909354732, 6.669413499359072],\n [7.2110919265617905, 6.890740332315202, 6.459084720114031, 5.665582459396683],\n [7.519229965867772, 7.801140344129115, 7.05712330829493, 6.355268378675618],\n [8.33004195106393, 8.297173184659577, 7.5513509515158415, 7.034238312317287],\n [7.211588189532609, 7.074555057498547, 6.4634351564889, 5.989064752433271],\n [8.422219823663514, 7.98678072414144, 7.662706602652522, 7.034613533842068],\n [7.075270831432405, 6.864063000298121, 6.2518443082043476, 5.519864481251135],\n [7.921802346821713, 8.08006630249417, 7.0324285951066665, 6.7593894401312715],\n [7.298602350566118, 7.800197298657011, 6.8758246911777166, 6.254008573221512],\n [8.550533304785503, 8.704828508593703, 7.644850549752333, 7.42817338154946],\n [6.841435198983401, 6.574339471119833, 6.204275227126536, 5.36469131211936],\n [7.615761886923018, 7.372951980711863, 6.7948105685756754, 6.270826396196433],\n [7.18625797335711, 7.452672161344297, 6.541755181923426, 6.17885618936786],\n [8.612367874213442, 8.449469004048737, 7.931515014446893, 7.4378345249761875],\n [9.051606337777951, 8.961623075159226, 8.305415457704663, 7.84405974877757],\n [6.4416756175369, 6.458195231957841, 5.702573370347473, 5.226691098316978],\n [6.228288638003642, 6.167971378672249, 5.795707263783902, 4.999861354544114],\n [8.375014878568667, 8.538771740626657, 7.3001867253040515, 7.282681151436034],\n [8.516310709008938, 8.383198671795675, 7.571113080033291, 7.209877886394474],\n [7.95952003643414, 8.141816882596, 7.093344197490603, 6.778164272628901],\n [7.767028143226407, 7.903896154454871, 6.919969957213367, 6.615241353056767],\n [6.938281207688682, 6.968752243650154, 6.676755690625425, 5.672048236449488],\n [8.912558726260771, 8.646773966926967, 8.331039037717368, 7.859190738503549],\n [8.768198273195503, 8.555137961667409, 7.99364101598471, 7.599133590918201],\n [8.02382557816781, 8.031613318250663, 7.553909619364843, 6.829961507134269],\n [8.359993134150574, 8.077564681957647, 7.374989032761616, 7.2692681282799985],\n [6.439791958497683,\n  6.492705271565958,\n  6.0115848227266975,\n  5.3170513196135145],\n [8.174113028846607, 7.727388807553281, 7.266489403696353, 6.784356993540796],\n [6.725465459655987, 6.488025530135636, 6.119949600581076, 5.326078724509641],\n [6.66399604794709, 6.605700725855135, 6.361033138927557, 5.319796699756453],\n [5.6727434952124876, 5.917007398014351, 5.476967099804606, 4.706424030388352],\n [8.23072233957819, 7.743148741848651, 7.523791550727087, 6.481138578093827],\n [6.779099225202222, 6.492055631467095, 6.174412256086268, 5.171848362368597],\n [8.187730219327053, 8.003700140574644, 7.084737793612608, 6.791134858720247],\n [8.19798596202287, 7.809858742026005, 7.4580001764388, 6.6638180013808075],\n [8.175961535178098, 8.196665586816875, 7.507937137677721, 7.08304736312374],\n [6.487255885786191, 6.7209436223901236, 6.140713863458121, 5.325476748397231],\n [6.840600860226536, 6.863601628188614, 6.374852953905967, 5.590559348025879],\n [6.781576474777189, 7.034504893949434, 6.617179715748908, 5.752562527504296],\n [7.828777700493823, 7.987633156703413, 7.02032911889269, 6.728378751142013],\n [7.801989155107546, 7.340220095676938, 7.14983799459167, 6.578474508646517],\n [7.281855223968853, 7.487786183863179, 6.646302161344561, 6.082403119833536],\n [6.255811773349433, 6.74177325822936, 6.162234120722837, 5.2159896238549255],\n [8.369688346595288, 8.324067778596577, 7.424257239096589, 7.093071761248677],\n [8.95577216509015, 8.700969619998325, 7.802187737567192, 7.570494961818611],\n [7.9992329411276515, 7.642835133757694, 7.208094009790805, 6.332865794314499],\n [6.628033751839141, 6.731123627984035, 6.065028597007115, 5.405577596777903],\n [6.59499977193847, 6.626380660584192, 6.295870161545449, 5.29503811845395],\n [6.276519826943456, 6.403357201024498, 6.034803718421097, 5.1035071578856614],\n [6.955657406338765, 7.529949562063998, 6.795323891637185, 6.206171889983814],\n [6.871465236528365, 7.077779380638277, 6.463263963606922, 5.631701745140977],\n [6.710302446788305, 7.208331567663606, 6.564082331388439, 5.76412866110598],\n [7.36126559426273, 7.605442357473584, 6.711197809588844, 6.2641029758839055],\n [6.675715769395996, 6.847265446400401, 6.044502100669407, 5.6083085004627735],\n [7.395568120600912, 7.387062757979715, 6.981774123192064, 6.519979877482437],\n\n*** WARNING: skipped 32314 bytes of output ***\n\n [8.320795772611966, 8.091773028340745, 7.506269266744527, 6.932298061361108],\n [7.7748021226917565, 7.876462174378876, 6.821216778524336, 6.593768364645361],\n [7.973732461497039, 7.632834744738371, 7.309808286176629, 6.4816754076970895],\n [7.281925051116175, 7.032167388962952, 6.830405771885678, 5.754880652794763],\n [8.01128024635545, 8.01358261452436, 7.269115302419806, 6.813414964848185],\n [7.279686756256827, 7.442920968143513, 6.612285870374507, 6.182549720332672],\n [8.734023265098106, 8.401493676930764, 7.942373798667286, 7.651788522554282],\n [8.576895085538943, 8.181478488963693, 7.904309926695259, 7.289872878687751],\n [7.737723713167598, 7.848455110284435, 7.445864638479529, 6.568644962891639],\n [7.0831320051566315,\n  6.724046578146885,\n  6.287973187007033,\n  5.4631003426810745],\n [7.96546846643835, 7.4842879794889345, 7.133050260246457, 6.729924988158662],\n [8.847284930693181, 8.643241989024972, 7.679705644463746, 7.534141532122623],\n [8.677504287260017, 8.436137187649978, 7.7750820599861825, 7.526465763511964],\n [7.431421042765348, 7.62327348722812, 7.138020574046672, 6.190783354857675],\n [7.920733297978902, 7.9389592244046, 7.286700825291514, 6.614900270539569],\n [8.835276574426409, 8.559554502027867, 8.25097013325762, 7.80861546238261],\n [6.30287096623095, 6.073020272311992, 5.317026650743934, 4.981473710397839],\n [6.33837591843735, 6.435420857109916, 6.050411115209079, 5.295834751217292],\n [6.6756732035322175,\n  6.712166218818603,\n  6.1047990913794346,\n  5.258035348617015],\n [8.084133396150287, 7.92710719880353, 7.35355625839436, 6.738367304706982],\n [7.302191767845203, 7.3516970260555725, 6.9686215782938, 6.044650284405354],\n [7.605453893433744, 7.830596994717866, 6.764111027115097, 6.522182472568701],\n [8.442137778302913, 8.491895762124495, 7.698418026894433, 7.17418632286351],\n [6.950391494851161, 6.7240938927204805, 6.485423571396027, 5.706935841729814],\n [6.438499678004102, 6.367664077008697, 5.918587873859302, 5.237383401939975],\n [6.9003526317216535, 7.110719814284292, 6.438840267274616, 5.587427003210587],\n [7.526209609221762, 7.487626002024778, 6.596978748773797, 6.191173793544518],\n [7.456401129303277, 7.424608847580367, 6.765250811943567, 6.109491164155887],\n [8.920643370769834, 8.716011253046174, 7.731785182107826, 7.58085364821108],\n [7.293284830957858, 7.513050529409304, 6.877890884351873, 6.15100586797878],\n [9.283608947826249, 8.853788430476559, 8.379888182960894, 8.194197239309004],\n [8.564857076647964, 8.294176521642568, 7.768470942860447, 7.303109754630216],\n [7.503373484758781, 7.169464571047596, 6.655590768735531, 5.852735518630609],\n [8.190127598123919, 8.007379178773409, 7.563214060819474, 6.8634015737572955],\n [7.443130518814106, 7.483169684412332, 6.856137290677853, 6.163444489485143],\n [7.872574835686962, 7.820368711753803, 7.350045453367559, 6.52267898403197],\n [7.105560419114438, 6.759186396736343, 6.476967329453043, 5.499927188983439],\n [6.967751073683644, 7.342021536172542, 6.707540238248679, 6.081643547884731],\n [7.211268168368557, 7.171179696831978, 6.146213796869608, 5.955017942539118],\n [7.774972834523243, 7.80018827587786, 6.860090013346103, 6.474985140806545],\n [7.018982737535205, 7.0497533907143985, 6.43255006682114, 5.810381005007142],\n [8.38288215021158, 8.12813939472526, 7.62542613345507, 7.159125009146455],\n [8.25775740229493, 7.981748544154774, 7.427975189214479, 7.128664738259791],\n [6.9752130130844, 7.1971013160782285, 6.542001346236726, 6.073354227773342],\n [8.203622710663586, 8.064191547832412, 7.389409836259299, 6.999641433818978],\n [8.493982701467166, 8.118175275627616, 7.658742561245452, 6.77286212966092],\n [7.752003439524246,\n  7.5310790324577646,\n  7.0862719086967365,\n  6.545992179587562],\n [7.924742799082852, 7.966128297885404, 6.931770125809681, 6.675492870937027],\n [7.119388974217619, 6.787174562509818, 6.2413279584306585, 5.627279611284096],\n [7.501799577196413, 7.364130646108928, 6.875220653901375, 6.08334602468383],\n [8.699231051975923, 8.142590640925132, 7.597534034769134, 7.219676368728037],\n [7.698642002841741, 7.607881795106844, 7.029718353522975, 6.246586823571755],\n [8.537021871345821, 8.221758540821842, 7.82307896505001, 7.393057903286498],\n [7.629168220940619, 7.239154536598835, 6.974816263196461, 6.484233708315267],\n [8.494136644022262, 8.305131881057047, 7.396317041856011, 7.057773145803352],\n [7.723467870325809, 7.654318765888067, 6.695710853918454, 6.422010599837396],\n [7.698923030207128, 7.665169593105067, 6.797043261310674, 6.189783268567674],\n [6.5180656797374175, 6.73177447845571, 6.674900549332855, 5.459786202757428],\n [6.592122749988384, 6.532665509916766, 5.877884618042732, 5.293375736115844],\n [8.042746941586445, 7.618689337741546, 7.15062762875794, 6.636207988128845],\n [8.212849160555091, 7.8947885391732076, 7.335898441642448, 6.720338106160384],\n [7.835316791641472, 7.465365396050347, 6.9625792582204795, 6.511311701062494],\n [7.274750371512405, 7.380197079810698, 6.737649921283159, 6.068833423516985],\n [8.047245232721645, 7.738650748899579, 7.113473686021533, 6.4750869706719145],\n [7.052386478485689, 6.776838418823039, 5.9836935516474705, 5.818463058563896],\n [7.470901647395925, 7.472341813016068, 6.691924054224754, 6.130918186532514],\n [7.832633617743775, 7.4361275547331225, 7.052779395408242, 6.507009794668844],\n [6.631958420133116, 6.915661413380735, 6.265133919378513, 5.503679348311498],\n [7.95377301709716, 7.476639922523052, 7.185139388391818, 6.604476912955832],\n [6.482834416469748, 6.897322889841329, 6.7402599737722655, 5.452604876579274],\n [6.3073855782661665, 6.461465989400031, 6.041034477239738, 5.143866231137999],\n [6.9046125061928825, 6.882071092059601, 6.272398505025447, 5.606223476651598],\n [8.038870638157352, 7.745601067201521, 7.229096672082907, 6.672054533247876],\n [6.398055172526902, 6.2675197010437635, 5.705466878134551, 4.98019750985558],\n [7.8179416093111715, 7.569053530420847, 7.399993319541945, 6.688320078788502],\n [8.738709694959864, 8.671299971302973, 7.800936201491577, 7.5438791737678175],\n [6.755865616951865, 6.701451008373596, 6.582937816755733, 5.5016060117281445],\n [8.628122624327862, 8.718517384972273, 7.776928986065094, 7.425152708475597],\n [7.921295561951983, 8.100577170148041, 7.075480525740412, 6.71785001278876],\n [8.913956943145665, 8.938873017801452, 8.093858346564208, 7.746642846426387],\n [7.7198436739859755,\n  7.399745037436385,\n  6.6979470959300205,\n  6.292240571010419],\n [7.636699703516993, 7.605917483819602, 6.7442936680204815, 6.301653486665389],\n [6.579116367696452, 6.900134530868798, 6.590268513573923, 5.53524146195886],\n [6.779827627567775, 6.8630099209339335, 6.220823515377619, 5.7956427396878],\n [8.049999160411172, 7.8283295538121696, 6.792641408347648, 6.658418166608983],\n [7.238980040276073, 7.539456476585797, 6.916900844670309, 6.107213175266227],\n [5.666985983868354, 5.813905641131884, 5.206270576326844, 4.615760220767221],\n [5.078396697978718, 5.0887492821323885, 4.825299548171374, 4.211285956188482],\n [6.861972989122312, 6.946541875024577, 6.458520925213935, 5.639237938054789],\n [7.88887926534423, 7.544004094820199, 7.086110087243674, 6.304364853620183],\n [8.526618343797113, 8.401303098229675, 7.509662551618592, 7.143964381952795],\n [7.374817815006573, 7.4969604796834455, 6.731022751021399, 6.046144404214687],\n [6.276778450436019, 6.442329665126071, 5.923713290893408, 5.179070398124999],\n [8.070327289757568, 8.349755080669635, 7.661368508651654, 7.082815240240079],\n [7.458786917570874, 7.5687274862443665, 6.89668766441089, 6.0471644923003876],\n [7.255392110154876, 7.081256864255219, 6.647637212673704, 5.792548923580493],\n [8.269285936057933, 8.03166420892575, 7.458228985352577, 6.962560320411817],\n [8.57691397713017, 8.414051294864821, 7.3732777855003, 7.279820520687644],\n [6.96977187610981, 6.550341574408883, 6.318189205332985, 5.27261619983915],\n [6.8105109793815135, 7.2419270743319055, 6.537356184507814, 5.76400345678497],\n [8.549507060940538, 8.524763215791584, 7.716896926867572, 7.303249554006066],\n [7.803319009541274, 7.641294301658371, 7.021983723777159, 6.388957246247845],\n [8.814984853241587, 8.874223704846568, 8.185066545236088, 7.697905428820119],\n [5.845870102499497, 6.534685842209316, 5.991096737076856, 5.221510916324489],\n [7.030593234499388, 7.235160856485655, 6.686224375499814, 5.822265853225888],\n [8.411645793525626, 8.222119507004823, 7.424374418470091, 7.075769631296],\n [8.28150879558206, 8.074073086494046, 7.5411571736278304, 6.899580981190225],\n [6.821447656330747, 6.947488221513444, 6.762709323019129, 5.640218434957249],\n [7.04132297080611, 6.989896548912277, 6.535615119063586, 5.784405106659488],\n [8.023387962826718, 7.592054877679668, 7.144291155352859, 6.305956930852173],\n [8.231701414228604, 8.242709323976264, 7.697824681566535, 6.92893432061891],\n [8.944346339408565, 8.516253634713395, 8.135888791446181, 7.902306655447644],\n [7.9287952845351946, 8.106112865224882, 7.068803273102046, 6.751513366974894],\n [7.289988098829886, 7.141899953233224, 6.956895900386677, 6.296750918273461],\n [8.034044434433918, 7.989888191287777, 7.02544308331042, 6.68463977633963],\n [8.061504079685141, 8.085798188000597, 7.003288477673631, 6.804353057466098],\n [7.210287210537372, 6.841655093327781, 6.3260815550103775, 5.667749126043354],\n [6.342674951111914, 6.044548892701432, 5.834060992835212, 4.882925104139276],\n [7.472390828637708, 7.378225382498449, 6.626615758841023, 6.239181437438943],\n [7.916157138813406, 7.62578513460306, 6.715300560314548, 6.453511313598884],\n [6.915403670062852,\n  7.1728956478705514,\n  6.607794497445301,\n  6.0508668970305814],\n [7.168693905006293, 7.266736470996973, 6.773764119194768, 6.067346419495883],\n [7.8829849394224905, 7.681942746987218, 7.162850162787194, 6.488589297669243],\n [7.7134253156499835, 7.73341866483947, 6.846695425433391, 6.556960698817363],\n [7.330808343708554, 7.1865054511434225, 6.550085311208721, 5.966475787544269],\n [6.601929757609033, 6.821831497363866, 6.296011584927262, 5.524974372117447],\n [6.644619253699433, 6.845921877561112, 6.2303126591766915, 5.362196732056697],\n [7.890401592809466, 7.915370598755256, 7.105213951989285, 6.621857526278111],\n [8.337385997370065, 8.221969860911589, 7.454347120423477, 7.106467840720359],\n [6.907824435402639, 6.434370913711653, 6.26631384313239, 5.313532177885122],\n [6.677945634779511, 6.643509361480138, 5.96859459747555, 5.540393460397178],\n [7.55235403952787, 7.143009617526416, 6.908659494460627, 6.280570398713228],\n [6.05998369089281, 6.249901294311099, 5.767611476481957, 4.979673765473897],\n [7.420317876195483, 6.754299406470281, 6.211837653823361, 5.607140672396675],\n [6.480119049148392, 6.249932653970343, 5.886970863422566, 5.029363496428453],\n [7.8584503820686535,\n  8.091150621630172,\n  7.0595468084951865,\n  6.644662467222863],\n [6.792328659794373, 6.91655620899116, 6.426880620082467, 5.728365911732834],\n [8.422822919778394, 8.089587205074501, 7.601126967591156, 7.184420755570724],\n [7.409438554057432, 7.7043128848241, 6.777197524738209, 6.462880332249838],\n [8.525862082312852, 8.25915790538524, 7.299320805272274, 7.0614012751251565],\n [8.889264916009257, 8.729034204309853, 8.05275852108559, 7.589362844780806],\n [7.729050724126363, 7.913783579929732, 7.6355423110433875, 6.74720603636546],\n [6.8166919373733155,\n  6.969002103110603,\n  6.2028524251448225,\n  5.576008246110924],\n [7.7307900358648896, 7.722828672963654, 7.044532610986859, 6.546323727787327],\n [5.037881408817933, 5.713881202109322, 5.1181605477130026, 4.504896631570298],\n [8.025357411212175, 7.70265860778204, 7.03396222336332, 6.480815753375114],\n [6.795930915583636, 6.579781087218943, 6.232401538511235, 5.3036721279064345],\n [8.783213620777326, 8.681568334112228, 7.653247155329493, 7.4972518320208765],\n [7.862611703412862, 7.52233191571504, 7.253435465703378, 6.677787985250228],\n [5.182994655923503, 6.0486554675878885, 5.856914621874278, 4.788576965847248],\n [7.398996256898245, 7.261651089623485, 6.637615479615754, 6.497706202125789],\n [7.719797042843073, 7.600196670473566, 6.564390420156703, 6.313878550645203],\n [7.1513786556513255,\n  7.1768044368359885,\n  6.710902098409161,\n  5.819142120052149],\n [6.25774421573815, 6.846420111024468, 6.366572846143844, 5.349199528164188],\n [8.115423012605282, 8.093779386328999, 7.049026807515426, 6.780964118608671],\n [7.375106898200814, 7.191696451299075, 6.564502293110715, 5.897371669907445],\n [8.32941757201716, 8.022681592145496, 7.591181130936734, 7.239586260318067],\n [7.597977710610172, 7.468318134913059, 7.1447403023823295, 6.249030827715222],\n [6.5828210326495515, 6.856046646141198, 6.272754716689725, 5.422602923966397],\n [8.298453237675673, 7.888069484603951, 7.591349281146987, 7.099514555284092],\n [7.390196168100841, 7.6785378582305785, 6.675605183421793, 6.331347321453893],\n [6.7377692660615125, 6.459624352148225, 5.995415188886542, 5.189036975228085],\n [7.996164851058695, 7.90956851162568, 6.971689930168928, 6.613242641380243],\n [6.353228057019012, 6.198232459537551, 6.2575357595449415, 5.008145194519971],\n [7.339165272243709, 7.690275998753342, 6.827312428265663, 6.2141182123265555],\n [7.397091509223567, 7.690626539225048, 6.820579125810768, 6.444561721035356],\n [7.828933586013207, 7.747322850036375, 6.9187416123816465, 6.504331602105938],\n [4.672984345909066, 5.208639350065642, 4.821471136899151, 4.173417548616285],\n [7.523283856232753, 7.517098511248227, 6.515354673941579, 6.411306678886266],\n [6.545651707342611, 6.434757132672867, 6.065246793081912, 5.191079947387003],\n [8.039181871144473, 7.954945333365149, 7.0054514643207275, 6.729365734735512],\n [7.651738904721916, 7.730026512505839, 6.80251651961914, 6.4191161655999265],\n [7.58503321920374, 7.7288620513343895, 7.096615580764646, 6.373198277725549],\n [6.758948174631824, 6.633768611183425, 6.600058516753239, 5.5073840876912685],\n [5.9605192239464895, 6.538897389428821, 6.44902959069122, 5.1971569703354445],\n [6.912433146692893, 6.99078741081106, 6.409508984862393, 5.851996371370615],\n [9.053220612291629, 8.933546827255137, 8.226867499645818, 7.8688823022744625],\n [7.827781019597786, 7.537930278994122, 6.849560662236129, 6.358346257654078],\n [8.320163860842111, 8.467628997598236, 7.561281745201252, 7.224589254313901],\n [7.196080225774477, 7.506968763634524, 6.457492891632608, 6.190717020053834],\n [8.082440446528839, 7.595128481895716, 7.442612338842549, 6.7020431769601725],\n [7.727057496847202, 7.508465341477873, 7.144975440053296, 6.221544921359171],\n [6.917361938930715, 6.640982323114543, 6.606937754111448, 5.759960990073021],\n [7.778045139330116, 7.650674635941565, 6.619385944846571, 6.387924282639628],\n [6.539328823282537, 6.40177843474088, 5.9720060896972225, 5.26122914923793],\n [7.830551596996669, 7.655134282079246, 6.574794653562641, 6.412533572188662],\n [8.753548322466788, 8.48810465440664, 8.13512525123263, 7.518901387043981],\n [6.94930911428416, 7.289849585181678, 6.952136796686978, 6.122973304123343],\n [8.66849849832807, 8.537314451759901, 7.739750848003123, 7.3658821348558],\n [7.582938838214589, 7.486862910106059, 7.139333353018782, 6.6298717491777035],\n [8.581033180986974, 8.563577781258017, 7.741734572602825, 7.259290031708343],\n [7.136784184025961, 7.268882365714627, 6.7043562648479735, 5.978105767520903],\n [6.709445031594053, 6.551542594340977, 5.811873246417987, 5.542892122601617],\n [6.268331278682644, 5.915558691791176, 5.6866657957377145, 4.830886631830942],\n [8.320934111669244, 8.20949872961671, 7.243505986452153, 6.997965295222289],\n [6.430837224688862, 6.23824551415656, 5.864218689229524, 4.962549124808799],\n [7.718387825791135, 7.634963584202753, 7.042926396753516, 6.285334328064124],\n [7.8194427136280344, 7.648059130326734, 7.009668862917058, 6.461857476136495],\n [8.694067190951058, 8.441028304186993, 7.3972427929846045, 7.264732477329926],\n [5.890616510260054, 5.718484029849077, 5.443037756221787, 4.84798991115141],\n [7.561936320909837, 7.531823397983475, 7.170350908442794, 6.333232376237852],\n [8.392639477729983, 8.052584732759849, 7.523763217367977, 7.301823376693396],\n [7.999358875105248, 8.013188583156808, 7.16257469145983, 6.735969801330907],\n [8.451669961098016, 8.388170883311156, 7.43906745589242, 7.100677686091162],\n [7.29915614022573, 7.21406788203419, 6.588461263934316, 5.894978136833645],\n [6.938112080120384, 7.295377234030161, 6.541946626828411, 5.671698905769114],\n [8.356191679684192, 8.328891389340798, 7.305700046471132, 7.115723510037373],\n [8.02889617690687, 7.753759337570759, 7.3217398921061525, 6.796577472101083],\n [8.68597728549254, 8.45847054665843, 7.8678904605720765, 7.475342361998193],\n [8.373656716639857, 8.319865749308738, 7.725553693047971, 6.965367659638132],\n [6.580776735301384, 6.45240975994597, 6.064302931971207, 5.29157047881562],\n [6.504873669626622, 6.151066651763639, 5.571101326338971, 4.989878113355427],\n [7.588060230523892, 7.431728606041162, 6.80941778744198, 6.35439229792698],\n [7.139299964077341, 7.398259118266691, 6.752528573851447, 6.175325877669337],\n [4.558242081743301, 5.201151323641559, 4.756643437599848, 4.191226647025545],\n [7.166331110973746,\n  7.2267341309282065,\n  6.7107109456006615,\n  5.863855216905454],\n [6.981186342923251, 7.087947337521006, 6.451903211575899, 5.721751779582378],\n [7.718010845881698, 7.669251190298022, 6.7082561302230035, 6.307607632952426],\n [7.154871447821799, 7.051851379426286, 6.635382864468885, 5.793112147305917],\n [6.861135720973135, 6.9244831055938185, 6.440595986225875, 5.4987774229967],\n [5.277007105295224, 5.7079668804098205, 5.300125169158925, 4.658159246511958],\n [5.835955723683138, 6.536407982654318, 5.99644239486116, 5.233340403990399],\n [8.462180022873204, 8.267167877576423, 7.264577145370145, 7.131575305345234],\n [7.707206783718034, 7.920917462272143, 6.914463348180114, 6.505083443149259],\n [7.852010501666706, 7.523013754481574, 7.042961935159273, 6.193919093302815],\n [4.781212119844239, 5.061498728554497, 4.85000643204654, 4.567994688421861],\n [8.985387484380858, 8.794297726450347, 7.825040879778781, 7.61384686010914],\n [8.091758185850106, 7.664989522640672, 7.17938920131623, 6.807895741409384],\n [8.048638666925216, 7.731381277293647, 7.495153437616741, 6.59190352485115],\n [5.9073468077974125, 6.515658121386842, 6.127969158277979, 5.281243554841344],\n [8.3926395375135, 8.052584732759849, 7.523763217367977, 7.301823376693396],\n [8.052804262790858, 8.130382600666225, 7.029279965380727, 6.877138707786302],\n [7.328524598430755, 7.390510859108911, 7.139410296101382, 6.187372012428001],\n [5.2133268991445885, 5.763900738874426, 5.16520736027443, 4.600803029625299],\n [8.606239944200814, 8.552020921572462, 7.542683475130968, 7.369309008996131],\n [7.349163108849161, 7.249766947880163, 6.792584644263932, 5.9873376849239595],\n [6.706611651611043, 6.47397464225211, 5.712667251237384, 5.46041317499824],\n [8.231976179270633, 8.198885740657449, 7.737400639679397, 7.026723480748105],\n [8.18465447696798, 7.859302168840632, 7.386843622584417, 6.5612822610032575],\n [7.779828182710993,\n  7.9491325729480415,\n  6.894305775789155,\n  6.6593053054882505],\n [7.319302841545299, 7.128123324825717, 6.634506550670492, 5.877631830278199],\n [9.265310648217016, 8.86342398837244, 8.395665579903538, 8.239744323318877],\n [7.860589645532849, 8.229495518926765, 7.498653921901048, 6.8354220148352125],\n [6.787635653745795, 6.93638877271728, 6.354796343448212, 5.80659059656985],\n [7.242883425638159, 7.5099631984652175, 6.935782387478261, 6.087936085550982],\n [6.97906050085852, 6.963730654425601, 6.44945795018746, 5.714123766438508],\n [7.762191148578236, 7.979894032937595, 7.626208757305684, 6.650515755250195],\n [8.153539069680189, 8.189785717714338, 7.570272182933785, 6.86377070002929],\n [7.081911763800485, 7.557095459656281, 6.856810682613512, 6.139541522302653],\n [8.865752895922363, 8.321183735268116, 7.982127351443437, 7.3253810254376],\n [6.6778306710772, 6.844047835328088, 6.585412044379, 5.6847825615956475],\n [7.4311891126129215,\n  7.415513674724907,\n  6.533187164391923,\n  6.0558952708808365],\n [7.27923144499484, 6.955649490306241, 6.442566444776852, 5.628845252454113],\n [4.771330861096046, 4.9253647972684895, 4.646208961413485, 3.882622320824585],\n [7.772491609344426, 7.795718149011958, 7.113024246895577, 6.546115454677186],\n [6.365310783980364,\n  6.5787922676477315,\n  5.944323132095028,\n  5.2144709552751936],\n [6.828903557596333, 6.646242493325606, 6.180633278355387, 5.48917354139432],\n [8.484333402763156, 8.397848254086028, 7.378778351787506, 7.305922406232813],\n [7.147157969275191, 7.527660870862893, 6.891668832167337, 6.284907744248656],\n [8.17042145648363, 8.128158595639428, 7.197341548691195, 6.847914934419155],\n [5.514784324773385, 5.383438530826524, 4.9420902386169745, 4.241355795378272],\n [5.365878889646497, 5.974570418285149, 5.264958310131803, 4.746068337391844],\n [3.236710709188877,\n  3.798554142741332,\n  3.8222933779302486,\n  3.0135262845224804],\n [7.986555786661318, 7.908631903136263, 7.310213959512482, 6.4694030356084875],\n [6.150398248671977, 6.206918755046426, 5.777803599156549, 5.089794852225806],\n [7.801917295320663, 7.517875364280574, 6.561897084554997, 6.414580011372102],\n [5.110402640528925, 5.388739303543766, 5.171837171575248, 4.139491036085052],\n [7.755321215450041, 7.665064015585613, 6.735814618210241, 6.4195900854840575],\n [7.671732649914412, 7.285394766781012, 6.544515542395784, 6.091736063255551],\n [9.256768181538272, 8.891289411339297, 8.332529313987273, 8.15678491322058],\n [7.623149351008042, 7.399938523439216, 6.993389692894776, 6.41799353467204],\n [5.164732704692328, 5.568963925421622, 5.221650235221496, 4.42064760018966],\n [8.922156730707588, 8.747387087272177, 7.834767303289715, 7.561960936879734],\n [8.405733921141781, 8.342969070477047, 7.600711362974791, 7.20256995790155],\n [6.059111329819424, 5.880449517640664, 5.608795209429145, 4.80793448214342],\n [6.700633778224935, 6.75442715581899, 6.225236801179654, 5.32220124902107],\n [7.598132953456753, 7.5719828939586105, 6.878445027897256, 6.296792858086216],\n [7.785009082038075, 7.472493501613424, 7.156331883661156, 6.731017939679056],\n [8.110904335290941, 8.314029661706508, 7.319588650171918, 6.889478183575312],\n [6.987107063267544, 6.773314473395165, 6.367517680909501, 5.487491531223845],\n [7.697098407390495, 7.8985652324511975, 7.115993042551837, 6.610278391334473],\n [8.178226672836374, 7.918377659975066, 7.379647582246122, 6.779756252001329],\n [8.557922547430145, 8.390746886758729, 7.5122679062225535, 7.160285008447506],\n [7.108411853719444, 6.720841180836593, 6.275368785505509, 5.522696816605853],\n [6.993621433931104, 6.8910852885668845, 6.372511995304288, 5.584950485227283],\n [8.40345557132635, 8.35562417843949, 7.334137002468344, 7.154295667667598],\n [8.499409789315369, 8.24627882343546, 7.684431181408966, 7.294654241020816],\n [7.0356844884434135, 7.122527771311582, 6.62478302378304, 5.736592601723843],\n [6.438800269525277, 6.858379969839779, 6.4769591750928175, 5.475603942937332],\n [8.167561233701692, 8.233268140950749, 7.348196774709734, 6.868701591774764],\n [8.018530611891164, 7.676282080917443, 7.28267081876143, 6.564907957815923],\n [7.741849418481826, 7.786166079331695, 7.333896071550411, 6.513107004647158],\n [6.5191591800384945, 6.316392134483895, 5.851494479486127, 5.265485536954804],\n [5.66356947546499, 6.2554186137279615, 5.803419876757111, 5.043963360365703],\n [7.407477804173633, 7.0862114703479655, 6.601720058929243, 5.949871536075833],\n [8.019174420580809, 7.698246358743374, 7.133065863067067, 6.864783574467197],\n [8.88101538856157, 8.717255296230073, 7.745543023948526, 7.550208842148295],\n [7.188406114673511, 7.381196534845291, 6.671490103790257, 5.82203272378672],\n [6.72495861500274, 6.9405502545158875, 6.291620693189129, 5.450439999364614],\n [7.834664493692478, 7.561809311593149, 7.042830929885449, 6.323234987352329],\n [7.550641787303519, 7.242325149444992, 6.673822396862861, 5.952295880932071],\n [6.839106391880009, 6.958051392628064, 6.229517572542689, 5.568490674473977],\n [7.790385593031282, 7.64700926945389, 7.290989191163177, 6.468963506370816],\n [6.8257509336349855,\n  6.946013437257845,\n  6.2649399186292785,\n  5.713430976651547],\n [8.872338648104673, 8.66562475325097, 7.774871847881993, 7.5562562276290555],\n [9.212086937810746, 8.998317947728557, 8.724118536224436, 8.11703737835796],\n [7.993473905303847, 7.516437276327985, 7.180215688322442, 6.269480918073472],\n [6.974822507154345, 6.620969964008758, 6.532765596138976, 5.751806744118578],\n [6.410961835913241, 6.165363135294296, 5.5509848361016525, 5.210077452330486],\n [8.538212161160182, 8.147467893191743, 7.634584624032361, 7.191022753135356],\n [8.867619696009914, 8.781356860470162, 8.513644518852857, 7.9265652023426085],\n [7.369135989594383, 7.509759305663187, 6.925051419811723, 6.146866488844508],\n [6.379000970134508, 6.613071456150257, 6.157098869773712, 5.4053572170418125],\n [7.794803415553216, 7.695135508054856, 6.783667245866176, 6.386403475129446],\n [7.831832309379799, 7.368433720895292, 6.887805040708287, 6.436257582190127],\n ...]\n</div>"]}}],"execution_count":94},{"cell_type":"code","source":["test = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','all-1825.csv'))\nchemprop_preds = pd.read_csv(os.path.join(VIRTUAL_SCREENING,'Chemprop-Feat_SLogP','fold_0/model_0/all-1825_preds.csv'))\ndiff_cp = test.filter(names).subtract(chemprop_preds.filter(names)).abs()\ndiff_cp.columns=diff_cp.columns+['diff']\ndiff_cp.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":95},{"cell_type":"code","source":["import seaborn as sns; sns.set(color_codes=True)\nplt.close()\nf, axes = plt.subplots(2, 4, sharex = True, sharey=True, figsize=(16,8))\n#plt.scatter(test[names[0]],diff_cp[diff_cp.columns[0]])\nfor i in range(4):\n  for j in range(2):\n    if j==0:\n      sns.kdeplot(test[names[i]], diff_cp[diff_cp.columns[i]], n_levels=10, cmap=\"Purples_d\",ax=axes[j][i],clip=((0,10),(0,1)))\n    if j==1:\n      sns.kdeplot(test[names[i]], shade=True, color='r',ax=axes[j][i])\nplt.tight_layout()\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":96},{"cell_type":"code","source":["len(np.mean(np.array(mean_ratios),axis=0))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[73]: 35\n</div>"]}}],"execution_count":97},{"cell_type":"code","source":["# args = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'JAK','all-1825_bin76.csv'),\n#                           #'--features_path',os.path.join(CHEMPROP_DIR,'JAK','SLogPall-1825.csv'),\n#                           '--checkpoint_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext','fold_0/model_0/model.pt'),\n#                           '--preds_path',os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext','all-1825_preds.csv')])\n# modify_predict_args(args)\n# make_predictions(args)\n#test = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','all-1825_bin76.csv'))\n\ntest = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','test-183_bin76.csv'))\nval =pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','val-182_bin76.csv'))\ntrues = test.append(val)\n#chemprop_preds = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext','all-1825_preds.csv'))\nchemprop_test = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext/fold_0/model_0','test_preds_bin76_ext.csv'))\nchemprop_val = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext/fold_0/model_0','val_preds_bin76_ext.csv'))\nchemprop_preds = chemprop_test.append(chemprop_val)\ndiff_test = test.filter(targets).subtract(chemprop_test.filter(targets)).abs()\ndiff_val = val.filter(targets).subtract(chemprop_val.filter(targets)).abs()\ndiff_cp = diff_test.append(diff_val)\ndiff_cp.columns\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[25]: Index([&#39;JAK1&#39;, &#39;JAK2&#39;, &#39;JAK3&#39;, &#39;TYK2&#39;], dtype=&#39;object&#39;)\n</div>"]}}],"execution_count":98},{"cell_type":"code","source":["#allsmi = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','all-1825_bin76.csv'))\nfps = get_fps([Chem.MolFromSmiles(smi) for smi in test.append(val).smiles], bit=True)\ndists = fps_distances(fps)\ndists[dists==0]=1"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Errors in conversion: 0\n</div>"]}}],"execution_count":99},{"cell_type":"code","source":["G2G = '/dbfs/FileStore/g2g'\n\nos.listdir(os.path.join(G2G,'models'))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[77]: \n[&#39;JAK1&#39;,\n &#39;JAK2&#39;,\n &#39;JAK3&#39;,\n &#39;TYK2&#39;,\n &#39;dpEC&#39;,\n &#39;dpEC1-all_pairs-JAK1&#39;,\n &#39;dpEC1-all_pairs-JAK2&#39;,\n &#39;dpEC1-all_pairs-JAK3&#39;,\n &#39;dpEC1to2-close_pairs-JAK1&#39;,\n &#39;dpEC1to2-close_pairs-JAK2&#39;,\n &#39;dpEC1to2-close_pairs-JAK3&#39;,\n &#39;dpEC1to2-close_pairs-TYK2&#39;,\n &#39;hyperopt&#39;,\n &#39;test-2e-dpEC1-all_pairs-JAK1&#39;,\n &#39;test-2e-dpEC1-close_pairs-JAK1&#39;]\n</div>"]}}],"execution_count":100},{"cell_type":"code","source":["#np.sum(dists<0.5,axis=1)\n#import seaborn as sns; sns.set(color_codes=True)\nplt.close()\nf, axes = plt.subplots(2, 5, sharex = True, sharey=True, figsize=(16,8))\n#plt.scatter(test[names[0]],diff_cp[diff_cp.columns[0]])\nfor i,ax in enumerate(axes.flat):\n  ax.scatter(np.sum(dists<i*0.1,axis=1),diff_cp.JAK1)\n  ax.set_title(str(i))\nplt.tight_layout()\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":101},{"cell_type":"code","source":["chemprop_preds = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','hyperopt_4x-bin76_ext','all-1825_preds.csv'))\ntest = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','all-1825_bin76.csv'))\nfps = get_fps([Chem.MolFromSmiles(smi) for smi in test.smiles], bit=True)\ndists = fps_distances(fps)\ndists[dists==0]=1\ndiff_cp = test.filter(targets).subtract(chemprop_preds.filter(targets)).abs()\ntargets=['JAK1','JAK2','JAK3','TYK2']\nfrom sklearn.metrics import roc_auc_score\n\nplt.close()\nf, axes = plt.subplots(1, 5, sharex = True, sharey=True, figsize=(16,4))\n#plt.axis([0,155,0.85,1.001])\n\n#colors = [plt.cm.Blues(np.linspace(0.2,1,len(idx))),plt.cm.Reds(np.linspace(0.2,1,len(idx)))]\nfor i,ax in enumerate(axes.flat):\n  for target in targets:\n    \n    ratio = []\n    for thresh in range(150):\n      n_neighbors = np.sum(dists<(i+2)*0.1,axis=1)\n      #n_true = diff_cp[(diff_cp.JAK1<0.5) & (n_neighbors>thresh)]\n      #n_false = diff_cp[(diff_cp.JAK1>0.5) & (n_neighbors>thresh)]\n      preds = chemprop_preds[target][n_neighbors>thresh]\n      truevals = test[target][n_neighbors>thresh]\n      try:\n        ratio.append(roc_auc_score(truevals,preds)-roc_auc_score(test[target],chemprop_preds[target]))\n      except:\n        continue#ratio.append(0)\n    ax.plot(range(len(ratio)),ratio,label=target)\n    \n    ax.set_xlim([0,155])\n    ax.set_ylim([-0.1,0.1])\n  ax.plot(range(150),np.ones(150),color='k',ls='--')\n  ax.set_title('Threshold: {:02.1f}'.format((i+2)*0.1))\n  ax.set_xlabel('Number of neighbors')\n  ax.set_ylabel('ROC/AUC score')\n  ax.legend()\nplt.tight_layout()\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":102},{"cell_type":"code","source":["targets=['JAK1','JAK2','JAK3','TYK2']\nfrom sklearn.metrics import roc_auc_score\n\nplt.close()\n\nf, axes = plt.subplots(1, 5, sharex = True, sharey=True, figsize=(16,4))\n\n\n#colors = [plt.cm.Blues(np.linspace(0.2,1,len(idx))),plt.cm.Reds(np.linspace(0.2,1,len(idx)))]\nfor i,ax in enumerate(axes.flat):\n  for target in targets:\n    \n    ratio = []\n    for thresh in range(50):\n      n_neighbors = np.sum(dists<(i+2)*0.1,axis=1)\n      #n_true = diff_cp[(diff_cp.JAK1<0.5) & (n_neighbors>thresh)]\n      #n_false = diff_cp[(diff_cp.JAK1>0.5) & (n_neighbors>thresh)]\n      preds = chemprop_preds[target][n_neighbors>thresh]\n      truevals = trues[target][n_neighbors>thresh]\n      try:\n        ratio.append(roc_auc_score(truevals,preds)-roc_auc_score(trues[target],chemprop_preds[target]))\n      except:\n        continue#ratio.append(0)\n    ax.plot(range(len(ratio)),ratio,label=target)\n    \n    ax.set_xlim([0,55])\n    ax.set_ylim([-0.3,0.3])\n  ax.plot(range(55),np.zeros(55),color='k',ls='--')\n  ax.set_title('Threshold: {:02.1f}'.format((i+2)*0.1))\n  ax.set_xlabel('Number of neighbors')\n  ax.set_ylabel('ROC/AUC score')\n  #plt.axis([0,55,0.7,1.001])\n  ax.legend()\nplt.tight_layout()\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":103},{"cell_type":"code","source":["plt.close()\nfor target in targets:\n    \n  ratio = []\n  for thresh in range(500):\n    n_neighbors = np.sum(dists<3*0.1,axis=1)\n    #n_true = diff_cp[(diff_cp.JAK1<0.5) & (n_neighbors>thresh)]\n    #n_false = diff_cp[(diff_cp.JAK1>0.5) & (n_neighbors>thresh)]\n    preds = chemprop_preds[target][n_neighbors>thresh]\n    truevals = test[target][n_neighbors>thresh]\n    try:\n      ratio.append(roc_auc_score(truevals,preds))\n    except:\n      continue#ratio.append(0)\n  plt.plot(range(len(ratio)),ratio)\nplt.axis([0,200,0.8,1.1])\n\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":104},{"cell_type":"code","source":["targets=['JAK1','JAK2','JAK3','TYK2']\nplt.close()\nf, axes = plt.subplots(2, 5, sharex = True, sharey=True, figsize=(16,8))\nfor i,ax in enumerate(axes.flat):\n  for target in targets:\n    \n  ratio = []\n  for thresh in range(len(diff_cp)):\n    n_neighbors = np.sum(dists<i*0.1,axis=1)\n    n_true = sum((diff_cp.JAK1<0.5) & (n_neighbors>thresh))\n    n_false = sum((diff_cp.JAK1>0.5) & (n_neighbors>thresh))\n    ratio.append(n_true/(n_false+1))\n  ax.scatter(range(len(diff_cp)),ratio)\n  ax.set_title(str(i))\nplt.tight_layout()\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":105},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":106},{"cell_type":"code","source":["#how number of true vs false change when looking at different distance thresholds\nplt.close()\nplt.scatter(np.sum(dists<0.4,axis=1),diff_cp.JAK1)\n#sns.kdeplot(np.sum(dists<0.4,axis=1),diff_cp.JAK1, n_levels=10, cmap=\"Purples_d\")\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":107},{"cell_type":"code","source":["plt.close()\nplt.figure(figsize=(6,3))\nplt.subplot(121)\nplt.scatter(dists.min(axis=1), np.round(diff_cp.JAK1))\nplt.subplot(122)\nplt.scatter(dists.min(axis=1), diff_cp.JAK1)\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":108},{"cell_type":"code","source":["plt.close()\nf, axes = plt.subplots(1, 1, sharex = True, sharey=True, figsize=(8,8))\nsns.kdeplot(dists.min(axis=1), diff_cp.JAK1, n_levels=10, cmap=\"Purples_d\",ax=axes)#,clip=((0,10),(0,1)))\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"code","source":["import seaborn as sns; sns.set(color_codes=True)\nplt.close()\nf, axes = plt.subplots(2, 4, sharex = True, sharey=True, figsize=(16,8))\n#plt.scatter(test[names[0]],diff_cp[diff_cp.columns[0]])\nfor i in range(4):\n  for j in range(2):\n    if j==0:\n      sns.kdeplot(test[names[i][:4]], diff_cp[diff_cp.columns[i][:4]], n_levels=10, cmap=\"Purples_d\",ax=axes[j][i],clip=((0,10),(0,1)))\n    if j==1:\n      sns.kdeplot(test[names[i][:4]], shade=True, color='r',ax=axes[j][i])\nplt.tight_layout()\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":110},{"cell_type":"markdown","source":["### HLM Regression"],"metadata":{}},{"cell_type":"code","source":["%sh ls /dbfs/FileStore/chemprop/HLM/checkpoints"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">depth_2_dropout_0.0_ffn_num_layers_1_hidden_size_1700\ndepth_2_dropout_0.15000000000000002_ffn_num_layers_1_hidden_size_1900\ndepth_3_dropout_0.05_ffn_num_layers_3_hidden_size_500\ndepth_3_dropout_0.1_ffn_num_layers_1_hidden_size_600\ndepth_3_dropout_0.25_ffn_num_layers_3_hidden_size_1300\ndepth_3_dropout_0.2_ffn_num_layers_2_hidden_size_1900\ndepth_3_dropout_0.30000000000000004_ffn_num_layers_2_hidden_size_900\ndepth_3_dropout_0.35000000000000003_ffn_num_layers_2_hidden_size_2200\ndepth_3_dropout_0.35000000000000003_ffn_num_layers_2_hidden_size_2400\ndepth_4_dropout_0.05_ffn_num_layers_2_hidden_size_1100\ndepth_4_dropout_0.15000000000000002_ffn_num_layers_2_hidden_size_2200\ndepth_4_dropout_0.1_ffn_num_layers_1_hidden_size_2400\ndepth_4_dropout_0.4_ffn_num_layers_2_hidden_size_1500\ndepth_5_dropout_0.05_ffn_num_layers_2_hidden_size_1200\ndepth_5_dropout_0.05_ffn_num_layers_2_hidden_size_800\ndepth_5_dropout_0.1_ffn_num_layers_2_hidden_size_1200\ndepth_5_dropout_0.25_ffn_num_layers_3_hidden_size_1000\ndepth_5_dropout_0.25_ffn_num_layers_3_hidden_size_2100\ndepth_5_dropout_0.2_ffn_num_layers_1_hidden_size_1400\ndepth_5_dropout_0.2_ffn_num_layers_3_hidden_size_400\ndepth_5_dropout_0.30000000000000004_ffn_num_layers_2_hidden_size_1900\ndepth_6_dropout_0.05_ffn_num_layers_2_hidden_size_400\ndepth_6_dropout_0.0_ffn_num_layers_1_hidden_size_1600\ndepth_6_dropout_0.0_ffn_num_layers_1_hidden_size_1700\ndepth_6_dropout_0.0_ffn_num_layers_1_hidden_size_1900\ndepth_6_dropout_0.0_ffn_num_layers_1_hidden_size_2000\ndepth_6_dropout_0.15000000000000002_ffn_num_layers_1_hidden_size_1200\ndepth_6_dropout_0.1_ffn_num_layers_1_hidden_size_1900\ndepth_6_dropout_0.2_ffn_num_layers_1_hidden_size_700\nfold_0\nfold_1\nfold_2\nfold_3\nfold_4\nquiet.log\nverbose.log\n</div>"]}}],"execution_count":112},{"cell_type":"code","source":["#{ \"depth\": 6, \"dropout\": 0.0, \"ffn_num_layers\": 1, \"hidden_size\": 1900 }\nargs = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'hlm_eh.csv'),\n                          '--checkpoint_dir',os.path.join(CHEMPROP_DIR,'HLM','checkpoints'),#,'fold_0/model_0/model.pt'\n                          #'--ensemble_size','10',\n                          '--preds_path',os.path.join(CHEMPROP_DIR,'HLM','checkpoints','fold_0/model_0/hlm_eh_preds.csv')])\nmodify_predict_args(args)\nmake_predictions(args)\n# test = pd.read_csv(os.path.join(CHEMPROP_DIR,'hlm_eh.csv'))\n# chemprop_preds = pd.read_csv(os.path.join(CHEMPROP_DIR,'HLM','checkpoints','fold_0/model_0/all-1825_preds.csv'))\n# diff_cp = test.filter(names).subtract(chemprop_preds.filter(names)).abs()\n# diff_cp.columns=diff_cp.columns+['diff']\n# diff_cp.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading training args\nLoading data\n\n\r  0%|          | 0/804 [00:00&lt;?, ?it/s]\n\r 32%|███▏      | 260/804 [00:00&lt;00:00, 2592.78it/s]\n\r 60%|█████▉    | 479/804 [00:00&lt;00:00, 2454.59it/s]\n\r 82%|████████▏ | 658/804 [00:00&lt;00:00, 2206.59it/s]\n\r100%|██████████| 804/804 [00:00&lt;00:00, 2094.06it/s]Validating SMILES\nTest size = 804\nPredicting with an ensemble of 190 models\n\n\r  0%|          | 0/190 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 40.20it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 40.36it/s]\n\n\r 82%|████████▏ | 14/17 [00:00&lt;00:00, 39.74it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 41.72it/s]\n\r  1%|          | 1/190 [00:01&lt;05:26,  1.73s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 42.86it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 40.87it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 25.95it/s]\n\n\r 94%|█████████▍| 16/17 [00:00&lt;00:00, 28.57it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 30.96it/s]\n\r  1%|          | 2/190 [00:03&lt;05:43,  1.83s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 42.08it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 41.39it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 40.31it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 41.71it/s]\n\r  2%|▏         | 3/190 [00:05&lt;05:45,  1.85s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 42.57it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 41.78it/s]\n\n\r 82%|████████▏ | 14/17 [00:00&lt;00:00, 40.65it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 42.15it/s]\n\r  2%|▏         | 4/190 [00:07&lt;05:45,  1.86s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 42.21it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 41.48it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 26.27it/s]\n\n\r 94%|█████████▍| 16/17 [00:00&lt;00:00, 28.49it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 31.12it/s]\n\r  3%|▎         | 5/190 [00:09&lt;05:59,  1.94s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 40.29it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 39.90it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 38.76it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 40.27it/s]\n\r  3%|▎         | 6/190 [00:11&lt;06:10,  2.01s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 24%|██▎       | 4/17 [00:00&lt;00:00, 39.59it/s]\n\n\r 47%|████▋     | 8/17 [00:00&lt;00:00, 39.71it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 38.83it/s]\n\n\r 94%|█████████▍| 16/17 [00:00&lt;00:00, 38.07it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 40.26it/s]\n\r  4%|▎         | 7/190 [00:14&lt;06:18,  2.07s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 40.68it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 40.17it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 38.35it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 39.97it/s]\n\r  4%|▍         | 8/190 [00:16&lt;06:27,  2.13s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 40.45it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 39.89it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 38.86it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 40.40it/s]\n\r  5%|▍         | 9/190 [00:18&lt;06:29,  2.15s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 40.37it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 40.06it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 39.14it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 39.91it/s]\n\r  5%|▌         | 10/190 [00:21&lt;06:53,  2.30s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 48.50it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 47.41it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 45.89it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 47.41it/s]\n\r  6%|▌         | 11/190 [00:21&lt;05:25,  1.82s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 47.42it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 47.00it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 46.03it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 48.01it/s]\n\r  6%|▋         | 12/190 [00:22&lt;04:18,  1.45s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 47.88it/s]\n\n\r 41%|████      | 7/17 [00:00&lt;00:00, 24.41it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 28.30it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 34.85it/s]\n\r  7%|▋         | 13/190 [00:23&lt;03:44,  1.27s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 47.85it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 47.05it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 45.87it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 47.44it/s]\n\r  7%|▋         | 14/190 [00:23&lt;03:08,  1.07s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 46.94it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 46.52it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 45.60it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 47.42it/s]\n\r  8%|▊         | 15/190 [00:24&lt;02:47,  1.04it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 47.44it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 46.95it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 45.43it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 47.29it/s]\n\r  8%|▊         | 16/190 [00:25&lt;02:30,  1.16it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 47.61it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 46.38it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 45.33it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 46.98it/s]\n\r  9%|▉         | 17/190 [00:25&lt;02:20,  1.23it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 48.15it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 46.91it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 45.79it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 47.33it/s]\n\r  9%|▉         | 18/190 [00:26&lt;02:10,  1.31it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 47.52it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 46.99it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 45.69it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 47.58it/s]\n\r 10%|█         | 19/190 [00:27&lt;02:05,  1.36it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 47.91it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 45.80it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 45.00it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 46.33it/s]\n\r 11%|█         | 20/190 [00:27&lt;02:00,  1.41it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 41.17it/s]\n\n\r 41%|████      | 7/17 [00:00&lt;00:00, 22.34it/s]\n\n\r 65%|██████▍   | 11/17 [00:00&lt;00:00, 25.53it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 28.10it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 30.86it/s]\n\r 11%|█         | 21/190 [00:29&lt;02:51,  1.02s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 40.63it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 40.42it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 39.28it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 40.72it/s]\n\r 12%|█▏        | 22/190 [00:31&lt;03:10,  1.13s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 40.51it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 39.69it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 38.73it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 40.22it/s]\n\r 12%|█▏        | 23/190 [00:32&lt;03:26,  1.23s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 40.43it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 40.20it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 39.05it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 40.62it/s]\n\r 13%|█▎        | 24/190 [00:34&lt;03:39,  1.32s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 40.88it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 39.73it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 38.82it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 40.23it/s]\n\r 13%|█▎        | 25/190 [00:35&lt;03:45,  1.36s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 24%|██▎       | 4/17 [00:00&lt;00:00, 34.38it/s]\n\n\r 47%|████▋     | 8/17 [00:00&lt;00:00, 34.20it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 33.35it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 32.21it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 34.02it/s]\n\r 14%|█▎        | 26/190 [00:37&lt;04:31,  1.66s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 24%|██▎       | 4/17 [00:00&lt;00:00, 33.60it/s]\n\n\r 47%|████▋     | 8/17 [00:00&lt;00:00, 33.64it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 32.61it/s]\n\n\r 94%|█████████▍| 16/17 [00:00&lt;00:00, 32.22it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 33.97it/s]\n\r 14%|█▍        | 27/190 [00:40&lt;05:03,  1.86s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 24%|██▎       | 4/17 [00:00&lt;00:00, 34.89it/s]\n\n\r 47%|████▋     | 8/17 [00:00&lt;00:00, 34.29it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 33.33it/s]\n\n\r 94%|█████████▍| 16/17 [00:00&lt;00:00, 32.53it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 34.17it/s]\n\r 15%|█▍        | 28/190 [00:42&lt;05:26,  2.01s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 18%|█▊        | 3/17 [00:00&lt;00:01, 13.25it/s]\n\n\r 41%|████      | 7/17 [00:00&lt;00:00, 16.16it/s]\n\n\r 65%|██████▍   | 11/17 [00:00&lt;00:00, 18.99it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 21.46it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 26.67it/s]\n\r 15%|█▌        | 29/190 [00:45&lt;05:47,  2.16s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 24%|██▎       | 4/17 [00:00&lt;00:00, 32.71it/s]\n\n\r 47%|████▋     | 8/17 [00:00&lt;00:00, 33.03it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 32.23it/s]\n\n\r 94%|█████████▍| 16/17 [00:00&lt;00:00, 31.72it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 33.58it/s]\n\n*** WARNING: skipped 134994 bytes of output ***\n\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 18%|█▊        | 3/17 [00:00&lt;00:00, 24.81it/s]\n\n\r 35%|███▌      | 6/17 [00:00&lt;00:00, 23.91it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 23.83it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 23.34it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 23.04it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 23.71it/s]\n\r 96%|█████████▋| 183/190 [06:55&lt;00:23,  3.38s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 18%|█▊        | 3/17 [00:00&lt;00:00, 24.74it/s]\n\n\r 35%|███▌      | 6/17 [00:00&lt;00:00, 24.69it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 24.32it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 23.09it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 22.86it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 24.21it/s]\n\r 97%|█████████▋| 184/190 [06:58&lt;00:19,  3.32s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 18%|█▊        | 3/17 [00:00&lt;00:00, 21.84it/s]\n\n\r 24%|██▎       | 4/17 [00:00&lt;00:01, 11.66it/s]\n\n\r 41%|████      | 7/17 [00:00&lt;00:00, 13.77it/s]\n\n\r 59%|█████▉    | 10/17 [00:00&lt;00:00, 15.45it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 16.83it/s]\n\n\r 94%|█████████▍| 16/17 [00:00&lt;00:00, 18.16it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 19.69it/s]\n\r 97%|█████████▋| 185/190 [07:02&lt;00:17,  3.54s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 18%|█▊        | 3/17 [00:00&lt;00:00, 23.96it/s]\n\n\r 35%|███▌      | 6/17 [00:00&lt;00:00, 24.23it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 24.05it/s]\n\n\r 65%|██████▍   | 11/17 [00:00&lt;00:00, 22.55it/s]\n\n\r 82%|████████▏ | 14/17 [00:00&lt;00:00, 22.48it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 24.07it/s]\n\r 98%|█████████▊| 186/190 [07:05&lt;00:14,  3.54s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 18%|█▊        | 3/17 [00:00&lt;00:00, 24.87it/s]\n\n\r 35%|███▌      | 6/17 [00:00&lt;00:00, 24.08it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 23.95it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 23.44it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 23.11it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 23.85it/s]\n\r 98%|█████████▊| 187/190 [07:09&lt;00:10,  3.45s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 18%|█▊        | 3/17 [00:00&lt;00:00, 24.78it/s]\n\n\r 35%|███▌      | 6/17 [00:00&lt;00:00, 24.79it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 24.35it/s]\n\n\r 65%|██████▍   | 11/17 [00:00&lt;00:00, 22.82it/s]\n\n\r 82%|████████▏ | 14/17 [00:00&lt;00:00, 22.66it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 24.23it/s]\n\r 99%|█████████▉| 188/190 [07:12&lt;00:06,  3.39s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 12%|█▏        | 2/17 [00:00&lt;00:00, 19.62it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 20.69it/s]\n\n\r 47%|████▋     | 8/17 [00:00&lt;00:00, 21.53it/s]\n\n\r 65%|██████▍   | 11/17 [00:00&lt;00:00, 21.79it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 21.11it/s]\n\n\r 94%|█████████▍| 16/17 [00:00&lt;00:00, 21.45it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 23.26it/s]\n\r 99%|█████████▉| 189/190 [07:15&lt;00:03,  3.40s/it]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 18%|█▊        | 3/17 [00:00&lt;00:00, 22.72it/s]\n\n\r 35%|███▌      | 6/17 [00:00&lt;00:00, 23.18it/s]\n\n\r 53%|█████▎    | 9/17 [00:00&lt;00:00, 23.24it/s]\n\n\r 71%|███████   | 12/17 [00:00&lt;00:00, 22.77it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 21.81it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 23.39it/s]\n\r100%|██████████| 190/190 [07:19&lt;00:00,  3.40s/it]Saving predictions to /dbfs/FileStore/chemprop/HLM/checkpoints/fold_0/model_0/hlm_eh_preds.csv\nOut[14]: \n[[73.19024570107283],\n [57.460097951448255],\n [70.39275368400008],\n [52.360304020891704],\n [50.89028466264032],\n [55.52897411022259],\n [69.28733610656764],\n [52.072853405568544],\n [52.6456002075161],\n [46.33457489692193],\n [66.36864712238327],\n [52.38407811353544],\n [51.47718268233877],\n [52.581327513306825],\n [52.296826172850906],\n [51.236396988424374],\n [52.31653039935984],\n [52.01760526205624],\n [44.67505212511402],\n [34.8588943331387],\n [46.26126038580533],\n [57.249810773272955],\n [62.00352244211601],\n [52.063400506301406],\n [72.3638502865574],\n [44.978253205277205],\n [51.75461515718555],\n [39.69310459812535],\n [72.44298566899353],\n [40.736505097108946],\n [62.86348145605966],\n [39.92601154051095],\n [51.35075437455644],\n [43.64137503312977],\n [55.89068600580114],\n [83.20558717916325],\n [60.44492231445262],\n [47.94002259179254],\n [48.09424319847054],\n [81.01887661790096],\n [43.6649521376303],\n [83.87230885655799],\n [58.05622489762478],\n [41.969229416490904],\n [40.90381096327225],\n [40.774369790972116],\n [45.97302823264538],\n [65.2928860355672],\n [47.32118147806045],\n [64.94042061586758],\n [41.5934355905329],\n [43.19846196158613],\n [34.03960467770533],\n [49.59517055655322],\n [81.5677664028604],\n [46.342338426678054],\n [59.10003258702804],\n [66.85495636182604],\n [54.08344591742223],\n [54.287495150215584],\n [34.98920681417181],\n [53.904817755401005],\n [48.818405011132704],\n [54.480153815126094],\n [64.47045920584358],\n [82.02353537462145],\n [63.782401324387465],\n [52.85796152704337],\n [53.65837543695847],\n [64.48380675492697],\n [63.701899104685715],\n [74.50441926944875],\n [90.09319558652379],\n [47.65866963414057],\n [46.42102380488797],\n [49.69569359217899],\n [47.90951705634296],\n [48.19169754258568],\n [83.986365760625],\n [76.68547637900245],\n [51.09116277283485],\n [43.955999507832395],\n [42.925359903572144],\n [43.148346758289414],\n [73.73231266586188],\n [56.218739621524534],\n [45.937103726148834],\n [43.40508527260455],\n [55.68084720019946],\n [63.731351144300874],\n [59.51693366465095],\n [74.22092745385041],\n [69.37745425140766],\n [44.437214604999795],\n [45.05536142517207],\n [44.35066224827168],\n [73.20712829578981],\n [77.11316111217305],\n [65.05063974125751],\n [51.537637920275024],\n [63.62870593435065],\n [86.36363498452985],\n [59.650004709888194],\n [45.616193967831684],\n [55.74616491278671],\n [52.54565433754776],\n [67.75872577892784],\n [44.84670708197518],\n [45.433918195889696],\n [59.55445702465407],\n [46.781395420879456],\n [47.577447578519354],\n [49.22224494247601],\n [47.84532503962763],\n [63.316331008929055],\n [64.98543739268243],\n [76.06896073397974],\n [55.41210930056153],\n [42.27013949542429],\n [45.421991987385006],\n [44.69944630142157],\n [42.81660503669803],\n [52.63400021892523],\n [48.366935378311055],\n [55.41210930056153],\n [38.906726483158316],\n [49.56514781150877],\n [37.4232667877568],\n [39.81749750204289],\n [71.93964294462282],\n [47.76751944932486],\n [67.34513402013596],\n [66.92306457233312],\n [40.490326615831215],\n [84.08112118820725],\n [82.5576378845222],\n [59.3645808415569],\n [83.73557419722123],\n [77.16785743997386],\n [44.53503842265816],\n [44.55204870995989],\n [76.07368880626166],\n [74.19015413322033],\n [46.639018688541846],\n [46.32452675634663],\n [79.71018608082602],\n [60.25883457474829],\n [53.362071144993905],\n [73.1116644245853],\n [85.5140342247504],\n [77.47698667284146],\n [84.48367103234821],\n [88.10205253929736],\n [74.31804541505582],\n [78.2469345546157],\n [92.3487075750008],\n [63.0790626210776],\n [46.078780560461986],\n [56.773692256619896],\n [74.33045007962627],\n [58.89560270085299],\n [45.51538025363195],\n [69.13658639337746],\n [49.39225508095344],\n [46.497311357039955],\n [52.888858893940224],\n [43.90874254425506],\n [47.98449025487623],\n [61.65748212207164],\n [59.365044359660935],\n [58.74036363184088],\n [61.374977498500265],\n [79.62851027629257],\n [45.29360456968748],\n [43.23120448952106],\n [82.29709602261846],\n [67.26210210959783],\n [76.55671299561699],\n [73.48273742111088],\n [71.38980495799046],\n [51.955119862998515],\n [56.95219852242228],\n [54.82444777850416],\n [74.46234332665738],\n [79.79685434004793],\n [78.12799513831852],\n [62.35598511262525],\n [60.07624560042841],\n [67.47723067752668],\n [65.58337994488491],\n [64.519433670802],\n [61.177895459355014],\n [80.95497137141793],\n [70.24452933809118],\n [79.4549149477132],\n [58.11202460204232],\n [66.21686876026496],\n [67.66216642415961],\n [66.44243453230749],\n [68.88101050251242],\n [60.136015824996655],\n [61.496134866639984],\n [60.65239392884618],\n [63.37120206911864],\n [61.281339031063965],\n [81.04863883313728],\n [77.05316608351579],\n [67.9360738955317],\n [64.94180700737806],\n [79.09040192892698],\n [63.2736169373544],\n [77.23055818824618],\n [72.45945562216157],\n [78.60503099911963],\n [65.10736613689586],\n [72.37932249055198],\n [60.07987167568262],\n [52.688246440599364],\n [68.95310303222634],\n [77.1014734028005],\n [76.92410896332639],\n [74.67275214948675],\n [75.36670917035855],\n [79.03396182909415],\n [85.15488101395887],\n [64.40268755082298],\n [71.83759990387567],\n [72.5282136377985],\n [50.52056912790944],\n [73.75922189841809],\n [74.45268214238084],\n [42.09483674667348],\n [74.58029590807747],\n [78.9249829662692],\n [60.71152286290092],\n [81.12401402827466],\n [72.35138833647255],\n [80.94750679704991],\n [76.66305202927022],\n [61.557365921017826],\n [58.42336247992193],\n [86.93361685402338],\n [54.841542839373],\n [59.17125669988891],\n [68.6668058676859],\n [55.9419562137641],\n [56.98848305858665],\n [56.51366041157987],\n [58.28194963770192],\n [46.6156305710388],\n [41.025440670643775],\n [81.97682000809966],\n [46.951431692838476],\n [57.581558973101004],\n [56.966433008119516],\n [57.17777586924692],\n [66.79306512611574],\n [54.34992100499519],\n [54.402436670100464],\n [63.40015087680527],\n [60.96614633890236],\n [74.50032483557388],\n [63.07486788061785],\n [62.000327948898615],\n [85.34448567907506],\n [76.63935315123021],\n [78.96137077487863],\n [67.57156222664833],\n [82.22439292875858],\n [83.73497568845218],\n [81.5372376176154],\n [90.0625058202028],\n [72.8342200691202],\n [80.51893751643827],\n [79.53831158348133],\n [61.48731666582896],\n [72.83092295965952],\n [64.71756705213225],\n [80.30925337425636],\n [82.51358121785628],\n [81.76891286641725],\n [64.03700180837112],\n [61.89291680514011],\n [52.43539074529638],\n [55.29562724263178],\n [51.66379011453074],\n [56.065348509521506],\n [76.62724659559332],\n [74.8876264024215],\n [80.91712079510488],\n [76.02244224272944],\n [58.255105439687085],\n [66.64563906406497],\n [58.403220918438535],\n [67.94715562691883],\n [57.212170469054016],\n [59.38930328936124],\n [65.19623542907829],\n [68.52957276400608],\n [82.97011111482236],\n [81.65195777990445],\n [60.523492191086305],\n [62.556646023855805],\n [63.30906196693922],\n [83.21126380896588],\n [77.08163267690271],\n [70.88982313270849],\n [69.2092294784301],\n [63.183755048131445],\n [76.45356415550121],\n [74.67664846007388],\n [79.93817104833396],\n [67.68004332162623],\n [58.70922763290921],\n [82.03624766569706],\n [56.712409461500954],\n [65.06511496426556],\n [73.15955789141611],\n [64.49740910351599],\n [82.12664929444105],\n [75.01532347130569],\n [58.2511998188754],\n [77.47726454776492],\n [78.08878486937411],\n [57.14450805851035],\n [70.8192350734706],\n [56.99681590927916],\n [66.5102912393376],\n [66.85959986181172],\n [78.08235405408645],\n [66.58049855332271],\n [54.718970431618544],\n [79.37573107292177],\n [62.72966579632144],\n [69.97892584758105],\n [57.212724421819594],\n [58.06648083736786],\n [75.58990787499091],\n [58.88374272996758],\n [61.11726243617486],\n [74.35605693953084],\n [63.232415187591684],\n [81.95994764241435],\n [58.59778620565547],\n [55.41831660457611],\n [63.20859133088831],\n [54.3251953500722],\n [81.77106686036329],\n [83.37701044507224],\n [88.97134608714518],\n [61.656557182384574],\n [66.03586929823862],\n [63.64475264299852],\n [63.43881645860316],\n [75.03249109459827],\n [64.56708827677934],\n [71.07722594985523],\n [57.12422677129199],\n [58.96646152107778],\n [70.95158408849846],\n [67.54126429070773],\n [65.49327801077764],\n [58.084687962953986],\n [58.35861091816674],\n [54.70850208873934],\n [76.53924643693958],\n [74.47217517667252],\n [69.63221585646262],\n [70.61216989853574],\n [68.51648698896524],\n [67.39271968830039],\n [54.533630136827256],\n [79.36391230829405],\n [63.10451963871305],\n [79.59691302760667],\n [79.43441978886311],\n [81.7577459254497],\n [81.6395805731324],\n [52.97793034913091],\n [65.68557443608726],\n [57.882398092699],\n [83.55346496959426],\n [62.38103848720939],\n [69.84610074191687],\n [72.08957980328334],\n [65.5545099182455],\n [75.86821362919844],\n [77.24594349709307],\n [76.52892863541915],\n [50.3648955087009],\n [55.13072752056611],\n [49.41549495830601],\n [68.26514823276845],\n [55.98247083263728],\n [52.757946389873084],\n [67.5047640320542],\n [61.59370066208975],\n [64.61479270352909],\n [64.30012156948649],\n [65.96641894461177],\n [67.05782667533958],\n [82.48501433700118],\n [81.06104314684683],\n [78.1814488395796],\n [78.49011061585816],\n [77.91831711348303],\n [79.6741354053879],\n [87.10517959188617],\n [48.47015364764984],\n [69.84050596188136],\n [73.62650750411943],\n [84.76745386211569],\n [87.54315298947292],\n [78.51814752510955],\n [72.0123891960298],\n [59.86332234822601],\n [73.96930412138222],\n [77.25863337884344],\n [41.71095730136259],\n [90.68935319512381],\n [84.16076256936913],\n [55.83287978458195],\n [77.62279863944757],\n [70.00221681045642],\n [62.5511120117583],\n [64.08809192395285],\n [87.86563519397511],\n [52.20070750255374],\n [56.35102403562796],\n [83.84947979758849],\n [69.48898957017778],\n [69.48898957017778],\n [63.438887676595925],\n [58.51535067147308],\n [40.4803977827955],\n [71.35887825819559],\n [74.98879916075359],\n [57.13543280132788],\n [56.056534769899066],\n [66.37065871781533],\n [58.39147610989791],\n [50.78270249818183],\n [66.02835711965267],\n [59.02095017081088],\n [72.7726445527655],\n [64.3454823820033],\n [53.03694243058464],\n [47.42921105554463],\n [80.96870205639534],\n [89.9148383349601],\n [63.40346753257332],\n [82.79355190078078],\n [87.52243669309075],\n [81.29618749630092],\n [80.87469900480478],\n [76.6773076908495],\n [81.14612962998827],\n [56.553380842564344],\n [58.55002321076399],\n [60.305322371215134],\n [61.98452680472422],\n [60.2339913803318],\n [63.21910552538504],\n [72.78482590338058],\n [84.19884060524805],\n [70.70920060100195],\n [63.21910552538504],\n [76.1376836138003],\n [78.92866391384432],\n [66.551753510015],\n [83.45158011688896],\n [66.62446369473278],\n [62.55111201796431],\n [59.81151337568028],\n [61.100185302511264],\n [86.05711263914478],\n [52.61438048402584],\n [67.73876391066054],\n [65.86400465767622],\n [42.74062039205033],\n [66.782177469721],\n [52.93734397045878],\n [53.649018463003756],\n [53.649018463003756],\n [53.649018463003756],\n [53.649018463003756],\n [71.78403482036335],\n [72.18496327444949],\n [81.42089972747864],\n [86.64962920311099],\n [64.05986727361454],\n [91.30919332162986],\n [55.03741156401446],\n [67.71409256864554],\n [76.57538091933209],\n [77.31176506908015],\n [91.57309806444313],\n [91.19854486827087],\n [85.21841129214738],\n [56.63537002695623],\n [53.03694248244908],\n [56.899621617331356],\n [77.3297934660266],\n [77.32520637428378],\n [80.02257895237571],\n [75.21332255369096],\n [57.990628069495116],\n [77.37214481098144],\n [77.37214481098144],\n [65.88850432579555],\n [76.43814547808067],\n [74.88120482355139],\n [46.49534661783618],\n [41.24425140618165],\n [83.73117519030586],\n [74.38787379538722],\n [45.86252358812847],\n [79.68993880893547],\n [42.531414672909236],\n [67.95621992303822],\n [62.52917132354367],\n [60.84691175310102],\n [82.42019544996913],\n [81.19320408474928],\n [81.36296632485262],\n [64.92511387418239],\n [79.57908806778318],\n [46.9076866234118],\n [45.71698952681092],\n [88.00334083046411],\n [72.81602041628292],\n [78.72661306264057],\n [79.39298036162641],\n [77.88466364926515],\n [66.77708551677975],\n [53.498917055480064],\n [47.268983086443626],\n [32.66897497717269],\n [42.22694773466871],\n [47.552903713956894],\n [51.21386711934121],\n [82.65314955266327],\n [52.079846876655346],\n [46.13829521381905],\n [83.78340863387481],\n [40.67693103988446],\n [87.96911543646114],\n [90.88173857544798],\n [85.57577562769455],\n [84.59606792462631],\n [76.36614033466836],\n [88.05698274252894],\n [84.45792716017291],\n [37.843408982058754],\n [79.8133758787931],\n [74.70367130472056],\n [76.63735874489066],\n [67.53945085509298],\n [38.43799626249406],\n [89.2533621935275],\n [68.81714477259861],\n [42.77785994141986],\n [77.59312284613732],\n [54.67132283307823],\n [58.982436706585894],\n [41.97903690952082],\n [58.5125829893578],\n [52.951623959240024],\n [80.11523491243135],\n [77.21429849138015],\n [77.80570820128962],\n [71.53230122272454],\n [41.35447165947115],\n [82.80873282752985],\n [82.52704872461354],\n [46.01855603603243],\n [95.13763034688759],\n [84.61849870377839],\n [79.07571699175556],\n [78.67942707261147],\n [77.07444280308728],\n [82.23567059569247],\n [81.94883387487435],\n [82.35162957960378],\n [79.7392807871711],\n [65.78155753531959],\n [80.30808508741227],\n [76.91210162262728],\n [67.34491515328085],\n [69.3537270167792],\n [61.593505300335075],\n [58.219569244226676],\n [76.85607160953397],\n [67.7996414022619],\n [90.05920655969679],\n [57.999138011633555],\n [73.01576877991373],\n [74.48282461384238],\n [76.69657857284436],\n [80.85866625307139],\n [76.30244351770587],\n [66.33542908349155],\n [78.2842839600028],\n [69.14333619607036],\n [67.30538344956992],\n [89.83949937933788],\n [73.25731262450626],\n [72.88654891148877],\n [84.64791678192493],\n [64.23554365473443],\n [87.89084910180556],\n [80.9595270424688],\n [33.1789487167197],\n [71.55000596460984],\n [79.85022589459918],\n [81.1297635679818],\n [66.35339024811258],\n [64.63392108074603],\n [66.01379245799122],\n [76.05094695760225],\n [92.47299711881338],\n [90.11949045962338],\n [88.73923869790484],\n [88.65331250567132],\n [93.3947369513327],\n [93.15837841410156],\n [85.67799836006479],\n [65.88574269808576],\n [47.429211015981366],\n [89.91483834559894],\n [82.79355190078078],\n [80.87469900480478],\n [76.6773076908495],\n [81.14612962998827],\n [56.553380842564344],\n [58.55002321076399],\n [60.305322371215134],\n [61.98452680472422],\n [60.2339913803318],\n [63.21910552538504],\n [72.78482590338058],\n [84.19884060524805],\n [70.70920060100195],\n [63.21910552538504],\n [76.1376836138003],\n [78.92866391384432],\n [66.551753510015],\n [83.45158011688896],\n [66.62446369473278],\n [62.55111201796431],\n [59.81151337568028],\n [61.100185302511264],\n [86.05711263914478],\n [52.61438048402584],\n [67.73876391066054],\n [65.86400465767622],\n [42.74062039205033],\n [66.782177469721],\n [52.93734397045878],\n [53.649018463003756],\n [53.649018463003756],\n [53.649018463003756],\n [53.649018463003756],\n [71.78403482036335],\n [72.18496327444949],\n [81.42089972747864],\n [86.64962920311099],\n [64.05986727361454],\n [91.30919332162986],\n [55.03741156401446],\n [67.71409256864554],\n [76.57538091933209],\n [77.31176506908015],\n [91.57309806444313],\n [91.19854486827087],\n [85.21841129214738],\n [56.63537002695623],\n [53.03694248244908],\n [56.899621617331356],\n [77.3297934660266],\n [77.32520637428378],\n [80.02257895237571],\n [75.21332255369096],\n [57.990628069495116],\n [77.37214481098144],\n [77.37214481098144],\n [65.88850432579555],\n [76.43814547808067],\n [74.88120482355139],\n [46.49534661783618],\n [41.24425140618165],\n [83.73117519030586],\n [74.38787379538722],\n [45.86252358812847],\n [79.68993880893547],\n [42.531414672909236],\n [67.95621992303822],\n [62.52917132354367],\n [60.84691175310102],\n [82.42019544996913],\n [81.19320408474928],\n [81.36296632485262],\n [64.92511387418239],\n [79.57908806778318],\n [46.9076866234118],\n [45.71698952681092],\n [88.00334083046411],\n [72.81602041628292],\n [78.72661306264057],\n [79.39298036162641],\n [77.88466364926515],\n [66.77708551677975],\n [53.498917055480064],\n [47.268983086443626],\n [32.66897497717269],\n [42.22694773466871],\n [47.552903713956894],\n [51.21386711934121],\n [82.65314955266327],\n [52.079846876655346],\n [46.13829521381905],\n [83.78340863387481],\n [40.67693103988446],\n [87.96911543646114],\n [90.88173857544798],\n [85.57577562769455],\n [84.59606792462631],\n [76.36614033466836],\n [88.05698274252894],\n [84.45792716017291],\n [37.843408982058754],\n [79.8133758787931],\n [74.70367130472056],\n [76.63735874489066],\n [67.53945085509298],\n [38.43799626249406],\n [89.2533621935275],\n [68.81714477259861],\n [42.77785994141986],\n [77.59312284613732],\n [54.67132283307823],\n [58.982436706585894],\n [41.97903690952082],\n [58.5125829893578],\n [52.951623959240024],\n [80.11523491243135],\n [77.21429849138015],\n [77.80570820128962],\n [71.53230122272454],\n [41.35447165947115],\n [82.80873282752985],\n [82.52704872461354],\n [46.01855603603243],\n [95.13763034688759],\n [84.61849870377839],\n [79.07571699175556],\n [78.67942707261147],\n [77.07444280308728],\n [82.23567059569247],\n [81.94883387487435],\n [82.35162957960378],\n [79.7392807871711],\n [65.78155753531959],\n [80.30808508741227],\n [76.91210162262728],\n [67.34491515328085],\n [69.3537270167792],\n [61.593505300335075],\n [58.219569244226676],\n [76.85607160953397],\n [67.7996414022619],\n [90.05920655969679],\n [57.999138011633555],\n [73.01576877991373],\n [74.48282461384238],\n [76.69657857284436],\n [80.85866625307139],\n [76.30244351770587],\n [66.33542908349155],\n [78.2842839600028],\n [69.14333619607036],\n [67.30538344956992],\n [89.83949937933788],\n [73.25731262450626],\n [72.88654891148877],\n [84.64791678192493],\n [64.23554365473443],\n [87.89084910180556],\n [80.9595270424688],\n [33.1789487167197],\n [71.55000596460984],\n [79.85022589459918],\n [81.1297635679818],\n [66.35339024811258],\n [64.63392108074603],\n [66.01379245799122],\n [76.05094695760225],\n [92.47299711881338],\n [90.11949045962338],\n [88.73923869790484],\n [88.65331246311585],\n [93.39473689459214],\n [93.158378456657],\n [65.88574269815503]]\n</div>"]}}],"execution_count":113},{"cell_type":"markdown","source":["# HLM Binary"],"metadata":{}},{"cell_type":"code","source":["parser = ArgumentParser()\nadd_train_args(parser)\nargs = parser.parse_args(['--data_path',os.path.join(CHEMPROP_DIR,'hlm_eh_binary.csv'),\n                          '--dataset_type','classification',\n                          '--checkpoint_path',os.path.join(CHEMPROP_DIR,'JAK','checkpoints','binary-hopt-prelu200e','model/fold_0/model_0/model.pt'),\n                          #'--multiclass_num_classes','2',\n                          #'--metric','auc',\n                          '--save_dir',os.path.join(CHEMPROP_DIR,'JAK','checkpoints','binary-hopt-prelu300e','model'),\n                          '--log_frequency','1',\n                          '--depth','7',\n                          '--dropout','0.05',\n                          '--hidden_size','2000',\n                          '--ffn_num_layers','5',\n                          '--save_smiles_splits',\n                          #'--split_type','scaffold_balanced',\n                          #'--num_folds','10',\n                          #'--split_sizes','1.0','0.0','0.0',\n                          '--activation','PReLU',\n                          \n                          '--epochs','100'])\nmodify_train_args(args)\nlogger = create_logger(name='train', save_dir=args.save_dir, quiet=args.quiet)\ncross_validate(args, logger)\n#10x cv 0.7612281044640512, 0.1151609264809672 - { \"depth\": 5, \"dropout\": 0.0, \"ffn_num_layers\": 1, \"hidden_size\": 2100 }\n#10x cv 0.7675366338044097, 0.11764713076888747 - { \"activation\": \"PReLU\", \"depth\": 7, \"dropout\": 0.05, \"ffn_num_layers\": 5, \"hidden_size\": 2000 }"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fold 0\nFold 0\n{&#39;activation&#39;: &#39;PReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu200e/model/fold_0/model_0/model.pt&#39;,\n &#39;checkpoint_paths&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu200e/model/fold_0/model_0/model.pt&#39;],\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/hlm_eh_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 7,\n &#39;dropout&#39;: 0.05,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 100,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2000,\n &#39;ffn_num_layers&#39;: 5,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2000,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu300e/model/fold_0&#39;,\n &#39;save_smiles_splits&#39;: True,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\n{&#39;activation&#39;: &#39;PReLU&#39;,\n &#39;atom_messages&#39;: False,\n &#39;batch_size&#39;: 50,\n &#39;bias&#39;: False,\n &#39;checkpoint_dir&#39;: None,\n &#39;checkpoint_path&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu200e/model/fold_0/model_0/model.pt&#39;,\n &#39;checkpoint_paths&#39;: [&#39;/dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu200e/model/fold_0/model_0/model.pt&#39;],\n &#39;config_path&#39;: None,\n &#39;cuda&#39;: True,\n &#39;data_path&#39;: &#39;/dbfs/FileStore/chemprop/hlm_eh_binary.csv&#39;,\n &#39;dataset_type&#39;: &#39;classification&#39;,\n &#39;depth&#39;: 7,\n &#39;dropout&#39;: 0.05,\n &#39;ensemble_size&#39;: 1,\n &#39;epochs&#39;: 100,\n &#39;features_generator&#39;: None,\n &#39;features_only&#39;: False,\n &#39;features_path&#39;: None,\n &#39;features_scaling&#39;: True,\n &#39;ffn_hidden_size&#39;: 2000,\n &#39;ffn_num_layers&#39;: 5,\n &#39;final_lr&#39;: 0.0001,\n &#39;folds_file&#39;: None,\n &#39;gpu&#39;: None,\n &#39;hidden_size&#39;: 2000,\n &#39;init_lr&#39;: 0.0001,\n &#39;log_frequency&#39;: 1,\n &#39;max_data_size&#39;: None,\n &#39;max_lr&#39;: 0.001,\n &#39;metric&#39;: &#39;auc&#39;,\n &#39;minimize_score&#39;: False,\n &#39;no_cache&#39;: False,\n &#39;num_folds&#39;: 1,\n &#39;num_lrs&#39;: 1,\n &#39;quiet&#39;: False,\n &#39;save_dir&#39;: &#39;/dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu300e/model/fold_0&#39;,\n &#39;save_smiles_splits&#39;: True,\n &#39;seed&#39;: 0,\n &#39;separate_test_features_path&#39;: None,\n &#39;separate_test_path&#39;: None,\n &#39;separate_val_features_path&#39;: None,\n &#39;separate_val_path&#39;: None,\n &#39;show_individual_scores&#39;: False,\n &#39;split_sizes&#39;: [0.8, 0.1, 0.1],\n &#39;split_type&#39;: &#39;random&#39;,\n &#39;test&#39;: False,\n &#39;test_fold_index&#39;: None,\n &#39;undirected&#39;: False,\n &#39;use_compound_names&#39;: False,\n &#39;use_input_features&#39;: None,\n &#39;val_fold_index&#39;: None,\n &#39;warmup_epochs&#39;: 2.0}\nLoading data\nLoading data\n\n\r  0%|          | 0/804 [00:00&lt;?, ?it/s]\n\r 32%|███▏      | 254/804 [00:00&lt;00:00, 2536.36it/s]\n\r 59%|█████▉    | 473/804 [00:00&lt;00:00, 2418.64it/s]\n\r 81%|████████  | 650/804 [00:00&lt;00:00, 2176.23it/s]\n\r100%|██████████| 804/804 [00:00&lt;00:00, 2056.78it/s]Warning: 2 SMILES are invalid.\nWarning: 2 SMILES are invalid.\nNumber of tasks = 1\nNumber of tasks = 1\nSplitting data with seed 0\nSplitting data with seed 0\nClass sizes\nClass sizes\nHLM_binary 0: 14.96%, 1: 85.04%\nHLM_binary 0: 14.96%, 1: 85.04%\nTotal size = 802 | train size = 641 | val size = 80 | test size = 81\nTotal size = 802 | train size = 641 | val size = 80 | test size = 81\nLoading model 0 from /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu200e/model/fold_0/model_0/model.pt\nLoading model 0 from /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu200e/model/fold_0/model_0/model.pt\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.05)\n      (act_func): PReLU(num_parameters=1)\n      (W_i): Linear(in_features=147, out_features=2000, bias=False)\n      (W_h): Linear(in_features=2000, out_features=2000, bias=False)\n      (W_o): Linear(in_features=2133, out_features=2000, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.05)\n    (1): Linear(in_features=2000, out_features=2000, bias=True)\n    (2): PReLU(num_parameters=1)\n    (3): Dropout(p=0.05)\n    (4): Linear(in_features=2000, out_features=2000, bias=True)\n    (5): PReLU(num_parameters=1)\n    (6): Dropout(p=0.05)\n    (7): Linear(in_features=2000, out_features=2000, bias=True)\n    (8): PReLU(num_parameters=1)\n    (9): Dropout(p=0.05)\n    (10): Linear(in_features=2000, out_features=2000, bias=True)\n    (11): PReLU(num_parameters=1)\n    (12): Dropout(p=0.05)\n    (13): Linear(in_features=2000, out_features=1, bias=True)\n  )\n)\nMoleculeModel(\n  (sigmoid): Sigmoid()\n  (encoder): MPN(\n    (encoder): MPNEncoder(\n      (dropout_layer): Dropout(p=0.05)\n      (act_func): PReLU(num_parameters=1)\n      (W_i): Linear(in_features=147, out_features=2000, bias=False)\n      (W_h): Linear(in_features=2000, out_features=2000, bias=False)\n      (W_o): Linear(in_features=2133, out_features=2000, bias=True)\n    )\n  )\n  (ffn): Sequential(\n    (0): Dropout(p=0.05)\n    (1): Linear(in_features=2000, out_features=2000, bias=True)\n    (2): PReLU(num_parameters=1)\n    (3): Dropout(p=0.05)\n    (4): Linear(in_features=2000, out_features=2000, bias=True)\n    (5): PReLU(num_parameters=1)\n    (6): Dropout(p=0.05)\n    (7): Linear(in_features=2000, out_features=2000, bias=True)\n    (8): PReLU(num_parameters=1)\n    (9): Dropout(p=0.05)\n    (10): Linear(in_features=2000, out_features=2000, bias=True)\n    (11): PReLU(num_parameters=1)\n    (12): Dropout(p=0.05)\n    (13): Linear(in_features=2000, out_features=1, bias=True)\n  )\n)\nNumber of parameters = 24,572,003\nNumber of parameters = 24,572,003\nMoving model to cuda\nMoving model to cuda\n\n\r  0%|          | 0/100 [00:00&lt;?, ?it/s]Epoch 0\nEpoch 0\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.2505e-05, PNorm = 130.3841, GNorm = 0.0354, lr_0 = 1.3750e-04\nLoss = 1.2505e-05, PNorm = 130.3841, GNorm = 0.0354, lr_0 = 1.3750e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.08it/s]Loss = 1.0041e-06, PNorm = 130.3882, GNorm = 0.0201, lr_0 = 1.7500e-04\nLoss = 1.0041e-06, PNorm = 130.3882, GNorm = 0.0201, lr_0 = 1.7500e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.10it/s]Loss = 7.7013e-06, PNorm = 130.3935, GNorm = 0.0560, lr_0 = 2.1250e-04\nLoss = 7.7013e-06, PNorm = 130.3935, GNorm = 0.0560, lr_0 = 2.1250e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.20it/s]Loss = 4.0115e-04, PNorm = 130.3968, GNorm = 0.3460, lr_0 = 2.5000e-04\nLoss = 4.0115e-04, PNorm = 130.3968, GNorm = 0.3460, lr_0 = 2.5000e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.22it/s]Loss = 6.7614e-08, PNorm = 130.4004, GNorm = 0.0010, lr_0 = 2.8750e-04\nLoss = 6.7614e-08, PNorm = 130.4004, GNorm = 0.0010, lr_0 = 2.8750e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.25it/s]Loss = 2.5689e-05, PNorm = 130.4050, GNorm = 0.4049, lr_0 = 3.2500e-04\nLoss = 2.5689e-05, PNorm = 130.4050, GNorm = 0.4049, lr_0 = 3.2500e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.27it/s]Loss = 1.4351e-06, PNorm = 130.4104, GNorm = 0.0309, lr_0 = 3.6250e-04\nLoss = 1.4351e-06, PNorm = 130.4104, GNorm = 0.0309, lr_0 = 3.6250e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.30it/s]Loss = 1.4454e-06, PNorm = 130.4164, GNorm = 0.0477, lr_0 = 4.0000e-04\nLoss = 1.4454e-06, PNorm = 130.4164, GNorm = 0.0477, lr_0 = 4.0000e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.32it/s]Loss = 5.6409e-08, PNorm = 130.4227, GNorm = 0.0001, lr_0 = 4.3750e-04\nLoss = 5.6409e-08, PNorm = 130.4227, GNorm = 0.0001, lr_0 = 4.3750e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.34it/s]Loss = 1.2831e-05, PNorm = 130.4305, GNorm = 0.0462, lr_0 = 4.7500e-04\nLoss = 1.2831e-05, PNorm = 130.4305, GNorm = 0.0462, lr_0 = 4.7500e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.35it/s]Loss = 1.8543e-03, PNorm = 130.4312, GNorm = 2.2459, lr_0 = 5.1250e-04\nLoss = 1.8543e-03, PNorm = 130.4312, GNorm = 2.2459, lr_0 = 5.1250e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.34it/s]Loss = 1.3291e-03, PNorm = 130.4333, GNorm = 0.8765, lr_0 = 5.5000e-04\nLoss = 1.3291e-03, PNorm = 130.4333, GNorm = 0.8765, lr_0 = 5.5000e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.39it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.92it/s]Validation auc = 0.703077\nValidation auc = 0.703077\n\n\r  1%|          | 1/100 [00:21&lt;35:59, 21.82s/it]Epoch 1\nEpoch 1\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 4.6133e-06, PNorm = 130.4359, GNorm = 0.3164, lr_0 = 5.8750e-04\nLoss = 4.6133e-06, PNorm = 130.4359, GNorm = 0.3164, lr_0 = 5.8750e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.15it/s]Loss = 2.6712e-03, PNorm = 130.4333, GNorm = 12.4998, lr_0 = 6.2500e-04\nLoss = 2.6712e-03, PNorm = 130.4333, GNorm = 12.4998, lr_0 = 6.2500e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.22it/s]Loss = 2.8575e-04, PNorm = 130.4326, GNorm = 0.7729, lr_0 = 6.6250e-04\nLoss = 2.8575e-04, PNorm = 130.4326, GNorm = 0.7729, lr_0 = 6.6250e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.28it/s]Loss = 5.8861e-06, PNorm = 130.4348, GNorm = 0.0842, lr_0 = 7.0000e-04\nLoss = 5.8861e-06, PNorm = 130.4348, GNorm = 0.0842, lr_0 = 7.0000e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.26it/s]Loss = 7.4422e-06, PNorm = 130.4404, GNorm = 0.0338, lr_0 = 7.3750e-04\nLoss = 7.4422e-06, PNorm = 130.4404, GNorm = 0.0338, lr_0 = 7.3750e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.30it/s]Loss = 2.8217e-03, PNorm = 130.4441, GNorm = 9.6196, lr_0 = 7.7500e-04\nLoss = 2.8217e-03, PNorm = 130.4441, GNorm = 9.6196, lr_0 = 7.7500e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.32it/s]Loss = 1.6431e-03, PNorm = 130.4486, GNorm = 12.0629, lr_0 = 8.1250e-04\nLoss = 1.6431e-03, PNorm = 130.4486, GNorm = 12.0629, lr_0 = 8.1250e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.34it/s]Loss = 3.4030e-03, PNorm = 130.4453, GNorm = 3.9777, lr_0 = 8.5000e-04\nLoss = 3.4030e-03, PNorm = 130.4453, GNorm = 3.9777, lr_0 = 8.5000e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.32it/s]Loss = 1.0038e-05, PNorm = 130.4506, GNorm = 0.1576, lr_0 = 8.8750e-04\nLoss = 1.0038e-05, PNorm = 130.4506, GNorm = 0.1576, lr_0 = 8.8750e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.33it/s]Loss = 5.5932e-08, PNorm = 130.4622, GNorm = 0.0010, lr_0 = 9.2500e-04\nLoss = 5.5932e-08, PNorm = 130.4622, GNorm = 0.0010, lr_0 = 9.2500e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.36it/s]Loss = 2.9030e-04, PNorm = 130.4740, GNorm = 9.2006, lr_0 = 9.6250e-04\nLoss = 2.9030e-04, PNorm = 130.4740, GNorm = 9.2006, lr_0 = 9.6250e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.34it/s]Loss = 5.3235e-03, PNorm = 130.4821, GNorm = 9.3894, lr_0 = 1.0000e-03\nLoss = 5.3235e-03, PNorm = 130.4821, GNorm = 9.3894, lr_0 = 1.0000e-03\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.33it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 24.01it/s]Validation auc = 0.690769\nValidation auc = 0.690769\n\n\r  2%|▏         | 2/100 [00:24&lt;26:05, 15.97s/it]Epoch 2\nEpoch 2\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 9.3041e-04, PNorm = 130.4958, GNorm = 21.2305, lr_0 = 9.9804e-04\nLoss = 9.3041e-04, PNorm = 130.4958, GNorm = 21.2305, lr_0 = 9.9804e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.09it/s]Loss = 2.0318e-03, PNorm = 130.5150, GNorm = 13.9396, lr_0 = 9.9609e-04\nLoss = 2.0318e-03, PNorm = 130.5150, GNorm = 13.9396, lr_0 = 9.9609e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.15it/s]Loss = 3.5402e-04, PNorm = 130.5395, GNorm = 2.2276, lr_0 = 9.9414e-04\nLoss = 3.5402e-04, PNorm = 130.5395, GNorm = 2.2276, lr_0 = 9.9414e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.19it/s]Loss = 1.6874e-03, PNorm = 130.5606, GNorm = 10.3648, lr_0 = 9.9220e-04\nLoss = 1.6874e-03, PNorm = 130.5606, GNorm = 10.3648, lr_0 = 9.9220e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.23it/s]Loss = 1.5750e-03, PNorm = 130.5852, GNorm = 3.2211, lr_0 = 9.9026e-04\nLoss = 1.5750e-03, PNorm = 130.5852, GNorm = 3.2211, lr_0 = 9.9026e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.26it/s]Loss = 6.2670e-03, PNorm = 130.6075, GNorm = 5.5499, lr_0 = 9.8832e-04\nLoss = 6.2670e-03, PNorm = 130.6075, GNorm = 5.5499, lr_0 = 9.8832e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.27it/s]Loss = 2.8249e-03, PNorm = 130.6479, GNorm = 8.0493, lr_0 = 9.8639e-04\nLoss = 2.8249e-03, PNorm = 130.6479, GNorm = 8.0493, lr_0 = 9.8639e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.33it/s]Loss = 1.1261e-03, PNorm = 130.7013, GNorm = 3.4535, lr_0 = 9.8446e-04\nLoss = 1.1261e-03, PNorm = 130.7013, GNorm = 3.4535, lr_0 = 9.8446e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.34it/s]Loss = 3.4011e-03, PNorm = 130.7522, GNorm = 14.9988, lr_0 = 9.8253e-04\nLoss = 3.4011e-03, PNorm = 130.7522, GNorm = 14.9988, lr_0 = 9.8253e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.35it/s]Loss = 2.1036e-03, PNorm = 130.8068, GNorm = 4.5722, lr_0 = 9.8061e-04\nLoss = 2.1036e-03, PNorm = 130.8068, GNorm = 4.5722, lr_0 = 9.8061e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.37it/s]Loss = 1.1198e-03, PNorm = 130.8675, GNorm = 0.5595, lr_0 = 9.7869e-04\nLoss = 1.1198e-03, PNorm = 130.8675, GNorm = 0.5595, lr_0 = 9.7869e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.36it/s]Loss = 3.1387e-03, PNorm = 130.9312, GNorm = 3.2163, lr_0 = 9.7678e-04\nLoss = 3.1387e-03, PNorm = 130.9312, GNorm = 3.2163, lr_0 = 9.7678e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.38it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.85it/s]Validation auc = 0.720513\nValidation auc = 0.720513\n\n\r  3%|▎         | 3/100 [00:43&lt;27:30, 17.02s/it]Epoch 3\nEpoch 3\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.0163e-03, PNorm = 131.0037, GNorm = 0.5106, lr_0 = 9.7487e-04\nLoss = 1.0163e-03, PNorm = 131.0037, GNorm = 0.5106, lr_0 = 9.7487e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.42it/s]Loss = 2.2133e-03, PNorm = 131.0699, GNorm = 9.6128, lr_0 = 9.7296e-04\nLoss = 2.2133e-03, PNorm = 131.0699, GNorm = 9.6128, lr_0 = 9.7296e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.38it/s]Loss = 1.0612e-03, PNorm = 131.1533, GNorm = 1.9252, lr_0 = 9.7106e-04\nLoss = 1.0612e-03, PNorm = 131.1533, GNorm = 1.9252, lr_0 = 9.7106e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.38it/s]Loss = 8.8004e-04, PNorm = 131.2338, GNorm = 2.8865, lr_0 = 9.6916e-04\nLoss = 8.8004e-04, PNorm = 131.2338, GNorm = 2.8865, lr_0 = 9.6916e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.37it/s]Loss = 3.4748e-04, PNorm = 131.3225, GNorm = 0.6212, lr_0 = 9.6726e-04\nLoss = 3.4748e-04, PNorm = 131.3225, GNorm = 0.6212, lr_0 = 9.6726e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.38it/s]Loss = 4.4432e-03, PNorm = 131.3486, GNorm = 37.8759, lr_0 = 9.6537e-04\nLoss = 4.4432e-03, PNorm = 131.3486, GNorm = 37.8759, lr_0 = 9.6537e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.36it/s]Loss = 1.9483e-03, PNorm = 131.3701, GNorm = 16.2651, lr_0 = 9.6348e-04\nLoss = 1.9483e-03, PNorm = 131.3701, GNorm = 16.2651, lr_0 = 9.6348e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.37it/s]Loss = 1.7565e-03, PNorm = 131.3825, GNorm = 31.7496, lr_0 = 9.6160e-04\nLoss = 1.7565e-03, PNorm = 131.3825, GNorm = 31.7496, lr_0 = 9.6160e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  4.39it/s]Loss = 3.3190e-04, PNorm = 131.4073, GNorm = 1.4751, lr_0 = 9.5972e-04\nLoss = 3.3190e-04, PNorm = 131.4073, GNorm = 1.4751, lr_0 = 9.5972e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  4.66it/s]Loss = 4.0470e-03, PNorm = 131.4292, GNorm = 4.9437, lr_0 = 9.5784e-04\nLoss = 4.0470e-03, PNorm = 131.4292, GNorm = 4.9437, lr_0 = 9.5784e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  4.85it/s]Loss = 1.1824e-03, PNorm = 131.4622, GNorm = 14.4934, lr_0 = 9.5597e-04\nLoss = 1.1824e-03, PNorm = 131.4622, GNorm = 14.4934, lr_0 = 9.5597e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.03it/s]Loss = 9.3979e-04, PNorm = 131.5056, GNorm = 1.9032, lr_0 = 9.5410e-04\nLoss = 9.3979e-04, PNorm = 131.5056, GNorm = 1.9032, lr_0 = 9.5410e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.11it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.66it/s]Validation auc = 0.717949\nValidation auc = 0.717949\n\n\r  4%|▍         | 4/100 [00:46&lt;20:14, 12.65s/it]Epoch 4\nEpoch 4\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.0198e-03, PNorm = 131.5599, GNorm = 2.1584, lr_0 = 9.5223e-04\nLoss = 1.0198e-03, PNorm = 131.5599, GNorm = 2.1584, lr_0 = 9.5223e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.34it/s]Loss = 1.2452e-03, PNorm = 131.6250, GNorm = 3.4981, lr_0 = 9.5037e-04\nLoss = 1.2452e-03, PNorm = 131.6250, GNorm = 3.4981, lr_0 = 9.5037e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.31it/s]Loss = 2.9349e-03, PNorm = 131.6798, GNorm = 6.1280, lr_0 = 9.4851e-04\nLoss = 2.9349e-03, PNorm = 131.6798, GNorm = 6.1280, lr_0 = 9.4851e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.30it/s]Loss = 3.3163e-04, PNorm = 131.7426, GNorm = 0.4020, lr_0 = 9.4665e-04\nLoss = 3.3163e-04, PNorm = 131.7426, GNorm = 0.4020, lr_0 = 9.4665e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.27it/s]Loss = 7.1310e-04, PNorm = 131.8077, GNorm = 2.4161, lr_0 = 9.4480e-04\nLoss = 7.1310e-04, PNorm = 131.8077, GNorm = 2.4161, lr_0 = 9.4480e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.31it/s]Loss = 6.8344e-04, PNorm = 131.8729, GNorm = 10.8591, lr_0 = 9.4295e-04\nLoss = 6.8344e-04, PNorm = 131.8729, GNorm = 10.8591, lr_0 = 9.4295e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.30it/s]Loss = 1.4005e-03, PNorm = 131.9357, GNorm = 15.6441, lr_0 = 9.4111e-04\nLoss = 1.4005e-03, PNorm = 131.9357, GNorm = 15.6441, lr_0 = 9.4111e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.33it/s]Loss = 2.7214e-04, PNorm = 132.0040, GNorm = 9.6631, lr_0 = 9.3927e-04\nLoss = 2.7214e-04, PNorm = 132.0040, GNorm = 9.6631, lr_0 = 9.3927e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.31it/s]Loss = 9.6644e-04, PNorm = 132.0656, GNorm = 4.5201, lr_0 = 9.3743e-04\nLoss = 9.6644e-04, PNorm = 132.0656, GNorm = 4.5201, lr_0 = 9.3743e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.28it/s]Loss = 6.1179e-04, PNorm = 132.1374, GNorm = 5.5515, lr_0 = 9.3560e-04\nLoss = 6.1179e-04, PNorm = 132.1374, GNorm = 5.5515, lr_0 = 9.3560e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.32it/s]Loss = 2.1422e-03, PNorm = 132.1973, GNorm = 3.6855, lr_0 = 9.3377e-04\nLoss = 2.1422e-03, PNorm = 132.1973, GNorm = 3.6855, lr_0 = 9.3377e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.36it/s]Loss = 9.9972e-04, PNorm = 132.2668, GNorm = 2.2049, lr_0 = 9.3194e-04\nLoss = 9.9972e-04, PNorm = 132.2668, GNorm = 2.2049, lr_0 = 9.3194e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.37it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 24.36it/s]Validation auc = 0.736410\nValidation auc = 0.736410\n\n\r  5%|▌         | 5/100 [01:05&lt;23:12, 14.66s/it]Epoch 5\nEpoch 5\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.1768e-03, PNorm = 132.3430, GNorm = 2.2071, lr_0 = 9.3012e-04\nLoss = 1.1768e-03, PNorm = 132.3430, GNorm = 2.2071, lr_0 = 9.3012e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  4.99it/s]Loss = 1.5128e-03, PNorm = 132.4081, GNorm = 20.9916, lr_0 = 9.2830e-04\nLoss = 1.5128e-03, PNorm = 132.4081, GNorm = 20.9916, lr_0 = 9.2830e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.07it/s]Loss = 1.0037e-03, PNorm = 132.4758, GNorm = 22.3693, lr_0 = 9.2648e-04\nLoss = 1.0037e-03, PNorm = 132.4758, GNorm = 22.3693, lr_0 = 9.2648e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.14it/s]Loss = 2.2171e-03, PNorm = 132.5379, GNorm = 30.3427, lr_0 = 9.2467e-04\nLoss = 2.2171e-03, PNorm = 132.5379, GNorm = 30.3427, lr_0 = 9.2467e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.22it/s]Loss = 1.1377e-05, PNorm = 132.6073, GNorm = 0.0389, lr_0 = 9.2286e-04\nLoss = 1.1377e-05, PNorm = 132.6073, GNorm = 0.0389, lr_0 = 9.2286e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.30it/s]Loss = 5.7025e-03, PNorm = 132.6559, GNorm = 6.3176, lr_0 = 9.2106e-04\nLoss = 5.7025e-03, PNorm = 132.6559, GNorm = 6.3176, lr_0 = 9.2106e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.31it/s]Loss = 1.9817e-03, PNorm = 132.7027, GNorm = 25.9255, lr_0 = 9.1925e-04\nLoss = 1.9817e-03, PNorm = 132.7027, GNorm = 25.9255, lr_0 = 9.1925e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.34it/s]Loss = 2.7581e-04, PNorm = 132.7568, GNorm = 1.5837, lr_0 = 9.1746e-04\nLoss = 2.7581e-04, PNorm = 132.7568, GNorm = 1.5837, lr_0 = 9.1746e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.33it/s]Loss = 2.2651e-03, PNorm = 132.8113, GNorm = 9.9994, lr_0 = 9.1566e-04\nLoss = 2.2651e-03, PNorm = 132.8113, GNorm = 9.9994, lr_0 = 9.1566e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.35it/s]Loss = 1.2160e-03, PNorm = 132.8692, GNorm = 3.7596, lr_0 = 9.1387e-04\nLoss = 1.2160e-03, PNorm = 132.8692, GNorm = 3.7596, lr_0 = 9.1387e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.37it/s]Loss = 7.6617e-04, PNorm = 132.9300, GNorm = 2.9616, lr_0 = 9.1208e-04\nLoss = 7.6617e-04, PNorm = 132.9300, GNorm = 2.9616, lr_0 = 9.1208e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.31it/s]Loss = 2.8674e-03, PNorm = 132.9914, GNorm = 8.6737, lr_0 = 9.1030e-04\nLoss = 2.8674e-03, PNorm = 132.9914, GNorm = 8.6737, lr_0 = 9.1030e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.33it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 24.56it/s]Validation auc = 0.690256\nValidation auc = 0.690256\n\n\r  6%|▌         | 6/100 [01:07&lt;17:10, 10.96s/it]Epoch 6\nEpoch 6\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.1411e-03, PNorm = 133.0557, GNorm = 1.3506, lr_0 = 9.0852e-04\nLoss = 1.1411e-03, PNorm = 133.0557, GNorm = 1.3506, lr_0 = 9.0852e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.35it/s]Loss = 1.6776e-03, PNorm = 133.1222, GNorm = 2.6106, lr_0 = 9.0674e-04\nLoss = 1.6776e-03, PNorm = 133.1222, GNorm = 2.6106, lr_0 = 9.0674e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.35it/s]Loss = 2.5273e-03, PNorm = 133.1857, GNorm = 17.9991, lr_0 = 9.0497e-04\nLoss = 2.5273e-03, PNorm = 133.1857, GNorm = 17.9991, lr_0 = 9.0497e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.35it/s]Loss = 5.0753e-04, PNorm = 133.2532, GNorm = 2.4854, lr_0 = 9.0320e-04\nLoss = 5.0753e-04, PNorm = 133.2532, GNorm = 2.4854, lr_0 = 9.0320e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.39it/s]Loss = 3.0659e-04, PNorm = 133.3217, GNorm = 1.1057, lr_0 = 9.0143e-04\nLoss = 3.0659e-04, PNorm = 133.3217, GNorm = 1.1057, lr_0 = 9.0143e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.39it/s]Loss = 1.2635e-03, PNorm = 133.3845, GNorm = 10.5289, lr_0 = 8.9967e-04\nLoss = 1.2635e-03, PNorm = 133.3845, GNorm = 10.5289, lr_0 = 8.9967e-04\n\n*** WARNING: skipped 224481 bytes of output ***\n\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.38it/s]Loss = 9.5367e-11, PNorm = 137.2271, GNorm = 0.0000, lr_0 = 1.2163e-04\nLoss = 9.5367e-11, PNorm = 137.2271, GNorm = 0.0000, lr_0 = 1.2163e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.35it/s]Loss = 4.2915e-10, PNorm = 137.2271, GNorm = 0.0000, lr_0 = 1.2139e-04\nLoss = 4.2915e-10, PNorm = 137.2271, GNorm = 0.0000, lr_0 = 1.2139e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.37it/s]Loss = 2.1103e-04, PNorm = 137.2272, GNorm = 0.0784, lr_0 = 1.2115e-04\nLoss = 2.1103e-04, PNorm = 137.2272, GNorm = 0.0784, lr_0 = 1.2115e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.38it/s]Loss = 3.5586e-04, PNorm = 137.2272, GNorm = 0.0877, lr_0 = 1.2092e-04\nLoss = 3.5586e-04, PNorm = 137.2272, GNorm = 0.0877, lr_0 = 1.2092e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.41it/s]Loss = 2.8419e-08, PNorm = 137.2273, GNorm = 0.0002, lr_0 = 1.2068e-04\nLoss = 2.8419e-08, PNorm = 137.2273, GNorm = 0.0002, lr_0 = 1.2068e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.41it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 22.84it/s]Validation auc = 0.691795\nValidation auc = 0.691795\n\n\r 92%|█████████▏| 92/100 [05:32&lt;00:18,  2.34s/it]Epoch 92\nEpoch 92\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.3755e-07, PNorm = 137.2273, GNorm = 0.0036, lr_0 = 1.2044e-04\nLoss = 1.3755e-07, PNorm = 137.2273, GNorm = 0.0036, lr_0 = 1.2044e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.47it/s]Loss = 2.3842e-09, PNorm = 137.2274, GNorm = 0.0001, lr_0 = 1.2021e-04\nLoss = 2.3842e-09, PNorm = 137.2274, GNorm = 0.0001, lr_0 = 1.2021e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.44it/s]Loss = 6.4323e-08, PNorm = 137.2274, GNorm = 0.0006, lr_0 = 1.1997e-04\nLoss = 6.4323e-08, PNorm = 137.2274, GNorm = 0.0006, lr_0 = 1.1997e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.43it/s]Loss = 1.9606e-04, PNorm = 137.2275, GNorm = 0.0764, lr_0 = 1.1974e-04\nLoss = 1.9606e-04, PNorm = 137.2275, GNorm = 0.0764, lr_0 = 1.1974e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.37it/s]Loss = 2.1545e-04, PNorm = 137.2276, GNorm = 0.0882, lr_0 = 1.1950e-04\nLoss = 2.1545e-04, PNorm = 137.2276, GNorm = 0.0882, lr_0 = 1.1950e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.37it/s]Loss = 2.0469e-04, PNorm = 137.2276, GNorm = 0.0623, lr_0 = 1.1927e-04\nLoss = 2.0469e-04, PNorm = 137.2276, GNorm = 0.0623, lr_0 = 1.1927e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.35it/s]Loss = 4.3392e-09, PNorm = 137.2277, GNorm = 0.0002, lr_0 = 1.1904e-04\nLoss = 4.3392e-09, PNorm = 137.2277, GNorm = 0.0002, lr_0 = 1.1904e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.36it/s]Loss = 1.4114e-08, PNorm = 137.2278, GNorm = 0.0001, lr_0 = 1.1880e-04\nLoss = 1.4114e-08, PNorm = 137.2278, GNorm = 0.0001, lr_0 = 1.1880e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.32it/s]Loss = 3.5842e-04, PNorm = 137.2278, GNorm = 0.0988, lr_0 = 1.1857e-04\nLoss = 3.5842e-04, PNorm = 137.2278, GNorm = 0.0988, lr_0 = 1.1857e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.35it/s]Loss = 3.7288e-04, PNorm = 137.2279, GNorm = 0.1477, lr_0 = 1.1834e-04\nLoss = 3.7288e-04, PNorm = 137.2279, GNorm = 0.1477, lr_0 = 1.1834e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.35it/s]Loss = 3.8147e-10, PNorm = 137.2279, GNorm = 0.0000, lr_0 = 1.1811e-04\nLoss = 3.8147e-10, PNorm = 137.2279, GNorm = 0.0000, lr_0 = 1.1811e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.36it/s]Loss = 5.2452e-10, PNorm = 137.2280, GNorm = 0.0000, lr_0 = 1.1788e-04\nLoss = 5.2452e-10, PNorm = 137.2280, GNorm = 0.0000, lr_0 = 1.1788e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.39it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r 50%|█████     | 1/2 [00:00&lt;00:00,  5.36it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00,  9.19it/s]Validation auc = 0.691795\nValidation auc = 0.691795\n\n\r 93%|█████████▎| 93/100 [05:34&lt;00:16,  2.38s/it]Epoch 93\nEpoch 93\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 4.0813e-04, PNorm = 137.2281, GNorm = 0.1443, lr_0 = 1.1765e-04\nLoss = 4.0813e-04, PNorm = 137.2281, GNorm = 0.1443, lr_0 = 1.1765e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.31it/s]Loss = 3.6355e-04, PNorm = 137.2281, GNorm = 0.1291, lr_0 = 1.1742e-04\nLoss = 3.6355e-04, PNorm = 137.2281, GNorm = 0.1291, lr_0 = 1.1742e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.33it/s]Loss = 1.0967e-09, PNorm = 137.2282, GNorm = 0.0000, lr_0 = 1.1719e-04\nLoss = 1.0967e-09, PNorm = 137.2282, GNorm = 0.0000, lr_0 = 1.1719e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.33it/s]Loss = 8.5831e-10, PNorm = 137.2282, GNorm = 0.0000, lr_0 = 1.1696e-04\nLoss = 8.5831e-10, PNorm = 137.2282, GNorm = 0.0000, lr_0 = 1.1696e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.35it/s]Loss = 0.0000e+00, PNorm = 137.2283, GNorm = 0.0000, lr_0 = 1.1673e-04\nLoss = 0.0000e+00, PNorm = 137.2283, GNorm = 0.0000, lr_0 = 1.1673e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.35it/s]Loss = 5.6272e-04, PNorm = 137.2283, GNorm = 0.0973, lr_0 = 1.1650e-04\nLoss = 5.6272e-04, PNorm = 137.2283, GNorm = 0.0973, lr_0 = 1.1650e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.36it/s]Loss = 3.6810e-08, PNorm = 137.2284, GNorm = 0.0015, lr_0 = 1.1627e-04\nLoss = 3.6810e-08, PNorm = 137.2284, GNorm = 0.0015, lr_0 = 1.1627e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.36it/s]Loss = 4.2915e-09, PNorm = 137.2284, GNorm = 0.0001, lr_0 = 1.1604e-04\nLoss = 4.2915e-09, PNorm = 137.2284, GNorm = 0.0001, lr_0 = 1.1604e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.35it/s]Loss = 7.9631e-09, PNorm = 137.2285, GNorm = 0.0001, lr_0 = 1.1582e-04\nLoss = 7.9631e-09, PNorm = 137.2285, GNorm = 0.0001, lr_0 = 1.1582e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.29it/s]Loss = 9.9269e-08, PNorm = 137.2285, GNorm = 0.0034, lr_0 = 1.1559e-04\nLoss = 9.9269e-08, PNorm = 137.2285, GNorm = 0.0034, lr_0 = 1.1559e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.34it/s]Loss = 2.2888e-09, PNorm = 137.2286, GNorm = 0.0000, lr_0 = 1.1537e-04\nLoss = 2.2888e-09, PNorm = 137.2286, GNorm = 0.0000, lr_0 = 1.1537e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.33it/s]Loss = 3.8147e-10, PNorm = 137.2286, GNorm = 0.0000, lr_0 = 1.1514e-04\nLoss = 3.8147e-10, PNorm = 137.2286, GNorm = 0.0000, lr_0 = 1.1514e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.35it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 24.62it/s]Validation auc = 0.691795\nValidation auc = 0.691795\n\n\r 94%|█████████▍| 94/100 [05:36&lt;00:14,  2.36s/it]Epoch 94\nEpoch 94\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 5.7503e-04, PNorm = 137.2287, GNorm = 0.0492, lr_0 = 1.1491e-04\nLoss = 5.7503e-04, PNorm = 137.2287, GNorm = 0.0492, lr_0 = 1.1491e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.23it/s]Loss = 4.7633e-08, PNorm = 137.2287, GNorm = 0.0003, lr_0 = 1.1469e-04\nLoss = 4.7633e-08, PNorm = 137.2287, GNorm = 0.0003, lr_0 = 1.1469e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.29it/s]Loss = 1.9608e-04, PNorm = 137.2288, GNorm = 0.0689, lr_0 = 1.1447e-04\nLoss = 1.9608e-04, PNorm = 137.2288, GNorm = 0.0689, lr_0 = 1.1447e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.32it/s]Loss = 4.7684e-10, PNorm = 137.2288, GNorm = 0.0000, lr_0 = 1.1424e-04\nLoss = 4.7684e-10, PNorm = 137.2288, GNorm = 0.0000, lr_0 = 1.1424e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.37it/s]Loss = 2.1656e-04, PNorm = 137.2289, GNorm = 0.0850, lr_0 = 1.1402e-04\nLoss = 2.1656e-04, PNorm = 137.2289, GNorm = 0.0850, lr_0 = 1.1402e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.40it/s]Loss = 2.3842e-10, PNorm = 137.2289, GNorm = 0.0000, lr_0 = 1.1379e-04\nLoss = 2.3842e-10, PNorm = 137.2289, GNorm = 0.0000, lr_0 = 1.1379e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.38it/s]Loss = 1.0490e-08, PNorm = 137.2290, GNorm = 0.0004, lr_0 = 1.1357e-04\nLoss = 1.0490e-08, PNorm = 137.2290, GNorm = 0.0004, lr_0 = 1.1357e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.37it/s]Loss = 2.0981e-09, PNorm = 137.2290, GNorm = 0.0000, lr_0 = 1.1335e-04\nLoss = 2.0981e-09, PNorm = 137.2290, GNorm = 0.0000, lr_0 = 1.1335e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.37it/s]Loss = 3.4332e-09, PNorm = 137.2291, GNorm = 0.0001, lr_0 = 1.1313e-04\nLoss = 3.4332e-09, PNorm = 137.2291, GNorm = 0.0001, lr_0 = 1.1313e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.36it/s]Loss = 3.5513e-04, PNorm = 137.2291, GNorm = 0.1314, lr_0 = 1.1291e-04\nLoss = 3.5513e-04, PNorm = 137.2291, GNorm = 0.1314, lr_0 = 1.1291e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.27it/s]Loss = 9.0599e-10, PNorm = 137.2291, GNorm = 0.0000, lr_0 = 1.1269e-04\nLoss = 9.0599e-10, PNorm = 137.2291, GNorm = 0.0000, lr_0 = 1.1269e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.29it/s]Loss = 9.7127e-08, PNorm = 137.2291, GNorm = 0.0009, lr_0 = 1.1247e-04\nLoss = 9.7127e-08, PNorm = 137.2291, GNorm = 0.0009, lr_0 = 1.1247e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.32it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 24.67it/s]Validation auc = 0.691795\nValidation auc = 0.691795\n\n\r 95%|█████████▌| 95/100 [05:39&lt;00:11,  2.35s/it]Epoch 95\nEpoch 95\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 1.9958e-04, PNorm = 137.2292, GNorm = 0.0686, lr_0 = 1.1225e-04\nLoss = 1.9958e-04, PNorm = 137.2292, GNorm = 0.0686, lr_0 = 1.1225e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.42it/s]Loss = 5.7220e-10, PNorm = 137.2293, GNorm = 0.0000, lr_0 = 1.1203e-04\nLoss = 5.7220e-10, PNorm = 137.2293, GNorm = 0.0000, lr_0 = 1.1203e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.40it/s]Loss = 3.5219e-04, PNorm = 137.2293, GNorm = 0.1279, lr_0 = 1.1181e-04\nLoss = 3.5219e-04, PNorm = 137.2293, GNorm = 0.1279, lr_0 = 1.1181e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.39it/s]Loss = 2.0368e-04, PNorm = 137.2293, GNorm = 0.0667, lr_0 = 1.1159e-04\nLoss = 2.0368e-04, PNorm = 137.2293, GNorm = 0.0667, lr_0 = 1.1159e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.38it/s]Loss = 2.0408e-04, PNorm = 137.2294, GNorm = 0.0750, lr_0 = 1.1137e-04\nLoss = 2.0408e-04, PNorm = 137.2294, GNorm = 0.0750, lr_0 = 1.1137e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.40it/s]Loss = 2.6369e-08, PNorm = 137.2294, GNorm = 0.0004, lr_0 = 1.1115e-04\nLoss = 2.6369e-08, PNorm = 137.2294, GNorm = 0.0004, lr_0 = 1.1115e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.40it/s]Loss = 5.8651e-09, PNorm = 137.2295, GNorm = 0.0000, lr_0 = 1.1093e-04\nLoss = 5.8651e-09, PNorm = 137.2295, GNorm = 0.0000, lr_0 = 1.1093e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.44it/s]Loss = 4.2915e-10, PNorm = 137.2295, GNorm = 0.0000, lr_0 = 1.1072e-04\nLoss = 4.2915e-10, PNorm = 137.2295, GNorm = 0.0000, lr_0 = 1.1072e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.38it/s]Loss = 2.1101e-07, PNorm = 137.2296, GNorm = 0.0052, lr_0 = 1.1050e-04\nLoss = 2.1101e-07, PNorm = 137.2296, GNorm = 0.0052, lr_0 = 1.1050e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.38it/s]Loss = 2.3842e-10, PNorm = 137.2297, GNorm = 0.0000, lr_0 = 1.1029e-04\nLoss = 2.3842e-10, PNorm = 137.2297, GNorm = 0.0000, lr_0 = 1.1029e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.40it/s]Loss = 2.4319e-09, PNorm = 137.2297, GNorm = 0.0001, lr_0 = 1.1007e-04\nLoss = 2.4319e-09, PNorm = 137.2297, GNorm = 0.0001, lr_0 = 1.1007e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.32it/s]Loss = 6.3165e-07, PNorm = 137.2299, GNorm = 0.0313, lr_0 = 1.0985e-04\nLoss = 6.3165e-07, PNorm = 137.2299, GNorm = 0.0313, lr_0 = 1.0985e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.34it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 24.59it/s]Validation auc = 0.691795\nValidation auc = 0.691795\n\n\r 96%|█████████▌| 96/100 [05:41&lt;00:09,  2.34s/it]Epoch 96\nEpoch 96\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 2.0221e-04, PNorm = 137.2300, GNorm = 0.0825, lr_0 = 1.0964e-04\nLoss = 2.0221e-04, PNorm = 137.2300, GNorm = 0.0825, lr_0 = 1.0964e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.39it/s]Loss = 4.7684e-11, PNorm = 137.2301, GNorm = 0.0000, lr_0 = 1.0942e-04\nLoss = 4.7684e-11, PNorm = 137.2301, GNorm = 0.0000, lr_0 = 1.0942e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.38it/s]Loss = 1.9568e-04, PNorm = 137.2303, GNorm = 0.0690, lr_0 = 1.0921e-04\nLoss = 1.9568e-04, PNorm = 137.2303, GNorm = 0.0690, lr_0 = 1.0921e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.39it/s]Loss = 3.7763e-04, PNorm = 137.2303, GNorm = 0.1158, lr_0 = 1.0900e-04\nLoss = 3.7763e-04, PNorm = 137.2303, GNorm = 0.1158, lr_0 = 1.0900e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.39it/s]Loss = 3.9862e-04, PNorm = 137.2304, GNorm = 0.1265, lr_0 = 1.0878e-04\nLoss = 3.9862e-04, PNorm = 137.2304, GNorm = 0.1265, lr_0 = 1.0878e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.39it/s]Loss = 2.0313e-08, PNorm = 137.2305, GNorm = 0.0010, lr_0 = 1.0857e-04\nLoss = 2.0313e-08, PNorm = 137.2305, GNorm = 0.0010, lr_0 = 1.0857e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.36it/s]Loss = 4.7207e-09, PNorm = 137.2305, GNorm = 0.0001, lr_0 = 1.0836e-04\nLoss = 4.7207e-09, PNorm = 137.2305, GNorm = 0.0001, lr_0 = 1.0836e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.36it/s]Loss = 2.0503e-08, PNorm = 137.2306, GNorm = 0.0004, lr_0 = 1.0815e-04\nLoss = 2.0503e-08, PNorm = 137.2306, GNorm = 0.0004, lr_0 = 1.0815e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.36it/s]Loss = 1.8567e-04, PNorm = 137.2307, GNorm = 0.0656, lr_0 = 1.0794e-04\nLoss = 1.8567e-04, PNorm = 137.2307, GNorm = 0.0656, lr_0 = 1.0794e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.34it/s]Loss = 1.9693e-08, PNorm = 137.2308, GNorm = 0.0008, lr_0 = 1.0772e-04\nLoss = 1.9693e-08, PNorm = 137.2308, GNorm = 0.0008, lr_0 = 1.0772e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.34it/s]Loss = 4.0244e-08, PNorm = 137.2308, GNorm = 0.0007, lr_0 = 1.0751e-04\nLoss = 4.0244e-08, PNorm = 137.2308, GNorm = 0.0007, lr_0 = 1.0751e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.34it/s]Loss = 4.5536e-08, PNorm = 137.2309, GNorm = 0.0003, lr_0 = 1.0730e-04\nLoss = 4.5536e-08, PNorm = 137.2309, GNorm = 0.0003, lr_0 = 1.0730e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.35it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 24.45it/s]Validation auc = 0.691795\nValidation auc = 0.691795\n\n\r 97%|█████████▋| 97/100 [05:43&lt;00:07,  2.34s/it]Epoch 97\nEpoch 97\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 3.7299e-04, PNorm = 137.2310, GNorm = 0.1173, lr_0 = 1.0709e-04\nLoss = 3.7299e-04, PNorm = 137.2310, GNorm = 0.1173, lr_0 = 1.0709e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.03it/s]Loss = 7.1997e-08, PNorm = 137.2311, GNorm = 0.0004, lr_0 = 1.0688e-04\nLoss = 7.1997e-08, PNorm = 137.2311, GNorm = 0.0004, lr_0 = 1.0688e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.08it/s]Loss = 2.3842e-10, PNorm = 137.2311, GNorm = 0.0000, lr_0 = 1.0667e-04\nLoss = 2.3842e-10, PNorm = 137.2311, GNorm = 0.0000, lr_0 = 1.0667e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.16it/s]Loss = 3.4668e-04, PNorm = 137.2312, GNorm = 0.1142, lr_0 = 1.0647e-04\nLoss = 3.4668e-04, PNorm = 137.2312, GNorm = 0.1142, lr_0 = 1.0647e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.20it/s]Loss = 6.5803e-09, PNorm = 137.2312, GNorm = 0.0000, lr_0 = 1.0626e-04\nLoss = 6.5803e-09, PNorm = 137.2312, GNorm = 0.0000, lr_0 = 1.0626e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.26it/s]Loss = 7.9198e-08, PNorm = 137.2313, GNorm = 0.0015, lr_0 = 1.0605e-04\nLoss = 7.9198e-08, PNorm = 137.2313, GNorm = 0.0015, lr_0 = 1.0605e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.29it/s]Loss = 2.1881e-04, PNorm = 137.2313, GNorm = 0.0803, lr_0 = 1.0584e-04\nLoss = 2.1881e-04, PNorm = 137.2313, GNorm = 0.0803, lr_0 = 1.0584e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.31it/s]Loss = 2.3603e-08, PNorm = 137.2314, GNorm = 0.0005, lr_0 = 1.0564e-04\nLoss = 2.3603e-08, PNorm = 137.2314, GNorm = 0.0005, lr_0 = 1.0564e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.31it/s]Loss = 4.1008e-09, PNorm = 137.2314, GNorm = 0.0001, lr_0 = 1.0543e-04\nLoss = 4.1008e-09, PNorm = 137.2314, GNorm = 0.0001, lr_0 = 1.0543e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.38it/s]Loss = 8.1062e-10, PNorm = 137.2315, GNorm = 0.0000, lr_0 = 1.0522e-04\nLoss = 8.1062e-10, PNorm = 137.2315, GNorm = 0.0000, lr_0 = 1.0522e-04\n\n\n\r 83%|████████▎ | 10/12 [00:02&lt;00:00,  4.41it/s]Loss = 1.9922e-04, PNorm = 137.2316, GNorm = 0.0730, lr_0 = 1.0502e-04\nLoss = 1.9922e-04, PNorm = 137.2316, GNorm = 0.0730, lr_0 = 1.0502e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  4.65it/s]Loss = 1.9578e-04, PNorm = 137.2316, GNorm = 0.0606, lr_0 = 1.0481e-04\nLoss = 1.9578e-04, PNorm = 137.2316, GNorm = 0.0606, lr_0 = 1.0481e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  4.80it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 24.27it/s]Validation auc = 0.691795\nValidation auc = 0.691795\n\n\r 98%|█████████▊| 98/100 [05:46&lt;00:04,  2.38s/it]Epoch 98\nEpoch 98\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 2.8610e-10, PNorm = 137.2317, GNorm = 0.0000, lr_0 = 1.0461e-04\nLoss = 2.8610e-10, PNorm = 137.2317, GNorm = 0.0000, lr_0 = 1.0461e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  4.93it/s]Loss = 9.5787e-08, PNorm = 137.2317, GNorm = 0.0004, lr_0 = 1.0440e-04\nLoss = 9.5787e-08, PNorm = 137.2317, GNorm = 0.0004, lr_0 = 1.0440e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.02it/s]Loss = 3.1471e-08, PNorm = 137.2318, GNorm = 0.0014, lr_0 = 1.0420e-04\nLoss = 3.1471e-08, PNorm = 137.2318, GNorm = 0.0014, lr_0 = 1.0420e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.04it/s]Loss = 6.4417e-08, PNorm = 137.2319, GNorm = 0.0013, lr_0 = 1.0399e-04\nLoss = 6.4417e-08, PNorm = 137.2319, GNorm = 0.0013, lr_0 = 1.0399e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.13it/s]Loss = 2.1092e-04, PNorm = 137.2320, GNorm = 0.0825, lr_0 = 1.0379e-04\nLoss = 2.1092e-04, PNorm = 137.2320, GNorm = 0.0825, lr_0 = 1.0379e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.19it/s]Loss = 0.0000e+00, PNorm = 137.2320, GNorm = 0.0000, lr_0 = 1.0359e-04\nLoss = 0.0000e+00, PNorm = 137.2320, GNorm = 0.0000, lr_0 = 1.0359e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.27it/s]Loss = 6.9517e-08, PNorm = 137.2321, GNorm = 0.0033, lr_0 = 1.0338e-04\nLoss = 6.9517e-08, PNorm = 137.2321, GNorm = 0.0033, lr_0 = 1.0338e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.34it/s]Loss = 1.3351e-09, PNorm = 137.2322, GNorm = 0.0000, lr_0 = 1.0318e-04\nLoss = 1.3351e-09, PNorm = 137.2322, GNorm = 0.0000, lr_0 = 1.0318e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.38it/s]Loss = 5.8228e-04, PNorm = 137.2322, GNorm = 0.1048, lr_0 = 1.0298e-04\nLoss = 5.8228e-04, PNorm = 137.2322, GNorm = 0.1048, lr_0 = 1.0298e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.37it/s]Loss = 1.5736e-09, PNorm = 137.2323, GNorm = 0.0000, lr_0 = 1.0278e-04\nLoss = 1.5736e-09, PNorm = 137.2323, GNorm = 0.0000, lr_0 = 1.0278e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.36it/s]Loss = 3.4212e-04, PNorm = 137.2323, GNorm = 0.1167, lr_0 = 1.0258e-04\nLoss = 3.4212e-04, PNorm = 137.2323, GNorm = 0.1167, lr_0 = 1.0258e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.37it/s]Loss = 2.1336e-04, PNorm = 137.2324, GNorm = 0.1156, lr_0 = 1.0238e-04\nLoss = 2.1336e-04, PNorm = 137.2324, GNorm = 0.1156, lr_0 = 1.0238e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.37it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 24.71it/s]Validation auc = 0.691795\nValidation auc = 0.691795\n\n\r 99%|█████████▉| 99/100 [05:48&lt;00:02,  2.37s/it]Epoch 99\nEpoch 99\n\n\n\r  0%|          | 0/12 [00:00&lt;?, ?it/s]Loss = 3.9432e-08, PNorm = 137.2324, GNorm = 0.0014, lr_0 = 1.0218e-04\nLoss = 3.9432e-08, PNorm = 137.2324, GNorm = 0.0014, lr_0 = 1.0218e-04\n\n\n\r  8%|▊         | 1/12 [00:00&lt;00:02,  5.45it/s]Loss = 1.7684e-04, PNorm = 137.2325, GNorm = 0.0535, lr_0 = 1.0198e-04\nLoss = 1.7684e-04, PNorm = 137.2325, GNorm = 0.0535, lr_0 = 1.0198e-04\n\n\n\r 17%|█▋        | 2/12 [00:00&lt;00:01,  5.42it/s]Loss = 2.0607e-04, PNorm = 137.2326, GNorm = 0.0831, lr_0 = 1.0178e-04\nLoss = 2.0607e-04, PNorm = 137.2326, GNorm = 0.0831, lr_0 = 1.0178e-04\n\n\n\r 25%|██▌       | 3/12 [00:00&lt;00:01,  5.38it/s]Loss = 3.7961e-04, PNorm = 137.2326, GNorm = 0.1167, lr_0 = 1.0158e-04\nLoss = 3.7961e-04, PNorm = 137.2326, GNorm = 0.1167, lr_0 = 1.0158e-04\n\n\n\r 33%|███▎      | 4/12 [00:00&lt;00:01,  5.40it/s]Loss = 1.2875e-09, PNorm = 137.2327, GNorm = 0.0000, lr_0 = 1.0138e-04\nLoss = 1.2875e-09, PNorm = 137.2327, GNorm = 0.0000, lr_0 = 1.0138e-04\n\n\n\r 42%|████▏     | 5/12 [00:00&lt;00:01,  5.37it/s]Loss = 1.9121e-08, PNorm = 137.2327, GNorm = 0.0004, lr_0 = 1.0118e-04\nLoss = 1.9121e-08, PNorm = 137.2327, GNorm = 0.0004, lr_0 = 1.0118e-04\n\n\n\r 50%|█████     | 6/12 [00:01&lt;00:01,  5.38it/s]Loss = 5.2452e-10, PNorm = 137.2328, GNorm = 0.0000, lr_0 = 1.0098e-04\nLoss = 5.2452e-10, PNorm = 137.2328, GNorm = 0.0000, lr_0 = 1.0098e-04\n\n\n\r 58%|█████▊    | 7/12 [00:01&lt;00:00,  5.42it/s]Loss = 3.8065e-04, PNorm = 137.2328, GNorm = 0.1110, lr_0 = 1.0079e-04\nLoss = 3.8065e-04, PNorm = 137.2328, GNorm = 0.1110, lr_0 = 1.0079e-04\n\n\n\r 67%|██████▋   | 8/12 [00:01&lt;00:00,  5.40it/s]Loss = 3.8624e-09, PNorm = 137.2328, GNorm = 0.0002, lr_0 = 1.0059e-04\nLoss = 3.8624e-09, PNorm = 137.2328, GNorm = 0.0002, lr_0 = 1.0059e-04\n\n\n\r 75%|███████▌  | 9/12 [00:01&lt;00:00,  5.40it/s]Loss = 2.1727e-04, PNorm = 137.2329, GNorm = 0.0854, lr_0 = 1.0039e-04\nLoss = 2.1727e-04, PNorm = 137.2329, GNorm = 0.0854, lr_0 = 1.0039e-04\n\n\n\r 83%|████████▎ | 10/12 [00:01&lt;00:00,  5.39it/s]Loss = 1.8167e-08, PNorm = 137.2329, GNorm = 0.0001, lr_0 = 1.0020e-04\nLoss = 1.8167e-08, PNorm = 137.2329, GNorm = 0.0001, lr_0 = 1.0020e-04\n\n\n\r 92%|█████████▏| 11/12 [00:02&lt;00:00,  5.36it/s]Loss = 2.7333e-07, PNorm = 137.2330, GNorm = 0.0136, lr_0 = 1.0000e-04\nLoss = 2.7333e-07, PNorm = 137.2330, GNorm = 0.0136, lr_0 = 1.0000e-04\n\n\n\r100%|██████████| 12/12 [00:02&lt;00:00,  5.35it/s]\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 24.70it/s]Validation auc = 0.691795\nValidation auc = 0.691795\n\n\r100%|██████████| 100/100 [05:51&lt;00:00,  2.35s/it]Model 0 best validation auc = 0.761026 on epoch 21\nModel 0 best validation auc = 0.761026 on epoch 21\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoving model to cuda\nMoving model to cuda\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.49it/s]Model 0 test auc = 0.943493\nModel 0 test auc = 0.943493\nEnsemble test auc = 0.943493\nEnsemble test auc = 0.943493\n1-fold cross validation\n1-fold cross validation\nSeed 0 ==&gt; test auc = 0.943493\nSeed 0 ==&gt; test auc = 0.943493\nOverall test auc = 0.943493 +/- 0.000000\nOverall test auc = 0.943493 +/- 0.000000\nOut[51]: (0.9434931506849314, 0.0)\n</div>"]}}],"execution_count":115},{"cell_type":"code","source":["from chemprop.parsing import parse_predict_args\nfrom chemprop.train import make_predictions\nfrom chemprop.parsing import add_predict_args\nfrom chemprop.parsing import modify_predict_args\nparser = ArgumentParser()\nadd_predict_args(parser)\n\nfor i in range(1,10):\n  args = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'JAK','checkpoints/binary-hopt-prelu100e/model/fold_'+str(i),'test_smiles.csv'),\n                            '--checkpoint_path',os.path.join(CHEMPROP_DIR,'JAK','checkpoints/binary-hopt-prelu100e/model/fold_'+str(i),'model_0/model.pt'),\n                            '--preds_path',os.path.join(CHEMPROP_DIR,'JAK','checkpoints/binary-hopt-prelu100e/model/fold_'+str(i),'test_preds.csv')])\n  modify_predict_args(args)\n  make_predictions(args)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading training args\nLoading data\n\n\n\r  0%|          | 0/81 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 81/81 [00:00&lt;00:00, 2045.00it/s]Validating SMILES\nTest size = 81\nPredicting with an ensemble of 1 models\n\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoving model to cuda\n\n\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.62it/s]\n\n\r100%|██████████| 1/1 [00:05&lt;00:00,  5.30s/it]Saving predictions to /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu100e/model/fold_1/test_preds.csv\nLoading training args\nLoading data\n\n\n\r  0%|          | 0/81 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 81/81 [00:00&lt;00:00, 1958.34it/s]Validating SMILES\nTest size = 81\nPredicting with an ensemble of 1 models\n\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoving model to cuda\n\n\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.26it/s]\n\n\r100%|██████████| 1/1 [00:05&lt;00:00,  5.08s/it]Saving predictions to /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu100e/model/fold_2/test_preds.csv\nLoading training args\nLoading data\n\n\n\r  0%|          | 0/81 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 81/81 [00:00&lt;00:00, 1990.77it/s]Validating SMILES\nTest size = 81\nPredicting with an ensemble of 1 models\n\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoving model to cuda\n\n\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.30it/s]\n\n\r100%|██████████| 1/1 [00:05&lt;00:00,  5.50s/it]Saving predictions to /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu100e/model/fold_3/test_preds.csv\nLoading training args\nLoading data\n\n\n\r  0%|          | 0/81 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 81/81 [00:00&lt;00:00, 2150.93it/s]Validating SMILES\nTest size = 81\nPredicting with an ensemble of 1 models\n\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoving model to cuda\n\n\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.55it/s]\n\n\r100%|██████████| 1/1 [00:05&lt;00:00,  5.30s/it]Saving predictions to /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu100e/model/fold_4/test_preds.csv\nLoading training args\nLoading data\n\n\n\r  0%|          | 0/81 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 81/81 [00:00&lt;00:00, 2125.89it/s]Validating SMILES\nTest size = 81\nPredicting with an ensemble of 1 models\n\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoving model to cuda\n\n\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.83it/s]\n\n\r100%|██████████| 1/1 [00:05&lt;00:00,  5.28s/it]Saving predictions to /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu100e/model/fold_5/test_preds.csv\nLoading training args\nLoading data\n\n\n\r  0%|          | 0/81 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 81/81 [00:00&lt;00:00, 1990.91it/s]Validating SMILES\nTest size = 81\nPredicting with an ensemble of 1 models\n\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoving model to cuda\n\n\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 22.82it/s]\n\n\r100%|██████████| 1/1 [00:05&lt;00:00,  5.26s/it]Saving predictions to /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu100e/model/fold_6/test_preds.csv\nLoading training args\nLoading data\n\n\n\r  0%|          | 0/81 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 81/81 [00:00&lt;00:00, 2175.25it/s]Validating SMILES\nTest size = 81\nPredicting with an ensemble of 1 models\n\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoving model to cuda\n\n\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.66it/s]\n\n\r100%|██████████| 1/1 [00:05&lt;00:00,  5.28s/it]Saving predictions to /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu100e/model/fold_7/test_preds.csv\nLoading training args\nLoading data\n\n\n\r  0%|          | 0/81 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 81/81 [00:00&lt;00:00, 2012.53it/s]Validating SMILES\nTest size = 81\nPredicting with an ensemble of 1 models\n\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoving model to cuda\n\n\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.34it/s]\n\n\r100%|██████████| 1/1 [00:04&lt;00:00,  4.98s/it]Saving predictions to /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu100e/model/fold_8/test_preds.csv\nLoading training args\nLoading data\n\n\n\r  0%|          | 0/81 [00:00&lt;?, ?it/s]\n\n\r100%|██████████| 81/81 [00:00&lt;00:00, 2050.16it/s]Validating SMILES\nTest size = 81\nPredicting with an ensemble of 1 models\n\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoving model to cuda\n\n\n\n\r  0%|          | 0/2 [00:00&lt;?, ?it/s]\n\n\n\r100%|██████████| 2/2 [00:00&lt;00:00, 23.62it/s]\n\n\r100%|██████████| 1/1 [00:04&lt;00:00,  4.89s/it]Saving predictions to /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu100e/model/fold_9/test_preds.csv\n</div>"]}}],"execution_count":116},{"cell_type":"code","source":["%run /Users/vxjdk@leo-pharma.com/mol_utils"],"metadata":{},"outputs":[],"execution_count":117},{"cell_type":"code","source":["%sh head /dbfs/FileStore/chemprop/JAK/checkpoints/binary-hopt-prelu100e/model/fold_9/test_smiles.csv"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">smiles\nCc1n[nH]c(c1-c1ccc(cc1)NC(=O)[C@H](Cc1cc(ccc1Cl)-c1cccc(c1)CN1C[C@H]2C[C@@H]1CN2)NC(=O)C1(F)CC1)C\nCc1n[nH]c(c1-c1ccc(cc1)NC(=O)[C@H](Cc1cc(ccc1Cl)-c1cnn(c1C)C)NC(=O)C1(F)CC1)C\nCc1n[nH]c(c1-c1ccc(cc1)NC(=O)[C@@H](NC(=O)c1ccnn1C)[C@@H]1COc2ccc(cc21)-c1cnn(c1)C(C)(C)C)C\nFC1(CC1)C(=O)N[C@@H]([C@@H]1CCCc2ccc(cc21)-c1ccnc(c1)N1C[C@H]2C[C@@H]1CO2)C(=O)Nc1ccc2c(c1)NC(=O)C12CCOCC1\nCN(C)C(=O)NC(Cc1cc(ccc1Cl)-c1cnn(c1)C)C(=O)Nc1ccc(cc1)CCC(=O)N\nCc1n[nH]c/2c1-c1ccc(cc1)NC(=O)C(Cc1cc(ccc1Cl)-c1cccc(c1)CNC\\C=C2)NC(=O)c1ccnn1C\nCn1nccc1C(=O)NC(Cc1cc(ccc1Cl)-c1cccc(c1)OCC(=O)O)C(=O)Nc1ccc(cc1)C[C@H](CO)NC(=O)OC(C)(C)C\nCc1n[nH]c(c1-c1ccc(cc1)NC(=O)[C@H](Cc1cc(ccc1Cl)-c1ccc(cc1)CN1C[C@H]2C[C@@H]1CO2)NC(=O)C1(F)CC1)C\nCOc1cccc(n1)-c1ccc(c(c1)CC(NC(=O)c1ccnn1C)C(=O)Nc1ccc(cc1)CCC(=O)N)Cl\n</div>"]}}],"execution_count":118},{"cell_type":"code","source":["true = pd.read_csv(os.path.join(CHEMPROP_DIR,'hlm_eh_binary.csv'))\n#pred = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','checkpoints/binary-hopt/model/fold_0/test_smiles.csv'))\npred = pd.DataFrame(columns=['smiles','HLM_binary'])\nfor i in range(1,10):\n  #print(pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','checkpoints/binary-hopt/model/fold_'+str(i),'test_smiles.csv')).head())\n  \n  pred = pred.append(pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','checkpoints/binary-hopt-prelu100e/model/fold_'+str(i),'test_preds.csv')))\npred = pred.rename(columns={'smiles':'smiles','HLM_binary':'pred'})\ntrue = true.merge(pred)\nfps = get_fps([Chem.MolFromSmiles(smi) for smi in true['smiles']], bit=True)\ndists = fps_distances(fps)\ncoords = get_embedding('mds', 2, dists)\n#true['diff'] = true.filter(['HLM_binary']).round(0).subtract(true.filter(['pred']).round(0)).abs()\ntrue['diff'] = abs(np.array(np.array(true.filter(['HLM_binary']).round(0)) - np.array(true.filter(['pred']).round(0))))\ncols = ['HLM_binary','pred','diff']\nf, axes = plt.subplots(1, 3, sharex = True, sharey=True, figsize=(20,12),squeeze=True)\ncorrections = [(0,1),(0,1),(-1,1)]\ntitles = ['True Values','Chemprop','dChemprop']\n#halves = [False,False,False,True,True,True]\nfor i, col in enumerate(cols):\n    #if halves[i]:\n      \n    f, ax, sc = plot_transformation_2D(coords,\n                                       coloring=np.round(true[col]).tolist(),\n                                       figure=(f, axes[i]),\n                                       scale='binary', \n                                       color_correction=corrections[i])\n    ax.set_title(titles[i])\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    ax.set_aspect('equal',adjustable='box')\n    f.colorbar(sc, cax=cax)\n\nf.tight_layout()\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["def plot_transformation_2D(coords, coloring=None, figure=None, scale=None, color_correction=None):\n    if figure:\n        fig, ax = figure\n    else:\n        fig = plt.figure(figsize = (8,8))\n        ax = fig.add_subplot(111)\n        ax.set_aspect('equal',adjustable='box')\n\n    if coloring:\n        cmap = plt.get_cmap('jet')\n        if scale == 'log':\n            coloring = np.log10(np.array(coloring).astype(float))\n        if scale == 'p':\n            coloring = -np.log10(np.array(coloring).astype(float)*1e-9)\n        coloring = np.around(np.array(coloring).astype(float), decimals = 1)\n        #ax.scatter(coords[trainsize:, 0], coords[trainsize:, 1], marker = '.', c=gensize#,cmap=colors.Colormap('jet'))\n        if scale == 'binary':\n            cmap=plt.get_cmap('bwr')\n        if color_correction:\n            norm = colors.Normalize(vmin=color_correction[0],vmax=color_correction[1])\n        sc = ax.scatter(coords[:, 0], \n                        coords[:, 1], \n                        marker = '.', \n                        c=coloring, \n                        cmap=cmap, \n                        norm=norm#, s=len(gensize)**2\n                  )\n        return fig, ax, sc\n    else:\n        ax.scatter(coords[:, 0], \n                   coords[:, 1], \n                   marker = '.', \n                   c='k'\n                )\n        #plt.colorbar(sc)\n        return fig, ax\nfor i, col in enumerate(cols):\n    #if halves[i]:\n      \n    f, ax, sc = plot_transformation_2D(coords,\n                                       coloring=np.round(true[col]).tolist(),\n                                       figure=(f, axes[i]),\n                                       scale='binary', \n                                       color_correction=corrections[i])\n    ax.set_title(titles[i])\n    divider = make_axes_locatable(ax)\n    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n    ax.set_aspect('equal',adjustable='box')\n    f.colorbar(sc, cax=cax)\n\nf.tight_layout()\ndisplay(f)"],"metadata":{},"outputs":[],"execution_count":120},{"cell_type":"code","source":["args = parser.parse_args(['--test_path',os.path.join(CHEMPROP_DIR,'hlm_eh_binary.smi'),\n                          '--checkpoint_path','/dbfs/FileStore/tables/binary_prelu100e-cdbb6.pt',#os.path.join(CHEMPROP_DIR,'JAK','checkpoints/binary-hopt-prelu100e/model/fold_0','model_0/model.pt'),#'/dbfs/FileStore/tables/binary_prelu100e-cdbb6.pt',\n                          '--preds_path',os.path.join(CHEMPROP_DIR,'hlm_eh_binary_preds.csv')])\nmodify_predict_args(args)\nmake_predictions(args)\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Loading training args\nLoading data\n\n\r  0%|          | 0/803 [00:00&lt;?, ?it/s]\n\r 33%|███▎      | 262/803 [00:00&lt;00:00, 2619.14it/s]\n\r 60%|██████    | 483/803 [00:00&lt;00:00, 2476.01it/s]\n\r 82%|████████▏ | 662/803 [00:00&lt;00:00, 2218.53it/s]\n\r100%|██████████| 803/803 [00:00&lt;00:00, 2077.20it/s]Validating SMILES\nTest size = 803\nPredicting with an ensemble of 1 models\n\n\r  0%|          | 0/1 [00:00&lt;?, ?it/s]Loading pretrained parameter &#34;encoder.encoder.cached_zero_vector&#34;.\nLoading pretrained parameter &#34;encoder.encoder.act_func.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_i.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_h.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.weight&#34;.\nLoading pretrained parameter &#34;encoder.encoder.W_o.bias&#34;.\nLoading pretrained parameter &#34;ffn.1.weight&#34;.\nLoading pretrained parameter &#34;ffn.1.bias&#34;.\nLoading pretrained parameter &#34;ffn.2.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.weight&#34;.\nLoading pretrained parameter &#34;ffn.4.bias&#34;.\nLoading pretrained parameter &#34;ffn.5.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.weight&#34;.\nLoading pretrained parameter &#34;ffn.7.bias&#34;.\nLoading pretrained parameter &#34;ffn.8.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.weight&#34;.\nLoading pretrained parameter &#34;ffn.10.bias&#34;.\nLoading pretrained parameter &#34;ffn.11.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.weight&#34;.\nLoading pretrained parameter &#34;ffn.13.bias&#34;.\nMoving model to cuda\n\n\n\r  0%|          | 0/17 [00:00&lt;?, ?it/s]\n\n\r 18%|█▊        | 3/17 [00:00&lt;00:00, 20.89it/s]\n\n\r 29%|██▉       | 5/17 [00:00&lt;00:00, 20.61it/s]\n\n\r 47%|████▋     | 8/17 [00:00&lt;00:00, 20.77it/s]\n\n\r 65%|██████▍   | 11/17 [00:00&lt;00:00, 20.52it/s]\n\n\r 76%|███████▋  | 13/17 [00:00&lt;00:00, 20.30it/s]\n\n\r 88%|████████▊ | 15/17 [00:00&lt;00:00, 20.12it/s]\n\n\r100%|██████████| 17/17 [00:00&lt;00:00, 21.35it/s]\n\r100%|██████████| 1/1 [00:06&lt;00:00,  6.27s/it]Saving predictions to /dbfs/FileStore/chemprop/hlm_eh_binary_preds.csv\nOut[44]: \n[[0.6911686658859253],\n [0.7632564306259155],\n [0.6500565409660339],\n [0.6973351240158081],\n [0.7092649340629578],\n [0.990444004535675],\n [0.6661577224731445],\n [0.663964033126831],\n [0.6548023819923401],\n [0.7593100666999817],\n [0.6598889827728271],\n [0.6875098347663879],\n [0.659458339214325],\n [0.6557130813598633],\n [0.684738039970398],\n [0.6449567675590515],\n [0.10628285259008408],\n [0.48254141211509705],\n [0.08388318866491318],\n [0.6375560760498047],\n [0.7149393558502197],\n [0.8482649326324463],\n [0.7093323469161987],\n [0.9921557307243347],\n [0.6294216513633728],\n [0.6771498322486877],\n [0.7178994417190552],\n [0.976884663105011],\n [0.6167119145393372],\n [0.8978360891342163],\n [0.7230321168899536],\n [0.7296144962310791],\n [0.6625226736068726],\n [0.7111787796020508],\n [0.9984069466590881],\n [0.8281614780426025],\n [0.6278284192085266],\n [0.6262041330337524],\n [0.9991239905357361],\n [0.39586570858955383],\n [0.9976396560668945],\n [0.6917388439178467],\n [0.42956238985061646],\n [0.5503103733062744],\n [0.5576819777488708],\n [0.4978344440460205],\n [0.6797346472740173],\n [0.00020894245244562626],\n [0.9689086079597473],\n [0.19885410368442535],\n [0.0823858305811882],\n [0.1815609335899353],\n [0.8146184682846069],\n [0.9982579350471497],\n [0.6945271492004395],\n [0.8334395289421082],\n [0.9686522483825684],\n [0.9170207977294922],\n [0.9194715023040771],\n [0.5731794238090515],\n [0.9283912181854248],\n [0.6522570252418518],\n [0.9110913276672363],\n [0.976696252822876],\n [0.9741744995117188],\n [0.18054187297821045],\n [0.9224990010261536],\n [0.9077692031860352],\n [0.9972412586212158],\n [0.9139145016670227],\n [0.996070146560669],\n [0.9999048709869385],\n [0.722547173500061],\n [0.6169044971466064],\n [0.6917563676834106],\n [0.609627366065979],\n [0.7078145146369934],\n [0.9960439205169678],\n [0.9859898090362549],\n [0.7084718942642212],\n [0.6556423902511597],\n [0.6275444030761719],\n [0.629576563835144],\n [0.9389843344688416],\n [0.7200656533241272],\n [0.6478068828582764],\n [0.598042905330658],\n [0.8824600577354431],\n [0.9944856762886047],\n [0.783547580242157],\n [0.9988721013069153],\n [0.9920693635940552],\n [0.00042422147816978395],\n [0.1842336654663086],\n [0.21688351035118103],\n [0.9769078493118286],\n [0.9939823746681213],\n [0.99647456407547],\n [0.6694183349609375],\n [0.9982871413230896],\n [0.9988152980804443],\n [0.99234539270401],\n [0.7760202884674072],\n [0.8526853919029236],\n [0.6359090209007263],\n [0.9921234250068665],\n [0.6980190873146057],\n [0.10787123441696167],\n [0.9163445830345154],\n [0.7536168098449707],\n [0.7514579892158508],\n [0.6536269187927246],\n [0.7092310190200806],\n [0.8960195183753967],\n [0.9708105325698853],\n [0.9907327890396118],\n [0.9725311398506165],\n [0.21011461317539215],\n [0.1881837695837021],\n [0.21689502894878387],\n [0.22097407281398773],\n [0.3457724452018738],\n [0.9063244462013245],\n [0.9725311398506165],\n [0.19503578543663025],\n [0.8991258144378662],\n [0.4896509349346161],\n [0.30162492394447327],\n [0.9107173085212708],\n [0.677385151386261],\n [0.9949427247047424],\n [0.8910984396934509],\n [0.43606677651405334],\n [0.9975759387016296],\n [0.9971990585327148],\n [0.8253602981567383],\n [0.9977978467941284],\n [0.9980504512786865],\n [0.6230181455612183],\n [0.6245623230934143],\n [0.9936822652816772],\n [0.992742657661438],\n [0.6456124782562256],\n [0.8091636896133423],\n [0.9965214729309082],\n [0.9368084073066711],\n [0.8674486875534058],\n [0.9514780044555664],\n [0.991176962852478],\n [0.9991886019706726],\n [0.9953054785728455],\n [0.9954990744590759],\n [0.9913181066513062],\n [0.9966715574264526],\n [0.9998108744621277],\n [0.9515433311462402],\n [0.7831081748008728],\n [0.9190828800201416],\n [0.9958535432815552],\n [0.929087221622467],\n [0.7088379859924316],\n [0.9675686359405518],\n [0.6428898572921753],\n [0.7190755009651184],\n [0.5458090305328369],\n [0.06088395044207573],\n [0.8285067677497864],\n [0.9515151381492615],\n [0.9439817667007446],\n [0.9866112470626831],\n [0.9589931964874268],\n [0.9967229962348938],\n [0.42533865571022034],\n [0.33180946111679077],\n [0.9985185265541077],\n [0.9799275398254395],\n [0.9953561425209045],\n [0.9845936298370361],\n [0.982239842414856],\n [0.9547939896583557],\n [0.8296644687652588],\n [0.8059684038162231],\n [0.9944801330566406],\n [0.9694222211837769],\n [0.9895306825637817],\n [0.9521129131317139],\n [0.9451332688331604],\n [0.9721531867980957],\n [0.8232401609420776],\n [0.9523729681968689],\n [0.7971970438957214],\n [0.9680736660957336],\n [0.9063787460327148],\n [0.9861916303634644],\n [0.9980839490890503],\n [0.955469012260437],\n [0.9487593173980713],\n [0.9462350010871887],\n [0.9523636102676392],\n [0.9418420791625977],\n [0.9510654211044312],\n [0.9595139026641846],\n [0.9349513649940491],\n [0.9219207167625427],\n [0.9961148500442505],\n [0.9862253665924072],\n [0.9510858654975891],\n [0.9466989040374756],\n [0.995915949344635],\n [0.9498966932296753],\n [0.9955319166183472],\n [0.9950693845748901],\n [0.9863395690917969],\n [0.7769444584846497],\n [0.9885779619216919],\n [0.9723929166793823],\n [0.2427815943956375],\n [0.9581601619720459],\n [0.9967184662818909],\n [0.9955203533172607],\n [0.9972561001777649],\n [0.9958047270774841],\n [0.9957002401351929],\n [0.9999841451644897],\n [0.9945595860481262],\n [0.9950113296508789],\n [0.9917927384376526],\n [0.6446748971939087],\n [0.9999738931655884],\n [0.9980372786521912],\n [0.6750096082687378],\n [0.9984764456748962],\n [0.9908863306045532],\n [0.9128435850143433],\n [0.9974760413169861],\n [0.9922475814819336],\n [0.9993528723716736],\n [0.9997410178184509],\n [0.9996059536933899],\n [0.9301607012748718],\n [0.9998984336853027],\n [0.9071595668792725],\n [0.9484350085258484],\n [0.9941493272781372],\n [0.9202104210853577],\n [0.9349237680435181],\n [0.929632842540741],\n [0.9010425806045532],\n [0.02991669625043869],\n [0.0115347970277071],\n [0.9964649677276611],\n [0.050504837185144424],\n [0.9284707903862],\n [0.9194899797439575],\n [0.923701286315918],\n [0.9230763912200928],\n [0.8947187066078186],\n [0.898476243019104],\n [0.9561347365379333],\n [0.9476568102836609],\n [0.9958798885345459],\n [0.953380286693573],\n [0.9968565702438354],\n [0.9979244470596313],\n [0.9870778322219849],\n [0.9940594434738159],\n [0.9939759373664856],\n [0.9964779019355774],\n [0.9967080354690552],\n [0.9806859493255615],\n [0.999845027923584],\n [0.9597743153572083],\n [0.9952036142349243],\n [0.995703399181366],\n [0.9581959843635559],\n [0.9947313070297241],\n [0.9874438643455505],\n [0.9951232075691223],\n [0.9959126114845276],\n [0.996715784072876],\n [0.8476994037628174],\n [0.8060601949691772],\n [0.8370739817619324],\n [0.905811607837677],\n [0.8624563813209534],\n [0.9178715348243713],\n [0.9993847608566284],\n [0.9740774631500244],\n [0.9967947602272034],\n [0.9943116903305054],\n [0.9205795526504517],\n [0.9926337599754333],\n [0.9013652801513672],\n [0.9891724586486816],\n [0.9250668287277222],\n [0.9380308389663696],\n [0.9184772372245789],\n [0.7149681448936462],\n [0.9920611381530762],\n [0.9923190474510193],\n [0.9382926225662231],\n [0.9567896127700806],\n [0.9566748142242432],\n [0.9964007139205933],\n [0.994170069694519],\n [0.9597961902618408],\n [0.9567424654960632],\n [0.9395345449447632],\n [0.9965648055076599],\n [0.9966251850128174],\n [0.9980337023735046],\n [0.9662126302719116],\n [0.9505524039268494],\n [0.9970614314079285],\n [0.94589763879776],\n [0.950809895992279],\n [0.9779627919197083],\n [0.980187714099884],\n [0.9986395239830017],\n [0.9770805835723877],\n [0.9410836696624756],\n [0.9967014193534851],\n [0.9942200183868408],\n [0.4432801604270935],\n [0.9920945763587952],\n [0.8411720991134644],\n [0.9622002243995667],\n [0.9601956605911255],\n [0.9965189695358276],\n [0.9875756502151489],\n [0.8608283400535583],\n [0.9688846468925476],\n [0.9555609822273254],\n [0.960582435131073],\n [0.9278978109359741],\n [0.8607290983200073],\n [0.9989583492279053],\n [0.8707013130187988],\n [0.9073230624198914],\n [0.998790442943573],\n [0.9858894348144531],\n [0.9999932050704956],\n [0.9205664992332458],\n [0.7617999911308289],\n [0.9459480047225952],\n [0.9068651795387268],\n [0.9973964691162109],\n [0.9999408721923828],\n [0.9999964237213135],\n [0.905737578868866],\n [0.9484251737594604],\n [0.9308833479881287],\n [0.9586353898048401],\n [0.9744062423706055],\n [0.8534095287322998],\n [0.8279200196266174],\n [0.6165850162506104],\n [0.7023570537567139],\n [0.9965512752532959],\n [0.9357781410217285],\n [0.9561657905578613],\n [0.8493983745574951],\n [0.8499987721443176],\n [0.7833160161972046],\n [0.9565185308456421],\n [0.9406022429466248],\n [0.9997304081916809],\n [0.9401933550834656],\n [0.9563405513763428],\n [0.9719322323799133],\n [0.8309563994407654],\n [0.9566206932067871],\n [0.7921832203865051],\n [0.9894133806228638],\n [0.9908547401428223],\n [0.9901459813117981],\n [0.9915437698364258],\n [0.8033548593521118],\n [0.9880073070526123],\n [0.8976848125457764],\n [0.9997484087944031],\n [0.9381434917449951],\n [0.9626667499542236],\n [0.9721064567565918],\n [0.7824867963790894],\n [0.9868268966674805],\n [0.9999834299087524],\n [0.9999983310699463],\n [0.6851640939712524],\n [0.8391239643096924],\n [0.6741500496864319],\n [0.9832321405410767],\n [0.8789304494857788],\n [0.8342412710189819],\n [0.9574287533760071],\n [0.9339989423751831],\n [0.9378137588500977],\n [0.9491977095603943],\n [0.9589129686355591],\n [0.9634925127029419],\n [0.9937533736228943],\n [0.9969209432601929],\n [0.9956693649291992],\n [0.9611660242080688],\n [0.9559360146522522],\n [0.9923712015151978],\n [0.9983057975769043],\n [0.6169518828392029],\n [0.9958837628364563],\n [0.9618079662322998],\n [0.9998652935028076],\n [0.9964800477027893],\n [0.9628498554229736],\n [0.9871528744697571],\n [0.942027747631073],\n [0.9753841757774353],\n [0.9990065693855286],\n [0.8018831014633179],\n [0.9961172342300415],\n [0.9909136295318604],\n [0.8306193947792053],\n [0.9833348989486694],\n [0.9873644709587097],\n [0.9589820504188538],\n [0.8240786194801331],\n [0.9993377327919006],\n [0.6252387166023254],\n [0.8714879155158997],\n [0.99992835521698],\n [0.9933597445487976],\n [0.9933597445487976],\n [0.9546242952346802],\n [0.9582839608192444],\n [0.3066052794456482],\n [0.9652870297431946],\n [0.9766223430633545],\n [0.9296020269393921],\n [0.919922947883606],\n [0.9908648133277893],\n [0.9322829246520996],\n [0.8277269601821899],\n [0.9159684181213379],\n [0.9712970852851868],\n [0.9246290326118469],\n [0.9457284808158875],\n [0.9307601451873779],\n [0.9304605722427368],\n [0.9329474568367004],\n [0.9999268054962158],\n [0.9778022766113281],\n [0.999998927116394],\n [0.9997460246086121],\n [0.9986526966094971],\n [0.9984472393989563],\n [0.9965067505836487],\n [0.9999756813049316],\n [0.9439817667007446],\n [0.9988553524017334],\n [0.7859033942222595],\n [0.9147104024887085],\n [0.917768120765686],\n [0.9531364440917969],\n [0.9312821626663208],\n [0.9999247789382935],\n [0.7967082262039185],\n [0.9531364440917969],\n [0.9915407299995422],\n [0.9879101514816284],\n [0.9913095235824585],\n [0.9963143467903137],\n [0.9784133434295654],\n [0.9589820504188538],\n [0.9299815893173218],\n [0.9441530108451843],\n [0.9986012578010559],\n [0.8882057070732117],\n [0.9774772524833679],\n [0.968814492225647],\n [0.7398639917373657],\n [0.985215425491333],\n [0.8955126404762268],\n [0.9664299488067627],\n [0.9664299488067627],\n [0.9664299488067627],\n [0.9664299488067627],\n [0.999283492565155],\n [0.9942089319229126],\n [0.9953911304473877],\n [0.9912359714508057],\n [0.8961824178695679],\n [0.9986593723297119],\n [0.5056362152099609],\n [0.9854009747505188],\n [0.9726574420928955],\n [0.9769162535667419],\n [0.9972906708717346],\n [0.9951545000076294],\n [0.9873265624046326],\n [0.7166955471038818],\n [0.9307601451873779],\n [0.9554734826087952],\n [0.999859094619751],\n [0.9998372793197632],\n [0.9950655102729797],\n [0.9785980582237244],\n [0.9636852145195007],\n [0.9998606443405151],\n [0.9998606443405151],\n [0.8737874627113342],\n [0.995572566986084],\n [0.9793767929077148],\n [0.7631585001945496],\n [0.6252793073654175],\n [0.9986295700073242],\n [0.9753912687301636],\n [0.2295815795660019],\n [0.9867478609085083],\n [0.6040903925895691],\n [0.9984679818153381],\n [0.9973270893096924],\n [0.9172027707099915],\n [0.9957578778266907],\n [0.9929617047309875],\n [0.9917696714401245],\n [0.9547461867332458],\n [0.9888089299201965],\n [0.7838687300682068],\n [0.7863653898239136],\n [0.9991762042045593],\n [0.9930627942085266],\n [0.9959638118743896],\n [0.9943898320198059],\n [0.9985299110412598],\n [0.9961965084075928],\n [0.737076461315155],\n [0.19860172271728516],\n [0.07665247470140457],\n [0.0700555294752121],\n [0.13646364212036133],\n [0.962788999080658],\n [0.9946257472038269],\n [0.6122642755508423],\n [0.167414590716362],\n [0.9975958466529846],\n [0.4012311100959778],\n [0.9925903081893921],\n [0.9991493225097656],\n [0.9991266131401062],\n [0.9950817823410034],\n [0.8267111778259277],\n [0.9985150694847107],\n [0.9898210763931274],\n [0.10376466065645218],\n [0.9973995685577393],\n [0.9962827563285828],\n [0.9944884777069092],\n [0.9967162609100342],\n [0.8881505131721497],\n [0.9968644976615906],\n [0.9375177621841431],\n [0.9417018890380859],\n [0.978391706943512],\n [0.9765871167182922],\n [0.9664884209632874],\n [0.07501375675201416],\n [0.9358800053596497],\n [0.8413920998573303],\n [0.9967875480651855],\n [0.9958276152610779],\n [0.9971799850463867],\n [0.9838968515396118],\n [0.6715706586837769],\n [0.9960113763809204],\n [0.9947344064712524],\n [0.9440557956695557],\n [0.999940037727356],\n [0.9988715052604675],\n [0.9959524869918823],\n [0.9946183562278748],\n [0.9965283274650574],\n [0.9976248145103455],\n [0.996896505355835],\n [0.9989181756973267],\n [0.9986119270324707],\n [0.9950892329216003],\n [0.9992191791534424],\n [0.9960353970527649],\n [0.981980562210083],\n [0.9879092574119568],\n [0.9659473896026611],\n [0.9613170027732849],\n [0.9961223006248474],\n [0.9609872102737427],\n [0.9998958110809326],\n [0.9591730237007141],\n [0.9916792511940002],\n [0.9896969199180603],\n [0.9968531727790833],\n [0.9963462948799133],\n [0.9947443008422852],\n [0.9984772801399231],\n [0.9851065874099731],\n [0.13655278086662292],\n [0.9991389513015747],\n [0.9992504715919495],\n [0.11853472888469696],\n [0.9920439124107361],\n [0.999213457107544],\n [0.9987819790840149],\n [0.9977217316627502],\n [0.9967555403709412],\n [0.03460221365094185],\n [0.9850998520851135],\n [0.9975468516349792],\n [0.9985957741737366],\n [0.9123159050941467],\n [0.999977707862854],\n [0.9998258948326111],\n [0.9061511158943176],\n [0.9999288320541382],\n [0.9999088048934937],\n [0.9982840418815613],\n [0.9994373917579651],\n [0.9998366832733154],\n [0.9998842477798462],\n [0.9978054165840149],\n [0.6221753358840942],\n [0.9304605722427368],\n [0.9999268054962158],\n [0.999998927116394],\n [0.9984472393989563],\n [0.9965067505836487],\n [0.9999756813049316],\n [0.9439817667007446],\n [0.9988553524017334],\n [0.7859033942222595],\n [0.9147104024887085],\n [0.917768120765686],\n [0.9531364440917969],\n [0.9312821626663208],\n [0.9999247789382935],\n [0.7967082262039185],\n [0.9531364440917969],\n [0.9915407299995422],\n [0.9879101514816284],\n [0.9913095235824585],\n [0.9963143467903137],\n [0.9784133434295654],\n [0.9589820504188538],\n [0.9299815893173218],\n [0.9441530108451843],\n [0.9986012578010559],\n [0.8882057070732117],\n [0.9774772524833679],\n [0.968814492225647],\n [0.7398639917373657],\n [0.985215425491333],\n [0.8955126404762268],\n [0.9664299488067627],\n [0.9664299488067627],\n [0.9664299488067627],\n [0.9664299488067627],\n [0.999283492565155],\n [0.9942089319229126],\n [0.9953911304473877],\n [0.9912359714508057],\n [0.8961824178695679],\n [0.9986593723297119],\n [0.5056362152099609],\n [0.9854009747505188],\n [0.9726574420928955],\n [0.9769162535667419],\n [0.9972906708717346],\n [0.9951545000076294],\n [0.9873265624046326],\n [0.7166955471038818],\n [0.9307601451873779],\n [0.9554734826087952],\n [0.999859094619751],\n [0.9998372793197632],\n [0.9950655102729797],\n [0.9785980582237244],\n [0.9636852145195007],\n [0.9998606443405151],\n [0.9998606443405151],\n [0.8737874627113342],\n [0.995572566986084],\n [0.9793767929077148],\n [0.7631585001945496],\n [0.6252793073654175],\n [0.9986295700073242],\n [0.9753912687301636],\n [0.2295815795660019],\n [0.9867478609085083],\n [0.6040903925895691],\n [0.9984679818153381],\n [0.9973270893096924],\n [0.9172027707099915],\n [0.9957578778266907],\n [0.9929617047309875],\n [0.9917696714401245],\n [0.9547461867332458],\n [0.9888089299201965],\n [0.7838687300682068],\n [0.7863653898239136],\n [0.9991762042045593],\n [0.9930627942085266],\n [0.9959638118743896],\n [0.9943898320198059],\n [0.9985299110412598],\n [0.9961965084075928],\n [0.737076461315155],\n [0.19860172271728516],\n [0.07665247470140457],\n [0.0700555294752121],\n [0.13646364212036133],\n [0.962788999080658],\n [0.9946257472038269],\n [0.6122642755508423],\n [0.167414590716362],\n [0.9975958466529846],\n [0.4012311100959778],\n [0.9925903081893921],\n [0.9991493225097656],\n [0.9991266131401062],\n [0.9950817823410034],\n [0.8267111778259277],\n [0.9985150694847107],\n [0.9898210763931274],\n [0.10376466065645218],\n [0.9973995685577393],\n [0.9962827563285828],\n [0.9944884777069092],\n [0.9967162609100342],\n [0.8881505131721497],\n [0.9968644976615906],\n [0.9375177621841431],\n [0.9417018890380859],\n [0.978391706943512],\n [0.9765871167182922],\n [0.9664884209632874],\n [0.07501375675201416],\n [0.9358800053596497],\n [0.8413920998573303],\n [0.9967875480651855],\n [0.9958276152610779],\n [0.9971799850463867],\n [0.9838968515396118],\n [0.6715706586837769],\n [0.9960113763809204],\n [0.9947344064712524],\n [0.9440557956695557],\n [0.999940037727356],\n [0.9988715052604675],\n [0.9959524869918823],\n [0.9946183562278748],\n [0.9965283274650574],\n [0.9976248145103455],\n [0.996896505355835],\n [0.9989181756973267],\n [0.9986119270324707],\n [0.9950892329216003],\n [0.9992191791534424],\n [0.9960353970527649],\n [0.981980562210083],\n [0.9879092574119568],\n [0.9659473896026611],\n [0.9613170027732849],\n [0.9961223006248474],\n [0.9609872102737427],\n [0.9998958110809326],\n [0.9591730237007141],\n [0.9916792511940002],\n [0.9896969199180603],\n [0.9968531727790833],\n [0.9963462948799133],\n [0.9947443008422852],\n [0.9984772801399231],\n [0.9851065874099731],\n [0.13655278086662292],\n [0.9991389513015747],\n [0.9992504715919495],\n [0.11853472888469696],\n [0.9920439124107361],\n [0.999213457107544],\n [0.9987819790840149],\n [0.9977217316627502],\n [0.9967555403709412],\n [0.03460221365094185],\n [0.9850998520851135],\n [0.9975468516349792],\n [0.9985957741737366],\n [0.9123159050941467],\n [0.999977707862854],\n [0.9998258948326111],\n [0.9061511158943176],\n [0.9999288320541382],\n [0.9999088048934937],\n [0.9982840418815613],\n [0.9994373917579651],\n [0.9998366832733154],\n [0.9998842477798462],\n [0.6221752762794495]]\n</div>"]}}],"execution_count":121},{"cell_type":"code","source":["pd.read_csv(os.path.join(CHEMPROP_DIR,'hlm_eh_binary.csv')).smiles.to_csv(os.path.join(CHEMPROP_DIR,'hlm_eh_binary.smi'),index=None)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":122},{"cell_type":"code","source":["true = pd.read_csv(os.path.join(CHEMPROP_DIR,'hlm_eh_binary.csv'))\n#pred = pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','checkpoints/binary-hopt/model/fold_0/test_smiles.csv'))\npred = pd.read_csv(os.path.join(CHEMPROP_DIR,'hlm_eh_binary_preds.csv'))#pd.DataFrame(columns=['smiles','HLM_binary'])\n# for i in range(1,10):\n#   #print(pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','checkpoints/binary-hopt/model/fold_'+str(i),'test_smiles.csv')).head())\n  \n#   pred = pred.append(pd.read_csv(os.path.join(CHEMPROP_DIR,'JAK','checkpoints/binary-hopt-prelu100e/model/fold_'+str(i),'test_preds.csv')))\npred = pred.rename(columns={'smiles':'smiles','HLM_binary':'pred'})\ntrue = true.merge(pred)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":123},{"cell_type":"code","source":["%sh head /dbfs/FileStore/chemprop/hlm_eh_binary.smi"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0,Cc1cccc(c1)-c1ccc(c(c1)CC(NC(=O)c1ccnn1C)C(=O)Nc1ccc(cc1)CCCO)Cl\n1,Cc1cccc(c1)-c1ccc(c(c1)CC(NC(=O)c1ccnn1C)C(=O)Nc1ccc(cc1)CCC(=O)N)Cl\n2,CNC(=O)CCc1ccc(cc1)NC(=O)C(Cc1cc(ccc1Cl)-c1cccc(c1)C)NC(=O)c1ccnn1C\n3,Cn1nccc1C(=O)NC(Cc1cc(ccc1Cl)-c1ccccc1)C(=O)Nc1ccc(cc1)CCC(=O)N\n4,Cn1ccc(n1)-c1ccc(c(c1)CC(NC(=O)c1ccnn1C)C(=O)Nc1ccc(cc1)CCC(=O)N)Cl\n5,Cn1nccc1C(=O)NC(Cc1cc(ccc1Cl)-c1ccnn1C)C(=O)Nc1ccc(cc1)CCC(=O)N\n6,CN(C)c1ncc(cn1)-c1ccc(c(c1)CC(NC(=O)c1ccnn1C)C(=O)Nc1ccc(cc1)CCC(=O)N)Cl\n7,Cn1nccc1C(=O)NC(Cc1cc(ccc1Cl)-c1cccc(c1)F)C(=O)Nc1ccc(cc1)CCC(=O)N\n8,Cn1nccc1C(=O)NC(Cc1cc(ccc1Cl)-c1ccncc1)C(=O)Nc1ccc(cc1)CCC(=O)N\n9,Cn1nccc1C(=O)NC(Cc1cc(ccc1Cl)-c1cn[nH]c1)C(=O)Nc1ccc(cc1)CCC(=O)N\n</div>"]}}],"execution_count":124},{"cell_type":"code","source":["web = pd.read_csv('/dbfs/FileStore/tables/20190624_predictions_hlm-3512e.csv').rename(columns={'HLM_binary':'web_pred'})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":125},{"cell_type":"code","source":["true.merge(web,on='smiles')"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[21]: \nEmpty DataFrame\nColumns: [smiles, HLM_binary, pred, web_pred]\nIndex: []\n</div>"]}}],"execution_count":126},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\nprint(roc_auc_score(true['HLM_binary'].tolist(), true['pred'].tolist()))\nprint(true.filter(['HLM_binary','pred']))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">0.9348548090301154\n      HLM_binary      pred\n0              1  0.691169\n1              1  0.763256\n2              0  0.650057\n3              0  0.697335\n4              1  0.709265\n5              1  0.990444\n6              1  0.666158\n7              1  0.663964\n8              1  0.654802\n9              1  0.759310\n10             1  0.659889\n11             1  0.687510\n12             1  0.659458\n13             1  0.655713\n14             1  0.684738\n15             1  0.644957\n16             0  0.106283\n17             1  0.482541\n18             0  0.083883\n19             0  0.637556\n20             1  0.714939\n21             1  0.848265\n22             0  0.709332\n23             1  0.992156\n24             0  0.629422\n25             1  0.677150\n26             1  0.717899\n27             1  0.976885\n28             0  0.616712\n29             1  0.897836\n...          ...       ...\n1199           1  0.999929\n1200           1  0.999929\n1201           1  0.999929\n1202           1  0.999929\n1203           1  0.999909\n1204           1  0.999909\n1205           1  0.999909\n1206           1  0.999909\n1207           1  0.998284\n1208           1  0.998284\n1209           1  0.998284\n1210           1  0.998284\n1211           1  0.999437\n1212           1  0.999437\n1213           1  0.999437\n1214           1  0.999437\n1215           1  0.999837\n1216           1  0.999837\n1217           1  0.999837\n1218           1  0.999837\n1219           1  0.999884\n1220           1  0.999884\n1221           1  0.999884\n1222           1  0.999884\n1223           1  0.997805\n1224           0  0.622175\n1225           0  0.622175\n1226           0  0.622175\n1227           0  0.622175\n1228           1  0.999927\n\n[1229 rows x 2 columns]\n</div>"]}}],"execution_count":127},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":128}],"metadata":{"name":"chemprop","notebookId":3193931697044536},"nbformat":4,"nbformat_minor":0}
